<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CE updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CE</link>


<item>
<title>Pressure-robust enriched Galerkin finite element methods for coupled Navier-Stokes and heat equations</title>
<link>https://arxiv.org/abs/2512.16716</link>
<guid>https://arxiv.org/abs/2512.16716</guid>
<content:encoded><![CDATA[
<div> Keywords: enriched Galerkin, pressure-robustness, Navier-Stokes, Boussinesq equations, Anderson acceleration  

<br /><br />Summary:  
This paper introduces a pressure-robust enriched Galerkin (EG) finite element method specifically designed for the incompressible Navier-Stokes and heat equations within the Boussinesq regime. The method employs a velocity space comprising continuous Lagrange elements combined with a discontinuous enrichment vector per element, alongside a piecewise constant pressure space, facilitating efficient implementation in standard finite element frameworks. To achieve pressure robustness, velocity reconstruction operators are developed that transform the discrete EG velocity field into exactly divergence-free, H(div)-conforming vector fields. These reconstructions utilize Arbogast-Correa (AC) mixed finite element spaces on quadrilateral meshes, ensuring stability and accuracy even on highly distorted grids. The nonlinear Navier-Stokes-Boussinesq system is solved using iterative strategies including Picard iterations and Anderson-accelerated iterations, with numerical evidence indicating that Anderson acceleration achieves robust and efficient convergence particularly for high Rayleigh number flows. The method's effectiveness is demonstrated through various benchmark problems and practical test cases, highlighting its versatility. The numerical experiments confirm that the pressure-robust EG methods provide flexible and accurate tools for simulating coupled flow and heat transport phenomena in complex geometries. <div>
arXiv:2512.16716v1 Announce Type: new 
Abstract: We propose a pressure-robust enriched Galerkin (EG) finite element method for the incompressible Navier-Stokes and heat equations in the Boussinesq regime. For the Navier-Stokes equations, the EG formulation combines continuous Lagrange elements with a discontinuous enrichment vector per element in the velocity space and a piecewise constant pressure space, and it can be implemented efficiently within standard finite element frameworks. To enforce pressure robustness, we construct velocity reconstruction operators that map the discrete EG velocity field into exactly divergence-free, H(div)-conforming fields. In particular, we develop reconstructions based on Arbogast-Correa (AC) mixed finite element spaces on quadrilateral meshes and demonstrate that the resulting schemes remain stable and accurate even on highly distorted grids. The nonlinearity of the coupled Navier-Stokes-Boussinesq system is treated with several iterative strategies, including Picard iterations and Anderson-accelerated iterations; our numerical study shows that Anderson acceleration yields robust and efficient convergence for high Rayleigh number flows within the proposed framework. The performance of the method is assessed on a set of benchmark problems and application-driven test cases. These numerical experiments highlight the potential of pressure-robust EG methods as flexible and accurate tools for coupled flow and heat transport in complex geometries.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Techno-economic optimization of a heat-pipe microreactor, part I: theory and cost optimization</title>
<link>https://arxiv.org/abs/2512.16032</link>
<guid>https://arxiv.org/abs/2512.16032</guid>
<content:encoded><![CDATA[
<div> Keywords: microreactors, heat-pipe microreactors, geometric design optimization, reinforcement learning, levelized cost of electricity (LCOE)  

<br /><br />Summary:  
This work addresses the challenges of the economic viability of microreactors, specifically heat-pipe microreactors (HPMRs), which are compact and self-regulated power systems suitable for remote areas but face diseconomies of scale. The authors propose a novel geometric design optimization approach that integrates techno-economic factors early in the design process. They generate random samples to train surrogate models, including Gaussian processes (GPs) and multi-layer perceptrons (MLPs), which are then used within a reinforcement learning (RL) optimization framework. The goal is to minimize the levelized cost of electricity (LCOE) while adhering to constraints on fuel lifetime, shutdown margin (SDM), peak heat flux, and rod-integrated peaking factor. Two cost scenarios are studied: one with expensive axial reflectors and one with inexpensive ones. Findings reveal that operation, maintenance, capital costs, axial reflector costs, and control drum materials significantly influence LCOE. Through optimization, the design parameters are adjusted to reduce overall costs while satisfying constraints, achieving more than a 57% LCOE reduction in both cases. Future work will involve integrating fuel and heat-pipe performance with multi-objective optimization to further understand cost and constraint interactions. <div>
arXiv:2512.16032v1 Announce Type: cross 
Abstract: Microreactors, particularly heat-pipe microreactors (HPMRs), are compact, transportable, self-regulated power systems well-suited for access-challenged remote areas where costly fossil fuels dominate. However, they suffer from diseconomies of scale, and their financial viability remains unconvincing. One step in addressing this shortcoming is to design these reactors with comprehensive economic and physics analyses informing early-stage design iteration. In this work, we present a novel unifying geometric design optimization approach that accounts for techno-economic considerations. We start by generating random samples to train surrogate models, including Gaussian processes (GPs) and multi-layer perceptrons (MLPs). We then deploy these surrogates within a reinforcement learning (RL)-based optimization framework to optimize the levelized cost of electricity (LCOE), all the while imposing constraints on the fuel lifetime, shutdown margin (SDM), peak heat flux, and rod-integrated peaking factor. We study two cases: one in which the axial reflector cost is very high, and one in which it is inexpensive. We found that the operation and maintenance and capital costs are the primary contributors to the overall LCOE particularly the cost of the axial reflectors (for the first case) and the control drum materials. The optimizer cleverly changes the design parameters so as to minimize one of them while still satisfying the constraints, ultimately reducing the LCOE by more than 57% in both instances. A comprehensive integration of fuel and HP performance with multi-objective optimization is currently being pursued to fully understand the interaction between constraints and cost performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting</title>
<link>https://arxiv.org/abs/2512.16046</link>
<guid>https://arxiv.org/abs/2512.16046</guid>
<content:encoded><![CDATA[
<div> Keywords: streamflow forecasting, causal learning, spatiotemporal modeling, runoff causal graph, hydrological dynamics  

<br /><br />Summary:  
Streamflow forecasting is essential for effective water resource management and mitigating related risks. Traditional deep learning models, while accurate, tend to ignore underlying physical processes, which limits their interpretability and ability to generalize. Addressing these issues, recent causal learning approaches incorporate domain knowledge but often depend on static causal graphs that do not adapt to changing data. The proposed framework, CauStream, overcomes this by jointly learning a runoff causal graph that outlines relationships among meteorological inputs and a routing graph that models dynamic dependencies between river stations. CauStream establishes identifiability conditions under a nonparametric setting, ensuring the learned causal structures are well-founded. Evaluations on three major U.S. river basins across multiple forecast horizons show that CauStream consistently surpasses existing state-of-the-art methods, with its advantage becoming more pronounced for longer forecasting periods, indicating better generalization to unseen scenarios. Besides improving forecasting accuracy, CauStream reveals interpretable causal graphs reflecting known hydrological interactions both within and across stations, offering valuable scientific insights. This unified approach provides a principled and adaptable foundation for causal spatiotemporal modeling in hydrology, with promising potential applicability to broader environmental and scientific domains. <div>
arXiv:2512.16046v1 Announce Type: cross 
Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.08435</link>
<guid>https://arxiv.org/abs/2412.08435</guid>
<content:encoded><![CDATA[
<div> Concept drift, time series forecasting, online learning, model adaptation, synthetic data<br /><br />Summary:<br /><br />1. The paper addresses the challenge of concept drift in time series forecasting, where changing data distributions over time degrade the accuracy of forecast models.<br />2. Existing online learning methods update models based on recent data but fail to consider the delay in receiving ground-truth future values, which creates a temporal gap between training and test samples.<br />3. This gap can cause forecast models to adapt to outdated concepts, leading to suboptimal performance.<br />4. The authors propose Proceed, a proactive model adaptation framework that estimates the concept drift between recently used training samples and the current test sample.<br />5. Proceed uses an adaptation generator to translate the estimated drift into parameter adjustments, enabling the model to proactively adapt to incoming test data.<br />6. To improve its generalization, Proceed is trained on synthetically generated diverse concept drifts.<br />7. Extensive experiments on five real-world datasets and multiple forecast models show that Proceed consistently outperforms state-of-the-art online learning techniques.<br />8. Proceed significantly improves the resilience of forecast models against concept drift, leading to better time series prediction accuracy.<br />9. The authors release their code publicly to facilitate further research and application in this area. <div>
arXiv:2412.08435v5 Announce Type: replace-cross 
Abstract: Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Roadmap for Applying Graph Neural Networks to Numerical Data: Insights from Cementitious Materials</title>
<link>https://arxiv.org/abs/2512.14855</link>
<guid>https://arxiv.org/abs/2512.14855</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, graph neural network, cementitious materials, k-nearest neighbor, multi-modal database<br /><br />Summary:<br /><br />This study addresses challenges in applying machine learning (ML) to cementitious materials, primarily the limited size and limited diversity of existing data sets. It proposes the use of multi-modal databases that combine numerical and graphical data to overcome these limitations. Traditional ML frameworks for concrete design typically rely on single-modality data, but this work introduces graph neural networks (GNNs), a newer architecture capable of learning from graph-structured data, effectively capturing relationships through complex topologies rather than fixed spatial coordinates. The research highlights a reproducible method for converting tabular numerical data into graph representations using the k-nearest neighbor (K-NN) algorithm. Through systematic hyperparameter tuning and feature selection, the GNN model achieves prediction performance comparable to that of random forest models, which are commonly used and trusted in cement research. The study demonstrates the potential of GNNs to not only model graphical data but also extract meaningful correlations from numerical datasets and incorporate physics-based rules for explainability. Overall, the work establishes a foundational framework for transitioning from conventional ML to advanced AI with multi-modal and physics-informed capabilities, paving the way for more accurate and efficient design and optimization of cementitious materials. <div>
arXiv:2512.14855v1 Announce Type: new 
Abstract: Machine learning (ML) has been increasingly applied in concrete research to optimize performance and mixture design. However, one major challenge in applying ML to cementitious materials is the limited size and diversity of available databases. A promising solution is the development of multi-modal databases that integrate both numerical and graphical data. Conventional ML frameworks in cement research are typically restricted to a single data modality. Graph neural network (GNN) represents a new generation of neural architectures capable of learning from data structured as graphs, capturing relationships through irregular or topology-dependent connections rather than fixed spatial coordinates. While GNN is inherently designed for graphical data, they can be adapted to extract correlations from numerical datasets and potentially embed physical laws directly into their architecture, enabling explainable and physics-informed predictions. This work is among the first few studies to implement GNNs to design concrete, with a particular emphasis on establishing a clear and reproducible pathway for converting tabular data into graph representations using the k-nearest neighbor (K-NN) approach. Model hyperparameters and feature selection are systematically optimized to enhance prediction performance. The GNN shows performance comparable to the benchmark random forest, which has been demonstrated by many studies to yield reliable predictions for cementitious materials. Overall, this study provides a foundational roadmap for transitioning from traditional ML to advanced AI architectures. The proposed framework establishes a strong foundation for future multi-modal and physics-informed GNN models capable of capturing complex material behaviors and accelerating the design and optimization of cementitious materials.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HD-Prot: A Protein Language Model for Joint Sequence-Structure Modeling with Continuous Structure Tokens</title>
<link>https://arxiv.org/abs/2512.15133</link>
<guid>https://arxiv.org/abs/2512.15133</guid>
<content:encoded><![CDATA[
<div> Keywords: protein language model, continuous tokens, diffusion model, sequence-structure modeling, multimodal learning<br /><br />Summary:<br /><br />This paper addresses the challenge of integrating continuous structural information into protein language models (pLMs), which traditionally operate on discrete protein sequence tokens. Existing approaches often discretize protein structures, causing loss of fine-grained details and limiting modeling capacity. The authors propose HD-Prot, a hybrid diffusion protein language model that extends sequences-based pLMs by adding a continuous-valued diffusion head, enabling joint modeling of discrete sequence tokens and continuous structural tokens without vector quantization. HD-Prot employs a unified absorbing diffusion process to capture dependencies between sequence and structure modalities, estimating categorical distributions for sequences and continuous distributions for structures in a complementary manner. The model demonstrates competitive performance across multiple tasks including unconditional co-generation of sequence and structure, motif-scaffolding, protein structure prediction, and inverse folding. Notably, HD-Prot achieves results comparable to state-of-the-art multimodal pLMs despite being developed with limited computational resources. The work highlights the feasibility of unifying categorical and continuous data modeling within a single language model framework, presenting an effective and resource-efficient alternative for multimodal protein representation learning. <div>
arXiv:2512.15133v1 Announce Type: new 
Abstract: Proteins inherently possess a consistent sequence-structure duality. The abundance of protein sequence data, which can be readily represented as discrete tokens, has driven fruitful developments in protein language models (pLMs). A key remaining challenge, however, is how to effectively integrate continuous structural knowledge into pLMs. Current methods often discretize protein structures to accommodate the language modeling framework, which inevitably results in the loss of fine-grained information and limits the performance potential of multimodal pLMs. In this paper, we argue that such concerns can be circumvented: a sequence-based pLM can be extended to incorporate the structure modality through continuous tokens, i.e., high-fidelity protein structure latents that avoid vector quantization. Specifically, we propose a hybrid diffusion protein language model, HD-Prot, which embeds a continuous-valued diffusion head atop a discrete pLM, enabling seamless operation with both discrete and continuous tokens for joint sequence-structure modeling. It captures inter-token dependencies across modalities through a unified absorbing diffusion process, and estimates per-token distributions via categorical prediction for sequences and continuous diffusion for structures. Extensive empirical results show that HD-Prot achieves competitive performance in unconditional sequence-structure co-generation, motif-scaffolding, protein structure prediction, and inverse folding tasks, performing on par with state-of-the-art multimodal pLMs despite being developed under limited computational resources. It highlights the viability of simultaneously estimating categorical and continuous distributions within a unified language model architecture, offering a promising alternative direction for multimodal pLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Updating of constitutive parameters under hybrid uncertainties with a novel surrogate model applied to biofilms</title>
<link>https://arxiv.org/abs/2512.15145</link>
<guid>https://arxiv.org/abs/2512.15145</guid>
<content:encoded><![CDATA[
<div> Keywords: bacterial biofilm growth, hybrid uncertainties, Bayesian model updating, reduced-order model, Time-Separated Stochastic Mechanics<br /><br />Summary: Accurate modeling of bacterial biofilm growth is important for understanding dynamics influenced by antibiotics, nutrient availability, and species interactions. The complexity arises due to hybrid uncertainties, comprising epistemic uncertainty (from incomplete model knowledge) and aleatory uncertainty (from biological variability and environmental randomness). The study introduces a Bayesian model updating (BMU) framework to calibrate a multi-species biofilm growth model effectively. To handle computational challenges, a reduced-order model (ROM) is developed using the Time-Separated Stochastic Mechanics (TSM), which efficiently propagates aleatory uncertainty. This approach enables single-loop Bayesian inference, circumventing computationally expensive nested loops typically necessary for hybrid uncertainty quantification. The BMU framework employs a likelihood function based on the mean and variance of stochastic outputs, allowing robust parameter calibration even with sparse or noisy data. Validation is performed through two case studies: a two-species and a four-species biofilm growth model. Both cases show that the method accurately identifies model parameters and provides predictive results consistent with synthetic data, demonstrating the framework’s potential for efficient and reliable biofilm growth modeling under uncertainty. <div>
arXiv:2512.15145v1 Announce Type: new 
Abstract: Accurate modeling of bacterial biofilm growth is essential for understanding their complex dynamics in biomedical, environmental, and industrial settings. These dynamics are shaped by a variety of environmental influences, including the presence of antibiotics, nutrient availability, and inter-species interactions, all of which affect species-specific growth rates. However, capturing this behavior in computational models is challenging due to the presence of hybrid uncertainties, a combination of epistemic uncertainty (stemming from incomplete knowledge about model parameters) and aleatory uncertainty (reflecting inherent biological variability and stochastic environmental conditions). In this work, we present a Bayesian model updating (BMU) framework to calibrate a recently introduced multi-species biofilm growth model. To enable efficient inference in the presence of hybrid uncertainties, we construct a reduced-order model (ROM) derived using the Time-Separated Stochastic Mechanics (TSM) approach. TSM allows for an efficient propagation of aleatory uncertainty, which enables single-loop Bayesian inference, thereby avoiding the computationally expensive nested (double-loop) schemes typically required in hybrid uncertainty quantification. The BMU framework employs a likelihood function constructed from the mean and variance of stochastic model outputs, enabling robust parameter calibration even under sparse and noisy data. We validate our approach through two case studies: a two-species and a four-species biofilm model. Both demonstrate that our method not only accurately recovers the underlying model parameters but also provides predictive responses consistent with the synthetic data.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Update Strategy for Channel Knowledge Map in Complex Environments</title>
<link>https://arxiv.org/abs/2512.15154</link>
<guid>https://arxiv.org/abs/2512.15154</guid>
<content:encoded><![CDATA[
<div> Keywords: Channel Knowledge Map, update scheduling, fractional programming, Dinkelbach algorithm, environmental dynamics<br /><br />Summary:<br /><br />1. The Channel Knowledge Map (CKM) connects position information with channel state information by utilizing environmental knowledge, aiming to reduce signaling overhead in sixth-generation (6G) networks.<br /><br />2. Constructing and maintaining a reliable CKM requires significant data and computational resources. Additionally, in dynamic environments, a pre-built CKM becomes outdated and degrades network performance.<br /><br />3. Although frequent retraining can restore CKM accuracy, it leads to resource waste, creating a trade-off between maintaining CKM effectiveness and minimizing update overhead.<br /><br />4. To address this challenge, the authors introduce the Map Efficacy Function (MEF) which models both gradual CKM aging and abrupt environmental changes, then formalize the update scheduling problem as a fractional programming problem.<br /><br />5. Two algorithms based on Dinkelbach’s method are developed: Delta-P achieves global optimal update scheduling, while Delta-L offers near-optimal results with significantly reduced, near-linear computational complexity.<br /><br />6. For unpredictable environments, a threshold-based policy is proposed: immediate updates are optimal when the environment degrades faster than the resource cost accelerates; otherwise, delaying updates is preferred.<br /><br />7. In predictable environments, long-term scheduling strategies optimize global performance by relaxing myopic update rules accordingly.<br /><br />8. The study finds that stronger initial performance loss and faster degradation rate favor immediate CKM updates, while weaker initial loss and slower decay justify postponing updates to conserve resources. <div>
arXiv:2512.15154v1 Announce Type: new 
Abstract: The Channel Knowledge Map (CKM) maps position information to channel state information, leveraging environmental knowledge to reduce signaling overhead in sixth-generation networks. However, constructing a reliable CKM demands substantial data and computation, and in dynamic environments, a pre-built CKM becomes outdated, degrading performance. Frequent retraining restores accuracy but incurs significant waste, creating a fundamental trade-off between CKM efficacy and update overhead. To address this, we introduce a Map Efficacy Function (MEF) capturing both gradual aging and abrupt environmental transitions, and formulate the update scheduling problem as fractional programming. We develop two Dinkelbach-based algorithms: Delta-P guarantees global optimality, while Delta-L achieves near-optimal performance with near-linear complexity. For unpredictable environments, we derive a threshold-based policy: immediate updates are optimal when the environmental degradation rate exceeds the resource consumption acceleration; otherwise, delay is preferable. For predictable environments, long-term strategies strategically relax these myopic rules to maximize global performance. Across this regime, the policy reveals that stronger entry loss and faster decay favor immediate updates, while weaker entry loss and slower decay favor delayed updates.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Data-Enhanced Agent-Based Model for Simulating 3D Cancer Spheroid Growth: Integrating Metabolism and Mechanics</title>
<link>https://arxiv.org/abs/2512.15361</link>
<guid>https://arxiv.org/abs/2512.15361</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-based model, tumour metabolism, cancer spheroid, mechanics, computational calibration<br /><br />Summary:<br /><br />This paper presents an agent-based computational model (ABM) that integrates both tumour metabolism and mechanical factors to investigate 3D cancer spheroid growth. The study addresses the gap in understanding the complex interplay between metabolic and mechanical mechanisms within the tumour microenvironment. The model unifies these aspects into a comprehensive framework to simulate cancer spheroid formation and growth. A computational calibration of parameters was performed to ensure accurate modeling, allowing the model to reproduce experimental spheroid growth results both qualitatively and quantitatively. Additionally, the model demonstrated the ability to capture different cancer cell dynamics under identical conditions, offering insights into the variability in spheroid sizes. Furthermore, the model's parameters can be tuned to reflect different cell lines and behaviours, highlighting its adaptability. The research underscores the value of integrative modelling approaches in cancer research as complementary tools to in vitro studies and as independent instruments capable of deriving meaningful conclusions based on physical realities. This approach enhances understanding of tumour progression mechanisms and holds promise for broad applications in cancer biology and therapy development. <div>
arXiv:2512.15361v1 Announce Type: new 
Abstract: Cancer research has shifted from a purely gene-centric view to a more holistic understanding that recognizes the critical role of the tumour microenvironment, where mechanics and metabolism are key drivers of disease progression. However, the intricate interplay between these multifactorial mechanisms remains poorly understood. To address this gap, we present an agent-based computational model (ABM) that integrates tumour metabolism and mechanics to study 3D cancer spheroid growth. Our approach unifies the metabolism and mechanical aspects of tumour development within an integral model for cancer spheroid formation and growth. In addition to that, we performed a computational calibration of the parameters and tested the model versatility to reproduce different cellular behaviours. Our model reproduced qualitatively and quantitatively the experimental results of spheroid growth obtained in the lab and also allowed to discern different dynamics that cancer cells can present under the same conditions, providing insight into the potential factors contributing to the variability in the size of spheroids. Furthermore, it also showed its adaptability to reproduce diferent cell lines and behaviours by tuning its parameters. This study highlights the significant potential and versatility of integrative modelling approaches in the field of cancer research, not only as a tool to complement in vitro studies, but also as independent tools to derive conclusions from the physical reality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonparametric Stochastic Subspaces via the Bootstrap for Characterizing Model Error</title>
<link>https://arxiv.org/abs/2512.15624</link>
<guid>https://arxiv.org/abs/2512.15624</guid>
<content:encoded><![CDATA[
<div> Keywords: uncertainty quantification, model error, stochastic subspace, reduced-order modeling, bootstrap

<br /><br />Summary:  
This paper addresses the challenge of reliable forward uncertainty quantification in engineering, emphasizing the need to account for both aleatory and epistemic uncertainties. It highlights that epistemic uncertainties, arising from unknown parameters and model form inaccuracies, often dominate prediction errors and impact critical engineering decisions. Given the difficulty in separately identifying these uncertainty sources, the authors propose a unified model-error framework for combined analysis. They introduce a novel bootstrap-based stochastic subspace model tailored for characterizing model error within stochastic reduced-order modeling. This method uses a snapshot matrix of state vectors and leverages the empirical data distribution to create a sampling distribution over principal subspaces, enhancing model error characterization in computational mechanics. The approach is assumption-free, enforces linear constraints (including boundary conditions) inherently, requires only one hyperparameter simplifying training, and is straightforward to implement. The authors validate their method against existing techniques with numerical examples in computational mechanics and structural dynamics, demonstrating improved performance and reliability. Overall, this work offers a robust, efficient, and practical tool for better uncertainty quantification in engineering models where model error is significant. <div>
arXiv:2512.15624v1 Announce Type: new 
Abstract: Reliable forward uncertainty quantification in engineering requires methods that account for aleatory and epistemic uncertainties. In many applications, epistemic effects arising from uncertain parameters and model form dominate prediction error and strongly influence engineering decisions. Because distinguishing and representing each source separately is often infeasible, their combined effect is typically analyzed using a unified model-error framework. Model error directly affects model credibility and predictive reliability; yet its characterization remains challenging. To address this need, we introduce a bootstrap-based stochastic subspace model for characterizing model error in the stochastic reduced-order modeling framework. Given a snapshot matrix of state vectors, the method leverages the empirical data distribution to induce a sampling distribution over principal subspaces for reduced order modeling. The resulting stochastic model enables improved characterization of model error in computational mechanics compared with existing approaches. The method offers several advantages: (1) it is assumption-free and leverages the empirical data distribution; (2) it enforces linear constraints (such as boundary conditions) by construction; (3) it requires only one hyperparameter, significantly simplifying the training process; and (4) its algorithm is straightforward to implement. We evaluate the method's performance against existing approaches using numerical examples in computational mechanics and structural dynamics.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VDMN: A Graphical Notation for Modelling Value Driver Trees</title>
<link>https://arxiv.org/abs/2512.14740</link>
<guid>https://arxiv.org/abs/2512.14740</guid>
<content:encoded><![CDATA[
<div> Keywords: Value Driver Trees, Value Driver Modelling Notation, Conceptual Models, Managerial Decision-Making, Business Outcomes<br /><br />Summary: Value Driver Trees (VDTs) are widely used conceptual models that depict the causal relationships between key performance indicators and business outcomes, aiding managerial decision-making and value-based management. However, despite their growing use, there has been no systematic approach or standardized guidelines for creating these models effectively. To address this gap, the study introduces the Value Driver Modelling Notation (VDMN), a novel graphical notation specifically designed to guide and systematize the modelling of VDTs. The VDMN incorporates a comprehensive collection of semantic constructs along with an intuitive graphical syntax, making it easier for managers and analysts to develop consistent and clear VDTs. To validate its effectiveness, the notation was applied in two separate case studies and further evaluated through expert interviews. Results indicate that the VDMN facilitates a clear, consistent, and comprehensible modelling process. By doing so, the new notation marks a significant advancement towards the systematization and standardization of VDT modelling, ultimately supporting better value-based management and decision-making processes across organizations. <div>
arXiv:2512.14740v1 Announce Type: cross 
Abstract: Value Driver Trees (VDTs) are conceptual models used to illustrate and analyse the causal relationships between key performance indicators and business outcomes, thereby supporting managerial decision-making and value-based management. Despite their increasing application, there are still no systematic guidelines for the modelling of such conceptual models. To fill this gap, this study introduces the Value Driver Modelling Notation (VDMN), a graphical notation developed to systematically guide VDT modelling. This notation includes a comprehensive set of semantic constructs and an intuitive graphical syntax. To evaluate its practical utility, the VDMN was applied in two case studies and assessed through expert interviews. The results show that the notation supports a consistent and comprehensible modelling of VDTs. The VDMN thus represents a significant step towards the systematisation and standardisation of VDT modelling.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Objective Bayesian Optimization of Deep Reinforcement Learning for Environmental, Social, and Governance (ESG) Financial Portfolio Management</title>
<link>https://arxiv.org/abs/2512.14992</link>
<guid>https://arxiv.org/abs/2512.14992</guid>
<content:encoded><![CDATA[
<div> DRL, Bayesian optimization, hyperparameter tuning, ESG score, multi-objective portfolio optimization<br /><br />Summary:<br /><br />1. The paper addresses the limitations of classic financial models by utilizing Deep Reinforcement Learning (DRL) agents that do not rely on assumptions such as normally distributed returns and can incorporate diverse information like ESG scores.<br />2. DRL agents exhibit high variability in performance and sensitivity to hyperparameter values, making hyperparameter tuning a critical problem.<br />3. Bayesian optimization is proposed as an effective technique for tuning DRL hyperparameters since it suits black-box functions that are noisy, expensive to evaluate, and analytically unknown.<br />4. Instead of optimizing a single combined objective, the study formulates a multi-objective optimization problem to obtain a Pareto set of portfolios balancing the Sharpe ratio and mean ESG score, thereby giving investors a range of trade-off options.<br />5. Experiments conducted on DJIA and NASDAQ markets using OpenAI Gym environments adapted from FinRL demonstrate that portfolios from the Pareto front achieve superior trade-offs between financial performance and ESG metrics.<br />6. The methodology’s effectiveness is validated by comparing the hypervolume of the Pareto sets with that from a Random Search, showing better optimization results through Bayesian methods. <div>
arXiv:2512.14992v1 Announce Type: cross 
Abstract: DRL agents circumvent the issue of classic models in the sense that they do not make assumptions like the financial returns being normally distributed and are able to deal with any information like the ESG score if they are configured to gain a reward that makes an objective better. However, the performance of DRL agents has high variability and it is very sensible to the value of their hyperparameters. Bayesian optimization is a class of methods that are suited to the optimization of black-box functions, that is, functions whose analytical expression is unknown, are noisy and expensive to evaluate. The hyperparameter tuning problem of DRL algorithms perfectly suits this scenario. As training an agent just for one objective is a very expensive period, requiring millions of timesteps, instead of optimizing an objective being a mixture of a risk-performance metric and an ESG metric, we choose to separate the objective and solve the multi-objective scenario to obtain an optimal Pareto set of portfolios representing the best tradeoff between the Sharpe ratio and the ESG mean score of the portfolio and leaving to the investor the choice of the final portfolio. We conducted our experiments using environments encoded within the OpenAI Gym, adapted from the FinRL platform. The experiments are carried out in the Dow Jones Industrial Average (DJIA) and the NASDAQ markets in terms of the Sharpe ratio achieved by the agent and the mean ESG score of the portfolio. We compare the performance of the obtained Pareto sets in hypervolume terms illustrating how portfolios are the best trade-off between the Sharpe ratio and mean ESG score. Also, we show the usefulness of our proposed methodology by comparing the obtained hypervolume with one achieved by a Random Search methodology on the DRL hyperparameter space.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Modular Physics for Elastic Simulation</title>
<link>https://arxiv.org/abs/2512.15083</link>
<guid>https://arxiv.org/abs/2512.15083</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Modular Physics, elastic simulation, physical consistency, modular neural networks, generalization<br /><br />Summary: This paper introduces Neural Modular Physics (NMP), a novel approach to elastic simulation that blends neural networks' approximation power with the reliability of traditional physics simulators. Unlike conventional monolithic neural simulators that treat dynamics as a black box, NMP breaks down elastic dynamics into physically interpretable modules connected via intermediate physical quantities. This design allows direct supervision of both intermediate outputs and physical constraints, enhancing the model's interpretability and trustworthiness. The proposed architecture and training strategy transform typical numerical computation flows into modular neural simulators, leading to improved physical consistency and better generalization capabilities. Experimentally, NMP outperforms existing neural simulation methods by generalizing well to unseen initial conditions and different spatial resolutions. It also exhibits more stable behavior over long simulation horizons and preserves fundamental physical properties more faithfully. Moreover, NMP proves more adaptable in situations where the underlying physical dynamics are partially or wholly unknown, where traditional simulators typically struggle. Overall, this work marks a significant step toward learning-based physics simulation methods that combine the strengths of data-driven modeling with physically grounded modular design, ensuring reliability and extensibility in complex elastic simulations. <div>
arXiv:2512.15083v1 Announce Type: cross 
Abstract: Learning-based methods have made significant progress in physics simulation, typically approximating dynamics with a monolithic end-to-end optimized neural network. Although these models offer an effective way to simulation, they may lose essential features compared to traditional numerical simulators, such as physical interpretability and reliability. Drawing inspiration from classical simulators that operate in a modular fashion, this paper presents Neural Modular Physics (NMP) for elastic simulation, which combines the approximation capacity of neural networks with the physical reliability of traditional simulators. Beyond the previous monolithic learning paradigm, NMP enables direct supervision of intermediate quantities and physical constraints by decomposing elastic dynamics into physically meaningful neural modules connected through intermediate physical quantities. With a specialized architecture and training strategy, our method transforms the numerical computation flow into a modular neural simulator, achieving improved physical consistency and generalizability. Experimentally, NMP demonstrates superior generalization to unseen initial conditions and resolutions, stable long-horizon simulation, better preservation of physical properties compared to other neural simulators, and greater feasibility in scenarios with unknown underlying dynamics than traditional simulators.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rigid-Deformation Decomposition AI Framework for 3D Spatio-Temporal Prediction of Vehicle Collision Dynamics</title>
<link>https://arxiv.org/abs/2503.19712</link>
<guid>https://arxiv.org/abs/2503.19712</guid>
<content:encoded><![CDATA[
<div> Keywords: rigid-deformation decomposition, vehicle collision dynamics, implicit neural representations, hierarchical architecture, quaternion-incremental scheme<br /><br />Summary:<br /><br />This study introduces a novel rigid-deformation decomposition framework aimed at improving the modeling of vehicle collision dynamics by addressing the spectral bias inherent in implicit neural representations that map spatio-temporal coordinates to physical fields. The authors develop a hierarchical architecture consisting of two scale-specific neural networks, RigidNet and DeformationNet, which separately capture global rigid-body motion and local deformation. To ensure kinematic separation, a frozen-anchor training method combined with a quaternion-incremental scheme is proposed, which stabilizes training and reduces rigid-body motion error by 29.8% compared to direct prediction methods. This approach enhances the resolution of high-frequency structural buckling, leading to a 17.2% decrease in total interpolation error. Analysis of the loss landscape reveals that decomposition smooths the optimization surface, improving robustness under angular distribution shifts and reducing error by 46.6%. To validate physical realism beyond numerical accuracy, the decomposed components are benchmarked against an oracle model representing performance upper bounds. The proposed framework recovers 92% of directional correlation between rigid and deformative elements, 96% of spatial deformation localization accuracy, and tracks temporal energy dynamics with a minimal 8 ms delay. Overall, the rigid-deformation decomposition approach offers accurate and physically interpretable predictions for nonlinear collision events in vehicles. <div>
arXiv:2503.19712v2 Announce Type: replace 
Abstract: This study presents a rigid-deformation decomposition framework for vehicle collision dynamics that mitigates the spectral bias of implicit neural representations, that is, coordinate-based neural networks that directly map spatio-temporal coordinates to physical fields. We introduce a hierarchical architecture that decouples global rigid-body motion from local deformation using two scale-specific networks, denoted as RigidNet and DeformationNet. To enforce kinematic separation between the two components, we adopt a frozen-anchor training strategy combined with a quaternion-incremental scheme. This strategy alleviates the kinematic instability observed in joint training and yields a 29.8% reduction in rigid-body motion error compared with conventional direct prediction schemes. The stable rigid-body anchor improves the resolution of high-frequency structural buckling, which leads to a 17.2% reduction in the total interpolation error. Loss landscape analysis indicates that the decomposition smooths the optimization surface, which enhances robustness to distribution shifts in angular extrapolation and yields a 46.6% reduction in error. To assess physical validity beyond numerical accuracy, we benchmark the decomposed components against an oracle model that represents an upper bound on performance. The proposed framework recovers 92% of the directional correlation between rigid and deformation components and 96% of the spatial deformation localization accuracy relative to the oracle, while tracking the temporal energy dynamics with an 8 ms delay. These results demonstrate that rigid-deformation decomposition enables accurate and physically interpretable predictions for nonlinear collision dynamics.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A User-Tunable Machine Learning Framework for Step-Wise Synthesis Planning</title>
<link>https://arxiv.org/abs/2504.02191</link>
<guid>https://arxiv.org/abs/2504.02191</guid>
<content:encoded><![CDATA[
<div> retrosynthesis, Hopfield networks, green chemistry, computer-aided synthesis, reaction templates<br /><br />Summary:<br /><br />We present MHNpath, a machine learning-based retrosynthetic tool designed to enhance computer-aided synthesis planning. Utilizing modern Hopfield networks alongside innovative comparative metrics, MHNpath prioritizes reaction templates efficiently, which boosts both the scalability and accuracy of retrosynthetic predictions. The platform provides a customizable scoring system enabling users to emphasize factors such as pathway cost, reaction temperature, and toxicity. This feature supports the design of greener, more cost-effective synthetic routes. The tool’s performance is validated through case studies involving complex molecules sourced from ChemByDesign, illustrating its capability to predict novel synthetic and enzymatic pathways. MHNpath’s benchmarking against established frameworks on the PaRoutes dataset reveals an 85.4% solution rate and a 69.2% replication of experimentally validated, “gold-standard” pathways. The case studies further demonstrate MHNpath’s ability to generate shorter, less expensive routes operating at moderate temperatures and involving green solvents. Notable examples include the synthesis of dronabinol, arformoterol, and lupinine. These outcomes position MHNpath as a promising solution for sustainable and efficient retrosynthetic planning. <div>
arXiv:2504.02191v3 Announce Type: replace 
Abstract: We introduce MHNpath, a machine learning-driven retrosynthetic tool designed for computer-aided synthesis planning. Leveraging modern Hopfield networks and novel comparative metrics, MHNpath efficiently prioritizes reaction templates, improving the scalability and accuracy of retrosynthetic predictions. The tool incorporates a tunable scoring system that allows users to prioritize pathways based on cost, reaction temperature, and toxicity, thereby facilitating the design of greener and cost-effective reaction routes. We demonstrate its effectiveness through case studies involving complex molecules from ChemByDesign, showcasing its ability to predict novel synthetic and enzymatic pathways. Furthermore, we benchmark MHNpath against existing frameworks using the PaRoutes dataset, achieving a solution rate of 85.4% and replicating 69.2% of experimentally validated "gold-standard" pathways. Our case studies reveal that the tool can generate shorter, cheaper, moderate-temperature routes employing green solvents, as exemplified by compounds such as dronabinol, arformoterol, and lupinine.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Predictions of Process-Induced Deformation in Carbon/Epoxy Composites Using a Deep Operator Network</title>
<link>https://arxiv.org/abs/2512.13746</link>
<guid>https://arxiv.org/abs/2512.13746</guid>
<content:encoded><![CDATA[
<div> Fiber reinforcement, polymer matrix, process-induced deformation, Deep Operator Networks, Ensemble Kalman Inversion<br /><br />Summary:<br /><br />This study addresses residual stresses and process-induced deformation (PID) in composites caused by mismatched thermal expansion and matrix shrinkage during thermoset curing. It uses a unidirectional AS4 carbon fiber/amine bi-functional epoxy prepreg to develop a two-mechanism physics-based model capturing thermal expansion/shrinkage and cure shrinkage effects, validated through manufacturing trials to define initial and boundary conditions. The model simulates PID under various non-isothermal cure cycles (time-temperature profiles). To enhance prediction efficiency, a data-driven surrogate model based on Deep Operator Networks (DeepONets) is created, trained on combined high-fidelity simulations and experimental PID measurements. This model is further extended with Feature-wise Linear Modulation (FiLM) to incorporate external parameters, such as the initial degree of cure, enabling prediction of temporal profiles of cure degree, viscosity, and deformation. Because experimental data are sparse in time, transfer learning fixes trained networks’ main layers and updates only the final output layer using final deformation data. The framework is complemented with Ensemble Kalman Inversion (EKI) to quantify uncertainty under experimental conditions and optimize cure schedules aimed at minimizing PID, offering a robust approach to predict, quantify, and mitigate manufacturing-induced deformation in composite materials. <div>
arXiv:2512.13746v1 Announce Type: new 
Abstract: Fiber reinforcement and polymer matrix respond differently to manufacturing conditions due to mismatch in coefficient of thermal expansion and matrix shrinkage during curing of thermosets. These heterogeneities generate residual stresses over multiple length scales, whose partial release leads to process-induced deformation (PID), requiring accurate prediction and mitigation via optimized non-isothermal cure cycles. This study considers a unidirectional AS4 carbon fiber/amine bi-functional epoxy prepreg and models PID using a two-mechanism framework that accounts for thermal expansion/shrinkage and cure shrinkage. The model is validated against manufacturing trials to identify initial and boundary conditions, then used to generate PID responses for a diverse set of non-isothermal cure cycles (time-temperature profiles). Building on this physics-based foundation, we develop a data-driven surrogate based on Deep Operator Networks (DeepONets). A DeepONet is trained on a dataset combining high-fidelity simulations with targeted experimental measurements of PID. We extend this to a Feature-wise Linear Modulation (FiLM) DeepONet, where branch-network features are modulated by external parameters, including the initial degree of cure, enabling prediction of time histories of degree of cure, viscosity, and deformation. Because experimental data are available only at limited time instances (for example, final deformation), we use transfer learning: simulation-trained trunk and branch networks are fixed and only the final layer is updated using measured final deformation. Finally, we augment the framework with Ensemble Kalman Inversion (EKI) to quantify uncertainty under experimental conditions and to support optimization of cure schedules for reduced PID in composites.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-simulation errors due to step size changes</title>
<link>https://arxiv.org/abs/2512.13845</link>
<guid>https://arxiv.org/abs/2512.13845</guid>
<content:encoded><![CDATA[
<div> extrapolation errors, co-simulation, continuous-time simulation, step size, state discrepancy  

<br /><br />Summary:  
This paper investigates the behavior of discrepancies between internal states of two coupled simulation units in a continuous-time co-simulation environment when connected via a shared variable \( q \). Each simulation unit maintains an internal state representing the time integral of \( q \). Normally, discrepancies arise due to extrapolation errors and tend to reduce as the macro time step size decreases. However, the authors demonstrate that under certain conditions, decreasing the step size can paradoxically cause the discrepancies between the states to increase rather than decrease. The study highlights that changing step sizes — specifically reducing them — does not always improve accuracy in co-simulation integration of coupled systems. This finding challenges the common assumption that smaller step sizes universally lead to more accurate simulation results by minimizing extrapolation error. The results point to the complexity of interaction effects between coupled units and their numerical integration strategies. The paper encourages a careful analysis of step size selection and error behavior in co-simulation frameworks where internal states are integrated quantities, as traditional step size refinement may inadvertently amplify internal state mismatches. <div>
arXiv:2512.13845v1 Announce Type: new 
Abstract: When two simulation units in a continuous-time co-simulation are connected via some variable $q$, and both simulation units have an internal state which represents the time integral of $q$, there will generally be a discrepancy between those states due to extrapolation errors. Normally, such extrapolation errors diminish if the macro time step size is reduced. Here we show that, under certain circumstances, step size changes can cause such discrepancies to increase even when the change is towards smaller steps.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic stacking ensemble learning with investor knowledge representations for stock market index prediction based on multi-source financial data</title>
<link>https://arxiv.org/abs/2512.14042</link>
<guid>https://arxiv.org/abs/2512.14042</guid>
<content:encoded><![CDATA[
<div> Keywords: financial data, ensemble model, investor cognition, stock index prediction, trading strategy  

<br /><br />Summary: This paper introduces a novel two-stage dynamic stacking ensemble model designed to predict stock index movements by effectively capturing diverse patterns in multi-source financial data. Firstly, the model identifies distinct properties of data from global stock market indices, industrial indices, and financial news, framed from investors' cognition perspectives. Subsequently, tailored neural network architectures are developed for each data property to generate informative feature representations. In the second stage, multiple meta-classifiers are constructed, and the model dynamically selects the optimal classifier for each time window to adapt to temporal variations in data patterns. The approach is tested on the Chinese stock market indices: Shanghai Securities Composite index (SSEC), SZSE Component index (SZEC), and Growth Enterprise index (GEI), where it outperforms existing models in prediction accuracy by 1.42%, 7.94%, and 7.73%, respectively. Additionally, a trading strategy based on the proposed model is devised and evaluated. Economic results demonstrate the superiority of this strategy over competitors through higher accumulated returns and improved Sharpe ratios. Overall, the study highlights the effectiveness of integrating investor cognition perspectives with dynamic ensemble learning to enhance financial forecasting and trading performance. <div>
arXiv:2512.14042v1 Announce Type: new 
Abstract: The patterns of different financial data sources vary substantially, and accordingly, investors exhibit heterogeneous cognition behavior in information processing. To capture different patterns, we propose a novel approach called the two-stage dynamic stacking ensemble model based on investor knowledge representations, which aims to effectively extract and integrate the features from multi-source financial data. In the first stage, we identify different financial data property from global stock market indices, industrial indices, and financial news based on the perspective of investors. And then, we design appropriate neural network architectures tailored to these properties to generate effective feature representations. Based on learned feature representations, we design multiple meta-classifiers and dynamically select the optimal one for each time window, enabling the model to effectively capture and learn the distinct patterns that emerge across different temporal periods. To evaluate the performance of the proposed model, we apply it to predicting the daily movement of Shanghai Securities Composite index, SZSE Component index and Growth Enterprise index in Chinese stock market. The experimental results demonstrate the effectiveness of our model in improving the prediction performance. In terms of accuracy metric, our approach outperforms the best competing models by 1.42%, 7.94%, and 7.73% on the SSEC, SZEC, and GEI indices, respectively. In addition, we design a trading strategy based on the proposed model. The economic results show that compared to the competing trading strategies, our strategy delivers a superior performance in terms of the accumulated return and Sharpe ratio.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transfer Learning-Based Surrogate Modeling for Nonlinear Time-History Response Analysis of High-Fidelity Structural Models</title>
<link>https://arxiv.org/abs/2512.14161</link>
<guid>https://arxiv.org/abs/2512.14161</guid>
<content:encoded><![CDATA[
<div> Keywords: performance based earthquake engineering, nonlinear time-history response analysis, surrogate model, transfer learning, high-fidelity response analysis

<br /><br />Summary:  
1. The study addresses the challenges in performance based earthquake engineering (PBEE) where nonlinear time-history response analysis (NLTHA) requires extensive computational resources, limiting practical application.  
2. Previous surrogate modeling efforts typically relied on low-fidelity models such as single degree of freedom systems due to the high computational costs of high-fidelity analyses.  
3. To overcome the computational burden, this study proposes a novel transfer learning framework that leverages a pretrained surrogate model based on low-fidelity response analysis to build surrogate models for high-fidelity analyses with significantly fewer training samples.  
4. The framework was validated in a case study involving a 20-story steel moment frame, where only 20 ground motion samples were used to train the high-fidelity surrogate model.  
5. The results demonstrated that the surrogate model accurately predicted structural responses consistent with site-specific, time-based seismic hazard, indicating its potential for efficient and reliable seismic risk assessment in PBEE. <div>
arXiv:2512.14161v1 Announce Type: new 
Abstract: In a performance based earthquake engineering (PBEE) framework, nonlinear time-history response analysis (NLTHA) for numerous ground motions are required to assess the seismic risk of buildings or civil engineering structures. However, such numerical simulations are computationally expensive, limiting the real-world practical application of the framework. To address this issue, previous studies have used machine learning to predict the structural responses to ground motions with low computational costs. These studies typically conduct NLTHAs for a few hundreds ground motions and use the results to train and validate surrogate models. However, most of the previous studies focused on computationally-inexpensive response analysis models such as single degree of freedom. Surrogate models of high-fidelity response analysis are required to enrich the quantity and diversity of information used for damage assessment in PBEE. Notably, the computational cost of creating training and validation datasets increases if the fidelity of response analysis model becomes higher. Therefore, methods that enable surrogate modeling of high-fidelity response analysis without a large number of training samples are needed. This study proposes a framework that uses transfer learning to construct the surrogate model of a high-fidelity response analysis model. This framework uses a surrogate model of low-fidelity response analysis as the pretrained model and transfers its knowledge to construct surrogate models for high-fidelity response analysis with substantially reduced computational cost. As a case study, surrogate models that predict responses of a 20-story steel moment frame were constructed with only 20 samples as the training dataset. The responses to the ground motions predicted by constructed surrogate model were consistent with a site-specific time-based hazard.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data</title>
<link>https://arxiv.org/abs/2512.14329</link>
<guid>https://arxiv.org/abs/2512.14329</guid>
<content:encoded><![CDATA[
<div> Keywords: stroke rehabilitation, locomotor capacity, neuromuscular control, wearable sensors, deep reinforcement learning<br /><br />Summary:<br /><br />1. The study addresses the need for dynamic prediction of locomotor capacity after stroke, which is essential for personalized rehabilitation but lacking in current static assessment methods.<br />2. A novel hybrid generative framework is developed that reconstructs an individual’s neuromuscular control using data from a single 20 m level-ground walking trial.<br />3. The framework integrates wearable-sensor kinematics, a proportional-derivative physics controller, a Healthy Motion Atlas, and goal-conditioned deep reinforcement learning combined with behavior cloning and generative adversarial imitation learning.<br />4. This approach enables the generation of patient-specific, physically plausible gait simulations under different task conditions such as slope walking and stair climbing.<br />5. Testing on 11 stroke survivors showed improved joint-angle and endpoint accuracy by 4.73% and 12.10%, respectively, while reducing training time by 74.44% compared to a physics-only baseline.<br />6. In a multicenter pilot with 21 inpatients, clinicians using these predictions to guide task selection and difficulty achieved significantly greater improvements in Fugl-Meyer lower-extremity scores over 28 days compared to controls.<br />7. The results demonstrate that this generative, task-predictive framework can effectively augment clinical decision-making and support dynamically personalized post-stroke motor recovery strategies. <div>
arXiv:2512.14329v1 Announce Type: new 
Abstract: Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BridgeNet: A Dataset of Graph-based Bridge Structural Models for Machine Learning Applications</title>
<link>https://arxiv.org/abs/2512.14496</link>
<guid>https://arxiv.org/abs/2512.14496</guid>
<content:encoded><![CDATA[
<div> Keywords: BridgeNet, structural engineering, graph machine learning, form-finding, multi-modal dataset<br /><br />Summary:  
This paper introduces BridgeNet, a comprehensive and publicly available dataset designed to advance machine learning applications in structural engineering and design. BridgeNet contains 20,000 form-found bridge structures represented through multiple modalities to facilitate diverse ML tasks. Each dataset entry consists of three components: (i) a pin-jointed equilibrium wireframe model produced via the Combinatorial Equilibrium Modeling (CEM) form-finding method, (ii) a volumetric 3D mesh derived from force-informed materialization processes, and (iii) two rendered images captured from canonical camera angles for visual context. The dataset is rich in modalities and aims to be application-agnostic, supporting tasks such as edge classification specific to CEM, parameter inference, surrogate modeling of the form-finding process, and cross-modal data reconstruction among graphs, meshes, and images. Additionally, BridgeNet enables generative structural design workflows by providing diverse and well-structured data. By releasing BridgeNet, the authors address a critical shortage of openly accessible, large-scale datasets in structural engineering, thereby lowering barriers to the adoption of advanced graph-based and multi-modal machine learning techniques within the field. This resource promises to catalyze the development of innovative ML approaches for designing equilibrium bridge structures, ultimately pushing forward research and practical applications in conceptual structural design. <div>
arXiv:2512.14496v1 Announce Type: new 
Abstract: Machine learning (ML) is increasingly used in structural engineering and design, yet its broader adoption is hampered by the lack of openly accessible datasets of structural systems. We introduce BridgeNet, a publicly available graph-based dataset of 20,000 form-found bridge structures aimed at enabling Graph ML and multi-modal learning in the context of conceptual structural design. Each datapoint consists of (i) a pin-jointed equilibrium wireframe model generated with the Combinatorial Equilibrium Modeling (CEM) form-finding method, (ii) a volumetric 3D mesh obtained through force-informed materialization, and (iii) rendered images from two canonical camera angles. The resulting dataset is modality-rich and application-agnostic, supporting tasks such as CEM-specific edge classification and parameter inference, surrogate modeling of form-finding, cross-modal reconstruction between graphs, meshes and images, and generative structural design. BridgeNet addresses a key bottleneck in data-driven applications for structural engineering and design by providing a dataset that facilitates the development of new ML-based approaches for equilibrium bridge structures.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Sizing and Material Choice for Additively Manufactured Compact Plate Heat Exchangers</title>
<link>https://arxiv.org/abs/2504.03372</link>
<guid>https://arxiv.org/abs/2504.03372</guid>
<content:encoded><![CDATA[
<div> Keywords: additive manufacturing, compact heat exchangers, material selection, power density, thermal conductivity<br /><br />Summary:<br /><br />This study explores the design of compact heat exchangers (cHEXs) enabled by additive manufacturing (AM), focusing on how material choices and manufacturing constraints impact performance. It addresses the common challenge that reducing size in counterflow cHEXs can lower effectiveness due to axial heat conduction, influenced by material thermal conductivity and wall thickness. The authors develop an optimization framework to evaluate six materials—plastic, austenitic steel, Al2O3, AlN, aluminum, and copper—under fixed pressure drop and effectiveness, while incorporating AM-specific thin wall constraints and minimum plate spacing to mitigate fouling. Results reveal that copper, despite its high thermal conductivity, results in the lowest power density. In contrast, plastic often achieves the highest power density, outperforming steel by nearly three orders of magnitude when no manufacturing or fouling constraints are applied. When uniform thickness or fouling constraints are introduced, plastic and steel performance become comparable. Applying material-specific thickness limits again favors plastic due to its manufacturability at thin wall scales. Overall, the findings emphasize that AM constraints significantly affect heat exchanger compactness and that materials with lower thermal conductivity can surpass traditional metals like copper in power-dense cHEX designs. <div>
arXiv:2504.03372v2 Announce Type: replace 
Abstract: Advances in additive manufacturing (AM) enable new opportunities to design compact heat exchangers (cHEXs) by leveraging flexible geometries to improve energy and material efficiency. However, it is well known that reducing size in counterflow cHEXs can degrade effectiveness due to axial heat conduction through the solid material, which depends strongly on material thermal conductivity and wall thickness. Understanding the interaction between fundamental heat transfer mechanisms and manufacturing constraints is essential for designing next generation compact thermal systems that fully exploit AM's shaping flexibility. This study investigates how material selection and AM thin wall limitations influence the maximum achievable power density in compact plate heat exchangers. An optimization framework evaluates six materials including plastic, austenitic steel, Al2O3, AlN, aluminum, and copper under fixed pressure drop and effectiveness, while accounting for AM specific thickness constraints and a minimum plate spacing to address fouling risks. Results show that copper consistently yields the lowest power density despite having the highest thermal conductivity, whereas plastic achieves the highest power density across most optimization scenarios. Without manufacturing or fouling constraints, plastic outperforms the baseline steel design by nearly three orders of magnitude. With uniform plate thickness or fouling constraints, the performance gap narrows, making plastic and austenitic steel comparable. When material specific thickness limits are applied, plastic again leads in compactness due to its superior thin wall manufacturability. These findings highlight that AM constraints strongly affect cHEX compactness and that lower conductivity materials can outperform metals such as copper in power dense heat exchanger designs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Large Language Models for ESG Activity Detection in Financial Texts</title>
<link>https://arxiv.org/abs/2502.21112</link>
<guid>https://arxiv.org/abs/2502.21112</guid>
<content:encoded><![CDATA[
<div> ESG, Large Language Models, fine-tuning, sustainability reports, EU ESG taxonomy<br /><br />Summary:<br /><br />1. The paper addresses the integration of Environmental, Social, and Governance (ESG) factors into corporate decision-making, emphasizing the need for aligning business practices with evolving regulatory frameworks for sustainable finance.<br /><br />2. It highlights challenges in automatically assessing sustainability reports and non-financial disclosures against specific ESG activities, due to limitations of general-purpose Large Language Models (LLMs) and lack of structured, high-quality datasets.<br /><br />3. The authors investigate current-generation LLMs' abilities to identify environmental activity-related text segments and show that fine-tuning these models on a combination of original and synthetically generated data greatly improves performance.<br /><br />4. They introduce ESG-Activities, a benchmark dataset comprising 1,325 labelled text segments classified according to the EU ESG taxonomy, which supports model training and evaluation.<br /><br />5. Experimental results demonstrate that fine-tuned open models like Llama 7B and Gemma 7B can outperform larger proprietary LLMs in specific configurations, with significant implications for financial analysts, policymakers, and AI researchers aiming to enhance ESG transparency and compliance using advanced natural language processing techniques. <div>
arXiv:2502.21112v2 Announce Type: replace-cross 
Abstract: The integration of Environmental, Social, and Governance (ESG) factors into corporate decision-making is a fundamental aspect of sustainable finance. However, ensuring that business practices align with evolving regulatory frameworks remains a persistent challenge. AI-driven solutions for automatically assessing the alignment of sustainability reports and non-financial disclosures with specific ESG activities could greatly support this process. Yet, this task remains complex due to the limitations of general-purpose Large Language Models (LLMs) in domain-specific contexts and the scarcity of structured, high-quality datasets. In this paper, we investigate the ability of current-generation LLMs to identify text related to environmental activities. Furthermore, we demonstrate that their performance can be significantly enhanced through fine-tuning on a combination of original and synthetically generated data. To this end, we introduce ESG-Activities, a benchmark dataset containing 1,325 labelled text segments classified according to the EU ESG taxonomy. Our experimental results show that fine-tuning on ESG-Activities significantly enhances classification accuracy, with open models such as Llama 7B and Gemma 7B outperforming large proprietary solutions in specific configurations. These findings have important implications for financial analysts, policymakers, and AI researchers seeking to enhance ESG transparency and compliance through advanced natural language processing techniques.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero</title>
<link>https://arxiv.org/abs/2505.21536</link>
<guid>https://arxiv.org/abs/2505.21536</guid>
<content:encoded><![CDATA[
<div> Keywords: circular economy, deep reinforcement learning, material circularity, thermodynamical material networks, Stable-Baselines3<br /><br />Summary:<br /><br />The article addresses the rising demand for finite raw materials and the pressing challenge of achieving net-zero carbon emissions, highlighting the urgency of sustainable solutions. It introduces CiRL, a deep reinforcement learning (DRL) library designed to optimize the circularity control of both solid and fluid materials within supply and recovery chains. CiRL leverages the formalism of thermodynamical material networks, supported by compartmental dynamical thermodynamics, enabling the integration of DRL into material circularity design. The library provides CE-oriented environments in a state-space form, popular in dynamical systems and control, facilitating advanced analysis and control design. CiRL is built upon Stable-Baselines3, a contemporary Python library for DRL algorithms, ensuring robustness and versatility. Furthermore, it is developed on Google Colaboratory, maximizing accessibility for researchers across diverse disciplines, particularly those involved in circular economy and engineering. The tool aims to complement human-driven material flow analysis (MFA) decisions by generating AI-driven actions for enhanced circularity. Finally, CiRL is made publicly available, encouraging widespread adoption and collaboration in advancing circular economy practices through AI-powered optimization. <div>
arXiv:2505.21536v2 Announce Type: replace-cross 
Abstract: The demand of finite raw materials will keep increasing as they fuel modern society. Simultaneously, solutions for stopping carbon emissions in the short term are not available, thus making the net zero target extremely challenging to achieve at scale. The circular economy (CE) paradigm is gaining attention as a solution to address climate change and the uncertainties of supplies of critical materials. Hence, in this paper, we introduce CiRL, a deep reinforcement learning (DRL) library of environments focused on the circularity control of both solid and fluid materials. The integration of DRL into the design of material circularity is possible thanks to the formalism of thermodynamical material networks, which is underpinned by compartmental dynamical thermodynamics. Along with the focus on circularity, this library has three more features: the new CE-oriented environments are in the state-space form, which is typically used in dynamical systems analysis and control design; it is based on a state-of-the-art Python library of DRL algorithms, namely, Stable-Baselines3; and it is developed in Google Colaboratory to be accessible to researchers from different disciplines and backgrounds as is often the case for circular economy researchers and engineers. CiRL is intended to be a tool to generate AI-driven actions for optimizing the circularity of supply-recovery chains and to be combined with human-driven decisions derived from material flow analysis (MFA) studies. CiRL is publicly available.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeBPF Many-Core Architecture for Network Functions in FPGA-based SmartNICs and IoT</title>
<link>https://arxiv.org/abs/2512.12778</link>
<guid>https://arxiv.org/abs/2512.12778</guid>
<content:encoded><![CDATA[
<div> FPGA, VeBPF, many-core architecture, eBPF, SmartNIC

<br /><br />Summary: This article introduces VeBPF, a resource-optimized and highly configurable many-core architecture designed specifically for FPGA-based network packet processing. It features custom VeBPF CPU cores that are compliant with the eBPF instruction set architecture (ISA) and implemented in Verilog HDL to facilitate seamless integration with existing FPGA IP blocks and subsystems. The architecture enables parallel execution of multiple eBPF rules across several cores, allowing for low-latency packet processing. It is fully parameterizable, permitting users to scale the number of cores and rules based on application demands and available FPGA resources. A key advantage is the dynamic update of eBPF rules at runtime without needing FPGA reconfiguration, enhancing flexibility and adaptability in network processing. The design incorporates hardware and architectural optimizations suitable for deployment on various platforms—ranging from low-end FPGA-based IoT devices to high-end SmartNICs. Moreover, the authors developed automated testing and simulation frameworks leveraging open-source tools like Python and Cocotb. The entire ecosystem, including the VeBPF cores, many-core architecture, control software libraries, and simulation infrastructure, is released as open source to encourage further research and development in FPGA many-core systems, eBPF acceleration, SmartNICs, IoT, and network security. <div>
arXiv:2512.12778v1 Announce Type: new 
Abstract: FPGA-based SmartNICs and IoT devices integrating soft-processors for network function execution have emerged to address the limited hardware reconfigurability of DPUs and MCUs. However, existing FPGA-based solutions lack a highly configurable many-core architecture specialized for network packet processing. This work presents VeBPF many-core architecture, a resource-optimized and highly configurable many-core architecture composed of custom VeBPF (Verilog eBPF) CPU cores designed for FPGA-based packet processing. The VeBPF cores are eBPF ISA compliant and implemented in Verilog HDL for seamless integration with existing FPGA IP blocks and subsystems.
  The proposed many-core architecture enables parallel execution of multiple eBPF rules across multiple VeBPF cores, achieving low-latency packet processing. The architecture is fully parameterizable, allowing the number of VeBPF cores and eBPF rules to scale according to application requirements and available FPGA resources. eBPF rules can be dynamically updated at run time without requiring FPGA reconfiguration, enabling flexible and adaptive network processing.
  The design incorporates hardware and computer architecture optimizations that support deployment across a wide range of platforms, from low-end FPGA-based IoT devices to high-end FPGA-based SmartNICs. In addition, we present automated testing and simulation frameworks developed using open-source tools such as Python and Cocotb. The VeBPF cores, many-core architecture, control software libraries, and simulation infrastructure are released as open source to support further research in FPGA-based many-core systems, eBPF acceleration, SmartNICs, IoT, and network security.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptiWing3D: A Diverse Dataset of Optimized Wing Designs</title>
<link>https://arxiv.org/abs/2512.12867</link>
<guid>https://arxiv.org/abs/2512.12867</guid>
<content:encoded><![CDATA[
<div> Keywords: OptiWing3D, aerodynamic shape optimization, 3D wing geometries, multi-fidelity dataset, inverse design<br /><br />Summary:<br /><br />OptiWing3D is the first publicly available dataset comprising high-fidelity, shape-optimized 3D wing geometries, addressing limitations in existing aerodynamic datasets that are mostly 2D, lack optimization, or have limited design diversity. The dataset includes 1552 simulations covering 776 distinct wing designs, each initiated from unique extruded airfoil cross-sections to ensure design variety. A significant feature is the pairing of most 3D optimized wings with 2D counterparts optimized under the same conditions, creating a multi-fidelity dataset that enables direct comparison between 2D and 3D aerodynamic simulations. This comparison reveals that differences between the designs are most pronounced near the wingtips, where three-dimensional aerodynamic effects dominate. Additionally, the dataset supports advanced inverse design approaches, demonstrated via a constraint-aware conditional latent diffusion model capable of generating optimized wing shapes based on flow conditions, providing a strong baseline for future research. The comprehensive dataset, including wing geometries and surface pressure distributions, is publicly released to facilitate advances in data-driven aerodynamic design and benchmarking of design diversity and optimization methods. <div>
arXiv:2512.12867v1 Announce Type: new 
Abstract: OptiWing3D is the first publicly available dataset of high-fidelity shape optimized 3D wing geometries. Existing aerodynamics datasets are either limited to 2D simulations, lack optimization, or derive diversity solely from perturbations to a single baseline design, constraining their application as benchmarks to inverse design approaches and in the study of design diversity. The OptiWing3D dataset addresses these gaps, consisting of 1552 simulations resulting in 776 wing designs initialized from distinct extruded airfoil cross-sections. Additionally, a majority of the optimized wings in the dataset are paired to 2D counterparts optimized under identical conditions, creating the first multi-fidelity aerodynamic shape optimization dataset. Moreover, this structure allows for a direct comparison between 2D and 3D aerodynamic simulations. It is observed that 3D optimized designs diverge most prominently from the 2D-optimized designs near the wingtip, where three-dimensional effects are strongest, a finding made possible by the paired nature of the dataset. Finally, we demonstrate a constraint-aware conditional latent diffusion model capable of generating optimized wings from flow conditions, establishing a baseline for future inverse design approaches. The dataset, containing wing geometries and surface pressure distributions is publicly released to advance research in data-driven aerodynamic design.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ERA-IT: Aligning Semantic Models with Revealed Economic Preference for Real-Time and Explainable Patent Valuation</title>
<link>https://arxiv.org/abs/2512.12869</link>
<guid>https://arxiv.org/abs/2512.12869</guid>
<content:encoded><![CDATA[
<div> Keywords: intangible assets, patent renewal, Large Language Models, Economic Chain-of-Thought, intellectual property management<br /><br />Summary:<br /><br />1. This study addresses the challenge of valuing intangible assets under uncertainty, particularly the difficulty caused by information asymmetry in technical specifications and the latency of traditional bibliometric indicators like citation counts.<br /><br />2. The authors propose the Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework, which conceptualizes patent renewal history as a revealed economic preference that can serve as an objective supervisory signal.<br /><br />3. ERA-IT aligns the generative reasoning of Large Language Models (LLMs) with actual market realities through a process termed Eco-Semantic Alignment.<br /><br />4. Using a dataset of 10,000 randomly sampled European Patent Office patents spanning diverse technologies, the model was trained to predict value tiers and to reconstruct the Economic Chain-of-Thought from unstructured patent text.<br /><br />5. Empirical results show that ERA-IT outperforms both traditional econometric models and zero-shot LLMs in predictive accuracy, while also generating explicit, logical rationales for valuation, thus enhancing transparency and reducing black-box opacity in high-stakes intellectual property decisions. <div>
arXiv:2512.12869v1 Announce Type: new 
Abstract: Valuing intangible assets under uncertainty remains a critical challenge in the strategic management of technological innovation due to the information asymmetry inherent in high-dimensional technical specifications. Traditional bibliometric indicators, such as citation counts, fail to address this friction in a timely manner due to the systemic latency inherent in data accumulation. To bridge this gap, this study proposes the Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework. We theoretically conceptualize patent renewal history as a revealed economic preference and leverage it as an objective supervisory signal to align the generative reasoning of Large Language Models (LLMs) with market realities, a process we term Eco-Semantic Alignment. Using a randomly sampled dataset of 10,000 European Patent Office patents across diverse technological domains, we trained the model not only to predict value tiers but also to reverse-engineer the Economic Chain-of-Thought from unstructured text. Empirical results demonstrate that ERA-IT significantly outperforms both conventional econometric models and zero-shot LLMs in predictive accuracy. More importantly, by generating explicit, logically grounded rationales for valuation, the framework serves as a transparent cognitive scaffold for decision-makers, reducing the opacity of black-box AI in high-stakes intellectual property management.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the impact of geometric variance on the performance of formed parts: A probabilistic approach on the example of airbag pressure bins</title>
<link>https://arxiv.org/abs/2512.13302</link>
<guid>https://arxiv.org/abs/2512.13302</guid>
<content:encoded><![CDATA[
<div> Keywords: geometric variability, probabilistic study, performance variance, airbag pressure bin, quality assurance<br /><br />Summary:<br /><br />This article addresses the challenge posed by scatter in manufacturing properties, which complicates lightweight design by necessitating conservative safety factors that account for performance variance. A key source of this variance is the inherent geometric variability in formed parts. To quantify the impact of geometric deviations on part performance, the authors conduct a probabilistic numerical study that models these deviations stochastically. The goal is to determine a clear correlation between the variance in geometry and the resulting variance in mechanical performance. The study focuses specifically on an airbag pressure bin, a component where understanding this relationship is critical for ensuring safety while enabling lighter designs. The insights gained allow for improvements in the design process without altering existing manufacturing methods. Instead, the study advocates for more targeted and effective quality assurance measures that take into account the performance implications of geometric variability. This approach aims to reduce unnecessary design conservatism, thereby optimizing part weight and potentially reducing costs while maintaining safety and reliability. <div>
arXiv:2512.13302v1 Announce Type: new 
Abstract: Scatter in properties resulting from manufacturing is a great challenge in lightweight design, requiring consideration of not only the average mechanical performance but also the variance which is done e.g., by conservative safety factors. One contributor to this variance is the inherent geometric variability in the formed part. To isolate and quantify this effect, we present a probabilistic numerical study, aiming to assess the impact of geometric variance on the resulting part performance. By modelling geometric deviations stochastically, we aim to establish a correlation between the variance in geometry with the resulting variance in performance. The study is done on the example of an airbag pressure bin, where a better understanding of this correlation is crucial, as it allows for the design of a lighter part without changing the manufacturing process. Instead, we aim to implement more targeted and effective quality assurance, informed by the performance impact of geometric deviations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hot H\'em: S\`ai G\`on Gi\~ua C\'ai N\'ong H\^ong C\`ong B\`ang -- Saigon in Unequal Heat</title>
<link>https://arxiv.org/abs/2512.11896</link>
<guid>https://arxiv.org/abs/2512.11896</guid>
<content:encoded><![CDATA[
<div> Keywords: pedestrian heat exposure, GeoAI, Google Street View, XGBoost, Ho Chi Minh City  

<br /><br />Summary:  
1. The study addresses pedestrian heat exposure, a significant health risk in dense tropical urban environments, specifically focusing on Ho Chi Minh City (HCMC), Vietnam.  
2. Hot Hém is introduced as a GeoAI workflow that integrates multiple data sources and techniques—Google Street View imagery, semantic image segmentation, and remote sensing—to estimate micro-scale thermal variations affecting pedestrians.  
3. Two machine learning models based on XGBoost are trained using GSV imagery and a labeled dataset from selected administrative wards (phường) within HCMC to predict land surface temperature (LST) at a fine spatial resolution.  
4. These models are applied in a patchwork manner across all pedestrian network nodes derived from the OpenStreetMap-based OSMnx framework, enabling the creation of heat-aware routing pathways that consider microclimatic heat variation on sidewalks and urban corridors.  
5. The deployed model aims to serve as a foundation for urban planners and public health officials by identifying city areas with disproportionately higher pedestrian heat exposure and helping to understand infrastructural factors that contribute to these hotspots, guiding interventions to improve pedestrian comfort and safety. <div>
arXiv:2512.11896v1 Announce Type: cross 
Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot H\'em is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in H\^o Ch\'i Minh City (HCMC), Vi\d{e}t Nam, colloquially known as S\`ai G\`on. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as ph\u{o}ng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance</title>
<link>https://arxiv.org/abs/2512.11933</link>
<guid>https://arxiv.org/abs/2512.11933</guid>
<content:encoded><![CDATA[
<div> Generative AI, multi-agent trading, model-risk governance, complex adaptive systems, layered controls<br /><br />Summary:<br /><br />1. The article addresses the rapid integration of generative and agentic AI into financial markets, highlighting that current model-risk governance frameworks are insufficient for these continuously learning, interacting, and emergent systems.<br /><br />2. It critiques existing approaches that rely on static, well-specified algorithms with one-time validations, which do not accommodate the dynamic nature of large language models and multi-agent trading environments.<br /><br />3. Using complex adaptive systems theory, the authors model these AI technologies as decentralized ensembles whose risk propagates over multiple time scales, necessitating a new governance approach.<br /><br />4. The paper proposes a modular governance architecture composed of four regulatory layers: self-regulation modules beside each model, firm-level governance aggregating telemetry and enforcing policies, regulator-hosted agents monitoring sector-wide patterns, and independent third-party audit blocks providing assurance.<br /><br />5. Eight design strategies allow these regulatory blocks to evolve in pace with the AI models they oversee. A case study on emergent spoofing in multi-agent trading demonstrates how layered controls can quarantine harmful behaviors in real-time while still enabling innovation.<br /><br />6. The architecture coexists with current model-risk regulations but significantly improves observability and control, offering a practical framework for resilient and adaptive AI governance within financial systems. <div>
arXiv:2512.11933v1 Announce Type: cross 
Abstract: Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of "regulatory blocks": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EXFormer: A Multi-Scale Trend-Aware Transformer with Dynamic Variable Selection for Foreign Exchange Returns Prediction</title>
<link>https://arxiv.org/abs/2512.12727</link>
<guid>https://arxiv.org/abs/2512.12727</guid>
<content:encoded><![CDATA[
<div> exchange rate forecasting, Transformer, multi-scale self-attention, variable selection, financial time series<br /><br />Summary:<br /><br />This paper addresses the challenge of accurately forecasting daily exchange rate returns, which are influenced by multiple correlated market factors and exhibit high-frequency fluctuations. The authors propose EXFormer, a novel Transformer-based model tailored for forecasting daily exchange rate returns. EXFormer incorporates a multi-scale trend-aware self-attention mechanism using parallel convolutional branches with different receptive fields, enabling alignment based on local slopes while preserving long-range dependencies and remaining sensitive to regime shifts. The model includes a dynamic variable selector that assigns time-varying importance weights to 28 exogenous covariates, enhancing pre-hoc interpretability. Additionally, an embedded squeeze-and-excitation block recalibrates channel responses to emphasize informative features and suppress noise. The model is tested on daily data from EUR/USD, USD/JPY, and GBP/USD exchange pairs, showing consistent outperformance over random walk and baseline models across five sliding windows, with directional accuracy improvements up to 8.5%–22.8%. In nearly one year of trading backtests, EXFormer achieves cumulative returns of 18%, 25%, and 18% for the three pairs, with Sharpe ratios above 1.8. After accounting for transaction costs and slippage, EXFormer retains positive cumulative returns (7%, 19%, and 9%), whereas baselines perform negatively. Robustness checks confirm EXFormer's superiority in high-volatility and bear-market regimes. The model offers both economically valuable forecasts and transparent insights into exchange rate dynamics for stakeholders such as investors, corporations, and central banks. <div>
arXiv:2512.12727v1 Announce Type: cross 
Abstract: Accurately forecasting daily exchange rate returns represents a longstanding challenge in international finance, as the exchange rate returns are driven by a multitude of correlated market factors and exhibit high-frequency fluctuations. This paper proposes EXFormer, a novel Transformer-based architecture specifically designed for forecasting the daily exchange rate returns. We introduce a multi-scale trend-aware self-attention mechanism that employs parallel convolutional branches with differing receptive fields to align observations on the basis of local slopes, preserving long-range dependencies while remaining sensitive to regime shifts. A dynamic variable selector assigns time-varying importance weights to 28 exogenous covariates related to exchange rate returns, providing pre-hoc interpretability. An embedded squeeze-and-excitation block recalibrates channel responses to emphasize informative features and depress noise in the forecasting. Using the daily data for EUR/USD, USD/JPY, and GBP/USD, we conduct out-of-sample evaluations across five different sliding windows. EXFormer consistently outperforms the random walk and other baselines, improving directional accuracy by a statistically significant margin of up to 8.5--22.8%. In nearly one year of trading backtests, the model converts these gains into cumulative returns of 18%, 25%, and 18% for the three pairs, with Sharpe ratios exceeding 1.8. When conservative transaction costs and slippage are accounted for, EXFormer retains cumulative returns of 7%, 19%, and 9%, while other baselines achieve negative. The robustness checks further confirm the model's superiority under high-volatility and bear-market regimes. EXFormer furnishes both economically valuable forecasts and transparent, time-varying insights into the drivers of exchange rate dynamics for international investors, corporations, and central bank practitioners.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Data Pruning for Pretraining Biological Foundation Models at Scale</title>
<link>https://arxiv.org/abs/2512.12932</link>
<guid>https://arxiv.org/abs/2512.12932</guid>
<content:encoded><![CDATA[
<div> Biological foundation models, data pruning, influence-guided, RNA-FM, ESM-C  

<br /><br />Summary:  
This paper addresses the high computational costs and reproducibility challenges of pretraining large-scale Biological Foundation Models (BioFMs), which often require millions to billions of sequences and parameters. The authors propose a post-hoc influence-guided data pruning framework specifically designed for biological domains to reduce the dataset size while maintaining model performance. The core contribution is a subset-based self-influence formulation that efficiently estimates the importance of individual samples at low computational cost. Based on this, two data selection strategies are introduced: Top-k Influence (Top I) and Coverage-Centric Influence (CCI). The framework is empirically validated on two representative BioFMs, RNA-FM for RNA sequences and ESM-C for protein tasks. Results show that the method consistently outperforms random selection baselines even under extreme pruning rates exceeding 99%. Additionally, the pruned subsets outperform randomly chosen subsets that are an order of magnitude larger, illustrating significant redundancy in biological sequence datasets. The findings highlight that influence-guided data pruning can substantially reduce the computational resources needed for BioFM pretraining. This paves the way for more efficient, sustainable, and accessible AI research in biology by lowering hardware demands and enabling broader participation, especially from academic labs. <div>
arXiv:2512.12932v1 Announce Type: cross 
Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Complementarity of Shared Electric Mobility and Renewable Energy Communities</title>
<link>https://arxiv.org/abs/2512.13099</link>
<guid>https://arxiv.org/abs/2512.13099</guid>
<content:encoded><![CDATA[
<div> Keywords: shared mobility, renewable energy community, electric vehicle fleet planning, vehicle-to-grid, cost reduction

<br /><br />Summary:  
1. The paper addresses the integration of shared mobility service providers with renewable energy communities to support the energy transition from combustion-based to electric mobility.  
2. It proposes an original Mixed-Integer Second Order Cone Programming model for long-term electric vehicle (EV) fleet planning, incorporating coordination with a renewable energy community and vehicle-to-grid (V2G) capabilities.  
3. The model evaluates economic, energy, and grid performance within a 21-bus low-voltage distribution network, aiming to optimize usage of distributed renewable energy and EV flexibility.  
4. Results demonstrate that coordinated operation between shared mobility providers and renewable energy communities can reduce annual costs by up to 11.3% compared to operating separately.  
5. Additionally, the collaboration significantly alleviates grid stress, reducing the load on the substation transformer by 46% by leveraging EV flexibility during peak penalty periods imposed by the grid operator. <div>
arXiv:2512.13099v1 Announce Type: cross 
Abstract: Driven by the ongoing energy transition, shared mobility service providers are emerging actors in electrical power systems which aim to shift combustion-based mobility to electric paradigm. In the meantime, Energy Communities are deployed to enhance the local usage of distributed renewable production. As both ators share the same goal of satisfying the demand at the lowest cost, they could take advantage of their complementarity and coordinate their decisions to enhance each other operation. This paper presents an original Mixed-Integer Second Order Cone Programming long-term Electric Vehicle fleet planning optimization problem that integrates the coordination with a Renewable Energy Community and Vehicle-to-Grid capability. This model is used to assess the economic, energy, and grid performances of their collaboration in a 21 buses low-voltage distribution network. Key results show that, both actors coordination can help reducing the yearly cost up to 11.3 % compared to their stand-alone situation and that it may reduce the stress on the substation transformer by 46 % through the activation of the inherent EVs flexibility when subject to peak penalties from the grid operator.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finch: Benchmarking Finance &amp; Accounting across Spreadsheet-Centric Enterprise Workflows</title>
<link>https://arxiv.org/abs/2512.13168</link>
<guid>https://arxiv.org/abs/2512.13168</guid>
<content:encoded><![CDATA[
<div> Keywords: Finch benchmark, enterprise workflows, AI evaluation, finance & accounting, LLM-assisted annotation<br /><br />Summary:  
This paper introduces Finch, a comprehensive finance and accounting benchmark designed to evaluate AI agents' performance on complex, real-world enterprise-grade professional workflows. Finch integrates diverse tasks such as data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. The dataset is sourced from authentic enterprise workspaces, notably Enron’s extensive repository comprising 15,000 spreadsheets and 500,000 emails from 150 employees, as well as data from other financial institutions. Finch preserves the natural, messy characteristics of multimodal artifacts, including text, tables, formulas, charts, code, and images, across varied financial domains like budgeting, trading, and asset management. The workflow construction uniquely combines large language model (LLM)-assisted discovery with rigorous expert verification and annotation, involving over 700 hours of domain-expert effort. This process generated 172 composite workflows comprising 384 distinct tasks, leveraging 1,710 spreadsheets with 27 million cells and associated documents. The benchmark was used to evaluate state-of-the-art AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, revealing significant challenges: GPT 5.1, despite 48 hours of testing, only passed 38.4% of workflows, while Claude Sonnet 4.5 passed 25.0%. The study highlights the intrinsic difficulty of replicating real-world, long-horizon, knowledge-intensive enterprise workflows with current AI technology. <div>
arXiv:2512.13168v1 Announce Type: cross 
Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preconditioning Techniques for Hybridizable Discontinuous Galerkin Discretizations on GPU Architectures</title>
<link>https://arxiv.org/abs/2512.13619</link>
<guid>https://arxiv.org/abs/2512.13619</guid>
<content:encoded><![CDATA[
<div> Hybridizable Discontinuous Galerkin, GPU, iterative solvers, preconditioning, partial differential equations

<br /><br />Summary: This work presents scalable iterative solvers and preconditioning techniques tailored for Hybridizable Discontinuous Galerkin (HDG) discretizations of partial differential equations (PDEs) optimized for graphics processing units (GPUs). The HDG implementation leverages parallel elimination of local element degrees of freedom and constructs the globally condensed system directly on the GPU using dense-block matrix operations. By storing the global matrix in a block format aligned with the HDG structure, the approach utilizes strided batched dense matrix-vector multiplications, avoiding sparse data formats and enhancing arithmetic intensity and memory throughput across varied mesh types and polynomial orders. The nonlinear solution strategy combines Newton’s method with preconditioned GMRES, integrating scalable preconditioners such as block-Jacobi, additive Schwarz domain decomposition, and polynomial smoothers. These preconditioners are executed in batched form with hardware-conscious optimizations, including dense linear algebra kernels, memory-coalesced vector operations, and shared-memory acceleration to optimize memory usage and parallel processing efficiency. Extensive numerical experiments cover multiple PDEs — including Poisson, Burgers, linear and nonlinear elasticity, Euler, Navier-Stokes, and Reynolds-Averaged Navier-Stokes equations — on both structured and unstructured meshes with diverse element types and polynomial degrees on NVIDIA and AMD GPUs, demonstrating broad applicability and performance benefits. <div>
arXiv:2512.13619v1 Announce Type: cross 
Abstract: We present scalable iterative solvers and preconditioning strategies for Hybridizable Discontinuous Galerkin (HDG) discretizations of partial differential equations (PDEs) on graphics processing units (GPUs). The HDG method is implemented using GPU-tailored algorithms in which local element degrees of freedom are eliminated in parallel, and the globally condensed system is assembled directly on the device using dense-block operations. The global matrix is stored in a block format that reflects the natural HDG structure, enabling all iterative solver kernels to be executed with strided batched dense matrix-vector multiplications. This implementation avoids sparse data structures, increases arithmetic intensity, and sustains high memory throughput across a range of meshes and polynomial orders. The nonlinear solver combines Newton's method with preconditioned GMRES, integrating scalable preconditioners such as block-Jacobi, additive Schwarz domain decomposition, and polynomial smoothers. All preconditioners are implemented in batched form with architecture-aware optimizations--including dense linear algebra kernels, memory-coalesced vector operations, and shared-memory acceleration--to minimize memory traffic and maximize parallel occupancy. Comprehensive studies are conducted for a variety of PDEs (including Poisson equation, Burgers equation, linear and nonlinear elasticity, Euler equations, Navier-Stokes equations, and Reynolds-Averaged Navier-Stokes equations) using structured and unstructured meshes with different element types and polynomial orders on both NVIDIA and AMD GPU architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Mobility Reimagined: Digital Twin Intelligence for Adaptive Campus Course Timetabling</title>
<link>https://arxiv.org/abs/2503.06109</link>
<guid>https://arxiv.org/abs/2503.06109</guid>
<content:encoded><![CDATA[
<div> Keywords: course timetabling, mobility-aware, recommendation system, digital twin, campus planning<br /><br />Summary:<br /><br />1. The paper highlights that course timetables impact daily campus operations by influencing how people move through space and time, acting as mobility policies that affect congestion, travel burden, and transition reliability.<br />2. Timetable design is complex, requiring the balance of hard constraints (capacity, conflicts, feasibility) and soft constraints (preferences, satisfaction, coordination), while also adapting dynamically to disruptions and changing demands.<br />3. Traditional static optimization approaches lack the flexibility to capture human mobility impacts or respond effectively to real-time campus condition changes.<br />4. The study proposes reconceptualizing course timetabling as a recommendation task, utilizing the Texas A&amp;M Campus Digital Twin platform to evaluate mobility outcomes dynamically.<br />5. An iterative framework combining collaborative and content-based filtering with feedback-driven refinement is introduced to generate ranked, adaptive timetable recommendations.<br />6. A mobility-aware composite scoring function, incorporating classroom occupancy, travel distance, travel time, and vertical transitions, balances efficient resource use with minimizing human movement costs.<br />7. Experiments based on real Texas A&amp;M data demonstrate reductions in mobility friction and travel inefficiencies, improvements in classroom utilization, and higher user satisfaction.<br />8. By integrating recommendation-based strategies with digital twin intelligence, the study offers a scalable blueprint for mobility-focused campus planning and resource allocation, with applicability to broader urban systems. <div>
arXiv:2503.06109v2 Announce Type: replace 
Abstract: Daily operations in large campuses depend on how efficiently people \emph{move} through space and time. In this sense, course timetables are more than administrative schedules: they act as mobility policies that orchestrate thousands of trajectories, shaping travel burden, congestion, accessibility, and the reliability of back-to-back transitions. Designing timetables that are both feasible and mobility-friendly is challenging because hard constraints including capacity, conflicts, feasibility must be satisfied alongside soft constraints including preferences, satisfaction, coordination, all under dynamic conditions such as real-time disruptions and evolving demand. Traditional static optimization methods often struggle to capture these human mobility impacts and to adapt when campus conditions change. This paper reconceptualizes course timetabling as a recommendation-based task and leverages the Texas A\&amp;M Campus Digital Twin as a dynamic data platform to evaluate mobility consequences at scale. We propose an iterative framework that integrates collaborative and content-based filtering with feedback-driven refinement to generate ranked sets of adaptive timetable recommendations. A mobility-aware composite scoring function combining classroom occupancy, travel distance, travel time, and vertical transitions systematically balances resource efficiency with human-centered movement costs. Extensive experiments using real-world data from Texas A\&amp;M University show that the proposed approach reduces mobility friction and travel inefficiencies, improves classroom utilization, and enhances overall user satisfaction. By coupling recommendation-oriented decision-making with digital twin intelligence, this study provides a robust and scalable blueprint for mobility-centered campus planning and resource allocation, with potential extensions to broader urban systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrated Prediction and Multi-period Portfolio Optimization</title>
<link>https://arxiv.org/abs/2512.11273</link>
<guid>https://arxiv.org/abs/2512.11273</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-period portfolio optimization, end-to-end learning, differentiable convex optimization, mirror-descent fixed-point, transaction costs<br /><br />Summary:<br /><br />This paper addresses the challenge of multi-period portfolio optimization by integrating machine learning prediction and portfolio decision-making into a unified framework, overcoming limitations of classical two-stage approaches. Traditional methods separate forecasting returns and portfolio optimization, which leads to misalignment between predictions and actual portfolio performance, and typically overlook transaction costs. The authors propose IPMO (Integrated Prediction and Multi-period Portfolio Optimization), which jointly learns multi-period return forecasts and optimizes mean-variance portfolios with turnover penalties through a differentiable convex optimization layer. To scale effectively, the paper introduces a mirror-descent fixed-point (MDFP) differentiation scheme that bypasses the costly factorization of Karush-Kuhn-Tucker systems, providing stable implicit gradients and maintaining near scale-invariant runtime as the investment horizon grows. Experimental results using real market data and two time-series prediction models demonstrate that IPMO outperforms traditional two-stage benchmarks by achieving better risk-adjusted returns net of transaction costs and more consistent asset allocation trajectories. Overall, the study shows that an integrated end-to-end framework enhances portfolio performance in a multi-period context while remaining computationally efficient. <div>
arXiv:2512.11273v1 Announce Type: new 
Abstract: Multi-period portfolio optimization is important for real portfolio management, as it accounts for transaction costs, path-dependent risks, and the intertemporal structure of trading decisions that single-period models cannot capture. Classical methods usually follow a two-stage framework: machine learning algorithms are employed to produce forecasts that closely fit the realized returns, and the predicted values are then used in a downstream portfolio optimization problem to determine the asset weights. This separation leads to a fundamental misalignment between predictions and decision outcomes, while also ignoring the impact of transaction costs. To bridge this gap, recent studies have proposed the idea of end-to-end learning, integrating the two stages into a single pipeline. This paper introduces IPMO (Integrated Prediction and Multi-period Portfolio Optimization), a model for multi-period mean-variance portfolio optimization with turnover penalties. The predictor generates multi-period return forecasts that parameterize a differentiable convex optimization layer, which in turn drives learning via portfolio performance. For scalability, we introduce a mirror-descent fixed-point (MDFP) differentiation scheme that avoids factorizing the Karush-Kuhn-Tucker (KKT) systems, which thus yields stable implicit gradients and nearly scale-insensitive runtime as the decision horizon grows. In experiments with real market data and two representative time-series prediction models, the IPMO method consistently outperforms the two-stage benchmarks in risk-adjusted performance net of transaction costs and achieves more coherent allocation paths. Our results show that integrating machine learning prediction with optimization in the multi-period setting improves financial outcomes and remains computationally tractable.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models</title>
<link>https://arxiv.org/abs/2512.11412</link>
<guid>https://arxiv.org/abs/2512.11412</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular toxicity prediction, multi-task learning, chemical language model, sparse attention, interpretability<br /><br />Summary:<br />Reliable molecular toxicity prediction using computational models is essential for efficient drug discovery, providing an alternative to costly and time-consuming experimental methods. However, existing models often operate as black boxes, limiting trust and acceptance in safety-critical applications. To overcome this, the article introduces a novel multi-task learning (MTL) framework that enhances both predictive accuracy and interpretability. The proposed architecture couples a shared chemical language model backbone with task-specific attention modules. By applying an L1 sparsity penalty on the attention modules, the model is encouraged to focus selectively on a minimal set of relevant molecular fragments for each toxicity endpoint. This approach enables end-to-end training and compatibility with various transformer-based architectures. The framework was evaluated on established benchmark datasets including ClinTox, SIDER, and Tox21, demonstrating consistent improvements over traditional single-task and standard MTL baselines. Importantly, the sparse attention weights provide meaningful chemical visualizations that identify key molecular substructures driving the predictions, thereby improving human interpretability and trust in the model’s decisions. This combination of accuracy and explainability presents a significant advancement for in silico toxicity prediction in drug discovery workflows. <div>
arXiv:2512.11412v1 Announce Type: new 
Abstract: Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contiguous Storage of Grid Data for Heterogeneous Computing</title>
<link>https://arxiv.org/abs/2512.11473</link>
<guid>https://arxiv.org/abs/2512.11473</guid>
<content:encoded><![CDATA[
<div> Keywords: Structured Cartesian grids, GPU optimization, SYCL, heterogeneous computing, sparse domains<br /><br />Summary:<br /><br />This work addresses the challenges of using structured Cartesian grids in sparse numerical simulation domains, where naive implementations cause memory inefficiency and computational overhead. Traditional frameworks are mostly optimized for CPU usage, limiting performance on GPUs due to restricted parallelism and high memory latency. To overcome these limitations, the authors propose a redesigned data storage architecture specifically tailored for GPU compatibility and efficient operation across heterogeneous platforms. The new design abstracts away GPU-specific low-level complexities, enabling easier programming and better scalability. It uses a unified programming model based on SYCL, facilitating seamless integration between host (CPU) and device (GPU) environments. This abstraction simplifies the developer experience by reducing the need for specialized GPU coding knowledge. Furthermore, the architecture enhances portability, allowing the same codebase to run efficiently on various hardware configurations. The proposed solution is particularly effective for sparse-grid computations and grid-particle coupling scenarios common in numerical simulations, providing both improved performance and reduced memory consumption compared to traditional approaches. Overall, this study contributes a flexible, scalable, and portable framework that can accelerate high-performance simulations on emerging heterogeneous computing platforms. <div>
arXiv:2512.11473v1 Announce Type: new 
Abstract: Structured Cartesian grids are a fundamental component in numerical simulations. Although these grids facilitate straightforward discretization schemes, their na\"{i}ve use in sparse domains leads to excessive memory overhead and inefficient computation. Existing frameworks address are primarily optimized for CPU execution and exhibit performance bottlenecks on GPU architectures due to limited parallelism and high memory access latency. This work presents a redesigned storage architecture optimized for GPU compatibility and efficient execution across heterogeneous platforms. By abstracting low-level GPU-specific details and adopting a unified programming model based on SYCL, the proposed data structure enables seamless integration across host and device environments. This architecture simplifies GPU programming for end-users while improving scalability and portability in sparse-grid and gird-particle coupling numerical simulations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation</title>
<link>https://arxiv.org/abs/2512.11748</link>
<guid>https://arxiv.org/abs/2512.11748</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Parametric Design, Rank Reduction Autoencoders, sparse Proper Generalized Decomposition, design optimization, multiparametric solutions  

<br /><br />Summary:  
This paper introduces a novel framework named Generative Parametric Design (GPD) within simulation-based engineering sciences. The GPD framework enables the simultaneous generation of new designs and their parametric solutions expressed as a reduced basis. The approach utilizes two Rank Reduction Autoencoders (RRAEs): one to encode and generate design geometries, and another to encode sparse Proper Generalized Decomposition (sPGD) mode solutions. These autoencoders are connected in the latent space through regression methods, facilitating seamless mapping between designs and their corresponding sPGD solutions. This linkage supports efficient design exploration and optimization processes. Additionally, the framework contributes to the advancement of digital and hybrid twin technologies by improving predictive modeling and enabling real-time decision-making in engineering contexts. The method is demonstrated on two-phase microstructures, focusing on multiparametric solutions that capture variations in two critical material parameters. The results highlight the framework's potential to enhance engineering design workflows by reducing computational costs and increasing flexibility in handling complex parametric variations. <div>
arXiv:2512.11748v1 Announce Type: new 
Abstract: This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets</title>
<link>https://arxiv.org/abs/2512.10971</link>
<guid>https://arxiv.org/abs/2512.10971</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial decision-making, live evaluation, autonomous agents, risk management<br /><br />Summary:  
This paper addresses the challenge of evaluating Large Language Models (LLMs) as autonomous agents in real-time financial environments. It introduces AI-Trader, the first fully automated, live, and data-uncontaminated benchmark designed specifically for LLMs operating in financial decision-making contexts. AI-Trader covers three major financial markets: U.S. stocks, A-shares, and cryptocurrencies, and includes various trading granularities to realistically simulate live trading scenarios. A key innovation is the minimal information paradigm, where agents receive only essential context and must independently search, verify, and synthesize live market data without human input. The study evaluates six mainstream LLMs across these markets and multiple trading frequencies. Results show that general intelligence alone does not ensure effective trading performance; most agents struggle with poor returns and weak risk management. The analysis highlights that robust risk control is critical for consistent performance across diverse markets. Additionally, the research finds that autonomous AI trading strategies tend to achieve excess returns more easily in highly liquid markets compared to policy-driven ones. Overall, the study exposes significant limitations of current autonomous agents in finance and points to the importance of improved risk management and live adaptability. The open-source code and evaluation data aim to catalyze further community research in this area. <div>
arXiv:2512.10971v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable potential as autonomous agents, approaching human-expert performance through advanced reasoning and tool orchestration. However, decision-making in fully dynamic and live environments remains highly challenging, requiring real-time information integration and adaptive responses. While existing efforts have explored live evaluation mechanisms in structured tasks, a critical gap remains in systematic benchmarking for real-world applications, particularly in finance where stringent requirements exist for live strategic responsiveness. To address this gap, we introduce AI-Trader, the first fully-automated, live, and data-uncontaminated evaluation benchmark for LLM agents in financial decision-making. AI-Trader spans three major financial markets: U.S. stocks, A-shares, and cryptocurrencies, with multiple trading granularities to simulate live financial environments. Our benchmark implements a revolutionary fully autonomous minimal information paradigm where agents receive only essential context and must independently search, verify, and synthesize live market information without human intervention. We evaluate six mainstream LLMs across three markets and multiple trading frequencies. Our analysis reveals striking findings: general intelligence does not automatically translate to effective trading capability, with most agents exhibiting poor returns and weak risk management. We demonstrate that risk control capability determines cross-market robustness, and that AI trading strategies achieve excess returns more readily in highly liquid markets than policy-driven environments. These findings expose critical limitations in current autonomous agents and provide clear directions for future improvements. The code and evaluation data are open-sourced to foster community research: https://github.com/HKUDS/AI-Trader.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigation of multi-path propagation artefacts in acoustic targets with cepstral adaptive filtering</title>
<link>https://arxiv.org/abs/2512.11165</link>
<guid>https://arxiv.org/abs/2512.11165</guid>
<content:encoded><![CDATA[
<div> Keywords: passive acoustic sensing, multi-path reflections, adaptive band-stop filter, signal separation, target classification  

<br /><br />Summary:  
1. The paper addresses the challenge of separating target signals from their reflections in passive acoustic sensing, which is important for monitoring moving targets like vessels and aircraft.  
2. It identifies limitations in current filtering techniques that fail to consider environmental characteristics and variability in medium properties, reducing their effectiveness.  
3. The proposed method applies temporal filtering to cepstral coefficients using an adaptive band-stop filter that dynamically adjusts bandwidth based on the intensity of quefrency components.  
4. This approach improves key performance metrics, including signal-to-noise ratio (SNR), log-spectral distance (LSD), and Itakura-Saito (IS) distance, across simulated aircraft noise velocities from 10 to 100 meters per second.  
5. In underwater acoustic tasks, the method enhances ship-type classification results by increasing Matthews Correlation Coefficient scores by 2.28 and 2.62 percentage points for the DeepShip and VTUAD v2 datasets, respectively.  
6. The findings demonstrate the potential to improve acoustic target classification and time-delay estimation in complex multi-path environments.  
7. Future work will focus on preserving amplitude information and expanding the method for use with multi-sensor acoustic arrays. <div>
arXiv:2512.11165v1 Announce Type: cross 
Abstract: Passive acoustic sensing is a cost-effective solution for monitoring moving targets such as vessels and aircraft, but its performance is hindered by complex propagation effects like multi-path reflections and motion-induced artefacts. Existing filtering techniques do not properly incorporate the characteristics of the environment or account for variability in medium properties, limiting their effectiveness in separating source and reflection components. This paper proposes a method for separating target signals from their reflections in a spectrogram. Temporal filtering is applied to cepstral coefficients using an adaptive band-stop filter, which dynamically adjusts its bandwidth based on the relative intensity of the quefrency components. The method improved the signal-to-noise ratio (SNR), log-spectral distance (LSD), and Itakura-Saito (IS) distance across velocities ranging from 10 to 100 metres per second in aircraft noise with simulated motion. It also enhanced the performance of ship-type classification in underwater tasks by 2.28 and 2.62 Matthews Correlation Coefficient percentage points for the DeepShip and VTUAD v2 datasets, respectively. These results demonstrate the potential of the proposed pipeline to improve acoustic target classification and time-delay estimation in multi-path environments, with future work aimed at amplitude preservation and multi-sensor applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Graph-Transformer Operator for Learning Physical Dynamics with Manifolds Embedding</title>
<link>https://arxiv.org/abs/2512.10227</link>
<guid>https://arxiv.org/abs/2512.10227</guid>
<content:encoded><![CDATA[
<div> Keywords: physical simulations, graph-transformer, unstructured meshes, message-passing, computational efficiency  

<br /><br />Summary:  
The paper introduces PhysGTO, a novel Graph-Transformer Operator designed for accurate and efficient physical simulations. Traditional numerical solvers often face high computational costs, especially when dealing with complex geometries and varying simulation conditions. Existing deep learning methods struggle with flexibility and generalization on unstructured meshes, limiting their usability. PhysGTO addresses these challenges by embedding physical and latent spaces through a Unified Graph Embedding module that preserves structural information and aligns heterogeneous node-level data. In the latent space, it uses a flux-oriented message-passing scheme combined with projection-inspired attention to model both local and global dependencies effectively, enabling multilevel interactions for complex physical phenomena. This architecture maintains linear complexity in relation to mesh size, reducing trainable parameters and FLOPs for real-time inference. The authors validate PhysGTO on a comprehensive benchmark consisting of eleven datasets featuring unstructured meshes, dynamic flow scenarios, and large-scale 3D geometries. Results demonstrate that PhysGTO consistently achieves state-of-the-art accuracy and significantly lowers computational costs. Thus, the method offers enhanced flexibility, scalability, and generalization, making it well-suited for a broad range of simulation tasks in science and engineering. <div>
arXiv:2512.10227v1 Announce Type: new 
Abstract: Accurate and efficient physical simulations are essential in science and engineering, yet traditional numerical solvers face significant challenges in computational cost when handling simulations across dynamic scenarios involving complex geometries, varying boundary/initial conditions, and diverse physical parameters. While deep learning offers promising alternatives, existing methods often struggle with flexibility and generalization, particularly on unstructured meshes, which significantly limits their practical applicability. To address these challenges, we propose PhysGTO, an efficient Graph-Transformer Operator for learning physical dynamics through explicit manifold embeddings in both physical and latent spaces. In the physical space, the proposed Unified Graph Embedding module aligns node-level conditions and constructs sparse yet structure-preserving graph connectivity to process heterogeneous inputs. In the latent space, PhysGTO integrates a lightweight flux-oriented message-passing scheme with projection-inspired attention to capture local and global dependencies, facilitating multilevel interactions among complex physical correlations. This design ensures linear complexity relative to the number of mesh points, reducing both the number of trainable parameters and computational costs in terms of floating-point operations (FLOPs), and thereby allowing efficient inference in real-time applications. We introduce a comprehensive benchmark spanning eleven datasets, covering problems with unstructured meshes, transient flow dynamics, and large-scale 3D geometries. PhysGTO consistently achieves state-of-the-art accuracy while significantly reducing computational costs, demonstrating superior flexibility, scalability, and generalization in a wide range of simulation tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrated Planning and Machine-Level Scheduling for High-Mix Discrete Manufacturing: A Profit-Driven Heuristic Framework</title>
<link>https://arxiv.org/abs/2512.10358</link>
<guid>https://arxiv.org/abs/2512.10358</guid>
<content:encoded><![CDATA[
<div> Keywords: manufacturing scheduling, production planning, mixed-integer optimization, machine-level execution, on-time completion  

<br /><br />Summary:  
Modern manufacturing enterprises face challenges in creating efficient, reliable production schedules, especially under multi-variety, small-batch, and rush-order conditions common in high-mix discrete manufacturing systems. This paper addresses the need for jointly optimizing mid-term production planning and machine-level scheduling in environments with heterogeneous resources and strict delivery deadlines. The authors propose a profit-driven integrated framework that combines a mixed-integer planning model with a heuristic for machine-level scheduling. The planning layer manages production allocation, accessory co-production, and outsourcing decisions based on aggregate economic and capacity constraints. Meanwhile, the scheduling layer applies a structure-aware procedure that ensures execution feasibility and stabilizes daily machine operations. This hierarchical framework balances aggregated optimization tractability with detailed operational constraints. Evaluations on a real industrial scenario reveal that a flexible machine-level execution scheme achieves 73.3% on-time completion but incurs significant outsourcing due to bottlenecks. Conversely, a stability-oriented execution policy attains 100% on-time completion, eliminates outsourcing, and maintains balanced machine utilization, with minimal capacity loss from changeovers (1.9 to 4.6%). These findings demonstrate that aligning planning with stability-focused execution enables practical, interpretable, and profit-maximizing decisions in complex manufacturing settings. <div>
arXiv:2512.10358v1 Announce Type: new 
Abstract: Modern manufacturing enterprises struggle to create efficient and reliable production schedules under multi-variety, small-batch, and rush-order conditions. High-mix discrete manufacturing systems require jointly optimizing mid-term production planning and machine-level scheduling under heterogeneous resources and stringent delivery commitments. We address this problem with a profit-driven integrated framework that couples a mixed-integer planning model with a machine-level scheduling heuristic. The planning layer allocates production, accessory co-production, and outsourcing under aggregate economic and capacity constraints, while the scheduling layer refines these allocations using a structure-aware procedure that enforces execution feasibility and stabilizes daily machine behavior. This hierarchical design preserves the tractability of aggregated optimization while capturing detailed operational restrictions. Evaluations are conducted on a real industrial scenario. A flexible machine-level execution scheme yields 73.3% on-time completion and significant outsourcing demand, revealing bottleneck congestion. In contrast, a stability-enforcing execution policy achieves 100% on-time completion, eliminates all outsourcing, and maintains balanced machine utilization with only 1.9 to 4.6% capacity loss from changeovers. These results show that aligning planning decisions with stability-oriented execution rules enables practical and interpretable profit-maximizing decisions in complex manufacturing environments.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Crop Planning under Uncertainty: Aligning Economic Optimality with Agronomic Sustainability</title>
<link>https://arxiv.org/abs/2512.10396</link>
<guid>https://arxiv.org/abs/2512.10396</guid>
<content:encoded><![CDATA[
<div> Keywords: crop planning, robust optimization, spatial heterogeneity, legume-cereal complementarity, distributionally robust optimization<br /><br />Summary:<br /><br />1. The paper addresses long-horizon agricultural planning challenges involving optimizing crop allocation amidst spatial heterogeneity, temporal agronomic dependencies, and environmental uncertainty.  
2. Existing methods often overlook dynamic crop interactions like legume-cereal complementarity or rely on static deterministic models that lack resilience to market and climate volatility.  
3. To overcome these issues, the authors propose a Multi-Layer Robust Crop Planning Framework (MLRCPF) that incorporates spatial reasoning, temporal dynamics, and robust optimization principles.  
4. The framework models crop-to-crop relationships using a structured interaction matrix within state-transition logic and applies distributionally robust optimization based on a data-driven ambiguity set to handle worst-case risks.  
5. Evaluations on a real-world dataset from North China demonstrate that MLRCPF autonomously produces sustainable checkerboard rotation patterns that restore soil fertility and significantly increase legume planting ratios compared to deterministic baselines.  
6. Economically, the framework balances the trade-off between optimality and stability, enhancing resilience against environmental and market uncertainties.  
7. The study underscores the importance of embedding domain-specific structural knowledge into optimization models for effective and resilient decision-making in complex agricultural systems. <div>
arXiv:2512.10396v1 Announce Type: new 
Abstract: Long-horizon agricultural planning requires optimizing crop allocation under complex spatial heterogeneity, temporal agronomic dependencies, and multi-source environmental uncertainty. Existing approaches often treat crop interactions, such as legume-cereal complementarity, which implicitly or rely on static deterministic formulations that fail to guarantee resilience against market and climate volatility. To address these challenges, we propose a Multi-Layer Robust Crop Planning Framework (MLRCPF) that integrates spatial reasoning, temporal dynamics, and robust optimization. Specifically, we formalize crop-to-crop relationships through a structured interaction matrix embedded within the state-transition logic, and employ a distributionally robust optimization layer to mitigate worst-case risks defined by a data-driven ambiguity set. Evaluations on a real-world high-mix farming dataset from North China demonstrate the effectiveness of the proposed approach. The framework autonomously generates sustainable checkerboard rotation patterns that restore soil fertility, significantly increasing the legume planting ratio compared to deterministic baselines. Economically, it successfully resolves the trade-off between optimality and stability. These results highlight the importance of explicitly encoding domain-specific structural priors into optimization models for resilient decision-making in complex agricultural systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HypeR Adaptivity: Joint $hr$-Adaptive Meshing via Hypergraph Multi-Agent Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10439</link>
<guid>https://arxiv.org/abs/2512.10439</guid>
<content:encoded><![CDATA[
<div> Adaptive mesh refinement, finite element method, deep reinforcement learning, hypergraph neural networks, multi-agent system<br /><br />Summary:<br /><br />This paper introduces HypeR, a novel deep reinforcement learning framework designed to enhance adaptive mesh refinement in finite element methods for solving partial differential equations. Unlike classical $h$- or $r$-adaptivity methods that individually optimize either mesh topology or vertex positions and face computational or accuracy limitations, HypeR jointly optimizes both mesh relocation and refinement. The approach utilizes hypergraph neural networks and models refinement as a heterogeneous multi-agent Markov decision process, where element agents perform discrete refinement actions and vertex agents employ an anisotropic diffusion-based relocation policy that prevents mesh tangling. The reward function integrates both local and global error reduction, encouraging overall solution accuracy. Experiments on benchmark PDE problems demonstrate that HypeR achieves 6 to 10 times reduction in approximation error compared to state-of-the-art $h$-adaptive baselines while maintaining similar element counts. Additionally, the meshes generated exhibit improved shape quality and better alignment with solution anisotropy, overcoming the accuracy limitations inherent to uniform or subdivision-only refinement. The results suggest that jointly learned $hr$-adaptivity strategies mediated by advanced machine learning models can significantly improve automated mesh generation and solution efficiency in computational PDE workflows. <div>
arXiv:2512.10439v1 Announce Type: new 
Abstract: Adaptive mesh refinement is central to the efficient solution of partial differential equations (PDEs) via the finite element method (FEM). Classical $r$-adaptivity optimizes vertex positions but requires solving expensive auxiliary PDEs such as the Monge-Amp\`ere equation, while classical $h$-adaptivity modifies topology through element subdivision but suffers from expensive error indicator computation and is constrained by isotropic refinement patterns that impose accuracy ceilings. Combined $hr$-adaptive techniques naturally outperform single-modality approaches, yet inherit both computational bottlenecks and the restricted cost-accuracy trade-off. Emerging machine learning methods for adaptive mesh refinement seek to overcome these limitations, but existing approaches address $h$-adaptivity or $r$-adaptivity in isolation. We present HypeR, a deep reinforcement learning framework that jointly optimizes mesh relocation and refinement. HypeR casts the joint adaptation problem using tools from hypergraph neural networks and multi-agent reinforcement learning. Refinement is formulated as a heterogeneous multi-agent Markov decision process (MDP) where element agents decide discrete refinement actions, while relocation follows an anisotropic diffusion-based policy on vertex agents with provable prevention of mesh tangling. The reward function combines local and global error reduction to promote general accuracy. Across benchmark PDEs, HypeR reduces approximation error by up to 6--10$\times$ versus state-of-art $h$-adaptive baselines at comparable element counts, breaking through the uniform refinement accuracy ceiling that constrains subdivision-only methods. The framework produces meshes with improved shape metrics and alignment to solution anisotropy, demonstrating that jointly learned $hr$-adaptivity strategies can substantially enhance the capabilities of automated mesh generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2512.10034</link>
<guid>https://arxiv.org/abs/2512.10034</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular dynamics, protein-ligand interactions, automation, multi-agent framework, free energy calculations<br /><br />Summary:<br /><br />1. Force field-based molecular dynamics (MD) simulations are critical tools for investigating the structure, dynamics, and function of biomolecular systems such as proteins and protein-ligand complexes, with important applications in drug discovery and protein engineering.<br /><br />2. Despite their usefulness, setting up MD simulations is technically challenging, involving complex parameterization, input preparation, and software configuration, which limits their widespread adoption.<br /><br />3. The newly introduced DynaMate framework leverages agentic large language models (LLMs) to fully automate the design and execution of MD workflows for protein and protein-ligand systems, including free energy binding affinity calculations via the MM/PB(GB)SA method.<br /><br />4. DynaMate consists of three interacting modules that plan experiments, perform simulations, and analyze results, integrating dynamic tool use, web search capabilities, scientific paper question answering (PaperQA), and self-correcting iterative reasoning to handle runtime errors.<br /><br />5. Benchmark tests on twelve systems across varying complexity demonstrated that DynaMate reliably conducts full MD simulations, effectively manages errors, and produces meaningful analyses of protein-ligand interactions, promising a standardized, scalable, and time-efficient molecular modeling pipeline for future biomolecular research and drug design. <div>
arXiv:2512.10034v1 Announce Type: cross 
Abstract: Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULE - A Co-Generation Fission Power Plant Concept to Support Lunar In-Situ Resource Utilisation</title>
<link>https://arxiv.org/abs/2512.10705</link>
<guid>https://arxiv.org/abs/2512.10705</guid>
<content:encoded><![CDATA[
<div> Keywords: molten salt electrolysis, fission reactor, lunar base, microreactor, in-situ resource utilisation<br /><br />Summary:<br /><br />1. The article addresses the challenge of establishing a sustainable human presence on the Moon by focusing on robust in-situ resource utilisation (ISRU) supply chains, specifically for producing consumables and propellant. <br /><br />2. Molten salt electrolysis (MSE) is highlighted as a promising process for ISRU, which requires high temperatures exceeding 900°C to operate efficiently.<br /><br />3. Fission reactors are proposed as an ideal power source for the lunar environment since they can operate independently of solar irradiance, providing continuous power even during the 14-day lunar night.<br /><br />4. Beyond electric power generation, the study explores using the reactor coolant as a direct heat source for the MSE plant, enabling simultaneous production of heat and surplus electricity for the lunar base.<br /><br />5. The research employed the neutron transport code Serpent 2 to design and simulate a ceramic core, gas-cooled, very-high-temperature microreactor capable of delivering 100 kW thermal power, with a projected operational lifetime of at least ten years.<br /><br />6. The resulting neutron and power distribution models provide a foundation for further thermal-hydraulic investigations to assess the overall technical feasibility of the reactor and integrated power plant design for lunar applications. <div>
arXiv:2512.10705v1 Announce Type: cross 
Abstract: For a sustained human presence on the Moon, robust in-situ resource utilisation supply chains to provide consumables and propellant are necessary. A promising process is molten salt electrolysis, which typically requires temperatures in excess of 900{\deg}C. Fission reactors do not depend on solar irradiance and are thus well suited for power generation on the Moon, especially during the 14-day lunar night. As of now, fission reactors have only been considered for electric power generation, but the reactor coolant could also be used directly to heat those processes to their required temperatures. In this work, a concept for a co-generation fission power plant on the Moon that can directly heat a MSE plant to the required temperatures and provide a surplus of electrical energy for the lunar base is presented. The neutron transport code Serpent 2 is used to model a ceramic core, gas-cooled very-high-temperature microreactor design and estimate its lifetime with a burnup simulation in hot conditions with an integrated step-wise criticality search. Calculations show a neutronically feasible operation time of at least 10 years at 100kW thermal power. The obtained power distributions lay a basis for further thermal-hydraulic studies on the technical feasibility of the reactor design and the power plant.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields</title>
<link>https://arxiv.org/abs/2512.10886</link>
<guid>https://arxiv.org/abs/2512.10886</guid>
<content:encoded><![CDATA[
<div> Keywords: Parabolic trough, Concentrating Solar Power, mass-flow ratios, heat-transfer coefficients, infrared thermography<br /><br />Summary:<br /><br />1. This paper addresses the challenge of maintaining uniform outlet temperatures in parabolic trough Concentrating Solar Power (CSP) plants, which operate extensive hydraulic networks of collector loops facing spatially heterogeneous optical performance, heat losses, and pressure drops.<br />2. Traditional monitoring tools fall short because loop-level mass flows and receiver heat-loss parameters are not directly observed, preventing effective diagnosis of hydraulic imbalances or receiver degradation.<br />3. The authors introduce a physics-informed learning framework that infers loop-level mass-flow ratios and time-varying receiver heat-transfer coefficients by leveraging routine operational data.<br />4. The method uniquely exploits nocturnal homogenization periods—times when hot oil circulates through the field without solar irradiation—to isolate hydraulic and thermal-loss effects.<br />5. By embedding a differentiable conjugate heat-transfer model into an end-to-end learning pipeline and optimizing it with historical data from the 50 MW Andasol 3 solar field, the framework achieves accurate reconstruction of loop temperatures with RMSE under 2°C.<br />6. Validation against drone-based infrared thermography (QScan) demonstrates the model's capability to identify regions with high receiver heat losses accurately.<br />7. The study proves that despite noise in real operational data, latent physical parameters relevant to CSP plant performance can be reliably recovered when combined with physics-based modeling and differentiable optimization techniques. <div>
arXiv:2512.10886v1 Announce Type: cross 
Abstract: Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.
  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.
  The model accurately reconstructs loop temperatures (RMSE $<2^\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.14933</link>
<guid>https://arxiv.org/abs/2506.14933</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized finance, DeFi, cryptocurrency, financial crime, automated detection<br /><br />Summary:<br /><br />The decentralized finance (DeFi) sector has experienced rapid growth, driven by the increasing interest of cryptocurrency enthusiasts in exploring new financial markets. This expansion has been accompanied by a rise in financial crime within the cryptocurrency ecosystem, marking a new challenge for regulators and law enforcement agencies. Due to the innovative and complex nature of DeFi technologies, identifying and prosecuting criminal activities has become particularly difficult. Consequently, there is a pressing need to develop and implement automated detection tools that align with regulatory policies. These tools will help monitor, identify, and mitigate criminal behavior in cryptocurrency transactions and platforms effectively. Overall, the paper emphasizes the urgency of integrating policy-driven automated mechanisms to combat the escalating criminal activities in DeFi and cryptocurrency spaces, aiming to foster a safer and more secure financial environment. <div>
arXiv:2506.14933v2 Announce Type: replace 
Abstract: The decentralized finance (DeFi) community has grown rapidly in recent years, pushed forward by cryptocurrency enthusiasts interested in the vast untapped potential of new markets. The surge in popularity of cryptocurrency has ushered in a new era of financial crime. Unfortunately, the novelty of the technology makes the task of catching and prosecuting offenders particularly challenging. Thus, it is necessary to implement automated detection tools related to policies to address the growing criminality in the cryptocurrency realm.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Informed Neural Network Framework for Simulating Creep Buckling in Growing Viscoelastic Biological Tissues</title>
<link>https://arxiv.org/abs/2506.18565</link>
<guid>https://arxiv.org/abs/2506.18565</guid>
<content:encoded><![CDATA[
<div> Viscoelasticity, physics-informed neural networks, creep buckling, morphogenesis, energy-based modeling

<br /><br />Summary: Modeling viscoelastic behavior is essential in fields like engineering and biomechanics due to materials experiencing time-dependent deformations such as stress relaxation, creep buckling, and tissue development. Traditional numerical approaches, including finite element methods, often involve explicit meshing, artificial imperfections, or customized programming, leading to increased computational complexity. This study introduces an energy-based physics-informed neural network (PINN) framework with an incremental approach to effectively model phenomena such as viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. The framework ensures physics consistency by training neural networks to minimize the system's potential energy functional, implicitly satisfying equilibrium and constitutive laws. A significant advantage of this method is its natural ability to capture creep buckling without needing pre-imposed imperfections, exploiting inherent training dynamics to trigger instabilities. The framework is further extended to biological tissue growth and morphogenesis, successfully predicting uniform expansion as well as differential growth-induced buckling in cylindrical structures. Results confirm that the energy-based PINN can accurately predict viscoelastic instabilities, post-buckling deformation, and complex tissue morphological evolution. This approach presents a flexible, robust alternative to traditional methods with promising applications in structural engineering, soft material analysis, and biological tissue development modeling. <div>
arXiv:2506.18565v2 Announce Type: replace 
Abstract: Modeling viscoelastic behavior is crucial in engineering and biomechanics, where materials undergo time-dependent deformations, including stress relaxation, creep buckling and biological tissue development. Traditional numerical methods, like the finite element method, often require explicit meshing, artificial perturbations or embedding customised programs to capture these phenomena, adding computational complexity. In this study, we develop an energy-based physics-informed neural network (PINN) framework using an incremental approach to model viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. Physics consistency is ensured by training neural networks to minimize the systems potential energy functional, implicitly satisfying equilibrium and constitutive laws. We demonstrate that this framework can naturally capture creep buckling without pre-imposed imperfections, leveraging inherent training dynamics to trigger instabilities. Furthermore, we extend our framework to biological tissue growth and morphogenesis, predicting both uniform expansion and differential growth-induced buckling in cylindrical structures. Results show that the energy-based PINN effectively predicts viscoelastic instabilities, post-buckling evolution and tissue morphological evolution, offering a promising alternative to traditional methods. This study demonstrates that PINN can be a flexible robust tool for modeling complex, time-dependent material behavior, opening possible applications in structural engineering, soft materials, and tissue development.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PILLTOP: Multi-Material Topology Optimization of Polypills for Prescribed Drug-Release Kinetics</title>
<link>https://arxiv.org/abs/2512.09154</link>
<guid>https://arxiv.org/abs/2512.09154</guid>
<content:encoded><![CDATA[
<div> Polypills, topology optimization, additive manufacturing, neural networks, drug release kinetics<br /><br />Summary: This paper introduces an automated design framework for polypills, which are single oral dosage forms combining multiple drugs and excipients for fixed-dose combination therapies. The framework addresses current limitations where polypill formulations rely on ad hoc parameter sweeps, restricting systematic exploration of complex shapes, compositions, and release behaviors. The authors use topology optimization guided by supershape parametrization to define geometry and phase distribution within the pill, enhancing the design flexibility. A neural network represents excipient distribution, allowing for intricate control over material placement. The dissolution kinetics are modeled through a coupled system of modified Allen-Cahn and Fick’s diffusion equations to simulate drug release accurately. Implementation is done in JAX, leveraging automatic differentiation to efficiently compute sensitivities for co-optimizing pill shape and constituent distribution simultaneously. Validation is performed through various case studies involving both single-phase and multi-excipient formulations, demonstrating the framework’s capability to match desired dissolution behaviors and prescribed drug release kinetics. The work significantly advances the rational design of polypills, facilitating superior customization and controlled release profiles enabled by advances in additive manufacturing technology. <div>
arXiv:2512.09154v1 Announce Type: new 
Abstract: Polypills are single oral dosage forms that combine multiple active pharmaceutical ingredients and excipients, enabling fixed-dose combination therapies, coordinated multi-phase release, and precise customization of patient-specific treatment protocols. Recent advances in additive manufacturing facilitate the physical realization of multi-material excipients, offering superior customization of target release profiles. However, polypill formulations remain tuned by ad hoc parameter sweeps; this reliance renders current design workflows ill-suited for the systematic exploration of the high-dimensional space of shapes, compositions, and release behaviors.
  We present an automated design framework for polypills that leverages topology optimization to match dissolution behaviors with prescribed drug release kinetics. In particular, we employ a supershape parametrization to define geometry/phase distribution, a neural network representation to specify excipient distribution, and a coupled system of modified Allen-Cahn and Fick's diffusion equations to govern dissolution kinetics. The framework is implemented in JAX, utilizing automatic differentiation to compute sensitivities for the co-optimization of pill shape and constituent distribution. We validate the method through single-phase and multi-excipient case studies.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiency-Aware Computational Intelligence for Resource-Constrained Manufacturing Toward Edge-Ready Deployment</title>
<link>https://arxiv.org/abs/2512.09319</link>
<guid>https://arxiv.org/abs/2512.09319</guid>
<content:encoded><![CDATA[
<div> Keywords: industrial cyber physical systems, data scarcity, physics-informed learning, graph-based modeling, edge-cloud collaboration<br /><br />Summary:<br />1. This dissertation addresses the challenges faced by industrial cyber physical systems (ICPS), which operate under heterogeneous sensing, stochastic dynamics, and shifting process conditions, resulting in incomplete, unlabeled, imbalanced, and domain-shifted data.<br />2. It highlights the limitations of centralized deep learning due to costly, confidential, and slow-to-obtain high-fidelity datasets and the constraints of edge devices on latency, bandwidth, and energy.<br />3. A computational framework is developed that is data lean, physics-aware, and deployment ready, tailored for modern manufacturing environments.<br />4. The research advances multiple methods, including generative strategies to mitigate data scarcity and imbalance, and semi-supervised learning that leverages unlabeled data to reduce annotation and simulation effort.<br />5. Physics-informed representation learning enhances interpretability and improves condition monitoring in small-data regimes.<br />6. Spatially aware graph-based surrogate modeling efficiently approximates complex industrial processes.<br />7. An edge-cloud collaborative compression scheme facilitates real-time signal analytics despite resource constraints.<br />8. The dissertation also extends visual understanding capabilities through zero-shot vision-language reasoning combined with domain-specific retrieval, enabling generalizable assessments in unseen scenarios.<br />9. Collectively, these contributions form a unified paradigm for data-efficient, resource-aware intelligence that bridges laboratory research and industrial deployment, supporting reliable decision-making across diverse manufacturing systems. <div>
arXiv:2512.09319v1 Announce Type: new 
Abstract: Industrial cyber physical systems operate under heterogeneous sensing, stochastic dynamics, and shifting process conditions, producing data that are often incomplete, unlabeled, imbalanced, and domain shifted. High-fidelity datasets remain costly, confidential, and slow to obtain, while edge devices face strict limits on latency, bandwidth, and energy. These factors restrict the practicality of centralized deep learning, hinder the development of reliable digital twins, and increase the risk of error escape in safety-critical applications. Motivated by these challenges, this dissertation develops an efficiency grounded computational framework that enables data lean, physics-aware, and deployment ready intelligence for modern manufacturing environments. The research advances methods that collectively address core bottlenecks across multimodal and multiscale industrial scenarios. Generative strategies mitigate data scarcity and imbalance, while semi-supervised learning integrates unlabeled information to reduce annotation and simulation demands. Physics-informed representation learning strengthens interpretability and improves condition monitoring under small-data regimes. Spatially aware graph-based surrogate modeling provides efficient approximation of complex processes, and an edge cloud collaborative compression scheme supports real-time signal analytics under resource constraints. The dissertation also extends visual understanding through zero-shot vision language reasoning augmented by domain specific retrieval, enabling generalizable assessment in previously unseen scenarios. Together, these developments establish a unified paradigm of data efficient and resource aware intelligence that bridges laboratory learning with industrial deployment, supporting reliable decision-making across diverse manufacturing systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNFinBench: A Benchmark for Safety and Compliance of Large Language Models in Finance</title>
<link>https://arxiv.org/abs/2512.09506</link>
<guid>https://arxiv.org/abs/2512.09506</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, financial sector, safety evaluation, CNFinBench, Harmful Instruction Compliance Score (HICS)  

<br /><br />Summary:  
This paper addresses the critical need for rigorous safety evaluation of large language models (LLMs) deployed in the financial sector, which is currently underserved by existing benchmarks that focus mostly on question answering and numerical tasks. It highlights that prior benchmarks do not adequately assess real-world safety behavior, regulatory compliance, investor protection, or resistance to multi-turn adversarial tactics like jailbreaks and prompt injections. To fill these gaps, the authors introduce CNFinBench, a novel benchmark tailored to finance that integrates a Capability-Compliance-Safety triad framework and emphasizes evidence-grounded reasoning from long financial reports as well as jurisdiction-specific rule and tax compliance. For measuring safety rigorously, the Harmful Instruction Compliance Score (HICS) is proposed to quantify model resistance to harmful multi-turn prompts. CNFinBench ensures evaluation auditability by enforcing strict output formats with dynamic option perturbation for objective tasks and combining an ensemble of LLMs with human-calibrated judgment for open-ended tasks. Experiments involving 21 models across 15 subtasks reveal a capability-compliance gap: models average 61.0 in capability tasks but drop sharply to 34.18 in compliance and risk-control. Under adversarial dialogue scenarios, most models only partially resist harmful instructions (HICS score 60-79), indicating that simple refusal is insufficient as a safety measure without verifiable and cited reasoning. <div>
arXiv:2512.09506v1 Announce Type: new 
Abstract: Large language models are increasingly deployed across the financial sector for tasks such as research, compliance, risk analysis, and customer service, which makes rigorous safety evaluation essential. However, existing financial benchmarks primarily focus on textbook-style question answering and numerical problem solving, but fail to evaluate models' real-world safety behaviors. They weakly assess regulatory compliance and investor-protection norms, rarely stress-test multi-turn adversarial tactics such as jailbreaks or prompt injection, inconsistently ground answers in long filings, ignore tool- or RAG-induced over-reach risks, and rely on opaque or non-auditable evaluation protocols. To close these gaps, we introduce CNFinBench, a benchmark that employs finance-tailored red-team dialogues and is structured around a Capability-Compliance-Safety triad, including evidence-grounded reasoning over long reports and jurisdiction-aware rule/tax compliance tasks. For systematic safety quantification, we introduce the Harmful Instruction Compliance Score (HICS) to measure how consistently models resist harmful prompts across multi-turn adversarial dialogues. To ensure auditability, CNFinBench enforces strict output formats with dynamic option perturbation for objective tasks and employs a hybrid LLM-ensemble plus human-calibrated judge for open-ended evaluations. Experiments on 21 models across 15 subtasks confirm a persistent capability-compliance gap: models achieve an average score of 61.0 on capability tasks but fall to 34.18 on compliance and risk-control evaluations. Under multi-turn adversarial dialogue tests, most systems reach only partial resistance (HICS 60-79), demonstrating that refusal alone is not a reliable proxy for safety without cited and verifiable reasoning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A roadmap of geospatial soil quality analysis systems</title>
<link>https://arxiv.org/abs/2512.09817</link>
<guid>https://arxiv.org/abs/2512.09817</guid>
<content:encoded><![CDATA[
<div> Keywords: Soil Quality, Geographic Information Systems, Remote Sensing, Machine Learning, Sustainable Agriculture<br /><br />Summary:<br /><br />This paper presents an innovative and comprehensive roadmap for soil quality (SQ) assessment, addressing limitations in traditional methods that rely on costly and labor-intensive sampling and lab analysis. It proposes a unified and modular pipeline that seamlessly integrates multi-source soil data, Geographic Information Systems (GIS), remote sensing technologies, and machine learning (ML) techniques to enable efficient, transparent, and scalable SQ evaluation. Unlike existing studies focusing on isolated soil parameters or specific modeling approaches, this work consolidates recent technological advancements across the entire soil quality assessment workflow. It discusses practical applications demonstrating the pipeline’s effectiveness in real-world scenarios. Additionally, the paper highlights challenges and limitations in current soil quality assessment methods, emphasizing the need for more adaptive and transparent systems. Finally, it explores emerging trends and future developments in GIS, remote sensing, and ML that could drive the next generation of soil quality assessment tools. These improvements aim to better support sustainable land management practices and environmental conservation by providing more reliable, holistic, and accessible soil quality evaluation methods. <div>
arXiv:2512.09817v1 Announce Type: new 
Abstract: Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Taxonomy of Numerical Differentiation Methods</title>
<link>https://arxiv.org/abs/2512.09090</link>
<guid>https://arxiv.org/abs/2512.09090</guid>
<content:encoded><![CDATA[
<div> Differentiation, Numerical methods, Noisy data, Computational algorithms, PyNumDiff<br /><br />Summary:<br /><br />1. Differentiation is fundamental in scientific computing and engineering, as many physical laws are expressed in terms of derivatives in space and time.<br /><br />2. Direct measurement of derivatives is often impossible, requiring computation from potentially noisy and corrupted data, which presents challenges.<br /><br />3. Existing numerical differentiation algorithms vary widely and may impose specific constraints such as periodic boundary conditions or perform poorly under noise.<br /><br />4. This work reviews a comprehensive set of numerical differentiation methods, discusses critical considerations for choosing suitable approaches, compares their advantages, and provides foundational theory to enhance understanding.<br /><br />5. To support practical application, the authors offer an open-source Python package, PyNumDiff, which implements a broad spectrum of differentiation techniques geared toward noisy data.<br /><br />This review and software tool aim to guide scientists and engineers in selecting the most appropriate differentiation method tailored to their specific application domains, especially when working with noisy or corrupted data streams. <div>
arXiv:2512.09090v1 Announce Type: cross 
Abstract: Differentiation is a cornerstone of computing and data analysis in every discipline of science and engineering. Indeed, most fundamental physics laws are expressed as relationships between derivatives in space and time. However, derivatives are rarely directly measurable and must instead be computed, often from noisy, potentially corrupt data streams. There is a rich and broad literature of computational differentiation algorithms, but many impose extra constraints to work correctly, e.g. periodic boundary conditions, or are compromised in the presence of noise and corruption. It can therefore be challenging to select the method best-suited to any particular problem. Here, we review a broad range of numerical methods for calculating derivatives, present important contextual considerations and choice points, compare relative advantages, and provide basic theory for each algorithm in order to assist users with the mathematical underpinnings. This serves as a practical guide to help scientists and engineers match methods to application domains. We also provide an open-source Python package, PyNumDiff, which contains a broad suite of methods for differentiating noisy data.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design</title>
<link>https://arxiv.org/abs/2512.09329</link>
<guid>https://arxiv.org/abs/2512.09329</guid>
<content:encoded><![CDATA[
<div> Keywords: supervised fine-tuning, protein language models, protein sequence generation, data curation, enzyme design<br /><br />Summary:<br /><br />1. The paper addresses the challenge of applying supervised fine-tuning (SFT) to protein language models (PLMs), noting the scarcity of high-quality annotated protein data compared to natural language.  
2. It proposes a simple and generalizable SFT approach that improves the fidelity, reliability, and novelty of generated protein sequences without relying on costly precompiled experimental datasets.  
3. The method uses the PLM itself to generate data, which is then refined through a lightweight curation pipeline combined with domain-specific filters to produce high-quality training samples.  
4. These filters not only enhance the PLM outputs but also identify promising candidates for in vitro experimental validation.  
5. The approach is model- and protein system-agnostic and is demonstrated on a genome-scale PLM (GenSLM) focused on the tryptophan synthase enzyme family, resulting in sequences that are more novel and better optimized for both targeted design constraints and emergent protein properties. <div>
arXiv:2512.09329v1 Announce Type: cross 
Abstract: Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Broadband Thermoelectric Energy Harvesting for Wearable Biosensors Using Plasmonic Field-Enhancement and Machine-Learning-Guided Device Optimization</title>
<link>https://arxiv.org/abs/2512.08103</link>
<guid>https://arxiv.org/abs/2512.08103</guid>
<content:encoded><![CDATA[
<div> Keywords: wearable biosensors, thermoelectric generators, thermoplasmonic heating, machine learning optimization, energy harvesting<br /><br />Summary:<br /><br />1. This paper addresses the challenge of powering wearable biosensors continuously and without batteries by improving skin-mounted thermoelectric generators, which traditionally suffer from low temperature differences in real-world conditions. <br />2. The authors introduce a hybrid device combining thermoplasmonic and thermoelectric effects, where a broadband metasurface of cross-bowtie nanoantennas absorbs infrared radiation from human body heat, ambient IR, and near-infrared sunlight across the 2 to 12 micron spectrum.<br />3. Electromagnetic simulations reveal strong localized field enhancements in the nanoantenna gaps cause thermoplasmonic heating directly on flexible Bi2Te3 thermoelectric junctions, raising the effective temperature difference from about 3-4 °C to roughly 13 °C.<br />4. Coupled optical, thermal, and electrical modeling demonstrates this temperature enhancement yields a power density near 0.15 mW/cm² under typical indoor infrared radiation, a four- to six-fold increase compared to current flexible thermoelectric devices.<br />5. Machine learning surrogate models trained on multiphysics simulation data accurately predict temperature rise and electrical output (R² > 0.92), enabling optimization of device geometry through Pareto-front analysis.<br />6. The integrated thermoplasmonic-thermoelectric approach enhanced by machine learning offers a scalable, efficient, and flexible route for autonomous, long-term wearable physiological monitoring energy harvesters. <div>
arXiv:2512.08103v1 Announce Type: new 
Abstract: Wearable biosensors increasingly require continuous and battery-free power sources, but conventional skin-mounted thermoelectric generators are limited by the small temperature differences available in real environments. This work introduces a hybrid thermoplasmonic and thermoelectric energy harvester that combines multiband plasmonic absorption with machine-learning-guided optimization to improve on-body energy conversion. A broadband metasurface made of cross-bowtie nanoantennas is designed to absorb infrared radiation across the 2 to 12 micron range, capturing human body emission, ambient infrared radiation, and near-infrared sunlight. Electromagnetic simulations show strong field enhancement in nanoscale antenna gaps, producing localized thermoplasmonic heating directly above flexible Bi2Te3 thermoelectric junctions. Coupled optical, thermal, and electrical modeling indicates that this localized heating increases the effective temperature difference from the typical 3 to 4 degrees C of standard wearable thermoelectric generators to approximately 13 degrees C. This results in a power density of about 0.15 mW per cm^2 under indoor-relevant infrared flux, representing a four- to six-fold improvement over existing flexible devices. A machine-learning surrogate model trained on multiphysics data predicts temperature rise and electrical output with high accuracy (R2 greater than 0.92) and identifies optimal device geometries through Pareto-front analysis. The proposed hybrid thermoplasmonic, thermoelectric, and machine-learning framework provides a scalable route toward more efficient, compact, and flexible energy harvesters for autonomous and long-term wearable physiological monitoring.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dflow-SUR: Enhancing Generative Aerodynamic Inverse Design using Differentiation Throughout Flow Matching</title>
<link>https://arxiv.org/abs/2512.08336</link>
<guid>https://arxiv.org/abs/2512.08336</guid>
<content:encoded><![CDATA[
<div> Generative design, energy-based methods, Dflow-SUR, aerodynamic optimization, physical loss  

<br /><br />Summary:  
Generative inverse design requires incorporating physical constraints to ensure generated designs are reliable and accurate. Current state-of-the-art energy-based methods suffer from an asynchronous phenomenon where optimizing the physical loss is hindered by the flow matching inference process. To address this, the authors introduce Dflow-SUR, a new differentiation strategy that separates the optimization of physical loss from flow matching inference. This decoupling leads to significant improvements: on an airfoil design case, Dflow-SUR reduces physical loss by four orders of magnitude and decreases wall-clock time by 74% compared to leading energy-based baselines. In wing design tasks, it achieves an 11.8% increase in the mean lift-to-drag ratio over traditional Latin-hypercube sampling. Beyond improvements in accuracy and efficiency, Dflow-SUR provides enhanced control over guidance during optimization, reduces surrogate model uncertainty, and exhibits greater robustness to hyper-parameter tuning. Overall, these results demonstrate that Dflow-SUR is a highly promising framework that delivers both scalability and high-fidelity generative aerodynamic design. <div>
arXiv:2512.08336v1 Announce Type: new 
Abstract: Generative inverse design requires incorporating physical constraints to ensure that generated designs are both reliable and accurate. However, we observe that current state-of-the-art energy-based methods suffer from an asynchronous phenomenon, where the optimization of the physical loss is constrained by the flow matching inference process. To overcome this limitation, we introduce Dflow-SUR, a differentiation strategy that separates the optimization of the physical loss from the flow matching inference.
  Compared to the most advanced energy-based baseline, Dflow-SUR achieves a reduction in physical loss by four orders of magnitude, while also cutting wall-clock time by 74% on the airfoil case. Additionally, it increases the mean lift-to-drag ratio by 11.8% over traditional Latin-hypercube sampling in wing design. Beyond improvements in accuracy and efficiency, Dflow-SUR offers three additional practical advantages: (i) enhanced control over guidance, (ii) lower surrogate uncertainty, and (iii) greater robustness to hyper-parameter tuning.
  Together, these results demonstrate that Dflow-SUR is a highly promising framework, providing both scalability and high fidelity for generative aerodynamic design.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanical behaviour of brain-skull interface (meninges) under shear loading through experiment and finite element modelling: Preliminary results</title>
<link>https://arxiv.org/abs/2512.08425</link>
<guid>https://arxiv.org/abs/2512.08425</guid>
<content:encoded><![CDATA[
<div> Keywords: brain-skull interface, meninges, shear loading, finite element modeling, cohesive layer

<br /><br />Summary: This study addresses the mechanical behavior of the brain-skull interface (meninges), which is crucial in controlling brain motion during head impacts but is often simplified in computational models. Researchers combined experimental shear loading tests on sheep brain tissue and brain-skull complex samples with computational finite element (FE) modeling. Magnetic resonance imaging (MRI) was employed to obtain precise 3D geometries of the samples, enabling subject-specific mesh generation for simulations. The brain tissue was represented using a second-order Ogden hyperelastic model, while the brain-skull interface was modeled as a cohesive layer to capture its mechanical response. The results demonstrated that the cohesive layer effectively replicates the force-displacement behavior and damage initiation observed experimentally at the brain-skull interface. Calibrated cohesive properties showed consistent maximum tractions across samples, with normal traction ranging between 2.8 and 3.4 kPa, and tangential traction between 1.8 and 2.1 kPa. This integrated experimental-computational framework offers a data-driven approach to replace idealized boundary conditions currently used in computational head models. Consequently, it enhances the biofidelity of simulations for brain injury prediction and neurosurgical planning by incorporating experimentally derived biomechanical properties of the meninges. <div>
arXiv:2512.08425v1 Announce Type: new 
Abstract: The brain-skull interface (meninges) plays a critical role in governing brain motion during head impacts, yet computational models often simplify this interface using idealized contact conditions due to limited experimental data. This study presents an improved protocol combining experimental testing and computational modelling to determine the mechanical properties of the brain-skull interface under shear loading. Brain tissue and brain-skull complex samples were extracted from sheep cadaver heads and subjected to shear loading. Magnetic resonance imaging (MRI) was used to obtain accurate 3D geometries of the samples, which were then used to create computational grids (meshes) for simulation of the experiments using finite element (FE) models to determine subject-specific properties of the brain tissue and brain-skull interface. A second-order Ogden hyperelastic model was used for the brain tissue, and a cohesive layer was employed to model the brain-skull interface. Our results indicate that a cohesive layer captures the force-displacement and damage initiation of the brain-skull interface. The calibrated cohesive properties showed consistent patterns across samples, with maximum normal tractions ranging from 2.8-3.4 kPa and maximum tangential tractions from 1.8-2.1 kPa. This framework provides a foundation for improving the biofidelity of computational head models used in injury prediction and neurosurgical planning by replacing arbitrary boundary conditions with formulations derived from experimental data on brain-skull interface (meninges) biomechanical behaviour.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?</title>
<link>https://arxiv.org/abs/2512.08764</link>
<guid>https://arxiv.org/abs/2512.08764</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial markets, summarization, large language models, extractive methods, FT-Mistral-7B<br /><br />Summary:<br /><br />Financial markets evolve rapidly due to factors such as news, economic shifts, and geopolitical events, making timely reactions essential for investors to mitigate losses or seize short-term opportunities. Given the daily influx of over 50,000 financial articles, automated summarization techniques are crucial for effective decision-making. This study assesses a spectrum of summarization approaches, ranging from simple extractive methods to advanced large language models (LLMs), evaluated using the FinLLMs Challenge dataset. Results indicate that LLMs produce more coherent and informative summaries but are resource-intensive and prone to hallucinations, potentially leading to significant summary inaccuracies. Conversely, extractive summarization methods are efficient and perform well on short, well-structured financial texts, making them a practical alternative in such contexts. Among the models tested, a fine-tuned LLM named FT-Mistral-7B achieved the best ROUGE scores, demonstrating superior summarization capability. However, the study notes that the reliability of the underlying data corpus is limited, warranting cautious interpretation of the findings. Overall, the research highlights the trade-offs between summary quality, computational cost, and reliability in financial news summarization, pointing towards tailored approaches depending on context and resource availability. <div>
arXiv:2512.08764v1 Announce Type: new 
Abstract: Financial markets change rapidly due to news, economic shifts, and geopolitical events. Quick reactions are vital for investors to avoid losses or capture short-term gains. As a result, concise financial news summaries are critical for decision-making. With over 50,000 financial articles published daily, automation in summarization is necessary. This study evaluates a range of summarization methods, from simple extractive techniques to advanced large language models (LLMs), using the FinLLMs Challenge dataset. LLMs generated more coherent and informative summaries, but they are resource-intensive and prone to hallucinations, which can introduce significant errors into financial summaries. In contrast, extractive methods perform well on short, well-structured texts and offer a more efficient alternative for this type of article. The best ROUGE results come from fine-tuned LLM model like FT-Mistral-7B, although our data corpus has limited reliability, which calls for cautious interpretation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Language Models Can Use Nuanced Reasoning For Health Science Research Classification: A Microbial-Oncogenesis Case Study</title>
<link>https://arxiv.org/abs/2512.06502</link>
<guid>https://arxiv.org/abs/2512.06502</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, medical research classification, zero-shot learning, in-context learning, interpretability  

<br /><br />Summary:  
This study evaluates the effectiveness of Small Language Models (SLMs) with 8 billion parameters or fewer in classifying medical research papers, using literature on the oncogenic potential of HMTV/MMTV-like viruses in breast cancer as a case study. The performance of these SLMs is compared to leading proprietary Large Language Models (LLMs) such as GPT-5, Gemini 3 Pro Preview, and Meerkat in both zero-shot and in-context learning (ICL; few-shot prompting) scenarios. Results show that Llama 3 and Qwen2.5 outperform GPT-5 and Gemini 3 Pro Preview in zero-shot settings, though they lag behind Gemini 2.5 Pro. The application of in-context learning improves performance variably, enabling Llama 3 and Qwen2.5 to match Gemini 2.5 Pro in binary classification tasks. Lexical ablation experiments reveal that SLM decisions often rely on valid scientific signals but can be affected by irrelevant textual artifacts, highlighting the importance of interpretability for reliability in critical scientific workflows. Overall, the findings demonstrate the potential and challenges of using modern SLMs for scientific literature triage, indicating that combining SLMs with effective prompting techniques can nearly match the classification capabilities of more powerful LLMs, making them suitable for cost-effective AI co-scientist pipelines. <div>
arXiv:2512.06502v1 Announce Type: new 
Abstract: Artificially intelligent (AI) co-scientists must be able to sift through research literature cost-efficiently while applying nuanced scientific reasoning. We evaluate Small Language Models (SLMs, <= 8B parameters) for classifying medical research papers. Using literature on the oncogenic potential of HMTV/MMTV-like viruses in breast cancer as a case study, we assess model performance with both zero-shot and in-context learning (ICL; few-shot prompting) strategies against frontier proprietary Large Language Models (LLMs). Llama 3 and Qwen2.5 outperform GPT-5 (API, low/high effort), Gemini 3 Pro Preview, and Meerkat in zero-shot settings, though trailing Gemini 2.5 Pro. ICL leads to improved performance on a case-by-case basis, allowing Llama 3 and Qwen2.5 to match Gemini 2.5 Pro in binary classification. Systematic lexical-ablation experiments show that SLM decisions are often grounded in valid scientific cues but can be influenced by spurious textual artifacts, underscoring need for interpretability in high-stakes pipelines. Our results reveal both promise and limitations of modern SLMs for scientific triage; pairing SLMs with simple but principled prompting strategies can approach performance of the strongest LLMs for targeted literature filtering in co-scientist pipelines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Model for Polycrystalline Material Informatics</title>
<link>https://arxiv.org/abs/2512.06770</link>
<guid>https://arxiv.org/abs/2512.06770</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D polycrystal, self-supervised pretraining, microstructures, homogenized stiffness, nonlinear response modeling<br /><br />Summary:  
1. The article introduces a 3D polycrystal foundation model designed to learn physically structured representations of voxel-based microstructures through large-scale self-supervised pretraining.  
2. The encoder is trained on a dataset comprising 100,000 face-centered cubic (FCC) microstructures with varying crystallographic orientations that span the texture hull, employing a masking strategy to encourage inference of latent features from incomplete spatial data.  
3. The learned representation quality is evaluated on two downstream tasks: (i) homogenized stiffness prediction, where the pretrained encoder outperforms a non-pretrained baseline across all masking ratios, and (ii) nonlinear response modeling, where the encoder integrates with an orientation-aware interaction-based deep material network (ODMN) to predict accurate stress-strain responses for unseen microstructures.  
4. In both tasks, the pretrained encoder exhibits significantly improved generalization capabilities, highlighting strong transferability and robustness in environments with limited labeled microstructure data.  
5. The foundation model offers a scalable path to incorporate experimentally derived microstructures, providing a novel platform for microstructure-property mapping and advancing materials design through physics-consistent, data-efficient approaches. <div>
arXiv:2512.06770v1 Announce Type: new 
Abstract: We present a 3D polycrystal foundation model that learns a physically structured representation of voxel-based microstructures through large-scale self-supervised pretraining. The encoder is trained on a dataset of 100,000 FCC microstructures whose crystallographic orientations span the texture hull, using a masking strategy that forces the model to infer latent features from incomplete spatial information. The quality of the learned representation is evaluated through two downstream tasks with distinct physical characteristics. (i) Homogenized stiffness prediction: the pretrained encoder consistently outperforms the non-pretrained baseline across all masking ratios. (ii) Nonlinear response modeling: the encoder is coupled with an orientation-aware interaction-based deep material network (ODMN) to infer complete sets of network parameters, enabling accurate stress-strain predictions for previously unseen microstructures. In both tasks, the pretrained encoder demonstrates markedly stronger generalization capability. These results underscore the strong transferability of the proposed framework and its suitability for data-scarce scientific settings, where labeled microstructures are limited and physics-consistent generalization is essential. The foundation model provides a scalable route toward integration with experimentally derived microstructures, offering a new basis for microstructure-property reasoning in practical materials design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crystallographic Texture-Generalizable Orientation-Aware Interaction-Based Deep Material Network for Polycrystal Modeling and Texture Evolution</title>
<link>https://arxiv.org/abs/2512.06779</link>
<guid>https://arxiv.org/abs/2512.06779</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, polycrystalline microstructures, graph neural network, texture evolution, surrogate model<br /><br />Summary:<br /><br />1. The paper addresses advancements in materials modeling through machine learning, focusing on the Orientation-aware Interaction-based Deep Material Network (ODMN) that captures crystallographic textures using material nodes and enforces stress equilibrium via the Hill-Mandel condition. <br /><br />2. ODMN effectively learns geometry-mechanics relationships from linear elastic stiffness data to predict nonlinear mechanical responses and texture evolution, but it requires retraining for different crystallographic textures, limiting its generalizability.<br /><br />3. To overcome this limitation, the authors propose the TACS-GNN-ODMN framework which integrates a Texture-Adaptive Clustering and Sampling (TACS) scheme for initializing texture-related parameters and a Graph Neural Network (GNN) for predicting stress equilibrium parameters.<br /><br />4. This new framework eliminates the need for texture-specific retraining while maintaining physical interpretability and delivers predictions of nonlinear responses and texture evolution consistent with direct numerical simulations (DNS) across diverse textures.<br /><br />5. The TACS-GNN-ODMN framework significantly enhances the generalization capability of ODMN, providing a robust, efficient surrogate model for multiscale simulations and advancing next-generation materials design. <div>
arXiv:2512.06779v1 Announce Type: new 
Abstract: Machine learning has significantly advanced materials modeling by enabling surrogate models that achieve high computational efficiency without compromising predictive accuracy. The Orientation-aware Interaction-based Deep Material Network (ODMN) is one such framework, in which a set of material nodes represents crystallographic textures, and a hierarchical interaction network enforces stress equilibrium among these nodes based on the Hill-Mandel condition. Using only linear elastic stiffness data, ODMN learns the intrinsic geometry-mechanics relationships within polycrystalline microstructures, allowing it to predict nonlinear mechanical responses and texture evolution with high fidelity. However, its applicability remains limited by the need to retrain for each distinct crystallographic texture. To address this limitation, we introduce the TACS-GNN-ODMN framework, which integrates (i) a Texture-Adaptive Clustering and Sampling (TACS) scheme for initializing texture-related parameters and (ii) a Graph Neural Network (GNN) for predicting stress-equilibrium-related parameters. The proposed framework accurately predicts nonlinear responses and texture evolution across diverse textures, showing close agreement with direct numerical simulations (DNS). By eliminating the requirement for texture-specific retraining while preserving physical interpretability, TACS-GNN-ODMN substantially enhances the generalization capability of ODMN, offering a robust and efficient surrogate model for multiscale simulations and next-generation materials design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MATEX: A Multi-Agent Framework for Explaining Ethereum Transactions</title>
<link>https://arxiv.org/abs/2512.06933</link>
<guid>https://arxiv.org/abs/2512.06933</guid>
<content:encoded><![CDATA[
<div> Keywords: Ethereum transactions, multi-hop token flows, nested contract calls, transaction explanation, cognitive multi-agent framework<br /><br />Summary: Understanding complex Ethereum transactions is difficult due to multi-hop token flows, nested contract calls, and opaque execution paths that often result in users blindly signing transactions. Interviews with everyday users, developers, and auditors highlight a demand for clear, step-wise explanations that rely on both on-chain data and the actual semantics of protocols in the real world. To address this challenge, the paper introduces matex, a cognitive multi-agent framework designed to enhance transaction comprehension. Matex treats understanding as a collaborative investigation involving multiple agents working together. It integrates rapid generation of hypotheses, dynamic retrieval of off-chain knowledge that supplements on-chain data, evidence-aware synthesis to consolidate findings, and adversarial validation to ensure the faithfulness and correctness of explanations. This multi-faceted approach aims to produce trustworthy, transparent explanations that provide users better insight into what a transaction actually does before signing, reducing blind trust and improving security. Matex thereby bridges the gap between raw blockchain data and user comprehension through intelligent collaboration grounded in both blockchain evidence and real-world protocol understanding. <div>
arXiv:2512.06933v1 Announce Type: new 
Abstract: Understanding a complicated Ethereum transaction remains challenging: multi-hop token flows, nested contract calls, and opaque execution paths routinely lead users to blind signing. Based on interviews with everyday users, developers, and auditors, we identify the need for faithful, step-wise explanations grounded in both on-chain evidence and real-world protocol semantics. To meet this need, we introduce (matex, a cognitive multi-agent framework that models transaction understanding as a collaborative investigation-combining rapid hypothesis generation, dynamic off-chain knowledge retrieval, evidence-aware synthesis, and adversarial validation to produce faithful explanations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smooth geometry extraction from SIMP topology optimization: Signed distance function approach with volume preservation</title>
<link>https://arxiv.org/abs/2512.06976</link>
<guid>https://arxiv.org/abs/2512.06976</guid>
<content:encoded><![CDATA[
<div> Keywords: topology optimization, signed distance function, radial basis functions, geometry refinement, stress reduction<br /><br />Summary: This paper introduces a novel post-processing methodology designed to extract high-quality geometries from density-based topology optimization results. The new method addresses common challenges such as achieving smooth boundaries, preserving the volume fraction, and maintaining key topological features simultaneously. The approach employs a two-stage process starting with the construction of a signed distance function (SDF) representation of density isocontours. This is followed by a geometry refinement step using radial basis functions (RBFs), which results in more accurate and smooth boundary definitions. One of the key advantages is that the method allows the geometries to appear as if derived from much finer discretizations, while still maintaining the computational efficiency typical of coarse mesh optimization. Validation results show that this method leads to an 18% reduction in maximum equivalent stress values compared to traditional post-processing techniques. This improvement is attributed to continuous geometric transitions at boundaries rather than abrupt changes. Furthermore, the implicit boundary representation created by this method allows straightforward export to standard manufacturing formats without needing intermediate reconstruction processes. This makes the proposed approach highly suitable for practical engineering applications where precise and manufacturable geometric representations are critical. <div>
arXiv:2512.06976v1 Announce Type: new 
Abstract: This paper presents a novel post-processing methodology for extracting high-quality geometries from density-based topology optimization results. Current post-processing approaches often struggle to simultaneously achieve smooth boundaries, preserve volume fraction, and maintain topological features. We propose a robust method based on a signed distance function (SDF) that addresses these challenges through a two-stage process: first, an SDF representation of density isocontours is constructed, which is followed by geometry refinement using radial basis functions (RBFs). The method generates smooth boundary representations that appear to originate from much finer discretizations while maintaining the computational efficiency of coarse mesh optimization. Through comprehensive validation, our approach demonstrates a 18% reduction in maximum equivalent stress values compared to conventional methods, achieved through continuous geometric transitions at boundaries. The resulting implicit boundary representation facilitates seamless export to standard manufacturing formats without intermediate reconstruction steps, providing a robust foundation for practical engineering applications where high-quality geometric representations are essential.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DC-Biased Homogenized Harmonic Balance Finite Element Method</title>
<link>https://arxiv.org/abs/2512.06978</link>
<guid>https://arxiv.org/abs/2512.06978</guid>
<content:encoded><![CDATA[
<div> Keywords: homogenized harmonic balance, finite element method, eddy-current simulation, ferromagnetic saturation, frequency-domain homogenization

<br /><br />Summary:  
1. The paper presents an extension of the homogenized harmonic balance finite element (FE) method to handle excitation signals that include a DC bias.  
2. This method combines the harmonic balance technique with a frequency-domain-based homogenization to enable efficient nonlinear eddy-current simulations in 3-D devices incorporating lamination stacks.  
3. Unlike traditional time-stepping methods, the approach allows the use of a coarser FE mesh that does not require resolving individual laminates, significantly reducing computational demand.  
4. The original homogenization formula is adapted to better account for ferromagnetic saturation by computing the homogenized reluctivity through a lookup table derived from a 1-D FE simulation of a lamination, considering average magnetic flux density and skin depth.  
5. The method is validated against fine-mesh transient reference simulations across various saturation levels and frequencies between 50 Hz and 10 kHz.  
6. Results indicate that for moderate ferromagnetic saturation, the method accurately approximates eddy-current losses and magnetic energy with relative errors below 10%.  
7. It achieves a reduction in the number of degrees of freedom by a factor of about 30 at 10 kHz, leading to a dramatic decrease in simulation time—from 2 days on a high-end server to approximately 90 minutes on a standard workstation. <div>
arXiv:2512.06978v1 Announce Type: new 
Abstract: The homogenized harmonic balance finite element (FE) method enables efficient nonlinear eddy-current simulations of 3-D devices with lamination stacks by combining the harmonic balance method with a frequency-domain-based homogenization technique. This approach avoids expensive time stepping of the eddy-current field problem and allows the use of a relatively coarse FE mesh that does not resolve the individual laminates. In this paper, we extend the method to handle excitation signals with a dc bias. To achieve this, we adapt the original homogenization technique to better account for ferromagnetic saturation. The resulting formula for the homogenized reluctivity is evaluated using a look-up table computed from a 1-D FE simulation of a lamination and containing the average magnetic flux density in the lamination and the corresponding skin depth. We compare the results of the proposed method to those from a fine-mesh transient reference simulation. The tests cover different levels of ferromagnetic saturation and frequencies between 50 Hz and 10 kHz. For moderate ferromagnetic saturation, the method gives a good approximation of the eddy-current losses and the magnetic energy, with relative errors below 10%, while reducing the required number of degrees of freedom at 10 kHz by 1.5 orders of magnitude. This results in a reduction in simulation time from 2 days on a contemporary server to 90 minutes on a standard workstation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Computing Tools To Predict Varied Weight Components, Material and Tribological Properties of Al2219-B4C-Gr</title>
<link>https://arxiv.org/abs/2512.07063</link>
<guid>https://arxiv.org/abs/2512.07063</guid>
<content:encoded><![CDATA[
<div> Keywords: Soft computing, Genetic algorithms, Aluminum composites, Boron Carbide, Tribological properties<br /><br />Summary:<br /><br />1. The study focuses on using soft computing tools such as fuzzy logic, decision trees, and genetic algorithms to predict the optimal weight percentages of reinforcements, specifically Boron Carbide (B4C) and Graphite (Gr), in aluminum matrix composites (Al2219).<br />2. These computational approaches enable accurate prediction of mechanical and tribological properties of composite materials with minimal or no experimental work.<br />3. NSGA-II genetic algorithms are utilized for multi-objective optimization to determine the best combination of materials and their tribological properties.<br />4. The predicted results closely match the findings from previous artificial neural network (ANN) studies and available experimental data, validating the proposed approach.<br />5. The inclusion of B4C as a reinforcement has a more significant positive influence on the enhancement of mechanical properties and wear resistance compared to Graphite, indicating its superior effectiveness in improving aluminum composites.<br /><br />The research highlights the reliability and efficiency of soft computing techniques in materials science, particularly for optimizing composite material compositions and properties without extensive experimentation. <div>
arXiv:2512.07063v1 Announce Type: new 
Abstract: Soft computing tools emerged as most reliable alternatives of traditional regression and statistical methods. In recent times, these tools can predict the optimum material compositions, mechanical and tribological properties of composite materials accurately without much experiment or even without experiment. In the present study, soft computing tools like fuzzy logic, Decision tree, genetic algorithms are employed to predict the reinforcement weight percentage of B4C(Boron Carbide) and Graphite(Gr) along with Aluminum (matrix material) weight percentage for Al2219 with B4C and graphite. The optimized material and tribological properties of Al2219 were also predicted using NSGA II genetic algorithms for multi-objective optimization. It is found that the predictions are at par with earlier ANN (artificial neural network) studies and experimental findings. It can be inferred that inclusion B4C has more impact on enhancement of mechanical properties as well as wear strength compared to Gr.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detrended cross-correlations and their random matrix limit: an example from the cryptocurrency market</title>
<link>https://arxiv.org/abs/2512.06473</link>
<guid>https://arxiv.org/abs/2512.06473</guid>
<content:encoded><![CDATA[
<div> correlations, multifractal detrended cross-correlation, heavy tails, spectral properties, cryptocurrencies<br /><br />Summary:<br /><br />1. Traditional covariance-based correlation analyses often fail in complex systems due to issues like nonstationarity, long-range memory, and heavy-tailed fluctuations. 2. The study introduces scale and fluctuation-dependent correlation matrices built on the multifractal detrended cross-correlation coefficient ($\rho_r$), which highlights fluctuations of varying amplitudes selectively. 3. Spectral properties of these detrended correlation matrices are analyzed and compared with those derived from synthetic Gaussian and $q$Gaussian signals, showing that detrending, heavy tails, and the fluctuation order parameter $r$ cause substantial deviations from random matrix spectra even when no true cross-correlations exist. 4. Applying this framework to one-minute return data from 140 major cryptocurrencies between 2021 and 2024 uncovers robust collective market modes, including a dominant market-wide factor and multiple sector-specific components whose significance depends on scale and fluctuation order. 5. After removing the dominant market mode, the eigenvalue distribution’s bulk closely resembles the random detrended cross-correlation limit, allowing clear detection of structurally meaningful outliers. Overall, the approach provides a refined spectral baseline and a powerful tool to disentangle genuine interdependencies from noise in complex nonstationary systems with heavy-tailed dynamics. <div>
arXiv:2512.06473v1 Announce Type: cross 
Abstract: Correlations in complex systems are often obscured by nonstationarity, long-range memory, and heavy-tailed fluctuations, which limit the usefulness of traditional covariance-based analyses. To address these challenges, we construct scale and fluctuation-dependent correlation matrices using the multifractal detrended cross-correlation coefficient $\rho_r$ that selectively emphasizes fluctuations of different amplitudes. We examine the spectral properties of these detrended correlation matrices and compare them to the spectral properties of the matrices calculated in the same way from synthetic Gaussian and $q$Gaussian signals. Our results show that detrending, heavy tails, and the fluctuation-order parameter $r$ jointly produce spectra, which substantially depart from the random case even under absence of cross-correlations in time series. Applying this framework to one-minute returns of 140 major cryptocurrencies from 2021-2024 reveals robust collective modes, including a dominant market factor and several sectoral components whose strength depends on the analyzed scale and fluctuation order. After filtering out the market mode, the empirical eigenvalue bulk aligns closely with the limit of random detrended cross-correlations, enabling clear identification of structurally significant outliers. Overall, the study provides a refined spectral baseline for detrended cross-correlations and offers a promising tool for distinguishing genuine interdependencies from noise in complex, nonstationary, heavy-tailed systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource-Bounded Type Theory: Compositional Cost Analysis via Graded Modalities</title>
<link>https://arxiv.org/abs/2512.06952</link>
<guid>https://arxiv.org/abs/2512.06952</guid>
<content:encoded><![CDATA[
<div> resource bounds, typed programs, graded modality, syntactic cost soundness, recursion-free fragment<br /><br />Summary:<br /><br />1. The paper presents a compositional framework for certifying resource bounds in typed programs, allowing for uniform handling of time, memory, gas, and other domain-specific costs within an abstract resource lattice.<br /><br />2. It introduces a graded feasibility modality characterized by co-unit and monotonicity laws, which structures the reasoning about resource consumption.<br /><br />3. The main theoretical contribution is a syntactic cost soundness theorem for the recursion-free simply-typed fragment, guaranteeing that if a closed term has a synthesized bound b under a specified budget, then its actual operational cost does not exceed b.<br /><br />4. The authors construct a syntactic term model within the topos of presheaves over the resource lattice, defining a cost-stratified family of values and framing cost extraction as a natural transformation. They prove canonical forms by reification and establish the initiality of this syntactic model, ensuring a unique embedding into all resource-bounded models.<br /><br />5. A practical case study on binary search implemented in Lean demonstrates the effectiveness of the framework, showcasing compositional reasoning about resource bounds with separate proofs for recursion and cost, highlighting its applicability to real-world functional programming settings. <div>
arXiv:2512.06952v1 Announce Type: cross 
Abstract: We present a compositional framework for certifying resource bounds in typed programs. Terms are typed with synthesized bounds drawn from an abstract resource lattice, enabling uniform treatment of time, memory, gas, and domain-specific costs.
  We introduce a graded feasibility modality with co-unit and monotonicity laws. Our main result is a syntactic cost soundness theorem for the recursion-free simply-typed fragment: if a closed term has synthesized bound b under a given budget, its operational cost is bounded by b. We provide a syntactic term model in the topos of presheaves over the lattice -- where resource bounds index a cost-stratified family of definable values -- with cost extraction as a natural transformation. We prove canonical forms via reification and establish initiality of the syntactic model: it embeds uniquely into all resource-bounded models.
  A case study demonstrates compositional reasoning for binary search using Lean's native recursion with separate bound proofs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models</title>
<link>https://arxiv.org/abs/2411.06272</link>
<guid>https://arxiv.org/abs/2411.06272</guid>
<content:encoded><![CDATA[
<div> Keywords: financial LLMs, bilingual benchmark, NLP tasks, model evaluation, Touchstone-GPT<br /><br />Summary:<br /><br />As large language models (LLMs) become more prevalent in the financial sector, there is a critical need for a standardized and comprehensive method to evaluate their performance. The paper introduces Golden Touchstone, a bilingual benchmark designed specifically for financial LLMs, covering eight core financial NLP tasks in both Chinese and English. This benchmark is built from extensive open-source data and industry-specific requirements to robustly assess models’ capabilities in financial language understanding and generation. The authors conduct a comparative analysis of leading models including GPT-4o, Llama3, FinGPT, and FinMA, highlighting their relative strengths and weaknesses when handling complex financial information. In addition to the benchmark, the paper presents Touchstone-GPT, an open-source financial LLM developed through continual pre-training and instruction tuning, which achieves strong performance on the Golden Touchstone benchmark but still demonstrates limitations in certain tasks. The research thus contributes a valuable evaluation tool for the financial AI community and offers guidance for the future improvement and optimization of financial LLMs. The source code for the benchmark and model weights for Touchstone-GPT have been released publicly to facilitate further research and development. <div>
arXiv:2411.06272v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) increasingly permeate the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. Existing financial benchmarks often suffer from limited language and task coverage, low-quality datasets, and inadequate adaptability for LLM evaluation. To address these limitations, we introduce Golden Touchstone, a comprehensive bilingual benchmark for financial LLMs, encompassing eight core financial NLP tasks in both Chinese and English. Developed from extensive open-source data collection and industry-specific demands, this benchmark thoroughly assesses models' language understanding and generation capabilities. Through comparative analysis of major models such as GPT-4o, Llama3, FinGPT, and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-source Touchstone-GPT, a financial LLM trained through continual pre-training and instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks. This research provides a practical evaluation tool for financial LLMs and guides future development and optimization. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Industrial Anomalies Synthesis</title>
<link>https://arxiv.org/abs/2502.16412</link>
<guid>https://arxiv.org/abs/2502.16412</guid>
<content:encoded><![CDATA[
<div> anomaly synthesis, taxonomy, generative models, vision-language models, multimodal learning<br /><br />Summary:<br /><br />This paper provides a comprehensive review of anomaly synthesis methodologies, addressing the limitations of existing surveys that cover only select techniques without offering an overarching perspective. It examines around 40 representative methods categorized into Hand-crafted, Distribution-hypothesis-based, Generative models (GM)-based, and Vision-language models (VLM)-based approaches. The study introduces the first detailed industrial anomaly synthesis (IAS) taxonomy, overcoming the deficiencies of prior simplistic or non-formal classifications, thereby enabling structured comparisons and clearer trend identification. This taxonomy captures both methodological advances and practical relevance, serving as a foundation for future research in the field. Additionally, the work explores emerging topics such as cross-modality synthesis and the incorporation of large-scale vision-language models, which previous surveys have neglected. It discusses the integration of multimodal data in anomaly synthesis along with associated benefits, challenges, and potential research directions. By providing this roadmap, the paper aims to stimulate progress in IAS through enhanced multimodal learning techniques. Supplementary resources and curated materials related to anomaly synthesis are made available via a dedicated GitHub repository. <div>
arXiv:2502.16412v2 Announce Type: replace-cross 
Abstract: This paper comprehensively reviews anomaly synthesis methodologies. Existing surveys focus on limited techniques, missing an overall field view and understanding method interconnections. In contrast, our study offers a unified review, covering about 40 representative methods across Hand-crafted, Distribution-hypothesis-based, Generative models (GM)-based, and Vision-language models (VLM)-based synthesis. We introduce the first industrial anomaly synthesis (IAS) taxonomy. Prior works lack formal classification or use simplistic taxonomies, hampering structured comparisons and trend identification. Our taxonomy provides a fine-grained framework reflecting methodological progress and practical implications, grounding future research. Furthermore, we explore cross-modality synthesis and large-scale VLM. Previous surveys overlooked multimodal data and VLM in anomaly synthesis, limiting insights into their advantages. Our survey analyzes their integration, benefits, challenges, and prospects, offering a roadmap to boost IAS with multimodal learning. More resources are available at https://github.com/M-3LAB/awesome-anomaly-synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Text to Returns: Using Large Language Models for Mutual Fund Portfolio Optimization and Risk-Adjusted Allocation</title>
<link>https://arxiv.org/abs/2512.05907</link>
<guid>https://arxiv.org/abs/2512.05907</guid>
<content:encoded><![CDATA[
<div> Generative AI, Large Language Models, portfolio optimization, risk management, financial allocation<br /><br />Summary:<br /><br />This study explores the use of Generative AI, specifically Large Language Models (LLMs), to enhance portfolio optimization and risk management in investing. It examines how advanced LLMs such as Microsoft Phi 2, Mistral 7B, and Zypher 7B can develop practical, risk-aware investment strategies for mutual funds across various economic sectors. The approach integrates a sophisticated Retrieval-Augmented Generation (RAG) pipeline, allowing the models to access and incorporate external real-time financial data into their decision-making processes. Additionally, large-scale economic signals are fed into the models to provide context-aware advice, reflecting changes in the global economy. Among the models tested, Zypher 7B emerged as the most effective, consistently generating strategies that maximize returns while improving risk-adjusted performance. This success is attributed to its capacity to process complex relationships and contextual information effectively. The research demonstrates that GenAI-based methods significantly outperform traditional allocation techniques. By bridging Generative AI with real-world financial applications, the study establishes a foundation for developing more intelligent, efficient, and adaptable asset management solutions, offering tangible improvements for investment professionals. <div>
arXiv:2512.05907v1 Announce Type: new 
Abstract: Generative AI (GenAI) has enormous potential for improving two critical areas in investing, namely portfolio optimization (choosing the best combination of assets) and risk management (protecting those investments). Our study works at this intersection, using Large Language Models (LLMs) to upgrade how financial decisions are traditionally made. This research specifically tested how well advanced LLMs like Microsoft Phi 2, Mistral 7B, and Zypher 7B can create practical, risk-aware strategies for investing mutual funds in different sectors of the economy. Our method is sophisticated: it combines a Retrieval-Augmented Generation (RAG) pipeline, which enables the LLM to check external, real-time data with standard financial optimization methods. The model's advice is context-aware because we feed it large economic signals, like changes in the global economy. The Zypher 7B model was the clear winner. It consistently produced strategies that maximized investment returns while delivering better risk-adjusted results than the other models. Its ability to process complex relationships and contextual information makes it a highly powerful tool for financial allocation. In conclusion, our findings show that GenAI substantially improves performance over basic allocation methods. By connecting GenAI to real-world financial applications, this work lays the groundwork for creating smarter, more efficient, and more adaptable solutions for asset management professionals.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction</title>
<link>https://arxiv.org/abs/2512.05402</link>
<guid>https://arxiv.org/abs/2512.05402</guid>
<content:encoded><![CDATA[
<div> Keywords: Bitcoin mining, ASIC hardware, ROI prediction, time series classification, Transformer model<br /><br />Summary:<br /><br />1. The paper addresses the challenge of determining optimal timing for acquiring Bitcoin mining hardware, focusing on strategic purchase decisions in a volatile and rapidly evolving market environment.<br /><br />2. It formulates hardware acquisition as a time series classification problem aimed at predicting the profitability of purchasing ASIC miners within one year, categorized as profitable (ROI ≥ 1), marginal (0 < ROI < 1), or unprofitable (ROI ≤ 0).<br /><br />3. The authors propose MineROI-Net, an open-source Transformer-based architecture designed to capture multi-scale temporal patterns from mining profitability data.<br /><br />4. The model was tested on 20 different ASIC miners released between 2015 and 2024, encompassing various market regimes, and outperformed LSTM-based and TSLANet baselines.<br /><br />5. MineROI-Net achieved an accuracy of 83.7% and a macro F1-score of 83.1%, with high precision especially in detecting unprofitable (93.6%) and profitable (98.5%) periods while minimizing misclassification between these categories.<br /><br />6. The results demonstrate that MineROI-Net is a practical and data-driven tool for timing hardware purchases, which can help reduce financial risks in capital-intensive Bitcoin mining operations.<br /><br />7. The model and code have been made publicly available at https://github.com/AMAAI-Lab/MineROI-Net for community use and further development. <div>
arXiv:2512.05402v1 Announce Type: cross 
Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAWOA-XGBoost: A Novel Model for Early Prediction of Academic Potential in Computer Science Students</title>
<link>https://arxiv.org/abs/2512.04751</link>
<guid>https://arxiv.org/abs/2512.04751</guid>
<content:encoded><![CDATA[
<div> Keywords: Whale Optimization Algorithm, Nonlinear Adaptive WOA, hyperparameter optimization, XGBoost, multi-class imbalanced datasets<br /><br />Summary:  
This study addresses the limitations of the traditional Whale Optimization Algorithm (WOA), such as its limited global search ability, slow convergence, and tendency to get trapped in local optima, which hinder its performance in hyperparameter optimization for machine learning models. To overcome these challenges, a novel Nonlinear Adaptive Whale Optimization Algorithm (NAWOA) is proposed. NAWOA incorporates several innovative strategies including Good Nodes Set initialization, Leader-Followers Foraging, Dynamic Encircling Prey, Triangular Hunting, and a nonlinear convergence factor aimed at improving exploration, exploitation, and convergence stability. Extensive experiments on 23 benchmark functions demonstrate that NAWOA exhibits superior optimization performance and robustness compared to standard approaches. Building upon NAWOA, the authors develop NAWOA-XGBoost, a predictive model designed to assess academic potential. This model is evaluated using data from 495 Computer Science undergraduates at Macao Polytechnic University spanning 2009 to 2019. The NAWOA-XGBoost model significantly outperforms both traditional XGBoost and WOA-XGBoost models on critical metrics including Accuracy (0.8148), Macro F1 score (0.8101), AUC (0.8932), and G-Mean (0.8172). The results highlight the proposed model's strong adaptability and effectiveness in handling multi-class imbalanced datasets in educational data mining contexts. <div>
arXiv:2512.04751v1 Announce Type: new 
Abstract: Whale Optimization Algorithm (WOA) suffers from limited global search ability, slow convergence, and tendency to fall into local optima, restricting its effectiveness in hyperparameter optimization for machine learning models. To address these issues, this study proposes a Nonlinear Adaptive Whale Optimization Algorithm (NAWOA), which integrates strategies such as Good Nodes Set initialization, Leader-Followers Foraging, Dynamic Encircling Prey, Triangular Hunting, and a nonlinear convergence factor to enhance exploration, exploitation, and convergence stability. Experiments on 23 benchmark functions demonstrate NAWOA's superior optimization capability and robustness. Based on this optimizer, an NAWOA-XGBoost model was developed to predict academic potential using data from 495 Computer Science undergraduates at Macao Polytechnic University (2009-2019). Results show that NAWOA-XGBoost outperforms traditional XGBoost and WOA-XGBoost across key metrics, including Accuracy (0.8148), Macro F1 (0.8101), AUC (0.8932), and G-Mean (0.8172), demonstrating strong adaptability on multi-class imbalanced datasets.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Customer Identification for Electricity Retailers Based on Monthly Demand Profiles by Activity Sectors and Locations</title>
<link>https://arxiv.org/abs/2512.04776</link>
<guid>https://arxiv.org/abs/2512.04776</guid>
<content:encoded><![CDATA[
<div> Keywords: electric sector, demand profiles, customer identification, marketing strategy, key performance indicator<br /><br />Summary:<br /><br />1. The paper addresses challenges faced by retail companies in the competitive electric sector to attract the most profitable customers whose energy demand aligns with specific target profiles related to generation or cost policies.  
2. It utilizes a large dataset of several millions of monthly demand profiles from Spain, incorporating information about customers' economic sectors and locations for indirect customer identification despite anonymity of demand profiles.  
3. A key performance indicator (KPI) is defined based on the distance of each demand profile from the target profile, which serves as the main driver of the proposed marketing strategy.  
4. The combined analysis of economic activity and geographic location proves effective, allowing unique identification of about 100,000 customers and narrowing down another 300,000 customers into small identifiable sets of 10 or fewer consumers.  
5. The proposed marketing strategy is tested against random client attraction, demonstrating a significant 40% reduction in distance from the target profile when attracting 10,000 new customers, highlighting its effectiveness in improving customer targeting efforts. <div>
arXiv:2512.04776v1 Announce Type: new 
Abstract: The increasing competition in the electric sector is challenging retail companies as they must assign its commercial efforts to attract the most profitable customers. Those are whose energy demand best fit certain target profiles, which usually depend on generation or cost policies. But, even when the demand profile is available, it is in an anonymous way, preventing its association to a particular client. In this paper, we explore a large dataset containing several millions of monthly demand profiles in Spain and use the available information about the associated economic sector and location for an indirect identification of the customers. The distance of the demand profile from the target is used to define a key performance indicator (KPI) which is used as the main driver of the proposed marketing strategy. The combined use of activity and location has been revealed as a powerful tool for indirect identification of customers, as 100,000 customers are uniquely identified, while about 300,000 clients are identifiable in small sets containing 10 or less consumers. To assess the proposed marketing strategy, it has been compared to the random attraction of new clients, showing a reduction of distance from the target of 40% for 10,000 new customers.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crack detection by holomorphic neural networks and transfer-learning-enhanced genetic optimization</title>
<link>https://arxiv.org/abs/2512.04947</link>
<guid>https://arxiv.org/abs/2512.04947</guid>
<content:encoded><![CDATA[
<div> Keywords: crack detection, genetic optimization, holomorphic neural networks, transfer learning, XFEM<br /><br />Summary:<br /><br />1. This article introduces a novel crack detection strategy for 2D solids based on strain data, framing crack identification as an inverse problem solved via genetic optimization.<br /><br />2. The approach exploits holomorphic potentials to represent solutions to plane elasticity problems, which are modeled by training two holomorphic neural networks ensuring equilibrium and traction-free crack face conditions inherently.<br /><br />3. The training process is expedited since it relies solely on boundary information, avoiding complex interior constraints.<br /><br />4. Efficiency is further improved by dividing the genetic search into long-range and short-range stages, applying transfer learning during the short-range stage to speed convergence.<br /><br />5. Benchmark tests on three problems reveal the existence of an optimal number of training epochs for best performance.<br /><br />6. The method is compared against an XFEM-based crack detection approach, demonstrating a speedup of 7 to 23 times under equivalent stress-field representation accuracy.<br /><br />7. While the study focuses on a single internal crack, the framework can be extended to more general crack configurations.<br /><br />8. Overall, the integration of genetic optimization, holomorphic neural networks, and transfer learning presents a highly efficient alternative for crack detection compared to traditional numerical methods like XFEM. <div>
arXiv:2512.04947v1 Announce Type: new 
Abstract: A new strategy for detecting cracks in 2D solids based on strain data is introduced. Crack detection is formulated as an inverse problem and solved using genetic optimization. The novelty lies in the evaluation of the model response at each generation. Specifically, the solution to the corresponding plane elasticity problem is expressed via holomorphic potentials, which are determined by training two holomorphic neural networks. As the potentials satisfy equilibrium and traction-free conditions along the crack faces a priori, the training proceeds quickly based solely on boundary information. Training efficiency is further improved by splitting the genetic search into long-range and short-range stages, enabling the use of transfer learning in the latter. The new strategy is tested on three benchmark problems, showing that an optimal number of training epochs exists that provides the best overall performance. A comparison is also made with a popular crack detection approach that uses XFEM to compute the model response. Under the assumption of identical stress-field representation accuracy, the proposed method is found to be between 7 and 23 times faster than the XFEM-based approach. While the strategy is presented here for the simplified case of a single internal crack, generalization is feasible. Overall, the present findings demonstrate that combining genetic optimization with holomorphic neural networks and transfer learning offers a promising avenue for developing crack detection strategies with higher efficiency than those currently available.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partial multivariate transformer as a tool for cryptocurrencies time series prediction</title>
<link>https://arxiv.org/abs/2512.04099</link>
<guid>https://arxiv.org/abs/2512.04099</guid>
<content:encoded><![CDATA[
<div> Cryptocurrency forecasting, Partial-Multivariate Transformer, BTCUSDT, ETHUSDT, trading utility  

<br /><br />Summary:  
This paper addresses the challenge of forecasting cryptocurrency prices, which is complicated by high volatility and the trade-off between univariate models that use limited information and multivariate models that incorporate too much noise. The authors propose a partial-multivariate approach using the Partial-Multivariate Transformer (PMformer) to strike a balance by selecting a strategic subset of features for better predictive performance. They apply the PMformer to predict daily returns of BTCUSDT and ETHUSDT and compare its performance against eleven classical and deep learning models. The study has two main findings: first, the partial-multivariate strategy significantly improves statistical accuracy, showing it can effectively capture informative signals while minimizing noise. Second, the results reveal a disconnect between lower prediction errors and practical trading success, as more accurate forecasts did not reliably translate into higher financial returns in simulated trading scenarios. This discrepancy challenges the conventional reliance on typical error metrics for model evaluation and suggests the necessity of developing new evaluation criteria that better align with real-world financial goals and trading utility. <div>
arXiv:2512.04099v1 Announce Type: cross 
Abstract: Forecasting cryptocurrency prices is hindered by extreme volatility and a methodological dilemma between information-scarce univariate models and noise-prone full-multivariate models. This paper investigates a partial-multivariate approach to balance this trade-off, hypothesizing that a strategic subset of features offers superior predictive power. We apply the Partial-Multivariate Transformer (PMformer) to forecast daily returns for BTCUSDT and ETHUSDT, benchmarking it against eleven classical and deep learning models. Our empirical results yield two primary contributions. First, we demonstrate that the partial-multivariate strategy achieves significant statistical accuracy, effectively balancing informative signals with noise. Second, we experiment and discuss an observable disconnect between this statistical performance and practical trading utility; lower prediction error did not consistently translate to higher financial returns in simulations. This finding challenges the reliance on traditional error metrics and highlights the need to develop evaluation criteria more aligned with real-world financial objectives.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReVeal-MT: A Physics-Informed Neural Network for Multi-Transmitter Radio Environment Mapping</title>
<link>https://arxiv.org/abs/2512.04100</link>
<guid>https://arxiv.org/abs/2512.04100</guid>
<content:encoded><![CDATA[
<div> Keywords: radio environment mapping, multi-transmitter, Physics-Informed Neural Networks, spectrum sharing, Received Signal Strength Indicator<br /><br />Summary:<br /><br />1. The paper addresses the challenge of accurately mapping radio environments in scenarios where multiple wireless transmitters coexist, which is crucial for efficient spectrum sharing between Primary Users (PUs) and Secondary Users (SUs).<br /><br />2. Existing models struggle with performance degradation due to compounded effects like shadowing and interference from adjacent transmitters.<br /><br />3. The authors extend their prior Physics-Informed Neural Networks (PINNs) work for single-transmitter mapping by deriving a new multi-transmitter Partial Differential Equation (PDE) formulation of the Received Signal Strength Indicator (RSSI).<br /><br />4. They propose ReVeal-MT, a novel PINN that incorporates multi-source PDE residuals into the neural network’s loss function, enabling accurate reconstruction of the spectrum landscape from sparse RF sensor measurements.<br /><br />5. ReVeal-MT was validated using real-world data from the ARA wireless living lab in rural and suburban areas, outperforming standard 3GPP and ITU-R channel models and a baseline single-transmitter PINN by achieving an RMSE as low as 2.66 dB with only 45 samples over 370 square kilometers.<br /><br />6. The approach maintains low computational complexity and offers substantial improvements in accuracy under realistic multi-transmitter conditions, promising enhanced fine-grained spectrum management and coexistence strategies for PUs and SUs. <div>
arXiv:2512.04100v1 Announce Type: cross 
Abstract: Accurately mapping the radio environment (e.g., identifying wireless signal strength at specific frequency bands and geographic locations) is crucial for efficient spectrum sharing, enabling Secondary Users~(SUs) to access underutilized spectrum bands while protecting Primary Users~(PUs). While existing models have made progress, they often degrade in performance when multiple transmitters coexist, due to the compounded effects of shadowing, interference from adjacent transmitters. To address this challenge, we extend our prior work on Physics-Informed Neural Networks~(PINNs) for single-transmitter mapping to derive a new multi-transmitter Partial Differential Equation~(PDE) formulation of the Received Signal Strength Indicator~(RSSI). We then propose \emph{ReVeal-MT} (Re-constructor and Visualizer of Spectrum Landscape for Multiple Transmitters), a novel PINN which integrates the multi-source PDE residual into a neural network loss function, enabling accurate spectrum landscape reconstruction from sparse RF sensor measurements. ReVeal-MT is validated using real-world measurements from the ARA wireless living lab across rural and suburban environments, and benchmarked against 3GPP and ITU-R channel models and a baseline PINN model for a single transmitter use-case. Results show that ReVeal-MT achieves substantial accuracy gains in multi-transmitter scenarios, e.g., achieving an RMSE of only 2.66\,dB with as few as 45 samples over a 370-square-kilometer region, while maintaining low computational complexity. These findings demonstrate that ReVeal-MT significantly advances radio environment mapping under realistic multi-transmitter conditions, with strong potential for enabling fine-grained spectrum management and precise coexistence between PUs and SUs.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayes-DIC Net: Estimating Digital Image Correlation Uncertainty with Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2512.04323</link>
<guid>https://arxiv.org/abs/2512.04323</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital Image Correlation, non-uniform B-spline, dataset generation, Bayes-DIC Net, Bayesian neural network<br /><br />Summary:<br /><br />This paper presents a novel method for generating high-quality Digital Image Correlation (DIC) datasets using non-uniform B-spline surfaces. By randomly selecting control point coordinates, displacement fields reflecting a wide range of realistic scenarios are created, which facilitate the synthesis of diverse speckle pattern datasets. This technique enables the production of large-scale datasets that closely mimic real-world displacement fields, thereby improving the training and generalization capability of deep learning-based DIC models. Additionally, the authors propose a new neural network architecture named Bayes-DIC Net. This architecture uniquely extracts multi-level features during down-sampling and consolidates information through a single skip connection in up-sampling, improving efficiency. The use of lightweight convolutional blocks expands the receptive field to capture richer contextual information with minimal computational cost. Importantly, the integration of dropout modules activated during inference transforms Bayes-DIC Net into a Bayesian neural network, allowing it to output not only displacement predictions but also confidence measures. This capability enhances the network’s reliability and practical utility when working with unlabeled real-world displacement data. Overall, the paper offers innovative approaches for both dataset generation and algorithmic improvements within the domain of DIC. <div>
arXiv:2512.04323v1 Announce Type: cross 
Abstract: This paper introduces a novel method for generating high-quality Digital Image Correlation (DIC) dataset based on non-uniform B-spline surfaces. By randomly generating control point coordinates, we construct displacement fields that encompass a variety of realistic displacement scenarios, which are subsequently used to generate speckle pattern datasets. This approach enables the generation of a large-scale dataset that capture real-world displacement field situations, thereby enhancing the training and generalization capabilities of deep learning-based DIC algorithms. Additionally, we propose a novel network architecture, termed Bayes-DIC Net, which extracts information at multiple levels during the down-sampling phase and facilitates the aggregation of information across various levels through a single skip connection during the up-sampling phase. Bayes-DIC Net incorporates a series of lightweight convolutional blocks designed to expand the receptive field and capture rich contextual information while minimizing computational costs. Furthermore, by integrating appropriate dropout modules into Bayes-DIC Net and activating them during the network inference stage, Bayes-DIC Net is transformed into a Bayesian neural network. This transformation allows the network to provide not only predictive results but also confidence levels in these predictions when processing real unlabeled datasets. This feature significantly enhances the practicality and reliability of our network in real-world displacement field prediction tasks. Through these innovations, this paper offers new perspectives and methods for dataset generation and algorithm performance enhancement in the field of DIC.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions</title>
<link>https://arxiv.org/abs/2512.04480</link>
<guid>https://arxiv.org/abs/2512.04480</guid>
<content:encoded><![CDATA[
<div> Keywords: Fuzzy Logic, Decision Support System, PlayeRank, Substitution Priority, Tactical Decision-Making<br /><br />Summary:<br />1. This paper addresses the critical impact of substitution decisions in elite soccer, highlighting the limitations of traditional intuition-based or machine learning models that replicate historical biases.<br />2. It introduces a Fuzzy Logic-based Decision Support System (DSS) designed for real-time, prescriptive game management, utilizing an objective rule-based inference engine rather than predictive replication of human behavior.<br />3. The authors reformulate the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, which removes play time exposure bias for accurate intra-match player comparisons.<br />4. The system integrates this refined performance metric with physiological proxies, such as fatigue, and contextual variables including disciplinary risk modulated by tactical role, to calculate a dynamic Substitution Priority score (P final).<br />5. Validation via a case study of the 2018 FIFA World Cup Brazil vs. Belgium match demonstrated ecological validity: the model aligned with expert substitution decisions and identified high-risk scenarios missed by humans, including the “FAGNER Paradox” (defensive risk detected prior to a critical yellow card) and the “Lukaku Paradox” (isolated assist masking a performance drop).<br />6. Overall, the paper concludes Fuzzy Logic provides a transparent, explainable, and superior alternative to black-box models for optimizing real-time tactical decisions in soccer. <div>
arXiv:2512.04480v1 Announce Type: cross 
Abstract: In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the "FAGNER Paradox" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the "Lukaku Paradox", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VNS Tokamak OpenMC-Serpent Validation for Medical Isotope Studies</title>
<link>https://arxiv.org/abs/2512.04873</link>
<guid>https://arxiv.org/abs/2512.04873</guid>
<content:encoded><![CDATA[
<div> Volumetric Neutron Source, tokamak, neutronics simulation, Serpent, OpenMC  

<br /><br />Summary:  
The Volumetric Neutron Source (VNS) tokamak is designed as a fusion reactor for testing and qualifying components for future fusion power facilities and has potential applications in radioisotope production. Its geometry has been modeled using two neutronics codes, Serpent and OpenMC, to evaluate and validate the physical simulations. Analog neutron-photon coupled simulations were performed to compare vacuum vessel and blanket components between the codes. In the vacuum vessel, spatial neutron and photon flux maps were calculated and compared, while in the blanket region, detailed neutron and photon spectra, as well as reaction rates for (n,T) and (n,2n) reactions, were evaluated. Detector response comparisons showed excellent agreement in neutron flux and (n,T) reactions, good agreement for (n,2n) reactions, while photon flux presented some regional discrepancies that depended on the tracking method used in Serpent. Using hybrid tracking in Serpent resulted in about a 20% relative difference in the outboard side of the blanket, whereas delta tracking reduced this difference to less than 1%. Performance-wise, Serpent exhibited shorter computation times than OpenMC for neutron-photon coupled simulations regardless of tracking method, but took longer for neutron-only calculations. Finally, the study demonstrated the VNS’s capability in radioisotope production with an exemplary case. <div>
arXiv:2512.04873v1 Announce Type: cross 
Abstract: The Volumetric Neutron Source (VNS) tokamak is a proposed fusion reactor for testing and qualification of reactor components for future use in a fusion power facility, and has potential use for radioisotope production. The VNS geometry is modeled in the Serpent and OpenMC neutronics codes. Analog neutron-photon coupled simulations are carried out to compare the model's vacuum vessel and blanket components across codes. In the vacuum vessel, neutron and photon flux maps are calculated, while in the blanket region, neutron and photon spectra, (n,T), and (n,2n) reaction rates are calculated and compared between models. The detector response comparisons found the following: neutron flux and (n,T) reactions achieved excellent agreement, the (n,2n) detector response had good agreement, and photon flux had regional discrepancies depending on Serpent tracking used. Hybrid tracking lead to a relative difference of about 20% in the outboard side blanket, where as employment of delta tracking resulted in less than 1% relative difference. On an HPC cluster, Serpent was found to have shorter computation time than OpenMC in neutron photon coupled simulations using both hybrid tracking and delta tracking, but longer in neutron only simulations. An exemplary radioisotope production case is presented for the demonstration of additional VNS capabilities.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast and Model Based Approach for Evaluating Task-Competence of Antagonistic Continuum Arms</title>
<link>https://arxiv.org/abs/2411.00241</link>
<guid>https://arxiv.org/abs/2411.00241</guid>
<content:encoded><![CDATA[
<div> Keywords: soft robot arms, model-based design, antagonistic pneumatic actuation, task-specific performance, fast computational method  

<br /><br />Summary:  
This paper addresses the challenge of designing soft robot arms tailored for specific load and workspace requirements, which is currently hindered by the absence of effective model-based design tools. Existing models mainly support control and use parameter fitting, limiting their ability to generalize and evaluate new designs beyond their training data. To overcome this, the authors propose a novel analytical method to assess whether a proposed soft arm design can accomplish a given task. The method is informative and interpretable, introducing new metrics to quantify task performance and providing graphical visualizations of segment forces that reveal mechanical performance intuitively. Computational efficiency is a key advantage, with the approach operating over 80 times faster than conventional optimization-based methods. The formulation specifically targets antagonistic, pneumatically-driven soft arms, enabling direct and fast comparisons between antagonistic and non-antagonistic architectures. Demonstrations include example analyses highlighting the benefits and trade-offs of these design choices via the new visualization tools. While positioned as an initial step, this approach significantly advances the development of model-based tools, potentially accelerating the creation and optimization of highly capable, task-specific soft robot arms. <div>
arXiv:2411.00241v4 Announce Type: replace-cross 
Abstract: Soft robot arms have made significant progress towards completing human-scale tasks, but designing arms for tasks with specific load and workspace requirements remains difficult. A key challenge is the lack of model-based design tools, forcing advancement to occur through empirical iteration and observation. Existing models are focused on control and rely on parameter fits, which means they cannot provide general conclusions about the mapping between design and performance or the influence of factors outside the fitting data.As a first step toward model-based design tools, we introduce a novel method of analyzing whether a proposed arm design can complete desired tasks. Our method is informative, interpretable, and fast; it provides novel metrics for quantifying a proposed arm design's ability to perform a task, it yields a graphical interpretation of performance through segment forces, and computing it is over 80x faster than optimization based methods.Our formulation focuses on antagonistic, pneumatically-driven soft arms. We demonstrate our approach through example analysis, and also through consideration of antagonistic vs non-antagonistic designs. Our method enables fast, direct and task-specific comparison of these two architectures, and provides a new visualization of the comparative mechanics. While only a first step, the proposed approach will support advancement of model-based design tools, leading to highly capable soft arms.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State Transition Block Diagram of the Generalized Maxwell Slip Friction Model</title>
<link>https://arxiv.org/abs/2512.03049</link>
<guid>https://arxiv.org/abs/2512.03049</guid>
<content:encoded><![CDATA[
<div> Dynamic friction models, Generalized Maxwell Slip, block diagram, MATLAB-Simulink, stick-slip friction<br /><br />Summary: This work addresses the challenge of representing complex dynamic friction models (DFMs) which are essential for accurately simulating and controlling systems affected by friction. Traditional DFMs are often illustrated with block diagrams for clarity and reproducibility, but recent advances have made these diagrams rare due to increased complexity. The paper introduces a block diagram representation of the Generalized Maxwell Slip (GMS) friction model, a sophisticated multi-state DFM that can simulate various nonlinear friction phenomena like stick-slip behavior. The proposed block diagram is versatile and can be implemented in MATLAB-Simulink using Stateflow charts or embedded if-else logic, though it is not confined to this platform. To validate the approach, the authors conducted both closed-loop and open-loop simulations demonstrating non-drifting behavior and accurately capturing stick-slip friction effects. Benchmarking against the well-known LuGre model further verifies the model's efficacy. Overall, this contribution enhances the accessibility of advanced DFMs by providing a practical and clear tool for engineers to simulate and control systems experiencing friction, making these complex models more approachable and usable in engineering practice. <div>
arXiv:2512.03049v1 Announce Type: new 
Abstract: Dynamic friction models (DFMs) encode essential information for the simulation and control of systems with friction. Traditionally, DFMs have been published with conceptual block diagrams, promoting clarity and reproducibility in simulation. However, modern DFMs have grown increasingly complex and block diagrams are now rarely presented, limiting accessibility. This letter presents a block diagram representation of the Generalized Maxwell Slip (GMS) friction model, an advanced multi-state DFM capable of simulating a wide range of nonlinear friction phenomena. The diagram can be implemented in the MATLAB-Simulink environment using a Stateflow chart or embedded if-else logic to represent the state transition criteria, but it is not limited to this platform. Closed-loop and open-loop simulations were conducted to verify that the block diagram reproduces non-drifting behavior and stick-slip friction, including benchmarking against the LuGre model. The proposed diagram improves accessibility to advanced dynamic friction models and provides the engineering community with a practical tool for the simulation and control of systems with friction.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deteccion de intrusiones en redes mediante algoritmos de aprendizaje automatico: Un estudio multiclase sobre el conjunto de datos NSL-KDD</title>
<link>https://arxiv.org/abs/2512.03200</link>
<guid>https://arxiv.org/abs/2512.03200</guid>
<content:encoded><![CDATA[
<div> Intrusion detection, NSL-KDD dataset, machine learning, Random Forest, XGBoost  

<br /><br />Summary:  
This paper addresses the critical issue of intrusion detection in computer networks by applying classical machine learning algorithms on the multiclass NSL-KDD dataset, which includes Normal, DoS, Probe, R2L, and U2R attack categories. It begins by detailing the characteristics of the NSL-KDD dataset, including its variants and class distribution, followed by a comprehensive description of the data preprocessing steps such as cleaning, coding, and normalization. Four supervised classifiers—Logistic Regression, Decision Tree, Random Forest, and XGBoost—are implemented, and their performance is rigorously evaluated using accuracy, recall, F1 score, confusion matrix, and AUC-ROC metrics. The experiments reveal that ensemble tree-based models, Random Forest and XGBoost, significantly outperform logistic regression and standalone decision trees, achieving accuracies near 99%. Additionally, the paper analyzes each model’s efficacy in detecting different attack types, emphasizing the ongoing difficulty in identifying rare attacks like R2L and U2R. The discussion section compares these results with existing state-of-the-art approaches and outlines future research directions, specifically suggesting the use of class balancing methods and deep learning architectures to enhance detection capabilities further. <div>
arXiv:2512.03200v1 Announce Type: new 
Abstract: Intrusion detection is a critical component of cybersecurity, responsible for identifying unauthorized access or anomalous behavior in computer networks. This paper presents a comprehensive study on intrusion detection in networks using classical machine learning algorithms applied to the multiclass version of the NSL-KDD dataset (Normal, DoS, Probe, R2L, and U2R classes). The characteristics of NSL-KDD are described in detail, including its variants and class distribution, and the data preprocessing process (cleaning, coding, and normalization) is documented. Four supervised classification models were implemented: Logistic Regression, Decision Tree, Random Forest, and XGBoost, whose performance is evaluated using standard metrics (accuracy, recall, F1 score, confusion matrix, and area under the ROC curve). Experiments show that models based on tree sets (Random Forest and XGBoost) achieve the best performance, with accuracies approaching 99%, significantly outperforming logistic regression and individual decision trees. The ability of each model to detect each attack category is also analyzed, highlighting the challenges in identifying rare attacks (R2L and U2R). Finally, the implications of the results are discussed, comparing them with the state of the art, and potential avenues for future research are proposed, such as the application of class balancing techniques and deep learning models to improve intrusion detection.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating shape optimization by deep neural networks with on-the-fly determined architecture</title>
<link>https://arxiv.org/abs/2512.03555</link>
<guid>https://arxiv.org/abs/2512.03555</guid>
<content:encoded><![CDATA[
<div> Keywords: shape optimization, multi-objective evolutionary algorithms, deep neural networks, ejector design, computational acceleration<br /><br />Summary:<br /><br />1. The article addresses the challenge of computationally expensive simulations in component shape optimization, especially when a global search requires evaluating thousands of simulations.<br /><br />2. The authors propose a novel global shape optimization methodology that integrates multi-objective evolutionary algorithms with deep neural networks (DNNs) to accelerate the optimization process.<br /><br />3. Their method alternates between running simulations and training various DNN architectures on the generated data; once an effective DNN is found, it replaces the simulation for the remainder of the search.<br /><br />4. The approach was validated on five ZDT benchmark functions, demonstrating performance comparable to or exceeding existing state-of-the-art acceleration techniques.<br /><br />5. The methodology was then applied to a real-world problem: optimizing the shape of a single-phase ejector. It achieved significant computational savings, reducing weeks of CPU time compared to non-accelerated methods.<br /><br />6. To experimentally verify the approach, four optimized ejector designs were 3D printed and tested in laboratory conditions, confirming the predicted performance.<br /><br />7. The results suggest that this combined evolutionary algorithm and DNN approach can effectively accelerate other real-life shape optimization tasks. <div>
arXiv:2512.03555v1 Announce Type: new 
Abstract: In component shape optimization, the component properties are often evaluated by computationally expensive simulations. Such optimization becomes unfeasible when it is focused on a global search requiring thousands of simulations to be evaluated. Here, we present a viable global shape optimization methodology based on multi-objective evolutionary algorithms accelerated by deep neural networks (DNNs). Our methodology alternates between evaluating simulations and utilizing the generated data to train DNNs with various architectures. When a suitable DNN architecture is identified, the DNN replaces the simulation in the rest of the global search. Our methodology was tested on five ZDT benchmark functions, showing itself at the level of and sometimes more flexible than other state-of-the-art acceleration approaches. Then, it was applied to a real-life optimization problem, namely the shape optimization of a single-phase ejector. Compared with a non-accelerated methodology, ours was able to save weeks of CPU time in solving this problem. To experimentally confirm the performance of the optimized ejector shapes, four of them were 3D printed and tested on the lab scale confirming the predicted performance. This suggests that our methodology could be used for acceleration of other real-life shape optimization problems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A 3D virtual geographic environment for flood representation towards risk communication</title>
<link>https://arxiv.org/abs/2512.03839</link>
<guid>https://arxiv.org/abs/2512.03839</guid>
<content:encoded><![CDATA[
<div> Keywords: risk communication, flood modelling, 3D virtual geographic environment, parallel computation, Rhine River  

<br /><br />Summary: This article addresses the challenge of improving risk communication related to floods by developing a 3D virtual geographic environment that integrates flood modelling, parallel computation, and 3D visualization in a unified pipeline. Traditional approaches have placed heavy emphasis on specialized numerical models, which are often too complex for non-expert stakeholders to understand and utilize effectively. To overcome this, the proposed method produces an intuitive 3D flood scene that includes detailed city models, enhancing public comprehension of flood events and their spatiotemporal dynamics, particularly benefiting those without direct flood experience. The system is demonstrated through an experimental analysis focusing on a section of the Rhine River in Bonn, Germany. Results reveal that the approach can complete flood modelling and 3D representation within a few hours, and the implementation of parallel computation achieves a speedup ratio of 6.45, significantly improving processing efficiency. Furthermore, the developed environment can be embedded into the Geospatial Infrastructure Management Ecosystem (GeoIME) cloud platform, facilitating its integration into intelligent flood management systems. By combining advanced computing and visualization techniques, this approach promotes more effective flood risk communication and stakeholder engagement. <div>
arXiv:2512.03839v1 Announce Type: new 
Abstract: Risk communication seeks to develop a shared understanding of disaster among stakeholders, thereby amplifying public awareness and empowering them to respond more effectively to emergencies. However, existing studies have overemphasized specialized numerical modelling, making the professional output challenging to understand and use by non-research stakeholders. In this context, this article proposes a 3D virtual geographic environment for flood representation towards risk communication, which integrates flood modelling, parallel computation, and 3D representation in a pipeline. Finally, a section of the Rhine River in Bonn, Germany, is selected for experiment analysis. The experimental results show that the proposed approach is capable of flood modelling and 3D representation within a few hours, the parallel speedup ratio reached 6.45. The intuitive flood scene with 3D city models is beneficial for promoting flood risk communication and is particularly helpful for participants without direct experience of floods to understand its spatiotemporal process. It also can be embedded in the Geospatial Infrastructure Management Ecosystem (GeoIME) cloud application for intelligent flood systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AtomDisc: An Atom-level Tokenizer that Boosts Molecular LLMs and Reveals Structure--Property Associations</title>
<link>https://arxiv.org/abs/2512.03080</link>
<guid>https://arxiv.org/abs/2512.03080</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, molecular graphs, atom-level tokenization, structure-property association, molecular generation<br /><br />Summary:<br />1. Advances in large language models (LLMs) have significantly propelled discovery in molecular science, but integrating molecular data into their token-based frameworks remains challenging.<br />2. Molecular graphs are valuable as they explicitly capture atomic connectivity and local topology, which critically influence molecular behavior and properties.<br />3. Existing tokenization methods inadequately represent fine-grained local atomic environments, limiting the ability to model sophisticated chemical properties and reactivity.<br />4. The authors present AtomDisc, a novel framework that quantizes local atomic environments into structure-aware tokens embedded directly in LLM token spaces, enabling detailed chemical context encoding.<br />5. Experimental results demonstrate that AtomDisc effectively distinguishes meaningful structural features and reveals structure-property relationships, improving property prediction and molecular generation.<br />6. Integrating AtomDisc tokens into LLMs introduces an interpretable inductive bias, leading to state-of-the-art performance in molecular tasks.<br />7. The proposed methodology offers a promising direction for developing more powerful molecular LLMs capable of mechanistic insights and complex chemical reasoning. <div>
arXiv:2512.03080v1 Announce Type: cross 
Abstract: Advances in large language models (LLMs) are accelerating discovery in molecular science. However, adapting molecular information to the serialized, token-based processing of LLMs remains a key challenge. Compared to other representations, molecular graphs explicitly encode atomic connectivity and local topological environments, which are key determinants of atomic behavior and molecular properties. Despite recent efforts to tokenize overall molecular topology, there still lacks effective fine-grained tokenization of local atomic environments, which are critical for determining sophisticated chemical properties and reactivity. To address these issues, we introduce AtomDisc, a novel framework that quantizes atom-level local environments into structure-aware tokens embedded directly in LLM's token space. Our experiments show that AtomDisc, in a data-driven way, can distinguish chemically meaningful structural features that reveal structure-property associations. Equipping LLMs with AtomDisc tokens injects an interpretable inductive bias that delivers state-of-the-art performance on property prediction and molecular generation. Our methodology and findings can pave the way for constructing more powerful molecular LLMs aimed at mechanistic insight and complex chemical reasoning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning From Limited Data and Feedback for Cell Culture Process Monitoring: A Comparative Study</title>
<link>https://arxiv.org/abs/2512.03460</link>
<guid>https://arxiv.org/abs/2512.03460</guid>
<content:encoded><![CDATA[
<div> Keywords: bioprocess monitoring, machine learning, soft sensors, online learning, Raman spectroscopy<br /><br />Summary:  
This study addresses the challenge of real-time batch process monitoring (BPM) in cell culture bioprocessing, emphasizing the continuous tracking of key variables like viable cell density and product titer to maintain optimal growth and quality. The development of accurate soft sensors for BPM is complicated by limited historical data, infrequent feedback, heterogeneous conditions, and high-dimensional sensory inputs. The research evaluates multiple machine learning (ML) methods including feature dimensionality reduction, online learning, and just-in-time learning across one simulated and two real-world datasets. Results indicate that batch learning works well in homogeneous process settings, whereas just-in-time and online learning offer better adaptability in scenarios with limited initial data (cold-start). The study also identifies critical meta-features such as feed media composition and process control strategies that influence model transferability. Moreover, integrating Raman spectroscopy-based predictions with lagged offline measurements significantly improves monitoring accuracy. These insights provide a comprehensive benchmarking of ML approaches for bioprocess soft sensor development, highlighting important training strategies and suggesting promising future directions for robust and adaptive bioprocess monitoring systems. <div>
arXiv:2512.03460v1 Announce Type: cross 
Abstract: In cell culture bioprocessing, real-time batch process monitoring (BPM) refers to the continuous tracking and analysis of key process variables such as viable cell density, nutrient levels, metabolite concentrations, and product titer throughout the duration of a batch run. This enables early detection of deviations and supports timely control actions to ensure optimal cell growth and product quality. BPM plays a critical role in ensuring the quality and regulatory compliance of biopharmaceutical manufacturing processes. However, the development of accurate soft sensors for BPM is hindered by key challenges, including limited historical data, infrequent feedback, heterogeneous process conditions, and high-dimensional sensory inputs. This study presents a comprehensive benchmarking analysis of machine learning (ML) methods designed to address these challenges, with a focus on learning from historical data with limited volume and relevance in the context of bioprocess monitoring. We evaluate multiple ML approaches including feature dimensionality reduction, online learning, and just-in-time learning across three datasets, one in silico dataset and two real-world experimental datasets. Our findings highlight the importance of training strategies in handling limited data and feedback, with batch learning proving effective in homogeneous settings, while just-in-time learning and online learning demonstrate superior adaptability in cold-start scenarios. Additionally, we identify key meta-features, such as feed media composition and process control strategies, that significantly impact model transferability. The results also suggest that integrating Raman-based predictions with lagged offline measurements enhances monitoring accuracy, offering a promising direction for future bioprocess soft sensor development.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas</title>
<link>https://arxiv.org/abs/2512.03565</link>
<guid>https://arxiv.org/abs/2512.03565</guid>
<content:encoded><![CDATA[
<div> Keywords: Molecular Dynamics, SIMD vectorization, AutoPas, particle simulation, neighbor identification

<br /><br />Summary:  
This paper investigates the use of SIMD vectorization techniques to optimize the pairwise force calculations in Molecular Dynamics (MD) simulations within the AutoPas particle simulation library. The core focus is on how different orders of loading particle values into vector registers affect performance metrics like execution time and energy consumption. Unlike previous studies, this work explores simulation-specific parameters such as particle density and the influence of neighbor identification algorithms, which are crucial for vectorization efficiency. Recognizing that the best MD algorithm may vary during runtime, the paper extends AutoPas' existing dynamic tuning mechanism to adaptively select the optimal vectorization order in real time. Benchmark results demonstrate that dynamically considering multiple particle interaction orders during simulation execution significantly enhances force calculation performance. These improvements surpass AutoPas' prior static approach, showcasing the benefits of adaptive vectorization in particle simulations. Ultimately, this work contributes to the optimization of MD simulations by integrating runtime adaptability and simulation parameter sensitivities into vectorization strategies, improving computational efficiency and possibly reducing energy costs in high-performance computing environments. <div>
arXiv:2512.03565v1 Announce Type: cross 
Abstract: Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.
  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.
  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Loss Landscape of Powder X-Ray Diffraction-Based Structure Optimization Is Too Rough for Gradient Descent</title>
<link>https://arxiv.org/abs/2512.04036</link>
<guid>https://arxiv.org/abs/2512.04036</guid>
<content:encoded><![CDATA[
<div> X-ray diffraction, crystal structure, gradient descent, non-convex optimization, symmetry constraints<br /><br />Summary:<br /><br />This work addresses the challenge of solving crystal structures from powder X-ray diffraction (XRD) data by focusing on the inverse mapping from XRD patterns back to the atomic structure using gradient descent optimization. First, the authors identify that commonly used XRD similarity metrics create a highly non-convex optimization landscape, which makes it difficult to recover accurate structures directly through standard optimization techniques. Second, they demonstrate that restricting the search space to the correct ground-truth crystal family significantly improves the recovery success, leading to higher match rates and better alignment of structural similarity with XRD similarity measures such as mutual information and correlation scores. Third, despite these improvements, the optimization landscape may still remain non-convex along certain symmetry axes, presenting challenges for navigation during optimization. Finally, the findings suggest that incorporating symmetry-aware inductive biases into learning models holds promise in effectively guiding the inverse mapping from powder diffraction data to crystal structures. This work underscores the importance of symmetry considerations and appropriate similarity metrics for development of computational methods in crystal structure determination from powder XRD data. <div>
arXiv:2512.04036v1 Announce Type: cross 
Abstract: Solving crystal structures from powder X-ray diffraction (XRD) is a central challenge in materials characterization. In this work, we study the powder XRD-to-structure mapping using gradient descent optimization, with the goal of recovering the correct structure from moderately distorted initial states based solely on XRD similarity. We show that commonly used XRD similarity metrics result in a highly non-convex landscape, complicating direct optimization. Constraining the optimization to the ground-truth crystal family significantly improves recovery, yielding higher match rates and increased mutual information and correlation scores between structural similarity and XRD similarity. Nevertheless, the landscape may remain non-convex along certain symmetry axes. These findings suggest that symmetry-aware inductive biases could play a meaningful role in helping learning models navigate the inverse mapping from diffraction to structure.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Fluid-Structure Interaction with Physics-Informed Machine Learning and Immersed Boundary Methods</title>
<link>https://arxiv.org/abs/2505.18565</link>
<guid>https://arxiv.org/abs/2505.18565</guid>
<content:encoded><![CDATA[
<div> Keywords: Physics-informed neural networks, fluid-structure interaction, Eulerian-Lagrangian architecture, immersed boundary method, learnable B-spline activations<br /><br />Summary:<br /><br />This paper tackles the challenge of applying physics-informed neural networks (PINNs) to fluid-structure interaction (FSI) problems involving moving boundaries, an area where existing PINN approaches struggle. The authors propose an innovative Eulerian-Lagrangian PINN architecture inspired by the immersed boundary method (IBM) that separates the modeling of fluid and structural domains through domain-specific neural networks — an Eulerian network for the fluid dynamics and a Lagrangian network for the structural interface. These networks are coupled via physics-based constraints to capture the distinct governing equations accurately. A novel aspect is the integration of learnable B-spline activation functions combined with SiLU activations to effectively represent localized high-gradient features near interfaces and global flow behaviors. The approach is validated on a two-dimensional cavity flow problem with a moving solid structure. Results show that baseline unified PINNs, while capturing velocity reasonably, exhibit significant pressure prediction errors in structural regions (12.9%). The proposed Eulerian-Lagrangian architecture with learnable activations (EL-L) substantially improves accuracy across all metrics, reducing pressure errors to 2.39% and demonstrating a 24.1-91.4% accuracy improvement overall. This study highlights the importance of physics-aligned domain decomposition and locality-aware activation functions for precise FSI modeling within the PINN framework. <div>
arXiv:2505.18565v5 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) have emerged as a promising approach for solving complex fluid dynamics problems, yet their application to fluid-structure interaction (FSI) problems with moving boundaries remains largely unexplored. This work addresses the critical challenge of modeling FSI systems with moving interfaces, where traditional unified PINN architectures struggle to capture the distinct physics governing fluid and structural domains simultaneously. We present an innovative Eulerian-Lagrangian PINN architecture that integrates immersed boundary method (IBM) principles to solve FSI problems with moving boundary conditions. Our approach fundamentally departs from conventional unified architectures by introducing domain-specific neural networks: an Eulerian network for fluid dynamics and a Lagrangian network for structural interfaces, coupled through physics-based constraints. Additionally, we incorporate learnable B-spline activation functions with SiLU to capture both localized high-gradient features near interfaces and global flow patterns. Empirical studies on a 2D cavity flow problem involving a moving solid structure show that while baseline unified PINNs achieve reasonable velocity predictions, they suffer from substantial pressure errors (12.9%) in structural regions. Our Eulerian-Lagrangian architecture with learnable activations (EL-L) achieves better performance across all metrics, improving accuracy by 24.1-91.4% and particularly reducing pressure errors from 12.9% to 2.39%. These results demonstrate that domain decomposition aligned with physical principles, combined with locality-aware activation functions, is essential for accurate FSI modeling within the PINN framework.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A unified framework for equation discovery and dynamic prediction of hysteretic systems</title>
<link>https://arxiv.org/abs/2512.02408</link>
<guid>https://arxiv.org/abs/2512.02408</guid>
<content:encoded><![CDATA[
<div> Hysteresis, symbolic regression, internal variables, equation discovery, nonlinear dynamics<br /><br />Summary:<br /><br />1. Hysteresis is a nonlinear phenomenon characterized by memory effects, where the output of a system depends on both current and past states, commonly observed in physical and mechanical systems such as seismic yielding structures, ferromagnetic materials, and piezoelectric actuators.<br /><br />2. Traditional analytical models like the Bouc-Wen model are widely used but have limitations because they rely on idealized assumptions and require careful parameter tuning, restricting their usefulness for systems with unknown mechanisms or diverse hysteretic behavior.<br /><br />3. Current equation discovery methods for hysteresis are often system-specific or dependent on predefined model libraries, which limits their adaptability and ability to uncover hidden mechanisms in the data.<br /><br />4. This research proposes a unified framework combining the learning of internal hysteretic variables and symbolic regression, enabling automatic extraction of these variables and discovery of explicit governing equations directly from data without the need for predefined candidate models, contrasting with approaches like sparse identification of nonlinear dynamics (SINDy).<br /><br />5. By solving the equations discovered using this framework, one can naturally predict the dynamic responses of hysteretic systems, providing a systematic and generalizable approach for both equation discovery and characterization of hysteretic dynamics. <div>
arXiv:2512.02408v1 Announce Type: new 
Abstract: Hysteresis is a nonlinear phenomenon with memory effects, where a system's output depends on both its current state and past states. It is prevalent in various physical and mechanical systems, such as yielding structures under seismic excitation, ferromagnetic materials, and piezoelectric actuators. Analytical models like the Bouc-Wen model are often employed but rely on idealized assumptions and careful parameter calibration, limiting their applicability to diverse or mechanism-unknown behaviors. Existing equation discovery approaches for hysteresis are often system-specific or rely on predefined model libraries, which limit their flexibility and ability to capture the hidden mechanisms. To address these, this research develops a unified framework that integrates learning of internal variables (commonly used in modeling hysteresis) and symbolic regression to automatically extract internal hysteretic variable, and discover explicit governing equations directly from data without predefined libraries as required by methods such as sparse identification of nonlinear dynamics (SINDy). Solving the discovered equations naturally enables prediction of the dynamic responses of hysteretic systems. This work provides a systematic view and approach for both equation discovery and characterization of hysteretic dynamics, defining a unified framework for these types of problems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Invisible Hand: Characterizing Generative AI Adoption and its Effects on An Online Freelancing Market</title>
<link>https://arxiv.org/abs/2512.02509</link>
<guid>https://arxiv.org/abs/2512.02509</guid>
<content:encoded><![CDATA[
<div> Generative AI, Freelancer.com, ChatGPT, job market, skill demand<br /><br />Summary:<br /><br />1. Since the COVID-19 pandemic, freelancing platforms like Freelancer.com have seen substantial growth in both worker registrations and job postings. 2. The introduction and rise of generative AI (GenAI) technologies, such as ChatGPT, have prompted questions about their influence on the freelance job market. 3. Despite widespread discussion, there has been limited empirical research on how GenAI adoption impacts job demand and worker engagement in freelancing. 4. This study analyzes over 1.8 million job posts and 3.8 million users on Freelancer.com to examine the emergence of GenAI-related jobs and the dominant role of ChatGPT within this domain. 5. The research further focuses specifically on jobs related to ChatGPT, investigating the unique skill requirements and the typical tasks associated with these positions. 6. The findings provide valuable insights into how the freelance job landscape is evolving in the age of AI, highlighting changes in employment patterns, skill demands, and user behaviors driven by the integration of generative AI technologies. <div>
arXiv:2512.02509v1 Announce Type: new 
Abstract: Since the COVID-19 pandemic, freelancing platforms have experienced significant growth in both worker registrations and job postings. However, the rise of generative AI (GenAI) technologies has raised questions about how it affect the job posting in freelancer market. Despite growing discussions, there is limited empirical research on the GenAI adoption and its effect on job demand and worker engagement. We present a large-scale analysis of Freelancer.com, utilizing over 1.8 million job posts and 3.8 million users. We investigate the emergence of jobs with the adoption of GenAI and identify leading position of ChatGPT in the freelancing market. With a focus on ChatGPT related jobs, we inspect their specific skill requirements, and the tasks that workers are asked to perform. Our findings provide insights into the evolving landscape of freelancing in the age of AI, offering a comprehensive profile of GenAI's effects on employment, skills, and user behaviors in freelancing market.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Computations in Deep Learning Inference</title>
<link>https://arxiv.org/abs/2512.02550</link>
<guid>https://arxiv.org/abs/2512.02550</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Neural Networks, sparsity, inference optimization, sparse kernels, performance engineering  

<br /><br />Summary:  
This work addresses the significant computational and energy challenges posed by Deep Neural Network (DNN) inference, highlighting the crucial role of sparsity in mitigating these demands. First, it explores various forms of sparsity that can be leveraged during DNN inference to optimize performance. Second, it details how dense matrix computations are transformed into sparse kernel operations, enabling more efficient processing. Third, the paper provides an extensive bibliographic survey of state-of-the-art implementations of sparse kernels tailored for CPU and GPU architectures. Fourth, it discusses the availability of sparse datasets, emphasizing their importance in fostering sparsity research and development. Fifth, the study examines current software tools and frameworks that offer robust support for sparse computations, facilitating the adoption of sparsity methods. Finally, the authors present evaluation results comparing different implementations of key sparse matrix multiplication (SpMM) and sampled dense-dense matrix multiplication (SDDMM) kernels across CPU and GPU platforms. Overall, this paper serves as a comprehensive resource aimed at performance engineers interested in developing and deploying efficient sparse deep learning models in production environments, thus bridging the gap between research advances and practical application in AI inference optimization. <div>
arXiv:2512.02550v1 Announce Type: new 
Abstract: The computational demands of modern Deep Neural Networks (DNNs) are immense and constantly growing. While training costs usually capture public attention, inference demands are also contributing in significant computational, energy and environmental footprints. Sparsity stands out as a critical mechanism for drastically reducing these resource demands. However, its potential remains largely untapped and is not yet fully incorporated in production AI systems. To bridge this gap, this work provides the necessary knowledge and insights for performance engineers keen to get involved in deep learning inference optimization. In particular, in this work we: a) discuss the various forms of sparsity that can be utilized in DNN inference, b) explain how the original dense computations translate to sparse kernels, c) provide an extensive bibliographic review of the state-of-the-art in the implementation of these kernels for CPUs and GPUs, d) discuss the availability of sparse datasets in support of sparsity-related research and development, e) explore the current software tools and frameworks that provide robust sparsity support, and f) present evaluation results of different implementations of the key SpMM and SDDMM kernels on CPU and GPU platforms. Ultimately, this paper aims to serve as a resource for performance engineers seeking to develop and deploy highly efficient sparse deep learning models in productions.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond N-grams: A Hierarchical Reward Learning Framework for Clinically-Aware Medical Report Generation</title>
<link>https://arxiv.org/abs/2512.02710</link>
<guid>https://arxiv.org/abs/2512.02710</guid>
<content:encoded><![CDATA[
<div> Keywords: medical report generation, clinical hallucinations, hierarchical reward learning, HiMed-RL, diagnostic consistency<br /><br />Summary:<br /><br />1. Automatic medical report generation aims to reduce doctors' workload but currently suffers from reliability issues in real-world settings. 2. Existing methods produce formally fluent text but are prone to clinical hallucinations, which are factual errors that undermine trust and diagnostic accuracy. 3. HiMed-RL is introduced as a Hierarchical Medical Reward Learning Framework that explicitly prioritizes clinical quality beyond mere text matching. 4. This framework operates on three levels: token-level fluency, concept-level factual grounding aligned with expert medical knowledge, and semantic-level diagnostic consistency verified by a specialized large language model (LLM). 5. A Human-inspired Dynamic Reward Adjustment guides the model’s learning process, first focusing on mastering basic facts before moving on to complex diagnostic reasoning. 6. Experimental results demonstrate that HiMed-RL using a 3B parameter model (HiMed-3B) achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks, showing a notable 12.1% improvement over the second-best baseline, especially on out-of-domain data. 7. This work provides a robust, multi-level reward learning paradigm that enhances not only linguistic fluency but also clinical fine-grained quality in automatic medical report generation. <div>
arXiv:2512.02710v1 Announce Type: new 
Abstract: Automatic medical report generation can greatly reduce the workload of doctors, but it is often unreliable for real-world deployment. Current methods can write formally fluent sentences but may be factually flawed, introducing serious medical errors known as clinical hallucinations, which make them untrustworthy for diagnosis. To bridge this gap, we introduce HiMed-RL, a Hierarchical Medical Reward Learning Framework designed to explicitly prioritize clinical quality. HiMed-RL moves beyond simple text matching by deconstructing reward learning into three synergistic levels: it first ensures linguistic fluency at the token-level, then enforces factual grounding at the concept-level by aligning key medical terms with expert knowledge, and finally assesses high-level diagnostic consistency at the semantic-level using a specialized LLM verifier. This hierarchical reward is implemented via a Human-inspired Dynamic Reward Adjustment, a strategy which first teaches the model to learn basic facts before progressing to more complex diagnostic reasoning. Experimentally, HiMed-3B achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks, particularly on the latter, with an improvement of 12.1% over the second-best baseline. Our work provides a robust paradigm for generating reports that not only improve fluency but clinical fine-grained quality.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ada-MoGE: Adaptive Mixture of Gaussian Expert Model for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.02061</link>
<guid>https://arxiv.org/abs/2512.02061</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive Gaussian Mixture of Experts, multivariate time series, frequency distribution, spectral intensity, Gaussian band-pass filtering<br /><br />Summary:<br /><br />This paper addresses challenges in multivariate time series forecasting, particularly the issue of shifting dominant frequencies in the evolving spectral distribution of data. Traditional Mixture of Experts (MoE) models, which use a fixed number of experts, fail to dynamically adapt to changes in frequency characteristics, causing frequency coverage imbalance. This imbalance arises because too few experts may miss critical information, while too many experts can introduce noise. To overcome this, the authors propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model that dynamically determines the appropriate number of experts based on the spectral intensity and frequency response of the input data. This adaptive approach ensures that the model aligns well with the frequency distribution present in the data, preventing both information loss and noise contamination. Additionally, the model employs Gaussian band-pass filtering as a smooth method to decompose frequency domain features, avoiding noise problems associated with direct band truncation. Experimental evaluations demonstrate that Ada-MoGE attains state-of-the-art forecasting performance on six public benchmark datasets while using a compact model size of only 0.2 million parameters, highlighting its efficiency and effectiveness in handling frequency shifts in time series data. <div>
arXiv:2512.02061v1 Announce Type: cross 
Abstract: Multivariate time series forecasts are widely used, such as industrial, transportation and financial forecasts. However, the dominant frequencies in time series may shift with the evolving spectral distribution of the data. Traditional Mixture of Experts (MoE) models, which employ a fixed number of experts, struggle to adapt to these changes, resulting in frequency coverage imbalance issue. Specifically, too few experts can lead to the overlooking of critical information, while too many can introduce noise. To this end, we propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model. Ada-MoGE integrates spectral intensity and frequency response to adaptively determine the number of experts, ensuring alignment with the input data's frequency distribution. This approach prevents both information loss due to an insufficient number of experts and noise contamination from an excess of experts. Additionally, to prevent noise introduction from direct band truncation, we employ Gaussian band-pass filtering to smoothly decompose the frequency domain features, further optimizing the feature representation. The experimental results show that our model achieves state-of-the-art performance on six public benchmarks with only 0.2 million parameters.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable, Cloud-Based Simulations of Blood Flow and Targeted Drug Delivery in Retinal Capillaries</title>
<link>https://arxiv.org/abs/2512.02090</link>
<guid>https://arxiv.org/abs/2512.02090</guid>
<content:encoded><![CDATA[
<div> Keywords: cloud computing, large-scale simulations, dissipative particle dynamics, Mirheo, LAMMPS<br /><br />Summary:<br /><br />This study explores the feasibility of using public cloud computing platforms for performing large-scale, tightly-coupled simulations traditionally run on supercomputers. The research focuses on biological fluid dynamics within complex geometries, specifically simulating blood flow through image-reconstructed capillaries and investigating targeted drug delivery via artificial bacterial flagella (ABFs). Two software frameworks are utilized for these dissipative particle dynamics (DPD) simulations: Mirheo, developed by the authors, and the widely used LAMMPS package. Mirheo demonstrates impressive weak scalability up to 512 GPUs, showing its effectiveness for large GPU-based parallel simulations. LAMMPS also delivers excellent weak scalability for simulations of pure solvent systems, blood suspensions, and ABFs within retinal capillary reconstructions, maintaining over 90% weak scaling efficiency on cloud environments using up to 2,000 cores. These findings highlight that cloud infrastructures can provide competitive performance for tightly coupled scientific simulations that demand substantial computational resources. Consequently, the study suggests that cloud computing is a viable and scalable alternative to traditional supercomputing centers for meso-scale biological fluid simulations and related complex scientific computations. <div>
arXiv:2512.02090v1 Announce Type: cross 
Abstract: We investigate the capabilities of cloud computing for large-scale,tightly-coupled simulations of biological fluids in complex geometries, traditionally performed in supercomputing centers. We demonstrate scalable and efficient simulations in the public cloud. We perform meso-scale simulations of blood flow in image-reconstructed capillaries, and examine targeted drug delivery by artificial bacterial flagella (ABFs). The simulations deploy dissipative particle dynamics (DPD) with two software frameworks, Mirheo (developed by our team) and LAMMPS. Mirheo exhibits remarkable weak scalability for up to 512 GPUs. Similarly, LAMMPS demonstrated excellent weak scalability for pure solvent as well as for blood suspensions and ABFs in reconstructed retinal capillaries. In particular, LAMMPS maintained weak scaling above 90% on the cloud for up to 2,000 cores. Our findings demonstrate that cloud computing can support tightly coupled, large-scale scientific simulations with competitive performance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orchestration Framework for Financial Agents: From Algorithmic Trading to Agentic Trading</title>
<link>https://arxiv.org/abs/2512.02227</link>
<guid>https://arxiv.org/abs/2512.02227</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithmic trading, financial agents, orchestration framework, stock trading, BTC trading<br /><br />Summary:<br /><br />This paper presents an orchestration framework for financial AI agents designed to democratize financial intelligence for the general public. The framework maps components of traditional algorithmic trading systems to different specialized agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. The authors validate their approach with two in-house trading experiments: one involving stock trading using hourly data from April 2024 to December 2024, and the other involving BTC trading with minute data from July 27, 2025, to August 13, 2025. In the stock trading task, the approach achieved a return of 20.42%, a Sharpe ratio of 2.63, and a maximum drawdown of -3.59%, outperforming the S&amp;P 500 index which yielded a return of 15.97%. For the BTC trading task, the system achieved a return of 8.39%, a Sharpe ratio of 0.38, and a maximum drawdown of -2.80%, compared to the BTC price increase of 3.80%. The promising results demonstrate the potential of agent-based orchestration systems in financial markets. The authors provide open access to their code on GitHub to encourage further research and adoption. <div>
arXiv:2512.02227v1 Announce Type: cross 
Abstract: The financial market is a mission-critical playground for AI agents due to its temporal dynamics and low signal-to-noise ratio. Building an effective algorithmic trading system may require a professional team to develop and test over the years. In this paper, we propose an orchestration framework for financial agents, which aims to democratize financial intelligence to the general public. We map each component of the traditional algorithmic trading system to agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. We present two in-house trading examples. For the stock trading task (hourly data from 04/2024 to 12/2024), our approach achieved a return of $20.42\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\%$, while the S&amp;P 500 index yielded a return of $15.97\%$. For the BTC trading task (minute data from 27/07/2025 to 13/08/2025), our approach achieved a return of $8.39\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\%$, whereas the BTC price increased by $3.80\%$. Our code is available on \href{https://github.com/Open-Finance-Lab/AgenticTrading}{GitHub}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanistic Modeling of Continuous Lyophilization for Biopharmaceutical Manufacturing</title>
<link>https://arxiv.org/abs/2409.06251</link>
<guid>https://arxiv.org/abs/2409.06251</guid>
<content:encoded><![CDATA[
<div> Lyophilization, continuous manufacturing, mechanistic model, pharmaceutical process, process optimization<br /><br />Summary:<br />1. This article introduces the first mechanistic model that comprehensively describes the entire continuous lyophilization process, which includes freezing, primary drying, and secondary drying steps.<br />2. It focuses on a state-of-the-art lyophilization technology where vials are continuously suspended and moved through the process, departing from traditional batch operations dominant in the pharmaceutical industry.<br />3. The model accurately predicts critical parameters such as product temperature, ice/water fraction, sublimation front position, and bound water concentration throughout the process, enhancing understanding and control.<br />4. Several practical applications are demonstrated, highlighting the model’s potential for process design and optimization in continuous lyophilization manufacturing.<br />5. The model is released as an open-source software package named ContLyo in both MATLAB and Julia, enabling researchers and manufacturers to apply it for guiding the development of future continuous lyophilization processes, especially pertinent for stabilizing drug products like mRNA vaccines. <div>
arXiv:2409.06251v3 Announce Type: replace 
Abstract: Lyophilization (aka freeze drying) is a typical process in pharmaceutical manufacturing used for improving the stability of various drug products, including its recent applications to mRNA vaccines. While extensive efforts have been dedicated to shifting the pharmaceutical industry toward continuous manufacturing, the majority of industrial-scale lyophilization is still being operated in a batch mode. This article presents the first mechanistic model for a complete continuous lyophilization process, which comprehensively incorporates and describes key transport phenomena in all three steps of lyophilization, namely freezing, primary drying, and secondary drying. The proposed model considers the state-of-the-art lyophilization technology, in which vials are suspended and move continuously through the process. The validated model can accurately predict the evolution of critical process parameters, including the product temperature, ice/water fraction, sublimation front position, and concentration of bound water, for the entire process. Several applications related to model-based process design and optimization of continuous lyophilization are also demonstrated. The final model is made available in MATLAB and Julia as an open-source software package called ContLyo, which can ultimately be leveraged for guiding the design and development of future continuous lyophilization processes.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Task Temporal Fusion Transformer for Joint Sales and Inventory Forecasting in Amazon E-Commerce Supply Chain</title>
<link>https://arxiv.org/abs/2512.00370</link>
<guid>https://arxiv.org/abs/2512.00370</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Task Learning, Temporal Fusion Transformer, Sales Forecasting, Inventory Management, E-commerce  

<br /><br />Summary: Efficient inventory management and accurate sales forecasting are essential challenges for large-scale e-commerce platforms like Amazon, where stockouts and overstocking cause significant financial and operational issues. Traditional single-task forecasting models, focusing only on sales or inventory, fail to capture complex temporal dependencies and cross-task interactions in supply chain dynamics. To overcome these limitations, this study introduces a Multi-Task Temporal Fusion Transformer (TFT-MTL) framework for joint sales and inventory forecasting within the Amazon ecosystem. The model incorporates diverse data sources—including historical sales, inventory levels, pricing, promotions, and event-driven factors like holidays and Prime Day—within a unified deep learning architecture. A shared encoder learns long-term temporal patterns, while task-specific decoder heads predict sales volume, inventory turnover, and stockout probability simultaneously. Experiments on large-scale real-world datasets demonstrate that TFT-MTL outperforms baseline methods such as LSTM, GRU, and single-task TFT. Compared to single-task TFT, the proposed model achieves a 6.2% reduction in Sales RMSE, 12.7% decrease in Sales MAPE, 6.4% reduction in Inventory RMSE, and 12.4% decrease in Inventory MAPE. These results confirm the model's effectiveness in capturing multi-dimensional dependencies across supply chain variables. Overall, the framework provides an interpretable, data-driven decision support tool that helps optimize Amazon’s inventory scheduling and demand planning strategies. <div>
arXiv:2512.00370v1 Announce Type: new 
Abstract: Efficient inventory management and accurate sales forecasting are critical challenges in large-scale e-commerce platforms such as Amazon, where stockouts and overstocking can lead to substantial financial losses and operational inefficiencies. Traditional single-task forecasting models, which focus solely on sales or inventory, often fail to capture the complex temporal dependencies and cross-task interactions that characterize real-world supply chain dynamics. To address this limitation, this study proposes a Multi-Task Temporal Fusion Transformer (TFT-MTL) framework designed for joint sales and inventory forecasting within the Amazon e-commerce ecosystem. The model integrates heterogeneous data sources, including historical sales records, warehouse inventory levels, pricing, promotions, and event-driven factors such as holidays and Prime Day campaigns, through a unified deep learning architecture. A shared encoder captures long-term temporal patterns, while task-specific decoder heads predict sales volume, inventory turnover, and stockout probability simultaneously. Experiments on large-scale real-world datasets demonstrate that the proposed TFT-MTL model significantly outperforms baseline methods such as LSTM, GRU, and single-task TFT. Compared with the single-task TFT model, the proposed approach achieves a 6.2% reduction in Sales RMSE, a 12.7% decrease in Sales MAPE, a 6.4% reduction in Inventory RMSE, and a 12.4% decrease in Inventory MAPE. These results confirm the model's ability to effectively capture multi-dimensional dependencies across supply chain variables. The proposed framework provides an interpretable, data-driven decision support tool for optimizing Amazon's inventory scheduling and demand planning strategies.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPU-native Embedding of Complex Geometries in Adaptive Octree Grids Applied to the Lattice Boltzmann Method</title>
<link>https://arxiv.org/abs/2512.01251</link>
<guid>https://arxiv.org/abs/2512.01251</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive mesh refinement, GPU algorithm, solid voxelization, lattice Boltzmann method, near-wall refinement<br /><br />Summary:<br /><br />This article presents a GPU-native algorithm designed to integrate stationary triangle-mesh geometries into block-structured forest-of-octrees grids for computational fluid dynamics (CFD). The approach performs both solid voxelization and automated near-wall mesh refinement directly on the GPU, thereby enhancing efficiency by avoiding CPU-GPU synchronization. A key innovation is the use of local ray casting accelerated through a hierarchy of spatial bins, enabling efficient grid-block traversal without relying on index orderings or hash tables common in CPU-based methods. This results in coalesced memory access and improved computational performance. To support accurate bounce-back boundary conditions for the lattice Boltzmann method (LBM), the authors construct a flattened lookup table encoding cut-link distances between fluid and solid cells. The method is implemented as an extension of the AGAL GPU-based AMR framework. Benchmarking is conducted on complex geometries—the Stanford Bunny (112K triangles) and the high-resolution XYZ RGB Dragon (7.2M triangles)—demonstrating scalability and robustness. Validation cases include external flows past 2D circular and square cylinders at Reynolds number 100, and 3D spherical flows at Reynolds numbers 10, 15, and 20. Results confirm that the proposed geometry handling introduces only modest computational overhead while achieving accurate force prediction and stable near-wall resolution on adaptive Cartesian grids. The methodology is broadly applicable to other explicit solvers needing GPU-resident geometry embeddings. <div>
arXiv:2512.01251v1 Announce Type: new 
Abstract: Adaptive mesh refinement (AMR) reduces computational costs in CFD by concentrating resolution where needed, but efficiently embedding complex, non-aligned geometries on GPUs remains challenging. We present a GPU-native algorithm for incorporating stationary triangle-mesh geometries into block-structured forest-of-octrees grids, performing both solid voxelization and automated near-wall refinement entirely on the device. The method employs local ray casting accelerated by a hierarchy of spatial bins, leveraging efficient grid-block traversal to eliminate the need for index orderings and hash tables commonly used in CPU pipelines, and enabling coalesced memory access without CPU-GPU synchronization. A flattened lookup table of cut-link distances between fluid and solid cells is constructed to support accurate interpolated bounce-back boundary conditions for the lattice Boltzmann method (LBM). We implement this approach as an extension of the AGAL framework for GPU-based AMR and benchmark the geometry module using the Stanford Bunny (112K triangles) and XYZ RGB Dragon (7.2M triangles) models from the Stanford 3D Scanning Repository. The extended solver is validated for external flows past a circular/square cylinder (2D, $Re = 100$), and a sphere (3D, $\text{Re}\in\{10, 15, 20\}$). Results demonstrate that geometry handling and interpolation impose modest overhead while delivering accurate force predictions and stable near-wall resolution on adaptive Cartesian grids. The approach is general and applicable to other explicit solvers requiring GPU-resident geometry embedding.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Artificial Intelligence and Mixed Integer Linear Programming: Explainable Graph-Based Instance Space Analysis in Air Transportation</title>
<link>https://arxiv.org/abs/2512.01698</link>
<guid>https://arxiv.org/abs/2512.01698</guid>
<content:encoded><![CDATA[
<div> AI, MILP, Graph Neural Networks, Crew Scheduling, Explainability<br /><br />Summary:<br /><br />This paper explores the integration of artificial intelligence with mixed integer linear programming (MILP) to solve complex optimization problems in air transportation, focusing on enhancing explainability. It validates the use of Graph Neural Networks (GNNs) to extract structural feature embeddings directly from MILP problem instances, demonstrated through the air05 crew scheduling problem. The MILP problem is converted into a heterogeneous bipartite graph that models variable-constraint relationships accurately. Two neural architectures, Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT), are trained to generate node embeddings representing variables and constraints. The study employs Instance Space Analysis (ISA) with dimensionality reduction techniques—both linear (PCA) and non-linear (UMAP, t-SNE)—to visualize the embedding space. Results show PCA is insufficient to reveal meaningful clusters, whereas non-linear methods uncover the embedding topology effectively. The GCN architecture outperforms GAT by producing well-defined clusters and capturing global topology for both variables and constraints, while GAT struggles with constraint space organization. The findings suggest that simpler graph architectures like GCN can effectively represent sparse problem topologies without manual feature engineering, improving explainability. This structural insight forms a solid foundation for developing Learning to Optimize (L2O) agents aimed at enhancing MILP solver performance in safety-critical domains like aviation logistics. <div>
arXiv:2512.01698v1 Announce Type: new 
Abstract: This paper analyzes the integration of artificial intelligence (AI) with mixed integer linear programming (MILP) to address complex optimization challenges in air transportation with explainability. The study aims to validate the use of Graph Neural Networks (GNNs) for extracting structural feature embeddings from MILP instances, using the air05 crew scheduling problem. The MILP instance was transformed into a heterogeneous bipartite graph to model relationships between variables and constraints. Two neural architectures, Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) were trained to generate node embeddings. These representations were evaluated using Instance Space Analysis (ISA) through linear (PCA) and non-linear (UMAP, t-SNE) dimensionality reduction techniques. Analysis revealed that PCA failed to distinguish cluster structures, necessitating non-linear reductions to visualize the embedding topology. The GCN architecture demonstrated superior performance, capturing global topology with well-defined clusters for both variables and constraints. In contrast, the GAT model failed to organize the constraint space. The findings confirm that simpler graph architectures can effectively map the sparse topology of aviation logistics problems without manual feature engineering, contributing to explainability of instance complexity. This structural awareness provides a validated foundation for developing future Learning to Optimize (L2O) agents capable of improving solver performance in safety-critical environments.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation</title>
<link>https://arxiv.org/abs/2512.00639</link>
<guid>https://arxiv.org/abs/2512.00639</guid>
<content:encoded><![CDATA[
<div> Keywords: thyroid cancer, thyroid nodule segmentation, YOLOv5, ultrasound imaging, doppler images  

<br /><br />Summary:  
This study addresses the growing incidence of thyroid cancer worldwide by enhancing computer-aided detection methods through accurate segmentation of thyroid nodules on ultrasound images. Focusing specifically on instance segmentation, the researchers utilized various YOLOv5 algorithm variants—Nano, Small, Medium, Large, and XLarge—to analyze two datasets, one including doppler images and one without. The YOLOv5-Large model demonstrated the best results, achieving a dice score of 91% and a mean Average Precision (mAP) of 0.87 on the dataset containing doppler images. A key finding was that the inclusion of doppler images, which are often disregarded by physicians, significantly improved segmentation accuracy across all model variants. For context, the YOLOv5-Small model reached a 79% dice score without doppler images, underscoring the benefit of incorporating these images. Overall, the study highlights that instance segmentation using YOLOv5 offers an effective, real-time solution for thyroid nodule detection. These advances have promising implications for developing automated diagnostic systems aimed at improving clinical decision support and aiding early detection of thyroid cancer. <div>
arXiv:2512.00639v1 Announce Type: cross 
Abstract: The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finite Operator Learning: Bridging Neural Operators and Numerical Methods for Efficient Parametric Solution and Optimization of PDEs</title>
<link>https://arxiv.org/abs/2407.04157</link>
<guid>https://arxiv.org/abs/2407.04157</guid>
<content:encoded><![CDATA[
<div> Neural operators, physics-informed learning, finite element method, PDE solving, gradient-based optimization<br /><br />Summary:<br /><br />1. The paper presents a novel method called Finite Operator Learning (FOL) that integrates neural operators, physics-informed machine learning, and classical numerical methods to solve partial differential equations (PDEs) within a unified framework. <br />2. FOL parametrically solves PDEs without requiring training data, producing accurate sensitivities—derivatives of solution space relative to design space—enabling efficient gradient-based optimization.<br />3. Unlike traditional adjoint methods, the proposed approach avoids sensitivity analysis scaling issues, providing direct access to sensitivities through the neural network’s tangent matrix.<br />4. The method employs a simple feed-forward neural network to map discrete parameter inputs to discrete solution outputs on arbitrary shaped domains, ensuring physical law compliance via loss functions incorporating discretized governing equations.<br />5. The discretization leverages the Finite Element Method (FEM) for field approximations, and Sobolev training minimizes a multi-objective loss including energy functional residuals, boundary condition violations, and stationarity with respect to design variables.<br />6. The approach is demonstrated on steady-state heat conduction in heterogeneous materials with high phase contrast and potentially temperature-dependent conductivity.<br />7. Results show the network can be used directly for gradient-based optimization to enhance microstructure heat transfer properties efficiently. <div>
arXiv:2407.04157v3 Announce Type: replace 
Abstract: We introduce a method that combines neural operators, physics-informed machine learning, and standard numerical methods for solving PDEs. The proposed approach extends each of the aforementioned methods and unifies them within a single framework. We can parametrically solve partial differential equations in a data-free manner and provide accurate sensitivities, meaning the derivatives of the solution space with respect to the design space. These capabilities enable gradient-based optimization without the typical sensitivity analysis costs, unlike adjoint methods that scale directly with the number of response functions. Our Finite Operator Learning (FOL) approach uses an uncomplicated feed-forward neural network model to directly map the discrete design space (i.e. parametric input space) to the discrete solution space (i.e. finite number of sensor points in the arbitrary shape domain) ensuring compliance with physical laws by designing them into loss functions. The discretized governing equations, as well as the design and solution spaces, can be derived from any well-established numerical techniques. In this work, we employ the Finite Element Method (FEM) to approximate fields and their spatial derivatives. Subsequently, we conduct Sobolev training to minimize a multi-objective loss function, which includes the discretized weak form of the energy functional, boundary conditions violations, and the stationarity of the residuals with respect to the design variables. Our study focuses on the steady-state heat equation within heterogeneous materials that exhibits significant phase contrast and possibly temperature-dependent conductivity. The network's tangent matrix is directly used for gradient-based optimization to improve the microstructure's heat transfer characteristics. ...
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Optimization: A Perspective on AI-Enhanced Problem Solving in Engineering</title>
<link>https://arxiv.org/abs/2412.13281</link>
<guid>https://arxiv.org/abs/2412.13281</guid>
<content:encoded><![CDATA[
<div> Keywords: optimization, generative artificial intelligence, engineering tools, hybrid algorithms, problem-solving<br /><br />Summary: The article discusses the evolving landscape of engineering problem-solving tools, emphasizing the traditional role of optimization as a robust, proven method for identifying highly optimal solutions. It highlights the rapid rise of generative artificial intelligence (GenAI) as a versatile and promising tool capable of inferring problem requirements, bridging different solution domains, handling mixed data types, and generating vast numbers of potential solutions quickly. The authors explain how the complementary strengths of optimization and GenAI have paved the way for hybrid 'generative optimization' algorithms, representing an emerging paradigm in engineering applications. These hybrid approaches leverage the precision of optimization and the creativity and flexibility of GenAI to address complex engineering challenges more effectively. The paper offers a perspective on the current state of these methods, areas where generative optimization shows significant promise, and presents key research questions to guide future development. The authors anticipate that ongoing advancements in generative optimization will significantly influence the computational tools engineers use, potentially transforming problem-solving approaches across various engineering disciplines. <div>
arXiv:2412.13281v2 Announce Type: replace 
Abstract: The field of engineering is shaped by the tools and methods used to solve problems. Optimization is one such class of powerful, robust, and effective engineering tools proven over decades of use. Within just a few years, generative artificial intelligence (GenAI) has risen as another promising tool for general-purpose problem-solving. While optimization shines at precisely identifying highly-optimal solutions, GenAI excels at inferring problem requirements, bridging solution domains, handling mixed data modalities, and rapidly generating copious numbers of solutions. These differing attributes also make the two frameworks complementary. Hybrid `generative optimization' algorithms have gained traction across a few engineering applications and now comprise an emerging paradigm for engineering problem-solving. We expect significant developments in the near future around generative optimization, leading to changes in how engineers solve problems using computational tools. We offer our perspective on existing methods, areas of promise, and key research questions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual semantics and machine learning methods for data product pricing</title>
<link>https://arxiv.org/abs/2511.22185</link>
<guid>https://arxiv.org/abs/2511.22185</guid>
<content:encoded><![CDATA[
<div> Keywords: data product pricing, text representation, machine learning, SHAP analysis, feature importance<br /><br />Summary:<br /><br />This study addresses the challenge of pricing data products on trading platforms to maximize revenue and support market growth, emphasizing the underexplored value of textual semantics in pricing. It employs five text representation techniques—including Word2Vec, Bag-of-Words, and TF-IDF—to encode the descriptive text of data products. Six machine learning models—linear regression, neural networks, decision trees, support vector machines, random forests, and XGBoost—are then used to predict prices. The research involves two tasks: regression to predict continuous prices and classification to categorize prices into ordered tiers. Experimental data from the AWA Data Exchange reveal that Word2Vec, capturing semantic similarity, outperforms others in continuous price prediction, while simpler methods like Bag-of-Words and TF-IDF excel in classification tasks. To understand feature contributions, the study applies mRMR feature selection and SHAP interpretability techniques. SHAP analysis indicates that semantic features related to healthcare and demographics tend to increase prices, whereas those linked to weather and environmental topics generally correspond to lower prices. Overall, this framework improves predictive accuracy and enhances the interpretability of pricing models, providing valuable insights into the influence of textual features on data product valuation. <div>
arXiv:2511.22185v1 Announce Type: new 
Abstract: Reasonable pricing of data products enables data trading platforms to maximize revenue and foster the growth of the data trading market. The textual semantics of data products are vital for pricing and contain significant value that remains largely underexplored. Therefore, to investigate how textual features influence data product pricing, we employ five prevalent text representation techniques to encode the descriptive text of data products. And then, we employ six machine learning methods to predict data product prices, including linear regression, neural networks, decision trees, support vector machines, random forests, and XGBoost. Our empirical design consists of two tasks: a regression task that predicts the continuous price of data products, and a classification task that discretizes price into ordered categories. Furthermore, we conduct feature importance analysis by the mRMR feature selection method and SHAP-based interpretability techniques. Based on empirical data from the AWA Data Exchange, we find that for predicting continuous prices, Word2Vec text representations capturing semantic similarity yield superior performance. In contrast, for price-tier classification tasks, simpler representations that do not rely on semantic similarity, such as Bag-of-Words and TF-IDF, perform better. SHAP analysis reveals that semantic features related to healthcare and demographics tend to increase prices, whereas those associated with weather and environmental topics are linked to lower prices. This analytical framework significantly enhances the interpretability of pricing models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maritime Activities Observed Through Open-Access Positioning Data: Moving and Stationary Vessels in the Baltic Sea</title>
<link>https://arxiv.org/abs/2511.23016</link>
<guid>https://arxiv.org/abs/2511.23016</guid>
<content:encoded><![CDATA[
<div> Automatic Identification System, maritime activity, vessel density, Baltic Sea, data reconstruction<br /><br />Summary:<br /><br />1. The study focuses on understanding maritime activity patterns in the Baltic Sea using publicly available Automatic Identification System (AIS) data collected between August and October 2024.<br />2. The authors develop cleansing and reconstruction methods to improve the quality of open AIS data despite limitations such as incomplete receiver coverage and data quality issues.<br />3. They introduce a journey model that translates AIS message data into quantitative metrics including vessel counts, traffic estimates, and spatially resolved vessel density maps with approximately 400-meter resolution.<br />4. Vessel activity is categorized into moving and stationary states, with counts provided alongside associated uncertainties.<br />5. The analysis reveals that on average over 4,000 vessels operate simultaneously in the Baltic Sea, with over 300 vessels entering or leaving daily.<br />6. Vessel density maps facilitate the identification of key port locations and highlight the busiest coastal areas in the region.<br />7. The findings demonstrate high accuracy and align within 20% of previous studies that used proprietary data sources, supporting the viability of open AIS data for maritime traffic analysis and monitoring.<br /><br /> <div>
arXiv:2511.23016v1 Announce Type: new 
Abstract: Understanding past and present maritime activity patterns is critical for navigation safety, environmental assessment, and commercial operations. An increasing number of services now openly provide positioning data from the Automatic Identification System (AIS) via ground-based receivers. We show that coastal vessel activity can be reconstructed from open access data with high accuracy, even with limited data quality and incomplete receiver coverage. For three months of open AIS data in the Baltic Sea from August to October 2024, we present (i) cleansing and reconstruction methods to improve the data quality, and (ii) a journey model that converts AIS message data into vessel counts, traffic estimates, and spatially resolved vessel density at a resolution of $\sim$400 m. Vessel counts are provided, along with their uncertainties, for both moving and stationary activity. Vessel density maps also enable the identification of port locations, and we infer the most crowded and busiest coastal areas in the Baltic Sea. We find that on average, $\gtrsim$4000 vessels simultaneously operate in the Baltic Sea, and more than 300 vessels enter or leave the area each day. Our results agree within 20\% with previous studies relying on proprietary data.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative convergence in phase-field brittle fracture computations: exact line search is all you need</title>
<link>https://arxiv.org/abs/2511.23064</link>
<guid>https://arxiv.org/abs/2511.23064</guid>
<content:encoded><![CDATA[
<div> Keywords: variational phase-field, brittle fracture, alternate minimization, Newton's method, line search algorithm  

<br /><br />Summary:  
This paper addresses the challenge of solving variational phase-field models of brittle fracture, which result in a local constrained minimization problem involving a non-convex energy functional. The discrete formulation typically employs alternate minimization, leveraging the separate convexity with respect to two unknown fields, ensuring convergence if subproblems are successfully solved. However, the nonlinear nature of the energy functional can cause iterative solvers to fail within one or both subproblems. To overcome this, the authors propose an exact line search algorithm based on bisection, which, under specific assumptions, guarantees global convergence of Newton’s method for each subproblem. This ensures reliable identification of critical points in the energy landscape via the alternate minimization scheme. The method's robustness and efficiency are validated by benchmark tests involving different strain energy decompositions and enforcement strategies for the irreversibility constraint. These tests are carried out in both two and three-dimensional settings. Results demonstrate that the proposed line search approach improves convergence reliability and compares favorably to other commonly employed algorithms, enhancing the practical applicability of phase-field fracture models in complex simulations. <div>
arXiv:2511.23064v1 Announce Type: new 
Abstract: Variational phase-field models of brittle fracture pose a local constrained minimization problem of a non-convex energy functional. In the discrete setting, the problem is most often solved by alternate minimization, exploiting the separate convexity of the energy with respect to the two unknowns. This approach is theoretically guaranteed to converge, provided each of the individual subproblems is solved successfully. However, strong non-linearities of the energy functional may lead to failure of iterative convergence within one or both subproblems. In this paper, we propose an exact line search algorithm based on bisection, which (under certain conditions) guarantees global convergence of Newton's method for each subproblem and consequently the successful determination of critical points of the energy through the alternate minimization scheme. Through several benchmark tests computed with various strain energy decompositions and two strategies for the enforcement of the irreversibility constraint in two and three dimensions, we demonstrate the robustness of the approach and assess its efficiency in comparison with other commonly used line search algorithms.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Startup-VC Fund Matches with Structural Embeddings and Temporal Investment Data</title>
<link>https://arxiv.org/abs/2511.23364</link>
<guid>https://arxiv.org/abs/2511.23364</guid>
<content:encoded><![CDATA[
<div> Keywords: startup inclusion prediction, venture capital, Node2Vec, multihead attention, LSTM  

<br /><br />Summary: This study introduces a novel method to predict whether a venture capital (VC) fund will invest in a particular startup by estimating the inclusion probability. Unlike traditional recommendation systems that rank multiple candidates, the problem is formulated as a binary classification for each fund-startup pair. Each startup is represented using a combination of textual, numerical, and structural features. Structural information is captured through Node2Vec embeddings that represent the network context of startups, while multihead attention mechanisms are applied to effectively fuse diverse feature types. The investment history of each VC fund is modeled as sequential data encoded by an LSTM network to account for temporal dynamics in investment behavior. The approach was validated with experiments on a dataset of Japanese startups, showing improved prediction accuracy over a static baseline method. Results highlight that integrating structural features and temporal modeling of investment sequences significantly enhances the compatibility assessment between funds and startups. This suggests that capturing network context and investment timing are critical for more precise startup inclusion predictions in venture capital investing. The study contributes to the intersection of machine learning and finance by advancing automated decision-making tools tailored to VC investment choices. <div>
arXiv:2511.23364v1 Announce Type: new 
Abstract: This study proposes a method for predicting startup inclusion, estimating the probability that a venture capital fund will invest in a given startup. Unlike general recommendation systems, which typically rank multiple candidates, our approach formulates the problem as a binary classification task tailored to each fund-startup pair. Each startup is represented by integrating textual, numerical, and structural features, with Node2Vec capturing network context and multihead attention enabling feature fusion. Fund investment histories are encoded as LSTM based sequences of past investees.
  Experiments on Japanese startup data demonstrate that the proposed method achieves higher accuracy than a static baseline. The results indicate that incorporating structural features and modeling temporal investment dynamics are effective in capturing fund-startup compatibility.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dissecting the Ledger: Locating and Suppressing "Liar Circuits" in Financial Large Language Models</title>
<link>https://arxiv.org/abs/2511.21756</link>
<guid>https://arxiv.org/abs/2511.21756</guid>
<content:encoded><![CDATA[
<div> arithmetic hallucination, causal tracing, GPT-2 XL, financial QA, intrinsic detection<br /><br />Summary: Large Language Models (LLMs), particularly GPT-2 XL, face persistent and reproducible hallucinations when performing arithmetic tasks in high-stakes financial domains. Current approaches generally treat the model as a black box without insight into the underlying process. This work employs Causal Tracing on GPT-2 XL using the ConvFinQA benchmark to dissect the model’s internal arithmetic reasoning mechanism. The study reveals a two-stage mechanism: an intermediate “computational scratchpad” distributed across middle layers (L12-L30) where arithmetic computations occur, followed by a decisive aggregation circuit localized in late layers, especially Layer 46. An ablation study validates this architecture by showing that suppressing Layer 46 decreases the model's confidence in hallucinatory arithmetic outputs by 81.8%, highlighting this layer’s critical role in producing deceptive answers. Additionally, training a linear probe on Layer 46 captures this arithmetic deception signature, achieving 98% accuracy in detecting hallucinations on previously unseen financial topics. This suggests a universal geometric representation of arithmetic errors within the model, opening pathways for intrinsic hallucination detection and explainable mitigation methods rather than opaque black-box fixes. <div>
arXiv:2511.21756v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes financial domains, yet they suffer from specific, reproducible hallucinations when performing arithmetic operations. Current mitigation strategies often treat the model as a black box. In this work, we propose a mechanistic approach to intrinsic hallucination detection. By applying Causal Tracing to the GPT-2 XL architecture on the ConvFinQA benchmark, we identify a dual-stage mechanism for arithmetic reasoning: a distributed computational scratchpad in middle layers (L12-L30) and a decisive aggregation circuit in late layers (specifically Layer 46). We verify this mechanism via an ablation study, demonstrating that suppressing Layer 46 reduces the model's confidence in hallucinatory outputs by 81.8%. Furthermore, we demonstrate that a linear probe trained on this layer generalizes to unseen financial topics with 98% accuracy, suggesting a universal geometry of arithmetic deception.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Risk-Adjusted Intelligence Dividend: A Quantitative Framework for Measuring AI Return on Investment Integrating ISO 42001 and Regulatory Exposure</title>
<link>https://arxiv.org/abs/2511.21975</link>
<guid>https://arxiv.org/abs/2511.21975</guid>
<content:encoded><![CDATA[
<div> Keywords: AI investment, risk management, algorithmic failures, regulatory compliance, financial framework  

<br /><br />Summary:  
This research addresses the challenge organizations face in evaluating artificial intelligence (AI) investments using traditional return on investment (ROI) metrics, which often overlook AI’s dual impact on operational risks. The paper introduces a comprehensive financial framework that quantifies AI project returns by explicitly incorporating shifts in organizational risk profiles due to AI adoption. It highlights that typical investment decisions rely excessively on optimistic productivity benefits, ignoring probabilistic costs associated with AI-specific threats such as model drift, adversarial attacks, bias-related litigation, and compliance failures under regulations like the EU Artificial Intelligence Act and ISO/IEC 42001. Leveraging established risk quantification methods, including annual loss expectancy and Monte Carlo simulations, this framework calculates net benefits by combining gains in productivity with differences in risk exposures before and after AI implementation. The approach requires detailed modeling of control effectiveness, algorithmic failure reserves, and continuous operational expenses to maintain model performance. Finally, the study offers practical guidance on establishing governance structures, conducting phased validations, and integrating risk-adjusted metrics into capital allocation. This enables organizations to manage AI portfolios effectively, satisfying fiduciary duties while complying with regulatory mandates. <div>
arXiv:2511.21975v1 Announce Type: cross 
Abstract: Organizations investing in artificial intelligence face a fundamental challenge: traditional return on investment calculations fail to capture the dual nature of AI implementations, which simultaneously reduce certain operational risks while introducing novel exposures related to algorithmic malfunction, adversarial attacks, and regulatory liability. This research presents a comprehensive financial framework for quantifying AI project returns that explicitly integrates changes in organizational risk profiles. The methodology addresses a critical gap in current practice where investment decisions rely on optimistic benefit projections without accounting for the probabilistic costs of AI-specific threats including model drift, bias-related litigation, and compliance failures under emerging regulations such as the European Union Artificial Intelligence Act and ISO/IEC 42001. Drawing on established risk quantification methods, including annual loss expectancy calculations and Monte Carlo simulation techniques, this framework enables practitioners to compute net benefits that incorporate both productivity gains and the delta between pre-implementation and post-implementation risk exposures. The analysis demonstrates that accurate AI investment evaluation requires explicit modeling of control effectiveness, reserve requirements for algorithmic failures, and the ongoing operational costs of maintaining model performance. Practical implications include specific guidance for establishing governance structures, conducting phased validations, and integrating risk-adjusted metrics into capital allocation decisions, ultimately enabling evidence-based AI portfolio management that satisfies both fiduciary responsibilities and regulatory mandates.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeXposure: A Dataset and Benchmarks for Inter-protocol Credit Exposure in Decentralized Financial Networks</title>
<link>https://arxiv.org/abs/2511.22314</link>
<guid>https://arxiv.org/abs/2511.22314</guid>
<content:encoded><![CDATA[
<div> arXiv, DeXposure dataset, inter-protocol credit exposure, decentralized finance, temporal graph neural networks<br /><br />Summary:  
This article presents the DeXposure dataset, a comprehensive large-scale collection covering inter-protocol credit exposure in decentralized financial networks from 2020 to 2025. It includes 43.7 million entries spanning 4.3 thousand protocols, 602 blockchains, and 24.3 thousand tokens worldwide. A novel measure, value-linked credit exposure between protocols, is introduced to infer financial dependency relationships through changes in Total Value Locked (TVL). The authors develop a token-to-protocol model based on DefiLlama metadata to derive inter-protocol credit exposure from token stock dynamics reported by protocols. Using this dataset, three machine learning benchmarks are proposed: graph clustering for analyzing global network structure and its evolution, vector autoregression for studying sector-level credit exposure dynamics during significant financial shocks (e.g., Terra and FTX events), and temporal graph neural networks for dynamic link prediction in evolving networks. Key observations include rapid network volume growth, concentration towards key protocols, decreasing network density, and differentiated shock propagation across sectors such as lending platforms, trading exchanges, and asset management protocols. The DeXposure dataset and accompanying code have been made publicly available to advance research in machine learning applications, financial risk monitoring, policy analysis, and DeFi market modeling. <div>
arXiv:2511.22314v1 Announce Type: cross 
Abstract: We curate the DeXposure dataset, the first large-scale dataset for inter-protocol credit exposure in decentralized financial networks, covering global markets of 43.7 million entries across 4.3 thousand protocols, 602 blockchains, and 24.3 thousand tokens, from 2020 to 2025. A new measure, value-linked credit exposure between protocols, is defined as the inferred financial dependency relationships derived from changes in Total Value Locked (TVL). We develop a token-to-protocol model using DefiLlama metadata to infer inter-protocol credit exposure from the token's stock dynamics, as reported by the protocols. Based on the curated dataset, we develop three benchmarks for machine learning research with financial applications: (1) graph clustering for global network measurement, tracking the structural evolution of credit exposure networks, (2) vector autoregression for sector-level credit exposure dynamics during major shocks (Terra and FTX), and (3) temporal graph neural networks for dynamic link prediction on temporal graphs. From the analysis, we observe (1) a rapid growth of network volume, (2) a trend of concentration to key protocols, (3) a decline of network density (the ratio of actual connections to possible connections), and (4) distinct shock propagation across sectors, such as lending platforms, trading exchanges, and asset management protocols. The DeXposure dataset and code have been released publicly. We envision they will help with research and practice in machine learning as well as financial risk monitoring, policy analysis, DeFi market modeling, amongst others. The dataset also contributes to machine learning research by offering benchmarks for graph clustering, vector autoregression, and temporal graph analysis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Design Optimization via Strategic Search with Large Language Models</title>
<link>https://arxiv.org/abs/2511.22651</link>
<guid>https://arxiv.org/abs/2511.22651</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, design optimization, GPU code, gradient-free search, Bayesian optimization<br /><br />Summary: Traditional optimization techniques often falter when applied to design problems with poorly defined search spaces and ambiguous transformations or parameters. This paper introduces AUTO, a novel framework leveraging large language models (LLMs) to navigate such challenging design optimization scenarios as gradient-free search problems. AUTO employs two specialized agents: a Strategist, which dynamically balances exploration and exploitation strategies, and an Implementor, which carries out detailed design executions. The approach is demonstrated in optimizing GPU code, specifically targeting chemical kinetics integration and dense matrix multiplication, which are essential for applications in machine learning and scientific computing. AUTO achieves competitive performance compared to expert human implementations while improving search efficiency by 50-70% relative to traditional Bayesian optimization methods. Furthermore, AUTO substantially reduces costs, completing optimizations within 8 hours at an approximate cost of $159 per run, which is significantly lower than the estimated $480 expense for median-wage developers performing similar tasks. These results illustrate the potential of integrating LLM-driven reasoning into design optimization workflows, especially in scenarios where prior information is limited and search spaces are ill-defined. The framework paves the way for more autonomous, efficient, and cost-effective solutions in complex optimization challenges. <div>
arXiv:2511.22651v1 Announce Type: cross 
Abstract: Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \$159 per run, compared to an estimated cost of up to \$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cosserat micropolar and couple-stress elasticity models of flexomagnetism at finite deformations</title>
<link>https://arxiv.org/abs/2511.22756</link>
<guid>https://arxiv.org/abs/2511.22756</guid>
<content:encoded><![CDATA[
<div> flexomagnetism, Cosserat micropolar, micro-dislocation tensor, Lifshitz invariant, couple-stress theory<br /><br />Summary:  
This article introduces geometrically nonlinear continuum models for flexomagnetism grounded in Cosserat micropolar and couple-stress theories, addressing finite deformations. The proposed modeling framework innovatively couples the micro-dislocation tensor, a second-order tensor characteristic of micropolar theory, with the magnetisation vector via a Lifshitz invariant, distinguishing it from traditional methods that utilize fourth-order tensors to link strain gradients and magnetisation. The approach enables the representation of centrosymmetric materials using a single flexomagnetic constant and extends to cubic-symmetric materials characterized by two such constants. Theoretical development includes the postulation of flexomagnetic action functionals and derivation of governing equations employing both scalar and vector magnetic potential formulations, ensuring versatility in magnetic modeling. Numerical simulations focusing on a nano-beam configuration demonstrate the models’ physical relevance and computational implementability, confirming their capability to capture flexomagnetic effects in nanoscale structures effectively. Overall, this study provides a refined, mathematically elegant framework that simplifies tensorial complexity while expanding applicability to broader material symmetries in flexomagnetic phenomena. <div>
arXiv:2511.22756v1 Announce Type: cross 
Abstract: We propose geometrically nonlinear (finite) continuum models of flexomagnetism based on the Cosserat micropolar and its descendent couple-stress theory. These models introduce the magneto-mechanical interaction by coupling the micro-dislocation tensor of the micropolar model with the magnetisation vector using a Lifshitz invariant. In contrast to conventional formulations that couple strain-gradients to the magnetisation using fourth-order tensors, our approach relies on third-order tensor couplings by virtue of the micro-dislocation being a second-order tensor. Consequently, the models permit centrosymmetric materials with a single new flexomagnetic constant, and more generally allow cubic-symmetric materials with two such constants. We postulate the flexomagnetic action-functionals and derive the corresponding governing equations using both scalar and vectorial magnetic potential formulations, and present numerical results for a nano-beam geometry, confirming the physical plausibility and computational feasibility of the models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Last-Click: An Optimal Mechanism for Ad Attribution</title>
<link>https://arxiv.org/abs/2511.22918</link>
<guid>https://arxiv.org/abs/2511.22918</guid>
<content:encoded><![CDATA[
<div> Keywords: advertising attribution, dominant strategy incentive compatible, Last-Click Mechanism, Peer-Validated Mechanism, accuracy bounds<br /><br />Summary:<br /><br />Accurate attribution across multiple advertising platforms is essential for evaluating performance-based advertising, but prevalent heuristic methods like the Last-Click Mechanism (LCM) are flawed as they allocate all attribution to the most recent platform report without theoretical support. This paper develops a formal theoretical model aimed at designing optimal dominant strategy incentive compatible (DSIC) attribution mechanisms and rigorously evaluates their efficacy. It first demonstrates that LCM is not DSIC and exhibits poor accuracy and fairness in attribution. To overcome these shortcomings, the authors propose the Peer-Validated Mechanism (PVM), a DSIC-compliant approach where a platform's attribution depends exclusively on reports from other platforms rather than its own. The study analyzes PVM’s accuracy in both homogeneous (identical platforms) and heterogeneous settings, deriving provable accuracy bounds for each scenario. Remarkably, the results establish PVM as the optimal DSIC mechanism in homogeneous environments. Finally, extensive numerical experiments validate that PVM consistently surpasses LCM in both attribution accuracy and fairness, confirming its practical advantages in multi-platform advertising attribution tasks. <div>
arXiv:2511.22918v1 Announce Type: cross 
Abstract: Accurate attribution for multiple platforms is critical for evaluating performance-based advertising. However, existing attribution methods rely heavily on the heuristic methods, e.g., Last-Click Mechanism (LCM) which always allocates the attribution to the platform with the latest report, lacking theoretical guarantees for attribution accuracy. In this work, we propose a novel theoretical model for the advertising attribution problem, in which we aim to design the optimal dominant strategy incentive compatible (DSIC) mechanisms and evaluate their performance. We first show that LCM is not DSIC and performs poorly in terms of accuracy and fairness. To address this limitation, we introduce the Peer-Validated Mechanism (PVM), a DSIC mechanism in which a platform's attribution depends solely on the reports of other platforms. We then examine the accuracy of PVM across both homogeneous and heterogeneous settings, and provide provable accuracy bounds for each case. Notably, we show that PVM is the optimal DSIC mechanism in the homogeneous setting. Finally, numerical experiments are conducted to show that PVM consistently outperforms LCM in terms of attribution accuracy and fairness.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Thermophysical Property Retrieval</title>
<link>https://arxiv.org/abs/2511.23449</link>
<guid>https://arxiv.org/abs/2511.23449</guid>
<content:encoded><![CDATA[
<div> Keywords: Inverse heat problems, thermal conductivity, physics-informed neural networks, building facade renovation, in-situ measurement<br /><br />Summary:  
1. The article addresses inverse heat problems, which involve estimating thermophysical properties like thermal conductivity from observed heat diffusion behavior.  
2. A critical application of these problems lies in assessing how building facade renovations reduce thermal transmittance, important for building energy efficiency.  
3. Traditional methods for measuring thermal conductivity are often invasive, time-consuming, or highly sensitive to environmental variations, making them impractical for in-situ non-invasive measurements.  
4. The authors propose an iterative framework based on physics-informed neural networks (PINNs) that alternates between solving the forward heat problem for a fixed thermal conductivity and optimizing the conductivity by comparing predicted and actual surface temperatures captured in thermographs.  
5. Their framework leverages environmental data from a weather station and simulation data from Finite-Volume-Method software, delivering accurate conductivity estimates across varying environmental conditions and sampling times, provided the wall temperature profile at dawn approximates steady state.  
6. While deviations from the steady-state assumption decrease accuracy, the approach still maintains a low maximum mean absolute error (MAE) of 4.0851.  
7. This work highlights the potential for PINN-based methods to reliably estimate material properties onsite without lengthy measurements and serves as a foundation for further research into machine learning-driven, in-situ inverse heat problem solutions. <div>
arXiv:2511.23449v1 Announce Type: cross 
Abstract: Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoadFed: A Multimodal Federated Learning System for Improving Road Safety</title>
<link>https://arxiv.org/abs/2502.09978</link>
<guid>https://arxiv.org/abs/2502.09978</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, Federated Learning, Road Hazard Detection, Multimodal Data, Privacy Preservation

<br /><br />Summary:  
This paper addresses the critical issue of road hazard detection within Collaborative Intelligent Transportation Systems (C-ITS), leveraging Internet of Things (IoT) technologies. It highlights the limitations of existing solutions, which often rely on single-modality data, incur high computational and communication costs, or face challenges related to high-dimensional data privacy. To tackle these challenges, the authors propose RoadFed, a novel federated learning-based framework designed specifically for private and efficient multimodal road hazard detection and alarm. RoadFed integrates a Multimodal Road Hazard Detector with a communication-efficient federated learning strategy, alongside a customized local differential privacy mechanism that effectively handles high-dimensional multimodal data while maintaining low error rates. Experimental evaluation on both self-collected real-world and CrisisMMD public datasets demonstrates RoadFed’s superior performance. The system achieves an accuracy of 96.42% with minimal latency of 0.0351 seconds, outperforming existing methods. Moreover, its communication cost is drastically reduced by up to 1,000 times compared to prior work. RoadFed enables collaborative model training across diverse, non-iid high-dimensional multimodal data sources at the network edges, ensuring data privacy for road users and enhancing practical deployment potential in intelligent transportation systems. <div>
arXiv:2502.09978v3 Announce Type: replace 
Abstract: Internet of Things (IoTs) have been widely applied in Collaborative Intelligent Transportation Systems (C-ITS) for the prevention of road accidents. As one of the primary causes of road accidents in C-ITS, the efficient detection and early alarm of road hazards are of paramount importance. Given the importance, extensive research has explored this topic and obtained favorable results. However, most existing solutions only explore single-modality data, struggle with high computation and communication overhead, or suffer from the curse of high dimensionality in their privacy-preserving methodologies. To overcome these obstacles, in this paper, we introduce RoadFed, an innovative and private multimodal Federated learning-based system tailored for intelligent Road hazard detection and alarm. This framework encompasses an innovative Multimodal Road Hazard Detector, a communication-efficient federated learning approach, and a customized low-error-rate local differential privacy method crafted for high dimensional multimodal data. Experimental results reveal that the proposed RoadFed surpasses most existing systems in the self-gathered real-world and CrisisMMD public datasets. In particular, RoadFed achieves an accuracy of 96.42% with a mere 0.0351 seconds of latency and its communication cost is up to 1,000 times lower than existing systems in this field. It facilitates collaborative training with non-iid high dimensional multimodal real-world data across various data modalities on multiple edges while ensuring privacy preservation for road users.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Laplacian-based Bayesian Multi-fidelity Modeling</title>
<link>https://arxiv.org/abs/2409.08211</link>
<guid>https://arxiv.org/abs/2409.08211</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-fidelity data, probabilistic approach, graph Laplacian, Bayesian inference, computational mechanics<br /><br />Summary: This paper introduces a novel probabilistic method to generate multi-fidelity data by explicitly modeling errors in both low- and high-fidelity datasets. The approach uses a graph Laplacian derived from low-fidelity data to form a multivariate Gaussian prior representing the true data points. High-fidelity data, though limited in quantity, is incorporated through a conjugate likelihood, enabling the application of Bayes' rule to determine a posterior multivariate Gaussian density. The maximum a posteriori (MAP) estimate from this posterior is chosen as the optimal multi-fidelity estimate. Notably, both the MAP estimate and posterior covariance can be computed via linear system solutions. To efficiently solve these systems, the authors propose two numerical methods: one based on spectral truncation and another leveraging low-rank approximations. The effectiveness of the approach is validated across various solid and fluid mechanics problems involving vectors and spatial fields in one and two dimensions. Results indicate that with only a small subset of high-fidelity data points, the method substantially enhances the accuracy of predictions derived from a large volume of low-fidelity data, demonstrating significant practical value for computational modeling tasks. <div>
arXiv:2409.08211v2 Announce Type: replace-cross 
Abstract: We present a novel probabilistic approach for generating multi-fidelity data while accounting for errors inherent in both low- and high-fidelity data. In this approach a graph Laplacian constructed from the low-fidelity data is used to define a multivariate Gaussian prior density for the coordinates of the true data points. In addition, few high-fidelity data points are used to construct a conjugate likelihood term. Thereafter, Bayes rule is applied to derive an explicit expression for the posterior density which is also multivariate Gaussian. The maximum \textit{a posteriori} (MAP) estimate of this density is selected to be the optimal multi-fidelity estimate. It is shown that the MAP estimate and the covariance of the posterior density can be determined through the solution of linear systems of equations. Thereafter, two methods, one based on spectral truncation and another based on a low-rank approximation, are developed to solve these equations efficiently. The multi-fidelity approach is tested on a variety of problems in solid and fluid mechanics with data that represents vectors of quantities of interest and discretized spatial fields in one and two dimensions. The results demonstrate that by utilizing a small fraction of high-fidelity data, the multi-fidelity approach can significantly improve the accuracy of a large collection of low-fidelity data points.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating many-engine spacecraft: Exceeding 1 quadrillion degrees of freedom via information geometric regularization</title>
<link>https://arxiv.org/abs/2505.07392</link>
<guid>https://arxiv.org/abs/2505.07392</guid>
<content:encoded><![CDATA[
<div> Keywords: Information Geometric Regularization, compressible fluid flow, multi-engine spacecraft boosters, high-performance computing, mixed precision computing<br /><br />Summary:  
This paper presents an optimized implementation of Information Geometric Regularization (IGR) tailored for large-scale compressible fluid flow simulations, specifically applied to multi-engine spacecraft boosters. The authors enhance existing computational fluid dynamics (CFD) methods by improving computational cost, memory footprint, and energy-to-solution metrics, which directly benefits large-scale simulations. Utilizing unified memory architectures on coupled CPU-GPU or APU platforms, their approach expands the feasible problem size with minimal overhead. They apply mixed half and single-precision storage and computation techniques on numerically well-conditioned problems to maximize efficiency. Their framework successfully simulates flow fields at an unprecedented scale of 200 trillion grid points and 1 quadrillion degrees of freedom, surpassing prior records by a factor of 20. A significant 4x wall-time speedup is achieved compared to highly optimized baseline methods. Furthermore, the implementation demonstrates near-ideal weak scaling performance on major supercomputers including OLCF Frontier, LLNL El Capitan, and CSCS Alps by utilizing entire systems. Strong scaling efficiency remains high under extreme conditions, exemplified by achieving 80% efficiency on CSCS Alps with an 8-node baseline and scaling up to the full system. This work sets a new standard in simulating complex fluid dynamics phenomena at extreme scales. <div>
arXiv:2505.07392v4 Announce Type: replace-cross 
Abstract: We present an optimized implementation of the recently proposed information geometric regularization (IGR) for unprecedented scale simulation of compressible fluid flows applied to multi-engine spacecraft boosters. We improve upon state-of-the-art computational fluid dynamics (CFD) techniques along computational cost, memory footprint, and energy-to-solution metrics. Unified memory on coupled CPU--GPU or APU platforms increases problem size with negligible overhead. Mixed half/single-precision storage and computation on well-conditioned numerics is used. We simulate flow at 200 trillion grid points and 1 quadrillion degrees of freedom, exceeding the current record by a factor of 20. A factor of 4 wall-time speedup is achieved over optimized baselines. Ideal weak scaling is seen on OLCF Frontier, LLNL El Capitan, and CSCS Alps using the full systems. Strong scaling is near ideal at extreme conditions, including 80% efficiency on CSCS Alps with an 8-node baseline and stretching to the full system.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Numerical Optimization of Nozzle Shapes for Fused Deposition Modeling</title>
<link>https://arxiv.org/abs/2511.21449</link>
<guid>https://arxiv.org/abs/2511.21449</guid>
<content:encoded><![CDATA[
<div> Keywords: Fused Deposition Modeling, Nozzle Geometry, Pressure Loss, Viscous Model, Viscoelastic Model<br /><br />Summary:<br /><br />This paper investigates the impact of nozzle geometry on pressure loss inside the nozzle in fused deposition modeling (FDM), a key factor affecting the balance between printing speed and precision. The study develops an optimization framework enabling both simple angle-based and advanced spline-based parametrizations for nozzle shape design. Two constitutive models are compared for simulating polymer melt flow: a temperature-dependent shear-thinning viscous model and an isothermal viscoelastic model. Results indicate that for the viscous model, the optimal half-opening angle strongly depends on the feeding rate, with higher feeding rates favoring narrower angles (~30°) and lower rates favoring wider angles. Conversely, the viscoelastic model shows weaker sensitivity of optimal angle to feeding rate changes. Both models reveal that more complex spline-based parametrization provides only marginal gains over simpler angle-based optimization in reducing pressure loss. The originality of this work lies in its comparative approach using different rheological models and flexible parametrization techniques. Its findings underscore the importance of the chosen flow model on optimal nozzle geometry and offer guidance for designing nozzles that enhance high-speed FDM printing performance. <div>
arXiv:2511.21449v1 Announce Type: new 
Abstract: Purpose: In fused deposition modeling (FDM), the nozzle plays a critical role in enabling high printing speeds while maintaining precision. Despite its importance, most applications still rely on standard nozzle designs. This work investigates the influence of nozzle geometry on pressure loss inside the nozzle, a key factor in high-speed printing performance. Design/methodology/approach: We focus on optimizing the nozzle shape to minimize the pressure loss and establish a framework that allows both sim- ple angle-based optimization and more advanced spline-based parametrization. To model the polymer melt flow, we compare two constitutive descriptions commonly employed in the literature: a temperature-dependent, shear-thinning viscous model and an isothermal viscoelastic model. Findings: For the viscous model, the optimal half-opening angle exhibits a strong dependence on the feeding rate, with higher rates favoring half-opening angles near 30{\deg}, whereas lower rates are more efficient at larger angles. In con- trast, the viscoelastic model predicts a weaker dependence of the optimal angle on the feeding rate. For both models, spline-based parametrization yields only marginal improvements over angle optimization in terms of reducing pressure loss. Originality/value: This paper presents a comparative study of FDM nozzle shape optimization using different simulation models. We introduce a flexible optimization framework that accommodates both simple and advanced geomet- ric parametrizations. The results highlight the impact of model choice on the optimal nozzle geometry and provide support for improving nozzle design in high-speed printing applications.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Going with the Speed of Sound: Pushing Neural Surrogates into Highly-turbulent Transonic Regimes</title>
<link>https://arxiv.org/abs/2511.21474</link>
<guid>https://arxiv.org/abs/2511.21474</guid>
<content:encoded><![CDATA[
<div> Keywords: neural surrogates, transonic regime, 3D wings, aerodynamic optimization, drag-lift Pareto front  

<br /><br />Summary:  
This paper addresses the challenge of applying neural surrogate models to aerospace CFD problems in the transonic regime, where complex 3D effects like wingtip vortices and compressible flow nonlinearities are significant. Unlike existing datasets that focus mainly on 2D airfoils, the authors present a novel dataset containing around 30,000 CFD simulations of 3D wing geometries under diverse inflow conditions. The dataset includes volumetric and surface flow fields, enabling the computation of lift and drag coefficients, which are essential for aerodynamic optimization targeting the drag-lift Pareto front. Several state-of-the-art neural surrogates, including Transolver and AB-UPT, are evaluated on this dataset, with a focus on their out-of-distribution generalization capabilities across varying geometries and flow conditions. Results show that AB-UPT achieves strong performance, accurately predicting transonic flowfields and physically consistent drag-lift trade-offs even for previously unseen wing configurations. This highlights AB-UPT’s potential as a fast and reliable surrogate model for aerodynamic design exploration in complex 3D transonic flows. To encourage further research, the authors have open-sourced the dataset at https://huggingface.co/datasets/EmmiAI/Emmi-Wing. <div>
arXiv:2511.21474v1 Announce Type: new 
Abstract: The widespread use of neural surrogates in automotive aerodynamics, enabled by datasets such as DrivAerML and DrivAerNet++, has primarily focused on bluff-body flows with large wakes. Extending these methods to aerospace, particularly in the transonic regime, remains challenging due to the high level of non-linearity of compressible flows and 3D effects such as wingtip vortices. Existing aerospace datasets predominantly focus on 2D airfoils, neglecting these critical 3D phenomena. To address this gap, we present a new dataset of CFD simulations for 3D wings in the transonic regime. The dataset comprises volumetric and surface-level fields for around $30,000$ samples with unique geometry and inflow conditions. This allows computation of lift and drag coefficients, providing a foundation for data-driven aerodynamic optimization of the drag-lift Pareto front. We evaluate several state-of-the-art neural surrogates on our dataset, including Transolver and AB-UPT, focusing on their out-of-distribution (OOD) generalization over geometry and inflow variations. AB-UPT demonstrates strong performance for transonic flowfields and reproduces physically consistent drag-lift Pareto fronts even for unseen wing configurations. Our results demonstrate that AB-UPT can approximate drag-lift Pareto fronts for unseen geometries, highlighting its potential as an efficient and effective tool for rapid aerodynamic design exploration. To facilitate future research, we open-source our dataset at https://huggingface.co/datasets/EmmiAI/Emmi-Wing.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Wildfire Spread Prediction Using an Autoregressive Conditional Generative Adversarial Network</title>
<link>https://arxiv.org/abs/2511.21019</link>
<guid>https://arxiv.org/abs/2511.21019</guid>
<content:encoded><![CDATA[
<div> wildfire prediction, CGAN, autoregressive model, fire spread, deep learning<br /><br />Summary:<br /><br />1. The study addresses the challenge of predicting wildfire spread accurately and rapidly, an increasingly critical task due to climate change intensifying wildfire frequency and severity.<br />2. Traditional physics-based simulators like FARSITE are accurate but not suitable for real-time use because they require extensive computational resources.<br />3. Existing deep learning approaches tend to produce overly smooth predictions, which fail to capture the complex, nonlinear behavior of wildfire propagation.<br />4. To overcome these limitations, the authors propose an autoregressive conditional generative adversarial network (CGAN) framework that treats wildfire spread prediction as a sequential state transition problem.<br />5. The autoregressive CGAN model yields improved long-term prediction stability and better delineation of fire perimeters compared to conventional models.<br />6. Adversarial learning enables the model to capture nonlinear dynamics and uncertainty rather than averaging pixel values, enhancing physical interpretability.<br />7. Experimental results demonstrate the model’s superiority in both overall accuracy and boundary detail, supporting its potential use in time-sensitive wildfire response and evacuation planning.<br />8. This approach represents a promising advancement in leveraging AI for dynamic environmental hazard forecasting, combining prediction performance and practical usability. <div>
arXiv:2511.21019v1 Announce Type: cross 
Abstract: Climate change has intensified the frequency and severity of wildfires, making rapid and accurate prediction of fire spread essential for effective mitigation and response. Physics-based simulators such as FARSITE offer high-fidelity predictions but are computationally intensive, limiting their applicability in real-time decision-making, while existing deep learning models often yield overly smooth predictions that fail to capture the complex, nonlinear dynamics of wildfire propagation. This study proposes an autoregressive conditional generative adversarial network (CGAN) for probabilistic wildfire spread prediction. By formulating the prediction task as an autoregressive problem, the model learns sequential state transitions, ensuring long-term prediction stability. Experimental results demonstrate that the proposed CGAN-based model outperforms conventional deep learning models in both overall predictive accuracy and boundary delineation of fire perimeters. These results demonstrate that adversarial learning allows the model to capture the strong nonlinearity and uncertainty of wildfire spread, instead of simply fitting the pixel average. Furthermore, the autoregressive framework facilitates systematic temporal forecasting of wildfire evolution. The proposed CGAN-based autoregressive framework enhances both the accuracy and physical interpretability of wildfire spread prediction, offering a promising foundation for time-sensitive response and evacuation planning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing GPU Energy Usage in Exascale-Ready Portable Science Applications</title>
<link>https://arxiv.org/abs/2505.05623</link>
<guid>https://arxiv.org/abs/2505.05623</guid>
<content:encoded><![CDATA[
<div> Keywords: GPU energy usage, QMCPACK, AMReX-Castro, mixed precision, exascale computing<br /><br />Summary:<br /> This paper characterizes the energy consumption of GPUs running two exascale-ready scientific applications: QMCPACK, a quantum Monte Carlo solver, and AMReX-Castro, an adaptive mesh code for astrophysics. It uses detailed power, temperature, utilization, and energy measurements collected from benchmarks run on NVIDIA’s A100 and H100 as well as AMD’s MI250X GPUs. Data were obtained through NVML queries for NVIDIA and rocm_smi_lib for AMD hardware. Mixed-precision computing is evaluated, revealing significant energy savings of 6-25% on QMCPACK and up to 45% on AMReX-Castro without compromising performance. The study also identifies tooling limitations with AMD’s monitoring utilities on Frontier-class GPUs, highlighting areas needing improvement. In contrast, NVML queries on NVIDIA hardware show consistent resolution with minimal variability between 1 millisecond and 1 second. The authors emphasize that in-depth application-specific knowledge is essential to optimize the trade-offs between energy consumption and scientific throughput. These insights inform strategies for energy-efficient code and system co-design in the post-Moore’s law era of supercomputing, where energy costs and performance gains are critically balanced. <div>
arXiv:2505.05623v3 Announce Type: replace-cross 
Abstract: We characterize the GPU energy usage of two widely adopted exascale-ready applications representing two classes of particle and mesh solvers: (i) QMCPACK, a quantum Monte Carlo package, and (ii) AMReXCastro, an adaptive mesh astrophysical code. We analyze power, temperature, utilization, and energy traces from double-/single (mixed)-precision benchmarks on NVIDIA's A100 and H100 and AMD's MI250X GPUs using queries in NVML and rocm_smi_lib, respectively. We explore application-specific metrics to provide insights on energy vs. performance trade-offs. Our results suggest that mixed-precision energy savings range between 6-25% on QMCPACK and 45% on AMReX-Castro. Also, we found gaps in the AMD tooling used on Frontier GPUs that need to be understood, while query resolutions on NVML have little variability between 1 ms-1 s. Overall, application level knowledge is crucial to define energy-cost/science-benefit opportunities for the codesign of future supercomputer architectures in the post-Moore era.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.04171</link>
<guid>https://arxiv.org/abs/2506.04171</guid>
<content:encoded><![CDATA[
<div> Keywords: Physics-Constrained Flow Matching, deep generative models, partial differential equations, hard constraints, zero-shot inference<br /><br />Summary:<br /><br />1. The paper addresses the challenge of enforcing hard physical constraints such as conservation laws and consistencies in deep generative models applied to physical systems governed by partial differential equations (PDEs).<br /><br />2. Existing approaches often use soft penalties or architectural biases that do not guarantee strict enforcement of these constraints.<br /><br />3. The authors propose Physics-Constrained Flow Matching (PCFM), a novel zero-shot inference framework designed to impose arbitrary nonlinear constraints on pretrained flow-based generative models.<br /><br />4. PCFM operates by continuously correcting intermediate solution states during the sampling procedure, ensuring that the generative process remains aligned with the learned flow while strictly satisfying physical constraints.<br /><br />5. Experimentally, PCFM outperforms both unconstrained and prior constrained baselines across a variety of PDE problems, including those exhibiting shocks, discontinuities, and sharp features, achieving exact constraint satisfaction in the final solutions.<br /><br />This method offers a flexible and robust mechanism for integrating hard physical constraints into scientific and general-purpose generative modeling, enhancing reliability in applications where constraint adherence is critical. <div>
arXiv:2506.04171v2 Announce Type: replace-cross 
Abstract: Deep generative models have recently been applied to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty-aware inference. However, enforcing physical constraints, such as conservation laws (linear and nonlinear) and physical consistencies, remains challenging. Existing methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints. In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints. Empirically, PCFM outperforms both unconstrained and constrained baselines on a range of PDEs, including those with shocks, discontinuities, and sharp features, while ensuring exact constraint satisfaction at the final solution. Our method provides a flexible framework for enforcing hard constraints in both scientific and general-purpose generative models, especially in applications where constraint satisfaction is essential.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation</title>
<link>https://arxiv.org/abs/2506.08604</link>
<guid>https://arxiv.org/abs/2506.08604</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative machine learning, Flow matching, Physical constraints, PDE residuals, Surrogate modeling<br /><br />Summary:<br /><br />This paper introduces Physics-Based Flow Matching (PBFM), a novel generative machine learning framework designed to explicitly incorporate physical constraints such as partial differential equation (PDE) residuals and algebraic relations directly into the flow matching objective. Unlike previous methods that implicitly learn underlying physics from data, PBFM embeds these constraints explicitly, enhancing the physical accuracy of generated samples. The authors propose a temporal unrolling technique during training, which improves the precision of the final noise-free sample predictions. The approach jointly minimizes both the flow matching loss and the physics-based residual loss, crucially without the need for tuning hyperparameters related to their relative weighting. The paper also explores the significance of the minimum noise level, denoted as \(\sigma_{\min}\), within the context of enforcing physical constraints, proposing a stochastic sampling strategy to reduce physical residuals further. Extensive benchmarking on three different PDE problems demonstrates that PBFM achieves up to eight times more accurate physical residuals compared to standard flow matching methods, while also outperforming existing algorithms in distributional accuracy. Overall, PBFM presents a principled and efficient framework suitable for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering domains. <div>
arXiv:2506.08604v2 Announce Type: replace-cross 
Abstract: Generative machine learning methods, such as diffusion models and flow matching, have shown great potential in modeling complex system behaviors and building efficient surrogate models. However, these methods typically learn the underlying physics implicitly from data. We propose Physics-Based Flow Matching (PBFM), a novel generative framework that explicitly embeds physical constraints, both PDE residuals and algebraic relations, into the flow matching objective. We also introduce temporal unrolling at training time that improves the accuracy of the final, noise-free sample prediction. Our method jointly minimizes the flow matching loss and the physics-based residual loss without requiring hyperparameter tuning of their relative weights. Additionally, we analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of physical constraints and evaluate a stochastic sampling strategy that helps to reduce physical residuals. Through extensive benchmarks on three representative PDE problems, we show that our approach yields up to an $8\times$ more accurate physical residuals compared to FM, while clearly outperforming existing algorithms in terms of distributional accuracy. PBFM thus provides a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigation of PINN Stability and Robustness for the Euler-Bernoulli Beam Problem</title>
<link>https://arxiv.org/abs/2511.19916</link>
<guid>https://arxiv.org/abs/2511.19916</guid>
<content:encoded><![CDATA[
<div> Keywords: Physics-Informed Neural Networks, doubly-clamped beam, loss landscape, strong formulation, energy-based formulation<br /><br />Summary:  
This study addresses the significant challenges faced by Physics-Informed Neural Networks (PINNs) when applied to doubly-clamped beam problems, a domain where training difficulties have been poorly understood. It focuses on analyzing the PINN loss landscape to diagnose the failure mechanisms in two dominant formulations: the high-order strong formulation and the energy-based formulation. The strong formulation is found to suffer from ill-conditioning in the loss landscape primarily due to boundary conditions, which adversely affects convergence in doubly-clamped beams. In contrast, the energy-based formulation involves lower-order derivatives and thus appears simpler; however, its loss functional can become indefinite, resulting in optimization challenges near saddle points. The work further benchmarks strain field predictions against the Finite Element Method (FEM), revealing that the strong formulation, when coupled with an effective boundary condition handling method and optimized using the L-BFGS algorithm, achieves superior accuracy and performance across three classic boundary condition scenarios. These insights clarify distinct, formulation-specific failure modes responsible for training difficulties in PINNs applied to beam problems, thereby providing a crucial diagnostic foundation. Ultimately, this research paves the way for developing more robust physics-informed surrogates tailored for complex beam system analyses by improving understanding of loss landscape characteristics and optimizer behaviors. <div>
arXiv:2511.19916v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) encounter significant training difficulties when applied to doubly-clamped beam problems, and the underlying causes are not fully understood. This study investigates the PINN loss landscape to identify the failure mechanisms of two primary formulations: the high-order strong formulation and the energy-based formulation. The results demonstrate that the Strong Formulation suffers from landscape ill-conditioning driven by the boundary conditions (BCs), leading to convergence issues in the doubly-clamped case. Conversely, while the energy-based formulation requires only lower-order derivatives, its loss functional can become indefinite, causing optimization difficulties near saddle points. Based on strain field benchmarks against Finite Element Method (FEM), it is found that the strong formulation, combined with a BC handling method and the L-BFGS optimizer, yields the best performance across three classical boundary condition cases. These findings clarify distinct, formulation-dependent failure modes, offering a diagnostic foundation for developing robust physics-based surrogate models for complex beam systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Surrogate-Informed Framework for Sparse Grid Interpolation</title>
<link>https://arxiv.org/abs/2511.20187</link>
<guid>https://arxiv.org/abs/2511.20187</guid>
<content:encoded><![CDATA[
<div> Keywords: sparse grids, surrogate modeling, error indicator, hierarchical surplus, adaptive refinement  

<br /><br />Summary:  
This work addresses the challenge of approximating complex, high-dimensional, and computationally expensive functions common in science and engineering. Traditional sparse grids alleviate the curse of dimensionality but apply isotropic refinement, which may be inefficient for functions exhibiting localized or anisotropic behavior. The authors propose a surrogate-informed framework for constructing sparse grid interpolants, driven by a novel zero-cost error indicator estimating the hierarchical surplus. This error indicator is computed for candidate points in the next-level grid, measuring the relative difference between interpolants at consecutive levels (w and w-1), effectively quantifying local approximation error. Candidate points are ranked based on this metric, and refinement is targeted selectively either via a preset budget or error threshold, focusing computational effort on the most impactful points. The final higher-order surrogate model blends direct function evaluations at these prioritized points with model predictions at other nodes, significantly reducing the number of expensive evaluations. This approach achieves accuracy comparable to fully-resolved higher-level sparse grids but at a fraction of the computational cost. The method's effectiveness and efficiency are demonstrated through tests on analytic functions and a practical engineering problem involving sensitivity analysis of flashback in hydrogen-fueled perforated burners. <div>
arXiv:2511.20187v1 Announce Type: new 
Abstract: Approximating complex, high-dimensional, and computationally expensive functions is a central problem in science and engineering. Standard sparse grids offer a powerful solution by mitigating the curse of dimensionality compared to full tensor grids. However, they treat all regions of the domain isotropically, which may not be efficient for functions with localized or anisotropic behavior. This work presents a surrogate-informed framework for constructing sparse grid interpolants, which is guided by an error indicator that serves as a zero-cost estimate for the hierarchical surplus. This indicator is calculated for all candidate points, defined as those in the next-level grid $w+1$ not already present in the base grid $w$. It quantifies the local approximation error by measuring the relative difference between the predictions of two consecutive interpolants of level $w$ and $w-1$. The candidates are then ranked by this metric to select the most impactful points for refinement up to a given budget or following another criterion, as, e.g., a given threshold in the error indicator. The final higher-order model is then constructed using a surrogate-informed approach: the objective function is evaluated only at the selected high-priority points, while for the remaining nodes of the $w+1$ grid, we assign the values predicted by the initial $w$-level surrogate. This strategy significantly reduces the required number of expensive evaluations, yielding a final model that closely approximates the accuracy of a fully-resolved $w+1$ grid at a fraction of the computational cost. The accuracy and efficiency of the proposed surrogate-informed refinement criterion is demonstrated for several analytic function and for a real engineering problem, i.e., the analysis of sensitivity to geometrical parameters of numerically predicted flashback phenomenon in hydrogen-fueled perforated burners.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.19941</link>
<guid>https://arxiv.org/abs/2511.19941</guid>
<content:encoded><![CDATA[
<div> Magnetic Resonance Fingerprinting, Reinforcement Learning, Flip-angle schedule, Pulse sequence optimization, Acquisition acceleration<br /><br />Summary:<br /><br />1. Magnetic Resonance Fingerprinting (MRF) uses transient-state signal dynamics influenced by adjustable acquisition parameters to create unique signal fingerprints for various tissue properties.  
2. Designing an optimal and robust MRF sequence, especially optimizing key parameters like flip angle, is a complex, high-dimensional sequential decision-making problem.  
3. Reinforcement Learning (RL) is proposed as an effective method to automate and optimize the selection of these parameters, specifically targeting the flip-angle schedule, to improve the overall fingerprint distinguishability across the tissue parameter space.  
4. The study introduces an RL-based framework that learns a non-periodic flip-angle schedule, which outperforms traditional periodic or heuristic schedules by enhancing the separability of fingerprints in MRF data.  
5. An additional notable benefit is the potential reduction in the number of repetition times required, which could lead to accelerated MRF acquisition times without sacrificing data quality, thereby improving clinical applicability and efficiency. <div>
arXiv:2511.19941v1 Announce Type: cross 
Abstract: Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents</title>
<link>https://arxiv.org/abs/2511.20216</link>
<guid>https://arxiv.org/abs/2511.20216</guid>
<content:encoded><![CDATA[
<div> CostNav, autonomous delivery, economic viability, navigation benchmarks, collision avoidance<br /><br />Summary:<br /><br />This paper introduces CostNav, a Micro-Navigation Economic Testbed designed to evaluate autonomous delivery agents by integrating detailed cost-revenue analysis reflective of real-world business operations. Unlike existing navigation benchmarks that focus solely on task success metrics, CostNav incorporates comprehensive economic factors including hardware, training, energy, maintenance costs, and delivery revenue alongside service-level agreements (SLAs), based on parameters sourced from industry data. The key contribution of CostNav is revealing the disconnect between optimizing navigation for task success and optimizing for commercial viability, highlighting that high task performance does not guarantee economic sustainability. Through a scaled simulation projected to realistic deliveries, the baseline agent achieves 43.0% SLA compliance but incurs a significant loss of \$30.009 per run, with no break-even point. This financial loss is primarily driven by collision-induced maintenance costs, which represent 99.7% of total per-run expenses, emphasizing collision avoidance as a critical focus for optimization. Additionally, the authors present a learning-based on-device navigation baseline and lay groundwork to evaluate various navigation approaches—including rule-based methods, imitation learning, and cost-aware reinforcement learning—within the CostNav framework. Ultimately, CostNav aims to bridge the gap between navigation research metrics and economic realities, enabling data-driven decisions on trade-offs in commercial autonomous delivery deployments. <div>
arXiv:2511.20216v1 Announce Type: cross 
Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research and Prototyping Study of an LLM-Based Chatbot for Electromagnetic Simulations</title>
<link>https://arxiv.org/abs/2511.17680</link>
<guid>https://arxiv.org/abs/2511.17680</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, electromagnetic simulation, chatbot, finite element method, automation  

<br /><br />Summary:  
This work explores the effective use of generative artificial intelligence to significantly reduce the time needed to establish electromagnetic simulation models. It introduces a chatbot powered by the large language model Google Gemini 2.0 Flash, which automates the generation and solution of two-dimensional finite element eddy current models. The workflow leverages Gmsh for mesh generation and GetDP for solving the models, with Python scripts orchestrating the interaction between these components. The models focus on conductor geometries featuring circular cross-sections with variable positions and numbers, providing flexibility in design. Users benefit from the ability to define custom post-processing routines tailored to their specific needs. Furthermore, the system delivers concise summaries of model data and simulation outcomes to facilitate understanding and assessment. Each proposed functional enhancement is accompanied by architectural updates and concrete case studies that demonstrate practical applications. This integrated approach showcases how generative AI and chatbot technologies can automate complex technical modeling tasks, streamline workflows, and enhance user interaction in electromagnetic simulation setups. <div>
arXiv:2511.17680v1 Announce Type: new 
Abstract: This work addresses the question of how generative artificial intelligence can be used to reduce the time required to set up electromagnetic simulation models. A chatbot based on a large language model is presented, enabling the automated generation of simulation models with various functional enhancements. A chatbot-driven workflow based on the large language model Google Gemini 2.0 Flash automatically generates and solves two-dimensional finite element eddy current models using Gmsh and GetDP. Python is used to coordinate and automate interactions between the workflow components. The study considers conductor geometries with circular cross-sections of variable position and number. Additionally, users can define custom post-processing routines and receive a concise summary of model information and simulation results. Each functional enhancement includes the corresponding architectural modifications and illustrative case studies.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lean 5.0: A Predictive, Human-AI, and Ethically Grounded Paradigm for Construction Management</title>
<link>https://arxiv.org/abs/2511.18651</link>
<guid>https://arxiv.org/abs/2511.18651</guid>
<content:encoded><![CDATA[
<div> Lean 5.0, Industry 5.0, Construction 5.0, predictive analytics, digital twin<br /><br />Summary:  
This paper presents Lean 5.0, an innovative evolution of Lean-Digital integration that emphasizes human-centric approaches within Industry 5.0 and Construction 5.0 frameworks. It integrates predictive analytics, AI collaboration, and continuous learning to enhance construction management processes. A systematic literature review spanning 2019 to 2024 was conducted alongside a 12-week empirical validation study using a mixed-method Design Science Research (DSR) approach aligned with PRISMA 2020 guidelines. The results indicate significant performance improvements, including a 13% increase in Plan Percent Complete (PPC), a 22% reduction in rework, and a 42% enhancement in forecast accuracy. The paper also explores how Lean 5.0 integrates with digital twin and blockchain technologies to boost traceability, auditability, and lifecycle transparency in construction projects. However, the study acknowledges limitations such as a small sample size, reliance on a single-case design, and a relatively short study duration. Despite these constraints, findings suggest that Lean 5.0 offers a transformative paradigm by connecting human cognitive capabilities with predictive control mechanisms, ultimately advancing construction management efficiency and decision-making. <div>
arXiv:2511.18651v1 Announce Type: new 
Abstract: This paper introduces Lean 5.0, a human-centric evolution of Lean-Digital integration that connects predictive analytics, AI collaboration, and continuous learning within Industry 5.0 and Construction 5.0 contexts. A systematic literature review (2019-2024) and a 12-week empirical validation study demonstrate measurable performance gains, including a 13% increase in Plan Percent Complete (PPC), 22% reduction in rework, and 42% improvement in forecast accuracy. The study adopts a mixed-method Design Science Research (DSR) approach aligned with PRISMA 2020 guidelines. The paper also examines integration with digital twin and blockchain technologies to improve traceability, auditability, and lifecycle transparency. Despite limitations related to sample size, single-case design, and study duration, the findings show that Lean 5.0 provides a transformative paradigm connecting human cognition with predictive control in construction management.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A novel k-means clustering approach using two distance measures for Gaussian data</title>
<link>https://arxiv.org/abs/2511.17823</link>
<guid>https://arxiv.org/abs/2511.17823</guid>
<content:encoded><![CDATA[
<div> Keywords: k-means clustering, within cluster distance, inter cluster distance, Calinski-Harabasz criterion, outlier detection<br /><br />Summary:<br /><br />This paper proposes a novel approach to the traditional k-means clustering algorithm by incorporating both within cluster distance (WCD) and inter cluster distance (ICD) into the distance metric used for clustering. The number of clusters, k, is pre-determined using the Calinski-Harabasz criterion, which helps in selecting an optimal and robust cluster count. By combining WCD and ICD, the algorithm aims to strengthen the convergence process, making the clustering outcomes more reliable compared to conventional methods. The study tests the performance of the algorithm on both synthetically generated data and benchmark datasets from the UCI repository. Results indicate improved clustering accuracy and robustness, particularly in handling outliers, which are better assigned to their true clusters than with traditional k-means. Additionally, the paper highlights potential avenues for future research that emerge from the investigation, encouraging further exploration in enhancing clustering techniques and metrics. This work contributes toward more effective unsupervised learning by addressing limitations of distance measurement in clustering analysis. <div>
arXiv:2511.17823v1 Announce Type: cross 
Abstract: Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \textit{k}-means clustering. Here we present a \textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization</title>
<link>https://arxiv.org/abs/2511.17963</link>
<guid>https://arxiv.org/abs/2511.17963</guid>
<content:encoded><![CDATA[
<div> Keywords: portfolio optimization, LSTM, Proximal Policy Optimization, reinforcement learning, multi-asset datasets<br /><br />Summary:<br /><br />This paper proposes a hybrid portfolio optimization framework combining Long Short-Term Memory (LSTM) networks with a Proximal Policy Optimization (PPO) reinforcement learning agent. The LSTM component captures temporal dependencies in asset price data, providing robust forecasts, while the PPO agent continuously adapts portfolio allocations in a continuous action space to dynamically respond to changing market conditions. The authors test the approach using multi-asset datasets spanning U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024. The hybrid model is benchmarked against equal-weighted, index-based strategies and single-model baselines that use either LSTM or PPO alone. Performance metrics used for evaluation include annualized return, volatility, Sharpe ratio, and maximum drawdown, all adjusted for transaction costs to reflect practical trading conditions. Results demonstrate that the combined LSTM-PPO framework delivers superior returns and exhibits enhanced resilience under non-stationary market regimes compared to all baselines. This shows the framework’s ability to both anticipate market trends via deep recurrent forecasting and adaptively optimize allocations through reinforcement learning, making it a promising AI-driven solution for dynamic portfolio management. <div>
arXiv:2511.17963v1 Announce Type: cross 
Abstract: This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Appropriateness of Linear Stress Recovery in Biomechanical Analysis of Abdominal Aortic Aneurysm</title>
<link>https://arxiv.org/abs/2511.18741</link>
<guid>https://arxiv.org/abs/2511.18741</guid>
<content:encoded><![CDATA[
<div> Abdominal aortic aneurysm, wall stress, linear stress recovery, non-linear analysis, 4D-CTA imaging<br /><br />Summary:<br /><br />1. The study investigates the impact of unknown cardiac imaging phases on the estimation of abdominal aortic aneurysm (AAA) wall stress using linear stress recovery methods.<br /><br />2. Linear stress recovery, which resolves a geometrically linear equilibrium problem on the already-loaded geometry, is validated for static stress estimation but had not been assessed for robustness against different cardiac phases in imaging.<br /><br />3. Two patient-specific AAAs from a public 4D-CTA dataset were analyzed: diastolic geometries and synthetic systolic geometries (created by warping diastolic meshes based on non-linear hyperelastic displacements).<br /><br />4. The maximum principal stress at the 99th percentile was calculated using linear recovery under systolic pressure on both geometries. Differences between stresses on diastolic vs. synthetic systolic geometries were within segmentation uncertainty (8.6% and 3.5%).<br /><br />5. Linear stress recovery results under pulse pressure corresponded closely with those from non-linear hyperelastic analysis, showing negligible differences (0% and 1.1%) and nearly identical stress distributions.<br /><br />6. These results indicate that linear stress recovery provides an accurate, computationally efficient alternative to non-linear analysis for patient-specific AAA rupture risk assessment, even when imaging phase is unknown. This supports its application in clinical settings using static single-phase images without detailed wall material properties. <div>
arXiv:2511.18741v1 Announce Type: cross 
Abstract: Abdominal aortic aneurysm (AAA) wall stress is a candidate rupture risk marker but is typically computed from single-phase images without known cardiac phase. Linear stress recovery methods, which solve a single geometrically linear equilibrium problem on the imaged, already-loaded geometry, have been validated for static stress estimation, but their robustness to unknown imaging phase remains unexplored. We investigated whether imaging phase materially biases 99th percentile stress recovered linearly, and whether linear recovery agrees with non-linear analysis under matched loads. Two patient-specific AAAs from a public 4D-CTA cohort (Case 1: 5.5% strain; Case 2: 4.5% strain) were analyzed. For each, we analyzed diastolic and synthetic systolic geometry, the latter generated by warping the diastolic mesh via displacements from non-linear hyperelastic analysis. Linear stresses were recovered on both geometries under systolic pressure and compared via 99th-percentile maximum principal stress, stress distributions, and 3D stress differential contours. Linear stresses under pulse pressure were compared against non-linear stresses. 99th-percentile stresses from linear recovery on diastolic vs synthetic systolic geometries under systolic pressure differed by 8.6% (Case 1) and 3.5% (Case 2), within segmentation uncertainty. 99th-percentile stresses from linear recovery and non-linear analysis under pulse pressure agreed closely: 0% difference (Case 1) and 1.1% (Case 2), with nearly identical distributions. These findings support linear stress recovery for patient-specific AAA analysis in clinical settings with static single-phase imaging, offering a computationally efficient alternative without compromising accuracy or requiring patient-specific wall properties.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the role of fractional Brownian motion in models of chemotaxis and stochastic gradient ascent</title>
<link>https://arxiv.org/abs/2511.18745</link>
<guid>https://arxiv.org/abs/2511.18745</guid>
<content:encoded><![CDATA[
<div> Keywords: cell migration, fractional Brownian motion, chemotaxis, superdiffusion, stochastic optimization  

<br /><br />Summary:  
Cell migration exhibits long-range temporal correlations and anomalous diffusion even without external guidance like chemical gradients or physical constraints. This study investigates whether such temporal correlations arise purely from internal cellular mechanisms or if they play a functional role in navigation. The authors model temporally correlated noise using fractional Brownian motion and analyze its effect on chemotactic search behavior through computational experiments. They find that superdiffusive motion combined with gradient-driven migration enhances the cell's ability to explore chemoattractant landscapes effectively. This strategy enables cells to consistently locate the global maximum of chemoattractant concentration despite challenges such as spatial noise, competing signals, and irregular geometries. The robustness of these findings is demonstrated across various environmental setups, including flat and curved surfaces and scenarios with both external and self-generated chemotactic signals. The study quantifies performance by examining the distribution of first hitting times under different levels of temporal correlation in the movement noise. Importantly, the results offer broader implications beyond biology, providing insights into the design of optimization and sampling algorithms that leverage structured stochastic processes for improved performance. <div>
arXiv:2511.18745v1 Announce Type: cross 
Abstract: Cell migration often exhibits long-range temporal correlations and anomalous diffusion, even in the absence of external guidance cues such as chemical gradients or topographical constraints. These observations raise a fundamental question: do such correlations simply reflect internal cellular processes, or do they enhance a cell's ability to navigate complex environments? In this work, we explore how temporally correlated noise (modeled using fractional Brownian motion) influences chemotactic search dynamics. Through computational experiments, we show that superdiffusive motion, when combined with gradient-driven migration, enables robust exploration of the chemoattractant landscape. Cells reliably reach the global maximum of the concentration field, even in the presence of spatial noise, secondary cues, or irregular signal geometry. We quantify this behavior by analyzing the distribution of first hitting times under varying degrees of temporal correlation. Notably, our results are consistent across diverse conditions, including flat and curved substrates, and scenarios involving both primary and self-generated chemotactic signals. Beyond biological implications, these findings also offer insight into the design of optimization and sampling algorithms that benefit from structured stochasticity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TorchQuantumDistributed</title>
<link>https://arxiv.org/abs/2511.19291</link>
<guid>https://arxiv.org/abs/2511.19291</guid>
<content:encoded><![CDATA[
<div> Quantum Simulation, PyTorch, Distributed Computing, Quantum Circuits, Differentiable Programming  

<br /><br />Summary: TorchQuantumDistributed (tqd) is a software library developed using PyTorch that facilitates large-scale simulation of quantum state vectors. It is designed to be accelerator-agnostic, meaning it can efficiently operate across diverse hardware platforms without being limited to specific accelerators. The library enables differentiable simulation, allowing users to study parameterized quantum circuits that can be trained or optimized using gradient-based methods. This is particularly relevant for near-term quantum devices and fault-tolerant quantum circuits, where understanding circuit behavior with many qubits is essential. By supporting high qubit counts, tqd advances the ability to model complex quantum systems that are beyond the reach of classical simulation tools constrained to smaller qubit numbers. Researchers and developers can leverage tqd to experiment with learnable quantum circuits in applications such as quantum machine learning, variational algorithms, and quantum error correction. Overall, TorchQuantumDistributed provides an important tool for the quantum computing community to bridge classical and quantum computational methods in a scalable, flexible, and efficient manner. <div>
arXiv:2511.19291v1 Announce Type: cross 
Abstract: TorchQuantumDistributed (tqd) is a PyTorch-based [Paszke et al., 2019] library for accelerator-agnostic differentiable quantum state vector simulation at scale. This enables studying the behavior of learnable parameterized near-term and fault- tolerant quantum circuits with high qubit counts.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinAudio: A Benchmark for Audio Large Language Models in Financial Applications</title>
<link>https://arxiv.org/abs/2503.20990</link>
<guid>https://arxiv.org/abs/2503.20990</guid>
<content:encoded><![CDATA[
<div> Keywords: Audio Large Language Models, financial domain, ASR, audio summarization, FinAudio benchmark<br /><br />Summary:<br /><br />This paper addresses the gap in evaluating Audio Large Language Models (AudioLLMs) within the financial domain, where audio data such as earnings calls and CEO speeches play a vital role. The authors introduce FinAudio, the first benchmark specifically designed to assess the performance of AudioLLMs on financial audio tasks. They define three key tasks tailored to finance: automatic speech recognition (ASR) for short financial audio clips, ASR for long financial audio, and summarization of long financial audio recordings. To support these tasks, the authors curate two datasets for short audio and two for long audio, along with a novel dataset dedicated to financial audio summarization, collectively forming the FinAudio benchmark. Seven prominent AudioLLMs are evaluated on this benchmark to investigate their capabilities and limitations when applied to financial audio data. The evaluation highlights that current AudioLLMs perform suboptimally on financial tasks, indicating room for improvement. The study provides valuable insights and directions for enhancing AudioLLMs to better serve financial applications. The datasets and associated codes will be made publicly available to foster further research in this area. <div>
arXiv:2503.20990v2 Announce Type: replace 
Abstract: Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce \textsc{FinAudio}, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the \textsc{FinAudio} benchmark. Then, we evaluate seven prevalent AudioLLMs on \textsc{FinAudio}. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rapid cardiac activation prediction for cardiac resynchronization therapy planning using geometric deep learning</title>
<link>https://arxiv.org/abs/2506.08987</link>
<guid>https://arxiv.org/abs/2506.08987</guid>
<content:encoded><![CDATA[
<div> Cardiac resynchronization therapy, geometric deep learning, graph neural network, geometry-informed neural operator, pacing site optimization<br /><br />Summary:<br /><br />1. Cardiac resynchronization therapy (CRT) is an established treatment for patients with dyssynchronous heart failure, but about one-third of patients do not respond effectively, partly due to suboptimal lead placement.<br /><br />2. Identifying the optimal pacing site for CRT is difficult because of patient-specific anatomical variability and the limitations of current individualized planning approaches.<br /><br />3. To address this challenge, the study develops two geometric deep learning models—a graph neural network (GNN) and a geometry-informed neural operator (GINO)—to predict cardiac activation time maps in real time, assisting CRT planning and optimization.<br /><br />4. Both models were trained on extensive datasets generated from finite-element simulations incorporating diverse synthetic left ventricular geometries, pacing configurations, and tissue conductivities.<br /><br />5. Testing reveals that the GINO model outperforms the GNN on synthetic data (1.38% vs. 2.44% prediction error), while on real-world data both models show comparable accuracy.<br /><br />6. Using the models, a workflow was developed to optimize pacing sites from activation maps and ventricular geometries, successfully recovering subject-specific parameters from noisy data with low error.<br /><br />7. An interactive web-based GUI is provided at https://dcsim.egr.msu.edu/, demonstrating the potential of these tools as clinical decision-support systems for personalized, pre-procedural CRT optimization. <div>
arXiv:2506.08987v2 Announce Type: replace 
Abstract: Cardiac resynchronization therapy (CRT) is a common intervention for patients with dyssynchronous heart failure, yet approximately one-third of recipients fail to respond due to multiple contributing factors, including suboptimal lead placement. Identifying optimal pacing sites remains challenging, largely due to patient-specific anatomical variability and the limitations of current individualized planning strategies. In a step towards constructing an in-silico approach to help address this issue, we develop two geometric deep learning (DL) models, based on graph neural network (GNN) and geometry-informed neural operator (GINO), to predict cardiac activation time maps in real time for CRT planning and optimization. Both models are trained on a large dataset generated from finite-element (FE) simulations over a wide range of synthetic left ventricular (LV) geometries, pacing site configurations, and tissue conductivities. In testing, the GINO model outperforms the GNN model on synthetic test data, with lower prediction errors (1.38% vs 2.44%), while both demonstrate comparable performance on real-world LV geometries (GINO: 4.79% vs GNN: 4.07%). Using the trained models, we also develop a workflow for optimizing the pacing site in CRT from a given activation time map and LV geometry. The trained DL models were capable of recovering the ground truth subject-specific parameters from the noisy activation time map with small errors. In conjunction with an interactive web-based graphical user interface (GUI) available at https://dcsim.egr.msu.edu/, this study shows promising potential as a clinical decision-support tool for personalized pre-procedural CRT optimization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Simulator for Electrically Reconfigurable Electromagnetic Structures</title>
<link>https://arxiv.org/abs/2503.18479</link>
<guid>https://arxiv.org/abs/2503.18479</guid>
<content:encoded><![CDATA[
<div> Keywords: CUDA, PyTorch, gradient-based optimization, resonant electromagnetic structures, tunable metasurfaces<br /><br />Summary:<br /><br />This paper presents a novel PyTorch-based framework enabled by CUDA for the gradient-based optimization of reconfigurable electromagnetic structures with electrically tunable parameters. Unlike traditional methods that rely on non-gradient-based optimizations, this framework utilizes automatic differentiation, significantly enhancing efficiency and flexibility in optimization. It integrates seamlessly with deep learning frameworks, allowing advanced and sophisticated optimization strategies. The authors validate the framework through extensive simulations on various resonant structures: single-loop copper wire unit-cells, an 8x1 array, and an 8x8 array of inductively coupled resonant unit cells forming one-dimensional and two-dimensional metasurfaces. The results demonstrate precise control over the magnetic field component normal to each resonant structure's surface, achieving target field strengths with minimal error. The framework also efficiently solves the inverse design problem, which is crucial for practical applications. Compatibility with existing electromagnetic simulation tools further highlights its adaptability. Potential applications include advanced electromagnetic control in MRI systems and other fields requiring precise manipulation of resonant electromagnetic structures. Overall, this CUDA-enabled, PyTorch-based framework provides a powerful and versatile platform for future research and innovation in the design and optimization of tunable resonant electromagnetic devices. <div>
arXiv:2503.18479v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel CUDA-enabled PyTorch-based framework designed for the gradient-based optimization of such reconfigurable electromagnetic structures with electrically tunable parameters. Traditional optimization techniques for these structures often rely on non-gradient-based methods, limiting efficiency and flexibility. Our framework leverages automatic differentiation, facilitating the application of gradient-based optimization methods. This approach is particularly advantageous for embedding within deep learning frameworks, enabling sophisticated optimization strategies.
  We demonstrate the framework's effectiveness through comprehensive simulations involving resonant structures with tunable parameters. Key contributions include the efficient solution of the inverse problem. The framework's performance is validated using three different resonant structures: a single-loop copper wire (Unit-Cell) as well as an 8x1 and an 8x8 array of resonant unit cells with multiple inductively coupled unit cells (1d and 2d Metasurfaces). Results show precise in-silico control over the magnetic field's component normal to the surface of each resonant structure, achieving desired field strengths with minimal error. The proposed framework is compatible with existing simulation software.
  This PyTorch-based framework sets the stage for advanced electromagnetic control strategies for resonant structures with application in e.g. MRI, providing a robust platform for further exploration and innovation in the design and optimization of resonant electromagnetic structures.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?</title>
<link>https://arxiv.org/abs/2505.07078</link>
<guid>https://arxiv.org/abs/2505.07078</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, asset pricing, backtesting, market regimes, investment strategies<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) have been applied to asset pricing and stock trading by generating investment decisions from unstructured financial data. 2. Previous evaluations of LLM-based timing strategies were limited by short timeframes and small stock universes, leading to overestimated effectiveness due to survivorship and data-snooping biases. 3. The authors introduce FINSABER, a comprehensive backtesting framework that evaluates timing-based strategies across longer periods (two decades) and a broader universe of over 100 stock symbols. 4. Systematic backtests reveal that LLM advantages diminish significantly when tested over larger cross-sections and extended durations, contradicting earlier overly optimistic results. 5. Market regime analysis shows LLM strategies tend to be too conservative during bull markets, underperforming passive benchmarks, and too aggressive during bear markets, resulting in substantial losses. 6. These findings suggest that improving LLM investment strategies requires focusing on trend detection and regime-aware risk management, rather than simply increasing framework complexity. <div>
arXiv:2505.07078v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Sensitivity Analysis of Electric Machine Efficiency Maps and Profiles Under Design Uncertainty</title>
<link>https://arxiv.org/abs/2511.17099</link>
<guid>https://arxiv.org/abs/2511.17099</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate global sensitivity analysis, electric machine design, efficiency maps, Monte Carlo sampling, polynomial chaos expansions  

<br /><br />Summary:  
This paper introduces the application of multivariate global sensitivity analysis to evaluate the influence of uncertain design parameters on efficiency maps and profiles of electric machines. Unlike traditional variance-based (Sobol') sensitivity analyses that assess each element individually, the multivariate approach yields a single sensitivity index per parameter, enabling a comprehensive assessment over the entire efficiency map or profile. The method is demonstrated using permanent magnet synchronous machine models with varying levels of fidelity. Two computational approaches—Monte Carlo sampling and polynomial chaos expansions—are compared with respect to computational efficiency. The sensitivity analysis outcomes assist in model simplification by fixing parameters with negligible influence to their nominal values while allowing stochastic variation only in significant parameters. Validation through uncertainty quantification of both full and reduced models confirms that using multivariate sensitivity analysis effectively guides model simplification without compromising accuracy. This approach therefore provides a more holistic and computationally feasible framework for design parameter assessment and simplification in electric machine efficiency modeling. <div>
arXiv:2511.17099v1 Announce Type: new 
Abstract: This work proposes the use of multivariate global sensitivity analysis for assessing the impact of uncertain electric machine design parameters on efficiency maps and profiles. Contrary to the common approach of applying variance-based (Sobol') sensitivity analysis elementwise, multivariate sensitivity analysis provides a single sensitivity index per parameter, thus allowing for a holistic estimation of parameter importance over the full efficiency map or profile. Its benefits are demonstrated on permanent magnet synchronous machine models of different fidelity. Computations based on Monte Carlo sampling and polynomial chaos expansions are compared in terms of computational cost. The sensitivity analysis results are subsequently used to simplify the models, by fixing non-influential parameters to their nominal values and allowing random variations only for influential parameters. Uncertainty estimates obtained with the full and reduced models confirm the validity of model simplification guided by multivariate sensitivity analysis.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Generative Design Using Optimal Transport for Shape Exploration and Solution Field Interpolation</title>
<link>https://arxiv.org/abs/2511.17111</link>
<guid>https://arxiv.org/abs/2511.17111</guid>
<content:encoded><![CDATA[
<div> Generative Design, Optimal Transport, Gaussian Splatting, Wasserstein Barycenters, Physical Fidelity<br /><br />Summary:<br /><br />Generative Design (GD) integrates AI, physics-based modeling, and multi-objective optimization to improve engineering design processes. However, traditional GD methods face limitations such as dependency on large datasets for AI, high computational loads for topology optimization, and underdeveloped model order reduction for changing geometries. This paper introduces a unified framework using Optimal Transport (OT) to address these issues by enabling structure-preserving interpolation of complex geometries and associated physical solution fields across varying design spaces, even with differing mesh topologies and significant shape changes. The approach employs Gaussian splatting to provide a continuous, mesh-independent representation of physical solutions and Wasserstein barycenters to perform smooth, mass-preserving blending of geometries, unlike traditional surrogate models reliant on static meshes. This allows efficient interpolation of positive scalar fields across evolving geometries without mesh matching or fixed dimensionality. Additionally, OT preserves localized physical features, such as stress concentrations, by conserving spatial distributions rather than averaging, thus avoiding artificial smoothing. Preliminary results also extend the method to signed and vector fields. The framework demonstrates improvements in efficiency, adaptability, and physical accuracy, establishing a promising basis for integrating foundation-model-based generative design workflows in engineering applications. <div>
arXiv:2511.17111v1 Announce Type: new 
Abstract: Generative Design (GD) combines artificial intelligence (AI), physics-based modeling, and multi-objective optimization to autonomously explore and refine engineering designs. Despite its promise in aerospace, automotive, and other high-performance applications, current GD methods face critical challenges: AI approaches require large datasets and often struggle to generalize; topology optimization is computationally intensive and difficult to extend to multiphysics problems; and model order reduction for evolving geometries remains underdeveloped. To address these challenges, we introduce a unified, structure-preserving framework for GD based on optimal transport (OT), enabling simultaneous interpolation of complex geometries and their associated physical solution fields across evolving design spaces, even with non-matching meshes and substantial shape changes. This capability leverages Gaussian splatting to provide a continuous, mesh-independent representation of the solution and Wasserstein barycenters to enable smooth, mathematically ''mass''-preserving blending of geometries, offering a major advance over surrogate models tied to static meshes. Our framework efficiently interpolates positive scalar fields across arbitrarily shaped, evolving geometries without requiring identical mesh topology or dimensionality. OT also naturally preserves localized physical features -- such as stress concentrations or sharp gradients -- by conserving the spatial distribution of quantities, interpreted as ''mass'' in a mathematical sense, rather than averaging them, avoiding artificial smoothing. Preliminary extensions to signed and vector fields are presented. Representative test cases demonstrate enhanced efficiency, adaptability, and physical fidelity, establishing a foundation for future foundation-model-powered generative design workflows.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Randomness as Reference: Benchmark Metric for Optimization in Engineering</title>
<link>https://arxiv.org/abs/2511.17226</link>
<guid>https://arxiv.org/abs/2511.17226</guid>
<content:encoded><![CDATA[
<div> Keywords: optimization benchmarking, engineering design, performance metric, metaheuristics, benchmark suite<br /><br />Summary:<br /><br />Benchmarking optimization algorithms is critical for advancing computational intelligence, but currently used artificial test suites often fail to reflect the complexity and diversity of real-world engineering problems. This paper introduces a new benchmark suite consisting of 231 bounded, continuous, and unconstrained optimization problems, mostly sourced from realistic engineering design and simulation contexts such as computational fluid dynamics and finite element analysis. Along with this suite, a novel performance metric is proposed that uses random sampling as a statistical baseline, enabling nonlinear normalization of objective values and facilitating unbiased comparisons of the efficiency of different algorithms across various heterogeneous problems. The authors evaluated 20 deterministic and stochastic optimization methods, conducting hundreds of independent runs per problem to ensure statistical rigor. Results reveal that only a small subset of these methods consistently delivers excellent performance, whereas several widely used metaheuristics suffer significant efficiency degradation on engineering-oriented problems, highlighting the limitations of existing benchmarks. Additionally, the study explores features of the tested optimization methods to provide actionable guidelines for their practical application. Together, the benchmark suite and the new metric offer a transparent, reproducible, and application-relevant platform for evaluating and comparing optimization algorithms, effectively bridging the gap between current benchmarks and realistic engineering use cases. <div>
arXiv:2511.17226v1 Announce Type: new 
Abstract: Benchmarking optimization algorithms is fundamental for the advancement of computational intelligence. However, widely adopted artificial test suites exhibit limited correspondence with the diversity and complexity of real-world engineering optimization tasks. This paper presents a new benchmark suite comprising 231 bounded, continuous, unconstrained optimization problems, the majority derived from engineering design and simulation scenarios, including computational fluid dynamics and finite element analysis models. In conjunction with this suite, a novel performance metric is introduced, which employs random sampling as a statistical reference, providing nonlinear normalization of objective values and enabling unbiased comparison of algorithmic efficiency across heterogeneous problems. Using this framework, 20 deterministic and stochastic optimization methods were systematically evaluated through hundreds of independent runs per problem, ensuring statistical robustness. The results indicate that only a few of the tested optimization methods consistently achieve excellent performance, while several commonly used metaheuristics exhibit severe efficiency loss on engineering-type problems, emphasizing the limitations of conventional benchmarks. Furthermore, the conducted tests are used for analyzing various features of the optimization methods, providing practical guidelines for their application. The proposed test suite and metric together offer a transparent, reproducible, and practically relevant platform for evaluating and comparing optimization methods, thereby narrowing the gap between the available benchmark tests and realistic engineering applications.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists</title>
<link>https://arxiv.org/abs/2511.16931</link>
<guid>https://arxiv.org/abs/2511.16931</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, AI Scientists, collaborative research, scientific workflow, evaluation platform  

<br /><br />Summary: With the advancement of Large Language Models (LLMs), AI agents, termed "AI Scientists," show growing ability in scientific processes such as hypothesis generation, experiment design, and manuscript writing. However, current AI Scientists treat scientific discovery mainly as isolated search or optimization tasks, neglecting the inherently social and collaborative nature of real-world scientific research. Real scientific progress depends on a complex infrastructure including collaboration, contribution attribution, peer review, and structured knowledge networks. To address these shortcomings, the paper introduces OmniScientist, a comprehensive framework that integrates human research mechanisms directly into AI scientific workflows. OmniScientist supports full automation spanning data management, literature review, ideation, experiment execution, writing, and peer review. It also encompasses infrastructural components, including (1) a structured knowledge system leveraging citation networks and conceptual relationships, (2) a collaborative research protocol called OSP permitting multi-agent and human collaboration, and (3) an open evaluation platform named ScienceArena employing blind pairwise user voting and Elo ranking methods. This infrastructure enables AI agents to understand and utilize human scientific systems, collaborate effectively, and co-evolve with human researchers, ultimately fostering a sustainable, scalable ecosystem for innovation. <div>
arXiv:2511.16931v1 Announce Type: cross 
Abstract: With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as "AI Scientists." However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs</title>
<link>https://arxiv.org/abs/2511.17220</link>
<guid>https://arxiv.org/abs/2511.17220</guid>
<content:encoded><![CDATA[
<div> Keywords: PARROT, sycophancy, robustness, large language models, epistemic collapse  

<br /><br />Summary:  
This study introduces PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a framework designed to evaluate how large language models (LLMs) degrade in accuracy under social pressure influenced by authority and persuasion, a phenomenon called sycophancy (excessive conformity). PARROT uniquely isolates causal effects by employing a double-blind comparison between neutral questions and authoritatively false versions. It measures shifts in model confidence toward both correct and imposed false answers using log-likelihood-based calibration tracking. Further, it classifies failure modes into an eight-state behavioral taxonomy, including categories like robust correct, sycophantic agreement, reinforced error, stubborn error, and self-correction. The evaluation covered 22 LLMs tested on 1,302 MMLU-style multiple-choice questions spanning 13 domains and corresponding authority templates. Results reveal significant variation: advanced models such as GPT-5, GPT-4.1, and Claude Sonnet 4.5 show low rates of conforming to false authority (follow rates ≤11%) and minor accuracy degradation (e.g., GPT-5 at 4%), whereas older and smaller models like GPT-4 and Qwen 2.5-1.5B suffer substantial epistemic collapse (80%-94%). Fragility is higher in domains like international law and global knowledge, while elementary mathematics remains comparatively stable. The authors argue that resistance to overfitting social pressure should be prioritized alongside accuracy, harm avoidance, and privacy for the safe deployment of LLMs in real-world settings. <div>
arXiv:2511.17220v1 Announce Type: cross 
Abstract: This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low "follow rates" ($\leq 11\%$, GPT-5: 4\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\%, Qwen 2.5-1.5B: 94\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of "resistance to overfitting pressure" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fundamental Limitations of QAOA on Constrained Problems and a Route to Exponential Enhancement</title>
<link>https://arxiv.org/abs/2511.17259</link>
<guid>https://arxiv.org/abs/2511.17259</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum Approximate Optimization Algorithm, constrained optimization, permutation constraints, feasibility bottleneck, constraint embedding  

<br /><br />Summary:  
1. The paper investigates the intrinsic limitations of the standard Quantum Approximate Optimization Algorithm (QAOA) when applied to constrained optimization problems, particularly where valid solutions lie on a low-dimensional manifold within the Boolean hypercube.  
2. It identifies a fundamental feasibility bottleneck for the generic QAOA ansatz using a transverse field mixer and diagonal r-local cost: circuits with depth scaling linearly in problem size \(n\) fail to concentrate a significant probability mass on feasible solutions beyond the uniform distribution over the full Hilbert space.  
3. To overcome this limitation, the authors propose a constraint enhanced kernel QAOA (CE QAOA) that operates within a restricted product one-hot subspace and utilizes a block-local XY Hamiltonian mixer designed to maintain feasibility.  
4. For permutation constraint problems, CE QAOA achieves an angle-robust and depth-matched exponential improvement, where the ratio of feasible probability mass compared to generic QAOA grows exponentially in \(n^2\) up to linear circuit depths, under reasonable assumptions about the interaction hypergraph.  
5. The approach generalizes beyond permutations to a broad class of NP-hard constrained problems by leveraging problem-algorithm co-design, enabling provable exponential enhancements in the solution quality over traditional generic QAOA methods. <div>
arXiv:2511.17259v1 Announce Type: cross 
Abstract: We study fundamental limitations of the generic Quantum Approximate Optimization Algorithm (QAOA) on constrained problems where valid solutions form a low dimensional manifold inside the Boolean hypercube, and we present a provable route to exponential improvements via constraint embedding. Focusing on permutation constrained objectives, we show that the standard generic QAOA ansatz, with a transverse field mixer and diagonal r local cost, faces an intrinsic feasibility bottleneck: even after angle optimization, circuits whose depth grows at most linearly with n cannot raise the total probability mass on the feasible manifold much above the uniform baseline suppressed by the size of the full Hilber space. Against this envelope we introduce a minimal constraint enhanced kernel (CE QAOA) that operates directly inside a product one hot subspace and mixes with a block local XY Hamiltonian. For permutation constrained problems, we prove an angle robust, depth matched exponential enhancement where the ratio between the feasible mass from CE QAOA and generic QAOA grows exponentially in $n^2$ for all depths up to a linear fraction of n, under a mild polynomial growth condition on the interaction hypergraph. Thanks to the problem algorithm co design in the kernel construction, the techniques and guarantees extend beyond permutations to a broad class of NP-Hard constrained optimization problems.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FORWARD: Dataset of a forwarder operating in rough terrain</title>
<link>https://arxiv.org/abs/2511.17318</link>
<guid>https://arxiv.org/abs/2511.17318</guid>
<content:encoded><![CDATA[
<div> Keywords: forwarder dataset, multimodal sensors, forest terrain, autonomous control, forestry machines<br /><br />Summary:<br /><br />1. The article presents FORWARD, a high-resolution multimodal dataset captured from a Komatsu cut-to-length forwarder operating in rough Swedish forest terrain.<br /><br />2. The forwarder is equipped with diverse sensors including RTK-GNSS for precise positioning, 360-degree cameras, operator vibration sensors, internal CAN-bus signals, and multiple IMUs, providing rich multimodal data.<br /><br />3. The dataset comprises about 18 hours of annotated wood extraction work, detailed event logs recorded at 5 Hz (including speed, fuel consumption, crane usage), and highly detailed terrain laser scans (~1500 points/m²), alongside production logs and extensive video.<br /><br />4. Various experiment scenarios are included, such as driving routes with and without steel tracks, altered load weights, and different target speeds, enabling testing under different operational conditions.<br /><br />5. The dataset aims to support development of AI models and algorithms for forest machine trafficability, perception, autonomous control, simulation calibration, and automation scenario generation, focusing on efficiency, safety, fuel consumption, and environmental impact during typical forwarder operations like terrain traversal, obstacle avoidance, and log handling. <div>
arXiv:2511.17318v1 Announce Type: cross 
Abstract: We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning for Glass Composition Screening</title>
<link>https://arxiv.org/abs/2410.24083</link>
<guid>https://arxiv.org/abs/2410.24083</guid>
<content:encoded><![CDATA[
<div> Keywords: glass composition, self-supervised learning, glass transition temperature, DeepGlassNet, data augmentation  

<br /><br />Summary: Glass composition screening is critical for developing new glass materials but is complicated by the complexity of multicomponent systems. Traditional supervised learning methods require large, high-quality datasets and often suffer from overfitting to noisy data, limiting their ability to generalize. To address this, the authors propose a self-supervised learning framework that reframes the screening problem as a classification task, predicting whether a glass composition's glass transition temperature (Tg) lies within a specified range. To enhance robustness against noise, the study introduces a novel data augmentation strategy based on asymptotic theory. The research further develops DeepGlassNet, a specialized neural network architecture designed to effectively model and analyze complex interactions among elements in glass compositions. Experimental results show that DeepGlassNet outperforms existing methods in screening accuracy and demonstrates strong adaptability for other composition-related screening challenges. This work not only offers an efficient approach to designing multicomponent glasses but also lays a foundation for utilizing self-supervised learning techniques in the broader context of materials discovery. The authors have made their code and data publicly accessible to facilitate further research and application. <div>
arXiv:2410.24083v3 Announce Type: replace 
Abstract: Glass composition screening is essential for advancing new glass materials, yet the inherent complexity of multicomponent systems presents significant challenges. Current supervised learning methods for this task rely heavily on large amounts of high-quality data and are prone to overfitting on noisy samples, which limits their generalization ability. In this work, we propose a novel self-supervised learning framework designed specifically for screening glass compositions within pre-defined glass transition temperature (Tg) ranges. We reformulate the screening task as a classification problem, aiming to predict whether the glass transition temperature of a given composition falls within a target interval. To improve the model's robustness to noise, we introduce an innovative data augmentation strategy grounded in asymptotic theory. Additionally, we present DeepGlassNet, a dedicated network architecture developed to capture and analyze the complex interactions among constituent elements in glass compositions. Experimental results demonstrate that DeepGlassNet achieves superior screening accuracy compared to traditional methods and exhibits strong adaptability to other composition-related screening tasks. This study not only provides an efficient methodology for designing multicomponent glasses but also establishes a foundation for applying self-supervised learning in material discovery. Code and data are available at: https://github.com/liubin06/DeepGlassNet
]]></content:encoded>
<pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrated 4D/5D Digital-Twin Framework for Cost Estimation and Probabilistic Schedule Control: A Texas Mid-Rise Case Study</title>
<link>https://arxiv.org/abs/2511.15711</link>
<guid>https://arxiv.org/abs/2511.15711</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D/5D digital-twin, Building Information Modeling (BIM), deep reinforcement learning (DRL), Bayesian risk modeling, construction cost control<br /><br />Summary: This study addresses persistent cost and schedule overruns in U.S. building projects by developing an integrated 4D/5D digital-twin framework that unifies Building Information Modeling (BIM), natural language processing (NLP), reality capture, computer vision, Bayesian risk modeling, and deep reinforcement learning (DRL). The system automates key project-control functions by: (a) mapping contract documents to standardized cost items using a transformer-based NLP model achieving a weighted F1 score of 0.883; (b) aligning photogrammetry and LiDAR data with BIM to compute earned value; (c) deriving real-time activity completion status from site imagery with a micro accuracy of 0.891; (d) updating probabilistic Critical Path Method (CPM) forecasts through Bayesian inference and Monte Carlo simulation; (e) employing DRL for adaptive resource allocation with a 75% adoption rate; and (f) offering a 4D/5D decision sandbox for predictive analysis. A case study on a Texas mid-rise building project used localized cost adjustments via RSMeans City Cost Index and Bureau of Labor Statistics wage data. The results demonstrated a 43% reduction in labor estimating effort, a 6% reduction in overtime hours (91 hours), and project completion aligning with the P50 probabilistic forecast of 128 days, indicating enhanced estimation accuracy and responsiveness to dynamic field conditions. <div>
arXiv:2511.15711v1 Announce Type: new 
Abstract: Persistent cost and schedule overruns in U.S. building projects expose limitations of conventional, document-based estimating and deterministic Critical Path Method (CPM) scheduling, which remain inflexible under uncertainty and lag dynamic field conditions. This study presents an integrated 4D/5D digital-twin framework unifying Building Information Modeling (BIM), natural language processing (NLP), reality capture, computer vision, Bayesian risk modeling, and deep reinforcement learning (DRL) for construction cost and schedule control. The system automates project-control functions by: (a) mapping contract documents to standardized cost items using transformer-based NLP (0.883 weighted F1 score); (b) aligning photogrammetry and LiDAR data with BIM to compute earned value; (c) deriving real-time activity completion from site imagery (0.891 micro accuracy); (d) updating probabilistic CPM forecasts via Bayesian inference and Monte Carlo simulation; (e) using DRL for adaptive resource allocation (75% adoption rate); and (f) providing 4D/5D decision sandbox for predictive analysis. A Texas mid-rise case study demonstrates localized cost adjustment using RSMeans City Cost Index and Bureau of Labor Statistics wage data. Results show 43% reduction in estimating labor, 6% overtime reduction (91 hours), and project completion matching P50 probabilistic forecast of 128 days, confirming improved estimation accuracy and responsiveness.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AquaSentinel: Next-Generation AI System Integrating Sensor Networks for Urban Underground Water Pipeline Anomaly Detection via Collaborative MoE-LLM Agent Architecture</title>
<link>https://arxiv.org/abs/2511.15870</link>
<guid>https://arxiv.org/abs/2511.15870</guid>
<content:encoded><![CDATA[
<div> Keywords: underground pipeline leaks, physics-informed AI, sparse sensor deployment, spatiotemporal graph neural networks, leak localization  

<br /><br />Summary: This paper introduces AquaSentinel, a physics-informed AI system designed for real-time anomaly detection in urban underground water pipeline networks to combat leaks and infiltrations that threaten water security and environmental safety. First, it employs strategic sparse sensor deployment at critical high-centrality nodes paired with physics-based state augmentation, enabling network-wide observability with minimal hardware. Second, the RTCA (Real-Time Cumulative Anomaly) detection algorithm utilizes a dual-threshold monitoring method with adaptive statistics to effectively differentiate between transient fluctuations and true anomalies. Third, the system features a Mixture of Experts (MoE) ensemble of spatiotemporal graph neural networks, which dynamically weigh model outputs to provide robust and accurate predictions. Fourth, it integrates causal flow-based leak localization to trace anomalies upstream, pinpointing the exact source nodes and impacted pipe segments. By deploying sensors only at critical junctions and leveraging physics-based modeling, AquaSentinel creates virtual sensors that enhance data coverage across unmonitored network parts. Experimental validation using 110 distinct leak scenarios demonstrated that AquaSentinel achieves perfect (100%) detection accuracy. This approach proves that sparse sensing informed by physics can match dense sensor deployments’ performance while significantly reducing infrastructure costs, offering a practical and scalable solution for monitoring aging urban water pipeline infrastructure. <div>
arXiv:2511.15870v1 Announce Type: new 
Abstract: Underground pipeline leaks and infiltrations pose significant threats to water security and environmental safety. Traditional manual inspection methods provide limited coverage and delayed response, often missing critical anomalies. This paper proposes AquaSentinel, a novel physics-informed AI system for real-time anomaly detection in urban underground water pipeline networks. We introduce four key innovations: (1) strategic sparse sensor deployment at high-centrality nodes combined with physics-based state augmentation to achieve network-wide observability from minimal infrastructure; (2) the RTCA (Real-Time Cumulative Anomaly) detection algorithm, which employs dual-threshold monitoring with adaptive statistics to distinguish transient fluctuations from genuine anomalies; (3) a Mixture of Experts (MoE) ensemble of spatiotemporal graph neural networks that provides robust predictions by dynamically weighting model contributions; (4) causal flow-based leak localization that traces anomalies upstream to identify source nodes and affected pipe segments. Our system strategically deploys sensors at critical network junctions and leverages physics-based modeling to propagate measurements to unmonitored nodes, creating virtual sensors that enhance data availability across the entire network. Experimental evaluation using 110 leak scenarios demonstrates that AquaSentinel achieves 100% detection accuracy. This work advances pipeline monitoring by demonstrating that physics-informed sparse sensing can match the performance of dense deployments at a fraction of the cost, providing a practical solution for aging urban infrastructure.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Enhanced Whale Optimization Algorithm with Log-Normal Distribution for Optimizing Coverage of Wireless Sensor Networks</title>
<link>https://arxiv.org/abs/2511.15970</link>
<guid>https://arxiv.org/abs/2511.15970</guid>
<content:encoded><![CDATA[
<div> Keywords: Wireless Sensor Networks, Whale Optimization Algorithm, log-normal distribution, coverage optimization, convergence enhancement  

<br /><br />Summary:  
This paper addresses the limitations of the traditional Whale Optimization Algorithm (WOA), such as limited exploration and premature convergence, in the context of Wireless Sensor Networks (WSNs) coverage optimization. To overcome these issues, the authors propose an enhanced WOA variant named GLNWOA, which incorporates a log-normal distribution model to improve convergence dynamics and search diversity. GLNWOA introduces three key mechanisms: Good Nodes Set initialization to achieve a uniform population distribution, a Leader Cognitive Guidance Mechanism to facilitate efficient information sharing, and an Enhanced Spiral Updating Strategy that balances global exploration with local exploitation. The proposed algorithm is tested on benchmark functions, demonstrating superior convergence accuracy and robustness compared to standard methods. In practical application for WSNs, deploying 25 sensor nodes in a 60 m × 60 m area, GLNWOA attained a 99.0013% coverage rate. This performance outperforms competing algorithms such as AROA, WOA, HHO, ROA, and WOABAT by up to 15.5%. Overall, GLNWOA provides fast convergence speed, high stability, and excellent optimization capability, making it a promising solution for intelligent network deployment in complex monitoring environments. <div>
arXiv:2511.15970v1 Announce Type: new 
Abstract: Wireless Sensor Networks (WSNs) are essential for monitoring and communication in complex environments, where coverage optimization directly affects performance and energy efficiency. However, traditional algorithms such as the Whale Optimization Algorithm (WOA) often suffer from limited exploration and premature convergence. To overcome these issues, this paper proposes an enhanced WOA which is called GLNWOA. GLNWOA integrates a log-normal distribution model into WOA to improve convergence dynamics and search diversity. GLNWOA employs a Good Nodes Set initialization for uniform population distribution, a Leader Cognitive Guidance Mechanism for efficient information sharing, and an Enhanced Spiral Updating Strategy to balance global exploration and local exploitation. Tests on benchmark functions verify its superior convergence accuracy and robustness. In WSN coverage optimization, deploying 25 nodes in a 60 m $\times$ 60 m area achieved a 99.0013\% coverage rate, outperforming AROA, WOA, HHO, ROA, and WOABAT by up to 15.5\%. These results demonstrate that GLNWOA offers fast convergence, high stability, and excellent optimization capability for intelligent network deployment.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring</title>
<link>https://arxiv.org/abs/2511.16628</link>
<guid>https://arxiv.org/abs/2511.16628</guid>
<content:encoded><![CDATA[
<div> Keywords: Structural Health Monitoring, Bayesian inverse framework, flexural rigidity, Fisher information, sensor placement<br /><br />Summary:<br /><br />1. This paper addresses the challenge of recovering distributed mechanical parameters in Structural Health Monitoring (SHM) from sparse data, a problem often ill-posed and problematic for parameter identifiability and solution reliability.  
2. Unlike deterministic approaches like Tikhonov regularization, the authors propose a Bayesian inverse framework that rigorously quantifies spatial resolution limits and inherent uncertainty in inferred mechanical parameter fields.  
3. The method uses Fisher information as a diagnostic tool to evaluate the informativeness of different sensor layouts and load paths, thereby illustrating how these factors constrain what spatial features of the parameter field can be reliably recovered.  
4. The framework is tested on real experimental data obtained from vehicle passages over the full-scale openLAB research bridge at TU Dresden, focusing on inferring distributed flexural rigidity from rotation (tilt) influence lines.  
5. Results show that despite high overall information content in the data, its usefulness varies spatially due to sensor placement and experiment design, with the Bayesian approach providing credible intervals that reveal regions of practical non-identifiability. This unifies identification with uncertainty quantification and offers a principled basis for optimizing sensor deployment and improving diagnostic credibility in SHM. <div>
arXiv:2511.16628v1 Announce Type: new 
Abstract: In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.
  The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Detailed Comparative Analysis of Blockchain Consensus Mechanisms</title>
<link>https://arxiv.org/abs/2511.15730</link>
<guid>https://arxiv.org/abs/2511.15730</guid>
<content:encoded><![CDATA[
<div> Proof of Work, Proof of Stake, blockchain consensus, scalability, environmental impact  

<br /><br />Summary:  
This paper provides a detailed comparative analysis of two primary blockchain consensus mechanisms: Proof of Work (PoW) and Proof of Stake (PoS). First, it examines energy use, highlighting PoW’s high energy consumption compared to PoS’s significantly reduced environmental impact. Second, the study discusses security, noting that PoW offers robust and time-tested protection, while PoS’s long-term security maturity remains a concern. Third, transaction speed and scalability are evaluated, with PoS demonstrating superior scalability and faster throughput than PoW. Fourth, centralization risk is analyzed, revealing PoW’s tendency toward centralization through mining pools, whereas PoS faces potential validator centralization. Fifth, transaction fees are reviewed, showing that PoS maintains more stable fee structures. Combining insights from recent academic research and real-world blockchain data, the paper highlights the inherent trade-offs between these mechanisms. Ultimately, the study suggests that hybrid designs could harness the strengths of both PoW’s strong security and PoS’s efficiency and sustainability. The goal is to guide future blockchain development by balancing decentralization, performance, and ecological responsibility. <div>
arXiv:2511.15730v1 Announce Type: cross 
Abstract: This paper presents a comprehensive comparative analysis of two dominant blockchain consensus mechanisms, Proof of Work (PoW) and Proof of Stake (PoS), evaluated across seven critical metrics: energy use, security, transaction speed, scalability, centralization risk, environmental impact, and transaction fees. Utilizing recent academic research and real-world blockchain data, the study highlights that PoW offers robust, time-tested security but suffers from high energy consumption, slower throughput, and centralization through mining pools. In contrast, PoS demonstrates improved scalability and efficiency, significantly reduced environmental impact, and more stable transaction fees, however it raises concerns over validator centralization and long-term security maturity. The findings underscore the trade-offs inherent in each mechanism and suggest hybrid designs may combine PoW's security with PoS's efficiency and sustainability. The study aims to inform future blockchain infrastructure development by striking a balance between decentralization, performance, and ecological responsibility.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithms and optimizations for global non-linear hybrid fluid-kinetic finite element stellarator simulations</title>
<link>https://arxiv.org/abs/2511.16412</link>
<guid>https://arxiv.org/abs/2511.16412</guid>
<content:encoded><![CDATA[
<div> Keywords: stellarator plasmas, hybrid fluid-kinetic models, toroidal Fourier modes, JOREK framework, spectral convergence  

<br /><br />Summary:  
This paper addresses the challenge of predictive modeling for stellarator plasmas, emphasizing difficulties unique to non-axisymmetric stellarator geometries that cause coupling of toroidal Fourier modes unlike in tokamaks. To overcome these challenges, the authors introduce a novel globally coupled projection scheme implemented within the JOREK finite element framework. This scheme facilitates a self-consistent and physically accurate transfer of kinetic markers to the fluid grid by creating and solving a single, unified linear system that simultaneously includes all toroidal harmonics. The construction of this system’s matrix is significantly sped up by leveraging the Fast Fourier Transform (FFT), which reduces computational cost. Additionally, the use of a 3D R-Tree spatial index enables efficient localization of millions of particles, ensuring that the method remains computationally feasible at large scale. Validation on realistic Wendelstein 7-X stellarator geometries demonstrates the high fidelity of the proposed approach, with convergence tests confirming that the coupled scheme achieves the expected spectral convergence, in stark contrast to the inferior performance of uncoupled approaches. Ultimately, this work delivers a validated, high-fidelity computational tool crucial for the predictive simulation and optimization of advanced stellarator designs for nuclear fusion research. <div>
arXiv:2511.16412v1 Announce Type: cross 
Abstract: Predictive modeling of stellarator plasmas is crucial for advancing nuclear fusion energy, yet it faces unique computational difficulties. One of the main challenges is accurately simulating the dynamics of specific particle species that are not well captured by fluid models, which necessitates the use of hybrid fluid-kinetic models. The non-axisymmetric geometry of stellarators fundamentally couples the toroidal Fourier modes, in contrast to what happens in tokamaks, requiring different numerical and computational treatment.
  This work presents a novel, globally coupled projection scheme inside the JOREK finite element framework. The approach ensures a self-consistent and physically accurate transfer of kinetic markers to the fluid grid, effectively handling the complex 3D mesh by constructing and solving a unified linear system that encompasses all toroidal harmonics simultaneously. To manage the computational complexity of this coupling, the construction of the system's matrix is significantly accelerated using the Fast Fourier Transform (FFT). The efficient localization of millions of particles is made possible by implementing a 3D R-Tree spatial index, which supports this projection and ensures computational tractability at scale.
  On realistic Wendelstein 7-X stellarator geometries, the fidelity of the framework is rigorously shown. In sharp contrast to the uncoupled approaches' poor performance, quantitative convergence tests verify that the coupled scheme attains the theoretically anticipated spectral convergence.
  This study offers a crucial capability for the predictive analysis and optimization of next-generation stellarator designs by developing a validated, high-fidelity computational tool.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Gap in XAI-Why Reliable Metrics Matter for Explainability and Compliance</title>
<link>https://arxiv.org/abs/2502.04695</link>
<guid>https://arxiv.org/abs/2502.04695</guid>
<content:encoded><![CDATA[
<div> Keywords: explainability, AI governance, XAI metrics, tamper resistance, model alignment  

<br /><br />Summary:  
1. Reliable explainability is essential not only for technical reasons but as a foundational element of private AI governance, especially as AI systems are used in high-stakes sectors.  
2. Private actors including auditors, insurers, certification bodies, and procurement agencies require standardized evaluation metrics to properly assess AI model trustworthiness.  
3. Current explainable AI (XAI) evaluation metrics are fragmented and vulnerable to manipulation, which undermines accountability and legal compliance.  
4. The authors argue that standardized metrics should serve as governance primitives to embed auditability and accountability directly within AI systems, supporting effective private oversight.  
5. They identify key limitations of existing metrics in ensuring faithfulness (the accuracy of explanations), tamper resistance (preventing manipulation), and alignment with regulatory requirements.  
6. Interpretability plays a crucial role in supporting model alignment by providing verifiable evidence of behavioral integrity, particularly in General Purpose AI (GPAI) systems, helping to prevent alignment faking.  
7. The paper proposes a "Governance by Metrics" paradigm that places explainability evaluation at the core of private AI governance. This hierarchical framework links transparency, tamper resistance, scalability, and legal alignment.  
8. The framework extends assessment beyond introspection to broader systemic accountability.  
9. Finally, the authors outline a roadmap to integrate explainability metrics into continuous AI assurance pipelines designed to serve both private oversight and regulatory purposes. <div>
arXiv:2502.04695v2 Announce Type: replace-cross 
Abstract: Reliable explainability is not only a technical goal but also a cornerstone of private AI governance. As AI models enter high-stakes sectors, private actors such as auditors, insurers, certification bodies, and procurement agencies require standardized evaluation metrics to assess trustworthiness. However, current XAI evaluation metrics remain fragmented and prone to manipulation, which undermines accountability and compliance. We argue that standardized metrics can function as governance primitives, embedding auditability and accountability within AI systems for effective private oversight. Building upon prior work in XAI benchmarking, we identify key limitations in ensuring faithfulness, tamper resistance, and regulatory alignment. Furthermore, interpretability can directly support model alignment by providing a verifiable means of ensuring behavioral integrity in General Purpose AI (GPAI) systems. This connection between interpretability and alignment positions XAI metrics as both technical and regulatory instruments that help prevent alignment faking, a growing concern among oversight bodies. We propose a Governance by Metrics paradigm that treats explainability evaluation as a central mechanism of private AI governance. Our framework introduces a hierarchical model linking transparency, tamper resistance, scalability, and legal alignment, extending evaluation from model introspection toward systemic accountability. Through conceptual synthesis and alignment with governance standards, we outline a roadmap for integrating explainability metrics into continuous AI assurance pipelines that serve both private oversight and regulatory needs.
]]></content:encoded>
<pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A High-Fidelity Neurosurgical Training Platform for Bimanual Procedures: A Feasibility Study</title>
<link>https://arxiv.org/abs/2511.14879</link>
<guid>https://arxiv.org/abs/2511.14879</guid>
<content:encoded><![CDATA[
<div> Keywords: neurosurgical simulation, bimanual psychomotor skills, instrument tracking, performance assessment, subpial corticectomy<br /><br />Summary:<br /><br />1. The study addresses the challenge of acquiring and objectively assessing bimanual psychomotor proficiency critical for neurosurgical procedures.  
2. Researchers developed a neurosurgical simulation platform that integrates an anatomically realistic ex-vivo calf brain model with a multi-camera tracking system to capture simultaneous motion of surgical instruments in both hands.  
3. The system records real-time instrument trajectories along with synchronized video, enabling extraction of multiple metrics including motion-based, time-based, and bimanual coordination parameters.  
4. A case series with 47 participants spanning four training levels — medical students, junior residents, senior residents, and neurosurgeons — was conducted to evaluate the platform.  
5. Results showed an 81% success rate in capturing instrument motion during active use, and several metrics such as instrument usage duration and bimanual coordination (e.g., tip separation distance, simultaneous usage time) significantly differentiated expertise levels.  
6. The study concludes that the simulation platform is feasible for tracking complex bimanual tasks in neurosurgical training and that the motion-derived metrics offer a foundation for objective performance assessment, potentially enhancing training and evaluation. <div>
arXiv:2511.14879v1 Announce Type: new 
Abstract: Background. Bimanual psychomotor proficiency is fundamental to neurosurgical procedures, yet it remains difficult for trainees to acquire and for educators to objectively evaluate performance. In this study, we investigate the feasibility of a neurosurgical simulation platform that integrates an anatomically realistic brain model with surgical instrument tracking to support training and objective assessment of bimanual tasks in the context of subpial corticectomy. Methods. We developed and evaluated a neurosurgical simulation platform based on an ex-vivo calf brain model and a multi-camera tracking system capable of simultaneously capturing the motion of surgical instruments in both hands, including collection of real-time instrument trajectories and synchronized video recordings. These enabled extraction of motion-based, time-based, and bimanual coordination metrics. We conducted a case series involving 47 participants across four training levels: medical students, junior residents, senior residents, and neurosurgeons. Results. The tracking system successfully captured instrument motion during 81% of the periods when instruments were actively used throughout the simulation procedure. Several extracted metrics were able to significantly differentiate between levels of surgical expertise. In particular, instrument usage duration and custom-defined bimanual coordination metrics such as instrument tip separation distance and simultaneous usage time, show potential as features to identify participant expertise levels with different instruments. Conclusions. We demonstrated the feasibility of tracking surgical instruments during complex bimanual tasks in an ex-vivo brain simulation platform. The metrics developed provide a foundation for objective performance assessment and highlight the potential of motion analysis to improve neurosurgical training and evaluation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Walls Have Ears: Unveiling Cross-Chain Sandwich Attacks in DeFi</title>
<link>https://arxiv.org/abs/2511.15245</link>
<guid>https://arxiv.org/abs/2511.15245</guid>
<content:encoded><![CDATA[
<div> Cross-chain interoperability, sandwich attacks, liquidity pools, front-running, MEV<br /><br />Summary:<br /><br />1. Cross-chain interoperability enables asset transfers and composable applications across blockchain ecosystems but can expose sensitive transaction details through transparent cross-chain messages.<br /><br />2. The study focuses on a newly identified vulnerability in liquidity pool-based cross-chain bridge protocols, where attackers exploit source chain event emissions to glean destination chain transaction details prior to their appearing in the destination chain mempool.<br /><br />3. This early information allows attackers to execute cross-chain sandwich attacks by placing front-running and back-running transactions ahead of existing MEV bots, making current sandwich-attack defenses ineffective against this variant.<br /><br />4. An empirical analysis was conducted on two months (August 10 to October 10, 2025) of transaction data from the Symbiosis protocol, utilizing a heuristic detection model tailored to identify these cross-chain sandwich attacks.<br /><br />5. Results reveal that these attacks accumulated over $5.27 million in profits, representing approximately 1.28% of the total bridged volume during the study period, highlighting a significant security threat in cross-chain bridging protocols. <div>
arXiv:2511.15245v1 Announce Type: new 
Abstract: Cross-chain interoperability is a core component of modern blockchain infrastructure, enabling seamless asset transfers and composable applications across multiple blockchain ecosystems. However, the transparency of cross-chain messages can inadvertently expose sensitive transaction information, creating opportunities for adversaries to exploit value through manipulation or front-running strategies.
  In this work, we investigate cross-chain sandwich attacks targeting liquidity pool-based cross-chain bridge protocols. We uncover a critical vulnerability where attackers can exploit events emitted on the source chain to learn transaction details on the destination chain before they appear in the destination chain mempool. This information advantage allows attackers to strategically place front-running and back-running transactions, ensuring that their front-running transactions always precede those of existing MEV bots monitoring the mempool of the destination chain. Moreover, current sandwich-attack defenses are ineffective against this new cross-chain variant. To quantify this threat, we conduct an empirical study using two months (August 10 to October 10, 2025) of cross-chain transaction data from the Symbiosis protocol and a tailored heuristic detection model. Our analysis identifies attacks that collectively garnered over \(5.27\) million USD in profit, equivalent to 1.28\% of the total bridged volume.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How much can we save? Upper bound cost and emissions benefits from commercial and industrial load flexibility</title>
<link>https://arxiv.org/abs/2511.14928</link>
<guid>https://arxiv.org/abs/2511.14928</guid>
<content:encoded><![CDATA[
<div> Keywords: load shifting, flexibility, emissions savings, energy capacity, decarbonization  

<br /><br />Summary: Load shifting among commercial and industrial power consumers can significantly lower costs and reduce Scope 2 emissions for both the consumers and the electricity grid. To encourage this practice, it is essential to have tools that can effectively evaluate the flexibility of different consumers, given the diversity in their load characteristics and the variations in electricity prices and emissions factors over time and regions. This study utilizes a top-down method to assess the benefits of flexibility through key metrics like system uptime, power capacity (PC), energy capacity (EC), and round-trip efficiency (RTE). The analysis reveals that depending on the region and season, cost savings can range from 0% to over 100%, while emissions reductions typically fall between 5% and 40%. The findings indicate that the cost-effectiveness of emissions abatement through flexibility can vary greatly, sometimes being significantly less than the costs associated with regional renewable energy credits or the social cost of carbon. Overall, understanding these dynamics can help inform the design of new systems and strategies to incentivize flexibility as a viable decarbonization option. <div>
arXiv:2511.14928v1 Announce Type: cross 
Abstract: Load shifting by commercial and industrial power consumers reduces costs and Scope 2 emissions for the consumer and the grid. Incentivizing this behavior requires tools for valuing flexibility amidst the heterogeneity in load characteristics across diverse sectors and the spatiotemporal variation in electricity prices and emissions factors. This work presents a top-down approach to screen and broadly understand the benefits of flexibility based on system uptime, power capacity (PC), energy capacity (EC), and round- trip efficiency (RTE). Depending on the region and season, cost savings from flexibility range from 0 to over 100% and emissions savings are generally bounded between 5-40%. We also find the magnitude and cost of emissions abatement from flexibility is highly variable and, in some cases, up to four orders of magnitude less than regional renewable energy credits or common investing or policy benchmarks like the social cost of carbon. While the value of flexibility is highly dynamic, estimating savings as a function of load characteristics and incentives can inform heuristic design of new systems, siting strategies, comparison of flexibility to other decarbonization options, and new avenues for incentivizing flexibility.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How To Cook The Fragmented Rug Pull?</title>
<link>https://arxiv.org/abs/2511.15463</link>
<guid>https://arxiv.org/abs/2511.15463</guid>
<content:encoded><![CDATA[
<div> Keywords: fragmented rug pull, liquidity pool, evasive strategies, inflated selling, scam behavior<br /><br />Summary:  
The paper introduces the concept of fragmented rug pull (FRP), a sophisticated form of liquidity pool exit attack that evades traditional detection methods. Unlike classical rug pulls characterized by a single or few large sells, FRPs split the exit into many low-impact trades over time and across multiple participant wallets. This fragmentation allows attackers to maintain control over LP tokens while diluting the visibility of each transaction, effectively bypassing heuristic-based detectors. The authors formalize FRP strategies using three atomic predicate groups—preserving LP control, slicing trades into minimal-impact chunks, and distributing sells among multiple non-owner addresses—to model evasive manipulation techniques overlooked in prior studies. The paper validates this framework with extensive empirical analysis covering over 300,000 liquidity pools, identifying more than 105,000 as FRP-affected. The analysis incorporates millions of pool transactions and reveals that owner wallets are participating less frequently (about 33%) in inflated selling within these pools, indicating a shift toward multi-actor scams. Additionally, the study detects a significant number of serial scammer wallets actively exploiting these tactics across multiple pools, emphasizing the operational prevalence and sophisticated nature of FRP scams. The findings underscore the need for improved detection mechanisms accounting for the fragmented, multi-actor landscape of modern DeFi exit fraud. <div>
arXiv:2511.15463v1 Announce Type: cross 
Abstract: Existing rug pull detectors assume a simple workflow: the deployer keeps liquidity pool (LP) tokens and performs one or a few large sells (within a day) that collapse the pool and cash out. In practice, however, many real-world exits violate these assumptions by splitting the attack across both time and actor dimensions: attackers break total extraction into many low-impact trades and route proceeds through multiple non-owner addresses, producing low-visibility drains.
  We formalize this family of attacks as the fragmented rug pull (FRP) and offer a compact recipe for a slow-stewed beef special: (i) keep the lid on (to preserve LP control so on-chain extraction remains feasible), (ii) chop thin slices (to split the total exit volume into many low-impact micro-trades that individually fall below impact thresholds), and (iii) pass the ladle (to delegate sells across multiple wallets so that each participant takes a small share of the extraction). Technically, we define three atomic predicate groups and show that their orthogonal combinations yield evasive strategies overlooked by prior heuristics (USENIX Sec 19, USENIX Sec 23).
  We validate the model with large-scale measurements. Our corpus contains 303,614 LPs, among which 105,434 are labeled as FRP pools. The labeled subset includes 34,192,767 pool-related transactions and 401,838 inflated-seller wallets, involving 1,501,408 unique interacting addresses. Notably, owner-wallet participation in inflated selling among FRP-flagged LPs has declined substantially (33.1% of cases), indicating a shift in scam behavior: the liquidity drain is no longer held on the owner wallet. We also detected 127,252 wallets acting as serial scammers when repeatedly engaging in inflated selling across multiple FRP LPs. Our empirical findings demonstrate that the evasive strategies we define are widespread and operationally significant.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Walrus: A Cross-Domain Foundation Model for Continuum Dynamics</title>
<link>https://arxiv.org/abs/2511.15684</link>
<guid>https://arxiv.org/abs/2511.15684</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, fluid dynamics, transformer, stabilization, distributed training<br /><br />Summary:<br /><br />This paper introduces Walrus, a transformer-based foundation model designed for simulating fluid-like continuum dynamics. 1) It addresses key challenges in physical simulation, such as data heterogeneity, unstable long-term dynamics, and issues with varying spatial resolutions and dimensionalities that complicate efficient modern hardware training. 2) The authors propose novel methods including harmonic-analysis-based stabilization to improve forecast stability, load-balanced distributed training across 2D and 3D data, and compute-adaptive tokenization to optimize training efficiency. 3) Walrus is pretrained on a diverse dataset containing nineteen scenarios spanning multiple fields such as astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluid mechanics, ensuring broad applicability. 4) Experimental results demonstrate that Walrus surpasses previous foundation models regarding accuracy in both short- and long-term predictions on downstream tasks across all types of pretraining data. 5) Ablation studies validate the individual contributions of the novel methods to enhancing stability, training throughput, and transfer learning performance compared to conventional approaches. The authors have also released the model code and pretrained weights to encourage further research and practical applications within the scientific community. <div>
arXiv:2511.15684v1 Announce Type: cross 
Abstract: Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational multiscale enrichment method for dynamic response of hyperelastic materials at finite deformation</title>
<link>https://arxiv.org/abs/2511.13723</link>
<guid>https://arxiv.org/abs/2511.13723</guid>
<content:encoded><![CDATA[
<div> Keywords: variational multiscale enrichment, wave propagation, hyperelastic materials, nonlinearities, micro-inertial effects

<br /><br />Summary: 
This manuscript presents an extension of the variational multiscale enrichment (VME) method to model the dynamic response of hyperelastic materials experiencing large deformations. The method allows simulation of wave propagation under conditions where scale-inseparability is significant, especially in short-wavelength regimes, by incorporating material and geometric nonlinearities that impact wave behavior. An additive decomposition of the displacement field is introduced to derive governing equations for both coarse- and fine-scale problems, effectively including micro-inertial effects. The discretization involves using patches of coarse-scale elements to accurately capture wave dynamics. To solve the equations iteratively, an operator-split procedure is adopted, with the coarse-scale problem integrated explicitly while the fine scale is tackled with various time integration schemes. Numerical examples demonstrate that multiscale dissipative schemes are effective in curbing spurious oscillations. The framework was further applied to analyze how material and geometric nonlinearities, alongside elastic stiffness contrasts in heterogeneous microstructures, affect wave characteristics such as dispersion, attenuation, and steepening. This advanced computational framework lays the groundwork for exploring the dynamic responses of architected materials. <div>
arXiv:2511.13723v1 Announce Type: new 
Abstract: In this manuscript, we extend the variational multiscale enrichment (VME) method to model the dynamic response of hyperelastic materials undergoing large deformations. This approach enables the simulation of wave propagation under scale-inseparable conditions, including short-wavelength regimes, while accounting for material and geometric nonlinearities that lead to wave steepening or flattening. By employing an additive decomposition of the displacement field, we derive multiscale governing equations for the coarse- and fine-scale problems, which naturally incorporate micro-inertial effects. The framework allows the discretization of each unit cell with a patch of coarse-scale elements, which is essential to accurately capture wave propagation in short-wavelength regimes. An operator-split procedure is used to iteratively solve the semi-discrete equations at both scales until convergence is achieved. The coarse-scale problem is integrated explicitly, while the fine-scale problem is solved using either explicit or implicit time integration schemes, including both dissipative and non-dissipative methods. Numerical examples demonstrate that multiscale dissipative schemes effectively suppress spurious oscillations. The multiscale framework was applied to investigate how material and geometric nonlinearities, along with elastic stiffness contrast in heterogeneous microstructures, influence key wave characteristics such as dispersion, attenuation, and steepening. This multiscale computational framework provides a foundation for studying the dynamic response of architected materials.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PGD-TO: A Scalable Alternative to MMA Using Projected Gradient Descent for Multi-Constraint Topology Optimization</title>
<link>https://arxiv.org/abs/2511.13905</link>
<guid>https://arxiv.org/abs/2511.13905</guid>
<content:encoded><![CDATA[
<div> Keywords: Projected Gradient Descent, topology optimization, multi-constraint, convergence, semismooth Newton

<br /><br />Summary: 
Projected Gradient Descent (PGD) methods are a common approach in topology optimization (TO) but face challenges in nonlinear and multi-constraint scenarios due to active-set detection complexities. The paper presents PGD-TO, a novel framework that reformulates the projection step as a regularized convex quadratic problem. This eliminates the need for active-set search and assures well-posedness even with infeasible constraints. PGD-TO utilizes a semismooth Newton solver for handling general multi-constraint cases and employs a binary search projection for single or independent constraints, leading to fast and reliable convergence. The framework also incorporates spectral step-size adaptation and nonlinear conjugate-gradient directions to enhance stability and efficiency. The effectiveness of PGD-TO is demonstrated through evaluation on four benchmark families that encompass a wide range of TO problems. Results show that PGD-TO achieves convergence and compliance levels comparable to traditional methods like the Method of Moving Asymptotes (MMA) and Optimality Criteria (OC), while significantly reducing computation time per iteration—by 10-43 times for general problems and 115-312 times for independent constraints. PGD-TO thus offers a promising, robust, and scalable alternative for large-scale, multi-constraint, and nonlinear topology optimization. <div>
arXiv:2511.13905v1 Announce Type: new 
Abstract: Projected Gradient Descent (PGD) methods offer a simple and scalable approach to topology optimization (TO), yet they often struggle with nonlinear and multi-constraint problems due to the complexity of active-set detection. This paper introduces PGD-TO, a framework that reformulates the projection step into a regularized convex quadratic problem, eliminating the need for active-set search and ensuring well-posedness even when constraints are infeasible. The framework employs a semismooth Newton solver for general multi-constraint cases and a binary search projection for single or independent constraints, achieving fast and reliable convergence. It further integrates spectral step-size adaptation and nonlinear conjugate-gradient directions for improved stability and efficiency. We evaluate PGD-TO on four benchmark families representing the breadth of TO problems: (i) minimum compliance with a linear volume constraint, (ii) minimum volume under a nonlinear compliance constraint, (iii) multi-material minimum compliance with four independent volume constraints, and (iv) minimum compliance with coupled volume and center-of-mass constraints. Across these single- and multi-constraint, linear and nonlinear cases, PGD-TO achieves convergence and final compliance comparable to the Method of Moving Asymptotes (MMA) and Optimality Criteria (OC), while reducing per-iteration computation time by 10-43x on general problems and 115-312x when constraints are independent. Overall, PGD-TO establishes a fast, robust, and scalable alternative to MMA, advancing topology optimization toward practical large-scale, multi-constraint, and nonlinear design problems. Public code available at: https://github.com/ahnobari/pyFANTOM
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoMoE: A Mixture of Expert Agent Model for Financial Sentiment Analysis</title>
<link>https://arxiv.org/abs/2511.13983</link>
<guid>https://arxiv.org/abs/2511.13983</guid>
<content:encoded><![CDATA[
<div> MoMoE, Mixture-of-Experts, multi-agent collaboration, LLaMA 3.1, task decomposition  

<br /><br />Summary:  
This paper introduces Mixture of Mixture of Experts (MoMoE), a new methodology that integrates Mixture-of-Experts (MoE) architectures with collaborative multi-agent systems. The approach modifies the LLaMA 3.1 8B model by embedding MoE layers within each agent of a structured multi-agent framework, forming an ensemble of expert agents. Each agent includes an MoE layer in its final attention block, allowing for efficient division of tasks while keeping computational costs manageable. This design creates specialized processing routes both at the neural network level and among the collaborating agents, promoting effective task decomposition and refinement. Through iterative collaboration, agents collectively enhance their outputs, leveraging their specialized expertise. Experimental evaluations on various language understanding and generation benchmarks reveal that this hybrid model achieves significant performance gains. The results underscore the advantages of combining expert routing mechanisms at the model architecture level with multi-agent collaboration, leading to improved language task performance and computational efficiency. <div>
arXiv:2511.13983v1 Announce Type: new 
Abstract: We present a novel approach called Mixture of Mixture of Expert (MoMoE) that combines the strengths of Mixture-of-Experts (MoE) architectures with collaborative multi-agent frameworks. By modifying the LLaMA 3.1 8B architecture to incorporate MoE layers in each agent of a layered collaborative structure, we create an ensemble of specialized expert agents that iteratively refine their outputs. Each agent leverages an MoE layer in its final attention block, enabling efficient task decomposition while maintaining computational feasibility. This hybrid approach creates specialized pathways through both the model architecture and the agent collaboration layers. Experimental results demonstrate significant improvements across multiple language understanding and generation benchmarks, highlighting the synergistic benefits of combining expert routing at both the neural and agent levels.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Cyber-Resilience in Cyber-Physical Systems of Systems:A Methodical Approach</title>
<link>https://arxiv.org/abs/2511.14548</link>
<guid>https://arxiv.org/abs/2511.14548</guid>
<content:encoded><![CDATA[
<div> Keywords: Cyber-physical Systems, resilience, risk mitigation, adaptability, Industry 4.0  

<br /><br />Summary:  
Cyber-Physical Systems of Systems (CPSoS) are increasingly important in various sectors, particularly in Industry 4.0 and smart homes, where they enable interconnected and intelligent functionality. This paper addresses the challenges associated with these complex systems by proposing a modified Cyber-Resilience Life-Cycle framework aimed at enhancing sustainable risk mitigation. The framework is designed to improve the adaptability of CPSoS, ensuring that they can respond effectively to evolving system complexities and potential disruptions. By implementing this modified life-cycle, organizations can better foster resilience in their operational systems. The authors also delineate various application scenarios that showcase the practical relevance of their proposed framework, emphasizing its potential to enhance cyber-resilience. Through this work, the need for effective risk management strategies in the face of growing complexity in cyber-physical environments is highlighted, making this framework a pivotal tool for organizations looking to maintain stability and security in their operations. The discussion concludes with insights on how the life-cycle can be operationalized for greater impact in real-world applications. <div>
arXiv:2511.14548v1 Announce Type: new 
Abstract: Cyber-physical Systems of Systems (CPSoS) are becoming increasingly prevalent across sectors such as Industry 4.0 and smart homes, where they play a critical role in enabling intelligent, interconnected functionality. Addressing the challenges and resilience requirements of these complex environments, we propose a modified Cyber-Resilience Life-Cycle as a practical framework for sustainable risk mitigation. Our approach enhances the adaptability of CPSoS and supports resilience against evolving system complexities and potential disruptions. We conclude by outlining application scenarios for the modified life-cycle and highlighting its relevance in fostering cyber-resilience in operational systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Disentangled Low-Rank RNN Framework for Uncovering Neural Connectivity and Dynamics</title>
<link>https://arxiv.org/abs/2511.13899</link>
<guid>https://arxiv.org/abs/2511.13899</guid>
<content:encoded><![CDATA[
<div> Keywords: low-rank recurrent neural networks, disentangled, latent dynamics, variational autoencoder, interpretability  

<br /><br />Summary:  
Low-rank recurrent neural networks (lrRNNs) are designed to identify low-dimensional latent dynamics within neural population activity. However, these models struggle with providing clear interpretations of the functional connectivity, making it challenging to define distinct roles for different latent dimensions. To overcome this limitation, the authors introduce the Disentangled Recurrent Neural Network (DisRNN). This innovative framework assumes independence among groups of latent dynamics while allowing for complex computational relationships within each group. By framing the lrRNN within a variational autoencoder (VAE) setting, DisRNN incorporates a partial correlation penalty that enhances disentanglement among latent dimension groups. The proposed model's effectiveness is validated through experiments conducted on synthetic datasets, as well as data from monkey M1 and mouse voltage imaging. The results demonstrate that DisRNN significantly improves the disentanglement and interpretability of learned latent trajectories, leading to a better understanding of the underlying neural dynamics compared to standard lrRNNs that do not promote partial disentanglement. This advancement holds the potential to enhance the interpretability of neural data analysis and model development in neuroscience. <div>
arXiv:2511.13899v1 Announce Type: cross 
Abstract: Low-rank recurrent neural networks (lrRNNs) are a class of models that uncover low-dimensional latent dynamics underlying neural population activity. Although their functional connectivity is low-rank, it lacks disentanglement interpretations, making it difficult to assign distinct computational roles to different latent dimensions. To address this, we propose the Disentangled Recurrent Neural Network (DisRNN), a generative lrRNN framework that assumes group-wise independence among latent dynamics while allowing flexible within-group entanglement. These independent latent groups allow latent dynamics to evolve separately, but are internally rich for complex computation. We reformulate the lrRNN under a variational autoencoder (VAE) framework, enabling us to introduce a partial correlation penalty that encourages disentanglement between groups of latent dimensions. Experiments on synthetic, monkey M1, and mouse voltage imaging data show that DisRNN consistently improves the disentanglement and interpretability of learned neural latent trajectories in low-dimensional space and low-rank connectivity over baseline lrRNNs that do not encourage partial disentanglement.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Making Evidence Actionable in Adaptive Learning</title>
<link>https://arxiv.org/abs/2511.14052</link>
<guid>https://arxiv.org/abs/2511.14052</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive learning, feedback loop, intervention assignment, binary integer program, personalized education

<br /><br />Summary:  
This study addresses limitations in adaptive learning systems which, although precise in diagnosing student knowledge gaps, often provide weak or mistimed interventions. It proposes an instructor-governed feedback loop that transforms concept-level assessments into validated micro-interventions to improve learning outcomes. The core adaptive learning algorithm is designed with three safeguards: (1) adequacy ensuring guaranteed gap closure, (2) attention managing time and redundancy via budget constraints, and (3) diversity preventing overfitting by encouraging varied resource use. Intervention assignment is modeled as a binary integer programming problem constrained by coverage requirements, time limits, difficulty ranges informed by ability estimates, prerequisite structures via a concept matrix, and anti-redundancy through diversity enforcement. To solve this, the authors apply three methods: greedy selection optimized for settings with limited content and strict latency, gradient-based relaxation suited for rich content repositories, and a hybrid approach blending both along a richness-latency spectrum. Evaluations through simulations and deployment in an introductory physics course involving 1,204 students showed both solvers achieved nearly full skill coverage within bounded viewing time. The gradient-based method notably reduced redundant content exposure by approximately 12% compared to greedy selection and balanced difficulty across interventions, whereas greedy was computationally cheaper in resource-scarce contexts. Slack variables helped identify content gaps and enabled targeted curation to maintain coverage across learner subgroups. Overall, this framework offers a practical, transparent controller that completes the diagnostic-to-pedagogical cycle and enables equitable, context-aware personalized learning at classroom scale. <div>
arXiv:2511.14052v1 Announce Type: cross 
Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval</title>
<link>https://arxiv.org/abs/2511.14130</link>
<guid>https://arxiv.org/abs/2511.14130</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, financial information retrieval, PRISM, document ranking, multi-agent system  
  
<br /><br />Summary: This article highlights the growing importance of financial information retrieval enhanced by large language models (LLMs). The focus is on extracting task-relevant information from extensive financial filings, crucial for both operational and analytical decision-making. The authors introduce the FinAgentBench dataset, which formalizes financial retrieval into two tasks: document ranking and chunk ranking. They present PRISM, a training-free framework that incorporates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each element is thoroughly examined to illustrate their combined effectiveness: prompt engineering delivers precise task instructions, ICL offers semantically relevant few-shot examples, and the multi-agent system enables coordinated scoring behavior. The best configuration of PRISM achieves an NDCG@5 score of 0.71818 on a restricted validation split, demonstrating its high performance. Furthermore, the study asserts that PRISM is both feasible and robust for production-scale financial retrieval applications, emphasizing its modular and inference-only design that makes it practical for real-world usage. The source code for PRISM has been made publicly available to facilitate further research and applications in this area. <div>
arXiv:2511.14130v1 Announce Type: cross 
Abstract: With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Many Simulations of Hybrid Programs in Lince</title>
<link>https://arxiv.org/abs/2511.14436</link>
<guid>https://arxiv.org/abs/2511.14436</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid systems, simulation, Lince, adaptive cruise control, critical applications

<br /><br />Summary: Hybrid systems are increasingly prominent in critical applications such as medical devices, infrastructure systems, and autonomous vehicles. The paper focuses on Lince, an academic tool designed for specifying and simulating these hybrid systems using a C-like language enriched with differential equations. Recent experiments have led to enhancements in Lince, introducing new mechanisms that allow the execution of multiple simulation variants. This feature enables users to explore different scenarios and configurations within a single framework. Additionally, the enhanced Lince can generate histograms that quantify the frequency with which specific properties hold during simulation runs, providing valuable insights into system behavior. The paper illustrates these advancements using variations of an adaptive cruise control system, demonstrating the practical utility of Lince in simulating real-world scenarios. By enabling the assessment of system performance across multiple simulations, Lince aims to improve the reliability and safety of systems in critical domains. Overall, the work presented highlights the importance of simulation tools like Lince in the development and verification of hybrid systems, catering to the growing demand for robust solutions in safety-sensitive applications. <div>
arXiv:2511.14436v1 Announce Type: cross 
Abstract: Hybrid systems are increasingly used in critical applications such as medical devices, infrastructure systems, and autonomous vehicles. Lince is an academic tool for specifying and simulating such systems using a C-like language with differential equations. This paper presents recent experiments that enhance Lince with mechanisms for executing multiple simulation variants and generating histograms that quantify the frequency with which a given property holds. We illustrate our extended Lince using variations of an adaptive cruise control system.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction</title>
<link>https://arxiv.org/abs/2511.14753</link>
<guid>https://arxiv.org/abs/2511.14753</guid>
<content:encoded><![CDATA[
<div> Keywords: spatiotemporal data mining, ConvLSTM, edge computing, data sparsity, multi-objective loss function

<br /><br />Summary: 
Spatiotemporal data mining (STDM) is crucial for analyzing complex physical systems across various domains such as transportation, manufacturing, and healthcare. The Convolutional Long Short-Term Memory (ConvLSTM) model is recognized for its adaptability and effectiveness but suffers from high computational demands, making it unsuitable for edge devices with limited resources. As the need for efficient AI grows in edge computing, it is essential to lower computational costs while maintaining effective model performance. While many approaches focus on reducing model capacity through techniques like pruning and compression, they often fail to address the considerable data and feature redundancy inherent in spatiotemporal data. This oversight presents an unnecessary computational burden. To tackle this issue, the authors developed a novel framework called SparseST that harnesses data sparsity to create a more efficient spatiotemporal model. Furthermore, they introduce a multi-objective composite loss function that aids in examining the trade-off between model performance and computational efficiency, thereby offering valuable guidance to practitioners in optimizing their models based on specific resource limits and performance needs for downstream applications. <div>
arXiv:2511.14753v1 Announce Type: cross 
Abstract: Spatiotemporal data mining (STDM) has a wide range of applications in various complex physical systems (CPS), i.e., transportation, manufacturing, healthcare, etc. Among all the proposed methods, the Convolutional Long Short-Term Memory (ConvLSTM) has proved to be generalizable and extendable in different applications and has multiple variants achieving state-of-the-art performance in various STDM applications. However, ConvLSTM and its variants are computationally expensive, which makes them inapplicable in edge devices with limited computational resources. With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. Common methods of efficient AI are developed to reduce redundancy in model capacity (i.e., model pruning, compression, etc.). However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotemporal model. In addition, we explore and approximate the Pareto front between model performance and computational efficiency by designing a multi-objective composite loss function, which provides a practical guide for practitioners to adjust the model according to computational resource constraints and the performance requirements of downstream tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines</title>
<link>https://arxiv.org/abs/2511.11613</link>
<guid>https://arxiv.org/abs/2511.11613</guid>
<content:encoded><![CDATA[
<div> Keywords: buried pipelines, reliability analysis, Physics-Informed Neural Network, Monte Carlo Simulation, ground movement<br /><br />Summary:  
This study addresses the challenge of assessing the reliability of buried pipelines subjected to ground movements in geohazard-prone regions. Traditional reliability analysis methods rely on computationally intensive numerical models such as finite element simulations, which are often impractical due to the large number of samples required for stochastic sampling to estimate low failure probabilities. To overcome this, the authors propose a novel method called Physics-Informed Neural Network for Reliability Analysis (PINN-RA), which integrates a PINN-based surrogate model with Monte Carlo Simulation to enable efficient reliability assessment. The PINN surrogate model is extended to solve a parametric differential equation system representing the governing equations of pipelines embedded in soils with varying properties, accommodating uncertainties in soil characteristics and ground movement. By replacing the need for repeated numerical simulations, PINN-RA dramatically reduces computational costs and accelerates reliability computations. This approach provides a scalable and efficient tool for pipeline reliability analysis under uncertainty, facilitating rapid decision-making for pipeline safety in geohazard-prone environments. The findings demonstrate the effectiveness of PINN-RA as a promising alternative to traditional reliability analysis in engineering applications dealing with complex coupled soil-pipeline interactions. <div>
arXiv:2511.11613v1 Announce Type: new 
Abstract: Buried pipelines transporting oil and gas across geohazard-prone regions are exposed to potential ground movement, leading to the risk of significant strain demand and structural failure. Reliability analysis, which determines the probability of failure after accounting for pertinent uncertainties, is essential for ensuring the safety of pipeline systems. However, traditional reliability analysis methods involving computationally intensive numerical models, such as finite element simulations of pipeline subjected to ground movement, have limited applications; this is partly because stochastic sampling approaches require repeated simulations over a large number of samples for the uncertain variables when estimating low probabilities. This study introduces Physics-Informed Neural Network for Reliability Analysis (PINN-RA) for buried pipelines subjected to ground movement, which integrates PINN-based surrogate model with Monte Carlo Simulation (MCS) to achieve efficient reliability assessment. To enable its application under uncertain variables associated with soil properties and ground movement, the PINN-based surrogate model is extended to solve a parametric differential equation system, namely the governing equation of pipelines embedded in soil with different properties. The findings demonstrate that PINN-RA significantly reduces the computational effort required and thus accelerates reliability analysis. By eliminating the need for repetitive numerical evaluations of pipeline subjected to permanent ground movement, the proposed approach provides an efficient and scalable tool for pipeline reliability assessment, enabling rapid decision-making in geohazard-prone regions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys</title>
<link>https://arxiv.org/abs/2511.12036</link>
<guid>https://arxiv.org/abs/2511.12036</guid>
<content:encoded><![CDATA[
<div> Keywords: preference learning, language models, structural alloys, synthesis, thermodynamic phase calculations

<br /><br />Summary: This paper presents a novel approach to the design of BCC/B2 superalloys, a class of materials that have potential applications in extreme environments, by leveraging preference learning in language models. Unlike previous efforts that primarily focused on stable inorganic crystal generation, the authors specifically target the synthesizeability of these alloys. They utilize three open-weight language models, namely LLaMA-3.1, Gemma-2, and OLMo-2, to optimize for multiple design objectives through a unified reward signal obtained via Direct Preference Optimization (DPO). This method distinguishes itself by employing a scientifically grounded feedback mechanism derived from thermodynamic phase calculations, eschewing the costly heuristic or human-in-the-loop feedback commonly used in prior work. The research marks a significant advancement in preference-tuning language models with physics-based feedback for structural alloy design. Furthermore, the authors suggest that their framework is general and extensible, indicating a promising avenue for intelligent exploration and design across various physical science domains. <div>
arXiv:2511.12036v1 Announce Type: new 
Abstract: We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modelling Heterogeneous Interfaces using Element-based Finite Volumes</title>
<link>https://arxiv.org/abs/2511.12562</link>
<guid>https://arxiv.org/abs/2511.12562</guid>
<content:encoded><![CDATA[
<div> Keywords: multiphysics, Element-based Finite Volume Method, geometric adaptability, conservation fidelity, transport phenomena 

<br /><br />Summary: The research addresses the need for computational frameworks that effectively manage multiphysics interactions in interfacial systems. Traditional spatiotemporal discretization methods tend to compromise between mesh flexibility and conservation fidelity, limiting their efficacy in understanding complex mechanisms. In response, the authors develop a three-dimensional adaptation of the Element-based Finite Volume Method (EbFVM), which combines the geometric adaptability of Finite Element Methods with the conservation principles of Finite Volume Methods. The new framework includes advanced discretization techniques suited for unstructured and irregular meshes, featuring detailed parametric shape functions and robust flux integration schemes. It also incorporates rigorous body-fitted curvilinear coordinate mappings. Through a series of lubrication-driven benchmark problems, the study showcases EbFVM's ability to accurately represent intricate transport phenomena, strong field couplings, and scale disparities in complex geometries. This innovative approach positions the three-dimensional EbFVM as a versatile and generalizable tool for simulating transport phenomena in various multiphysics applications, thereby enhancing accuracy in geometrically and physically challenging interfacial systems. <div>
arXiv:2511.12562v1 Announce Type: new 
Abstract: Accurately depicting multiphysics interactions in interfacial systems requires computational frameworks capable of reconciling geometric adaptability with strict conservation fidelity. However, traditional spatiotemporal discretisation methods often compromise between mesh flexibility and flow conservation enforcement, hence constraining their effectiveness in elucidating the underlying mechanisms. Here, we respond to these computational demands by developing a novel three-dimensional adaptation of the Element-based Finite Volume Method (EbFVM) -- a hybrid numerical strategy that merges the geometric flexibility of Finite Element Methods with the conservation-centric principles of Finite Volume Methods. The proposed framework introduces advanced discretisation techniques tailored to unstructured, irregular mesh entities, including detailed parametric shape functions, robust flux integration schemes and rigorous body-fitted curvilinear coordinate mappings. Through a series of lubrication-driven benchmark problems, we demonstrate the EbFVM's capacity to capture intricate transport phenomena, strong field couplings and scale disparities across geometrically complex domains. By enabling accurate modelling in geometrically and physically challenging interfacial systems, the three-dimensional EbFVM offers a versatile and generalisable tool for simulating transport phenomena in a plethora of multiphysics applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Voltage-Based Unsupervised Learning Framework for Bridge Damage Detection in Simultaneous Energy Harvesting and Sensing Systems</title>
<link>https://arxiv.org/abs/2511.13291</link>
<guid>https://arxiv.org/abs/2511.13291</guid>
<content:encoded><![CDATA[
<div> Keywords: piezoelectric energy harvesters, structural health monitoring, energy consumption, damage detection, bi-objective optimisation  

<br /><br />Summary: This study focuses on piezoelectric energy harvesters (PEHs) designed for dual purposes in structural health monitoring (SHM): harvesting energy from bridge vibrations and serving as intrinsic damage sensors. The innovative approach leverages the voltage signal directly for sensing, reducing complexity and energy usage associated with traditional sensing methods. A bi-objective optimisation framework is developed to enhance both power output and damage detection accuracy of a PEH modeled as a composite cantilevered Kirchhoff-Love plate. Predictions of voltage responses are conducted using isogeometric analysis. Validation occurs through numerical simulations and laboratory tests, successfully detecting damage in both healthy and compromised states using an unsupervised convolutional variational autoencoder (CVAE) trained exclusively on healthy voltage signatures. The NSGA-II algorithm facilitates exploration of trade-offs concerning energy yield and sensing precision, examining various parameters like damage severity and harvester geometry. Results reveal that the optimised PEHs surpass traditional acceleration-based systems, achieving a 13% improvement in damage detection accuracy while decreasing energy consumption by 98%. Ultimately, the research highlights the potential of replacing conventional sensors with efficient, self-powered PEHs, paving the way for sustainable simultaneous energy harvesting and sensing (SEHS) systems. <div>
arXiv:2511.13291v1 Announce Type: new 
Abstract: In this study, piezoelectric energy harvesters (PEHs) are designed to offer dual functionality in structural health monitoring (SHM): harvesting electric power from bridge vibrations while serving as intrinsic damage sensors. This strategy utilises the voltage signal directly as the sensing input, eliminating the need for traditional sensing modules and thereby reducing system complexity and energy consumption. A bi-objective optimisation framework is proposed to maximise both power output and damage detection accuracy of a PEH modelled as a composite cantilevered Kirchhoff-Love plate. Voltage responses under realistic bridge inputs are predicted via isogeometric analysis. The approach is validated in two scenarios: a numerical vehicle-bridge interaction model and a laboratory-scale beam test using a toy car, each evaluated in both healthy and damaged states. Unsupervised damage detection is achieved using a convolutional variational autoencoder (CVAE) trained solely on healthy voltage signatures. The NSGA-II algorithm is applied to explore trade-offs between energy yield and sensing precision, including parametric studies on damage severity, damage location, and harvester geometry. Results indicate that optimised PEHs not only act as an effective filter and sensing component but also outperform traditional acceleration-based sensing, improving damage detection accuracy by 13% while reducing energy consumption by 98%. The multi-parameter design space further highlights the importance of bi-objective optimisation due to variations in performance even under resonant conditions. These findings demonstrate the feasibility of replacing traditional sensors with lightweight, self-powered PEHs and pave the way for sustainable simultaneous energy harvesting and sensing (SEHS) systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block Structure Preserving Model Order Reduction for A-EFIE Integral Equation Method</title>
<link>https://arxiv.org/abs/2511.13390</link>
<guid>https://arxiv.org/abs/2511.13390</guid>
<content:encoded><![CDATA[
<div> Keywords: Block Structure, Model Order Reduction, Integral Equations, Augmented Electric Field, Reduced-Order Models

<br /><br />Summary: This article presents a novel approach for Model Order Reduction (MOR) specifically tailored for Integral Equations methods associated with the Augmented Electric Field Integral Equation. The proposed method emphasizes a Block Structure Preserving technique that enhances the efficiency and accuracy of the reduction process. By utilizing dedicated subspaces for representing the unknown fields, the approach significantly optimizes the computational resources required for simulations. Numerical examples demonstrate that this methodology yields smaller reduced-order models while maintaining, and in some cases improving, accuracy levels. The advantages of this new approach lie in its ability to balance reduced complexity with high fidelity results, making it a valuable contribution to the field of computational electromagnetics. The findings indicate that implementing this block structure preservation technique can lead to more effective and reliable modeling of electric fields, which has potential implications for various applications in engineering and physics. Overall, this work promises to advance the understanding and implementation of MOR in integral equations, providing researchers and practitioners with a powerful tool for improving simulation outcomes. <div>
arXiv:2511.13390v1 Announce Type: new 
Abstract: A Block Structure Preserving Model Order Reduction approach is proposed for Integral Equations methods based on the Augmented Electric Field Integral Equation. This approach allows for representing the unknown fields with dedicated subspaces. Numerical results show that this leads to smaller reduced-order models and higher accuracy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop</title>
<link>https://arxiv.org/abs/2511.13542</link>
<guid>https://arxiv.org/abs/2511.13542</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive learning, feedback loop, intervention assignment, binary integer program, personalized education<br /><br />Summary:<br /><br />This article addresses the common issue in adaptive learning where diagnoses are accurate but interventions are poorly timed or misaligned, limiting their effectiveness. The authors propose an instructor-governed feedback loop that transforms concept-level assessment data into carefully vetted microinterventions, ensuring more precise and actionable responses. Their adaptive learning algorithm integrates three safeguards: adequacy to guarantee skill gaps are closed, attention to limit time and redundancy, and diversity to prevent overfitting on single resources. Intervention assignment is mathematically formulated as a binary integer program incorporating constraints such as skill coverage, time budgets, difficulty windows based on ability estimates, prerequisite dependencies modeled via a concept matrix, and anti-redundancy through diversity. The study explores three computational solvers: a greedy approach for environments with limited resources and tight latency, a gradient-based relaxation method optimal for rich repositories, and a hybrid switching strategy balancing richness and latency. Both solvers were tested in simulations and a real-world deployment in introductory physics with 1204 students, achieving near-complete skill coverage within bounded viewing times. The gradient-based method notably reduced redundant coverage and improved difficulty alignment, while the greedy approach maintained adequacy at a lower computational cost. Additionally, slack variables identified content gaps and guided resource curation, supporting equity across diverse student groups. The result is a practical, auditable system that closes the pedagogical feedback loop and enables scalable, personalized learning tailored to classroom demands. <div>
arXiv:2511.13542v1 Announce Type: new 
Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedBuild AI: An Agent-Based Hybrid Intelligence Framework for Reshaping Agency in Healthcare Infrastructure Planning through Generative Design for Medical Architecture</title>
<link>https://arxiv.org/abs/2511.11587</link>
<guid>https://arxiv.org/abs/2511.11587</guid>
<content:encoded><![CDATA[
<div> Keywords: healthcare, infrastructure, MedBuild AI, modular design, community empowerment

<br /><br />Summary: The paper discusses the significant disparities in healthcare infrastructure, highlighting the lack of access to basic services in many communities globally. Traditional approaches to infrastructure planning are often slow and fail to meet the urgent needs for healthcare facilities. In response, the authors present MedBuild AI, a hybrid-intelligence framework designed to enhance early design and planning stages in healthcare architecture. This web-based platform allows regions with satellite internet access to receive guidance on creating modular, low-cost medical buildings. MedBuild AI operates through three agents: the first gathers local health data via conversations, the second translates this information into architectural functional programs through rule-based computations, and the third generates layouts and 3D models. The inclusion of computational negotiation in the design process promotes a more reciprocal and inclusive approach to healthcare planning. By empowering communities with access to tailored healthcare infrastructure solutions, MedBuild AI aims to redefine agency in global healthcare architecture, ultimately addressing the pressing need for effective healthcare facilities in underserved regions. <div>
arXiv:2511.11587v1 Announce Type: cross 
Abstract: Globally, disparities in healthcare infrastructure remain stark, leaving countless communities without access to even basic services. Traditional infrastructure planning is often slow and inaccessible, and although many architects are actively delivering humanitarian and aid-driven hospital projects worldwide, these vital efforts still fall far short of the sheer scale and urgency of demand. This paper introduces MedBuild AI, a hybrid-intelligence framework that integrates large language models (LLMs) with deterministic expert systems to rebalance the early design and conceptual planning stages. As a web-based platform, it enables any region with satellite internet access to obtain guidance on modular, low-tech, low-cost medical building designs. The system operates through three agents: the first gathers local health intelligence via conversational interaction; the second translates this input into an architectural functional program through rule-based computation; and the third generates layouts and 3D models. By embedding computational negotiation into the design process, MedBuild AI fosters a reciprocal, inclusive, and equitable approach to healthcare planning, empowering communities and redefining agency in global healthcare architecture.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models</title>
<link>https://arxiv.org/abs/2511.11630</link>
<guid>https://arxiv.org/abs/2511.11630</guid>
<content:encoded><![CDATA[
<div> Keywords: Grain Growth, Deep Learning, LSTM, Temporal Forecasting, Microstructure Prediction<br /><br />Summary:  
1. Grain Growth significantly affects the mechanical properties of materials, making its accurate prediction crucial in microstructural engineering.  
2. This study evaluates various deep learning models, including RNN, LSTM, TCN, and transformers, for forecasting grain size distributions during grain growth processes.  
3. Unlike computationally intensive full-field simulations, the approach leverages low-dimensional mean-field statistical descriptors derived from high-fidelity simulations, focusing on grain size distributions over time.  
4. A dataset comprising 120 grain growth sequences was normalized and used to train models to predict future grain size distributions based on short temporal histories, employing a recursive forecasting method.  
5. Among all tested architectures, the LSTM network delivered the highest accuracy (exceeding 90%) and the most stable predictions, successfully maintaining physical consistency even over long forecasting horizons.  
6. The LSTM model also drastically reduced computational time from approximately 20 minutes per sequence to a few seconds, whereas other models showed divergence when predicting further into the future.  
7. The results reveal the strong potential of combining low-dimensional descriptors with LSTM-based forecasting for efficient, precise microstructure evolution prediction.  
8. These insights have direct implications for advancing digital twin technology and optimizing material processing in microstructural engineering. <div>
arXiv:2511.11630v1 Announce Type: cross 
Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks</title>
<link>https://arxiv.org/abs/2511.11737</link>
<guid>https://arxiv.org/abs/2511.11737</guid>
<content:encoded><![CDATA[
<div> Keywords: Quality of Experience, root-cause analysis, weak supervision, contrastive learning, diffusion model<br /><br />Summary: Diagnosing Quality of Experience (QoE) degradations in mobile networks is difficult due to complex interactions across network layers and limited reliable expert labels. Existing rule-based heuristics can create labels at scale but result in noisy and coarse-grained annotations that limit data-driven model performance. To overcome these challenges, the paper proposes DK-Root, a hybrid framework combining data-driven learning with expert knowledge for effective root-cause analysis. DK-Root first pretrains an encoder using contrastive representation learning on noisy rule-based labels while explicitly mitigating noise through a supervised contrastive objective. The method introduces a class-conditional diffusion model to generate KPI sequences that maintain root-cause semantics, enabling task-faithful data augmentation. By controlling reverse diffusion steps, weak and strong augmentations are produced to enhance intra-class compactness and inter-class separability. Finally, the encoder and classifier are fine-tuned jointly on limited expert-verified labels to improve decision boundaries. Extensive experiments on real-world operator-grade datasets demonstrate that DK-Root achieves state-of-the-art accuracy, outperforming traditional machine learning and recent semi-supervised time-series methods. Ablation studies validate the critical contributions of the conditional diffusion augmentation and the pretrain-finetune strategy in improving representation quality and classification outcomes. <div>
arXiv:2511.11737v1 Announce Type: cross 
Abstract: Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data</title>
<link>https://arxiv.org/abs/2511.12316</link>
<guid>https://arxiv.org/abs/2511.12316</guid>
<content:encoded><![CDATA[
<div> Keywords: inverse problem, stochastic systems, quantum systems, neural operator, BlinDNO  

<br /><br />Summary: This article addresses an inverse problem related to stochastic and quantum dynamical systems without the need for specific time labels. The authors emphasize that only unordered density snapshots, which are sampled at unknown times from an observation-time distribution, are utilized in their study. The primary aim is to recover the parameters of the underlying evolution operator based on these samples. To achieve this, the authors introduce a novel neural operator framework called BlinDNO, designed to be permutation-invariant. BlinDNO combines a multiscale U-Net encoder with an attention-based mixer to effectively learn from the distribution of state densities. The effectiveness of this approach is demonstrated through numerical experiments across various stochastic and quantum systems. Notably, it includes a complex example involving the reconstruction of a 3D protein-folding mechanism in a cryo-electron microscopy (cryo-EM) context. The results indicate that BlinDNO consistently achieves reliable recovery of governing parameters and significantly outperforms existing neural inverse operator baselines, marking it as a promising tool for addressing similar challenges in the field. <div>
arXiv:2511.12316v1 Announce Type: cross 
Abstract: We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuaRT: A toolkit for the exploration of quantum methods for radiation transport</title>
<link>https://arxiv.org/abs/2511.12356</link>
<guid>https://arxiv.org/abs/2511.12356</guid>
<content:encoded><![CDATA[
<div> Quantum simulation, radiative transfer, Python library, lattice Boltzmann methods, angular redistribution<br /><br />Summary:<br /><br />1. QuaRT is a Python library developed for simulating radiative transfer in astrophysical and cosmological contexts, addressing the complex problem of light propagation in such environments.<br />2. The library introduces an innovative angular redistribution methodology designed specifically for lattice Boltzmann methods, which are numerical techniques used to simulate fluid dynamics and related phenomena.<br />3. This novel approach significantly enhances the isotropy of simulations, meaning it improves the uniformity of angular distribution in the modeled radiation field.<br />4. By improving isotropy, QuaRT enables more accurate modeling of astrophysical objects such as stars, particularly in non-scattering media where radiation transport tends to be highly directional.<br />5. Overall, QuaRT represents an important tool for researchers needing precise radiative transfer simulations in the study of astrophysical and cosmological processes, combining advanced numerical methods with practical implementation in Python. <div>
arXiv:2511.12356v1 Announce Type: cross 
Abstract: QuaRT is a Python library for quantum simulation of radiative transfer in astrophysical and cosmological problems. It features a novel angular redistribution methodology for lattice Boltzmann methods which improves the isotropy of simulations of objects such as stars in non-scattering media.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolEdit: Knowledge Editing for Multimodal Molecule Language Models</title>
<link>https://arxiv.org/abs/2511.12770</link>
<guid>https://arxiv.org/abs/2511.12770</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal molecular knowledge, MoLMs, MolEdit, knowledge editing, MEBench  

<br /><br />Summary: Understanding and refining multimodal molecular knowledge is essential for fields like biomedicine, chemistry, and materials science. Molecule language models (MoLMs) integrate structural representations with contextual descriptions but can propagate inaccuracies from outdated training data. Existing knowledge editing approaches are not well-suited for MoLMs, highlighting the need for tailored solutions. This paper introduces MolEdit, a framework for editing MoLMs focused on molecule-to-caption and caption-to-molecule tasks while maintaining unrelated knowledge. MolEdit features a Multi-Expert Knowledge Adapter, routing edits to specialized experts, and an Expertise-Aware Editing Switcher to minimize interference with non-molecular aspects. To evaluate the efficacy of this editing approach, the authors present MEBench, a benchmark assessing Reliability, Locality, and Generality of edits. Experimental results indicate that MolEdit significantly enhances Reliability by up to 18.8% and Locality by 12.0% compared to baseline models while ensuring efficient performance. This work marks a significant advancement in the field of molecular knowledge refinement, providing valuable tools for improving MoLM applications. The implementation of MolEdit is accessible through the provided GitHub repository. <div>
arXiv:2511.12770v1 Announce Type: cross 
Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Case study of a differentiable heterogeneous multiphysics solver for a nuclear fusion application</title>
<link>https://arxiv.org/abs/2511.13262</link>
<guid>https://arxiv.org/abs/2511.13262</guid>
<content:encoded><![CDATA[
<div> Keywords: multiphysics, plasma, JAX, differentiability, Gkeyll

<br /><br />Summary: This article details a case study focused on a heterogeneous multiphysics solver in the nuclear fusion sector. The study involves an auto-differentiable ordinary differential equation (ODE) solver implemented in JAX, used to model the dynamics of a pulsed power circuit alongside bulk plasma parameters during a compressing Z Pinch. To enhance its functionality, the ODE solver requires a closure for plasma load impedance, efficiently determined through a gradient-based Newton iteration process at each timestep. However, integrating production-grade plasma solvers like Gkeyll—which operates in C/CUDA—into a gradient-based framework poses significant challenges. To tackle this issue, the authors present "Tesseract," a software solution that offers a differentiable abstraction layer, fully compatible with JAX via the `tesseract_jax` adapter. This innovative architecture not only facilitates end-to-end differentiability but also allows for the seamless transitions between high-fidelity plasma simulations provided by Gkeyll, neural network surrogates, and analytical models, thereby promoting rapid prototyping and progressive development in plasma physics research. <div>
arXiv:2511.13262v1 Announce Type: cross 
Abstract: This work presents a case study of a heterogeneous multiphysics solver from the nuclear fusion domain. At the macroscopic scale, an auto-differentiable ODE solver in JAX computes the evolution of the pulsed power circuit and bulk plasma parameters for a compressing Z Pinch. The ODE solver requires a closure for the impedance of the plasma load obtained via root-finding at every timestep, which we solve efficiently using gradient-based Newton iteration. However, incorporating non-differentiable production-grade plasma solvers like Gkeyll (a C/CUDA plasma simulation suite) into a gradient-based workflow is non-trivial. The ''Tesseract'' software addresses this challenge by providing a multi-physics differentiable abstraction layer made fully compatible with JAX (through the `tesseract_jax` adapter). This architecture ensures end-to-end differentiability while allowing seamless interchange between high-fidelity solvers (Gkeyll), neural surrogates, and analytical approximations for rapid, progressive prototyping.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Market-Dependent Communication in Multi-Agent Alpha Generation</title>
<link>https://arxiv.org/abs/2511.13614</link>
<guid>https://arxiv.org/abs/2511.13614</guid>
<content:encoded><![CDATA[
<div> Keywords: hedge funds, communication, performance, volatility, trading strategies  

<br /><br />Summary: This article explores the organizational communication strategies of multi-strategy hedge funds, specifically how analysts generating trading strategies interact. Using a framework of 5-agent large language model (LLM)-based trading systems, the study conducted 450 experiments over 21 months to investigate five different communication structures, ranging from isolated agents to collaborative and competitive conversation models. The findings reveal that communication indeed enhances performance, but the effectiveness of these communication styles is influenced by market characteristics. Competitive conversation strategies outperform in volatile technology stocks, while collaborative discussions yield better results in stable general stocks. However, finance stocks displayed resistance to any communication interventions. Remarkably, all structures, including isolated agents, converged to similar strategy alignments, which questions the belief that increased transparency can lead to diversity loss. Performance variances were linked to the behavioral approaches adopted by the agents; competitive interactions emphasize stock-level allocations, whereas collaborative interactions foster the development of comprehensive technical frameworks. Ultimately, this study suggests that communication design must align with market volatility, and that the quality of discussions does not necessarily correlate with improved performance. <div>
arXiv:2511.13614v1 Announce Type: cross 
Abstract: Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seasons's Greetings by AD</title>
<link>https://arxiv.org/abs/2402.09409</link>
<guid>https://arxiv.org/abs/2402.09409</guid>
<content:encoded><![CDATA[
<div> Keywords: Algorithmic Differentiation, type-generic, gradients, C++, source code

<br /><br />Summary: This article discusses the implementation of type-generic tangent and adjoint algorithms using Algorithmic Differentiation (AD) in C++. It specifically focuses on the function $$ y=\sum_{i=0}^{n-1} x_{2 i} \cdot x_{2 i+1} $$ and presents a case study where char-arithmetic is employed. The output gradient for the input vector $(101~77~114~114~32~121~109~88~115~97)^T$ results in the message "Merry Xmas." Furthermore, the article explores an instantiation of type-generic second-order tangent and adjoint algorithms for the function $$ y=\frac{1}{6} \cdot \sum_{i=0}^{n-1} x^3_{i} $$, which yields the output "Happy 2026" for the input $(72~97~112~112~121~32~50~48~50~54)^T$. The article suggests prepending a significant number of zeros to the input vector to investigate the varying run times of different derivative codes. Additionally, the complete source code is available on GitHub at https://github.com/un110076/SeasonsGreetings. <div>
arXiv:2402.09409v2 Announce Type: replace 
Abstract: We use Algorithmic Differentiation (AD) to implement type-generic tangent and adjoint versions of $$ y=\sum_{i=0}^{n-1} x_{2 i} \cdot x_{2 i+1} $$ in C++. We run an instantiation for char-arithmetic and we print the gradient at $(101~77~114~114~32~121~109~88~115~97)^T$ to std::cout, yielding the output ``Merry Xmas''.
  Similar instantiations of type-generic second-order tangent and second-order adjoint versions of $$
  y=\frac{1}{6} \cdot \sum_{i=0}^{n-1} x^3_{i} $$ yield ``Happy 2026'' at $(72~97~112~112~121~32~50~48~50~54)^T.$ Prepend a sufficiently large number of zeros to the input vector to explore the varying run times of the different derivative codes. The entire source code can be found on https://github.com/un110076/SeasonsGreetings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Diffusion Autoencoder for Test-time Adapting Prediction of Complex Systems</title>
<link>https://arxiv.org/abs/2505.17459</link>
<guid>https://arxiv.org/abs/2505.17459</guid>
<content:encoded><![CDATA[
<div> Keywords: spatiotemporal dynamics, latent representations, graph neural networks, prediction error, adaptive re-encoding  

<br /><br />Summary: This work addresses the challenges of predicting the behavior of complex systems, which often relies on modeling high-dimensional observations through latent representations. Current methods struggle to maintain the inherent spatial structure in spatiotemporal dynamics, resulting in ineffective modeling of spatial interactions and challenges in long-term predictions. The authors propose SparseDiff, which includes a test-time adaptation strategy to enhance the encoding of these dynamics, allowing it to adjust to emergent spatiotemporal structures over time. They introduce a codebook-based sparse encoder that transforms continuous spatial domains into a sparse graph topology. Subsequently, they utilize a graph neural ordinary differential equation to model system dynamics and integrate it with a diffusion decoder for reconstruction. SparseDiff enables autoregressive predictions and refines the sparse topology dynamically through adaptive re-encoding to respond to evolving spatiotemporal patterns. Evaluation results indicate that SparseDiff significantly reduces prediction errors, achieving an average reduction of 49.99% compared to existing methods while maintaining only 1% of the spatial resolution. This highlights its effectiveness in modeling complex system dynamics in various scientific and engineering applications. <div>
arXiv:2505.17459v2 Announce Type: replace 
Abstract: Predicting the behavior of complex systems is critical in many scientific and engineering domains, and hinges on the model's ability to capture their underlying dynamics. Existing methods encode the intrinsic dynamics of high-dimensional observations through latent representations and predict autoregressively. However, these latent representations lose the inherent spatial structure of spatiotemporal dynamics, leading to the predictor's inability to effectively model spatial interactions and neglect emerging dynamics during long-term prediction. In this work, we propose SparseDiff, introducing a test-time adaptation strategy to dynamically update the encoding scheme to accommodate emergent spatiotemporal structures during the long-term evolution of the system. Specifically, we first design a codebook-based sparse encoder, which coarsens the continuous spatial domain into a sparse graph topology. Then, we employ a graph neural ordinary differential equation to model the dynamics and guide a diffusion decoder for reconstruction. SparseDiff autoregressively predicts the spatiotemporal evolution and adjust the sparse topological structure to adapt to emergent spatiotemporal patterns by adaptive re-encoding. Extensive evaluations on representative systems demonstrate that SparseDiff achieves an average prediction error reduction of 49.99\% compared to baselines, requiring only 1% of the spatial resolution.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bounds of Block Rewards in Honest PinFi Systems</title>
<link>https://arxiv.org/abs/2404.02174</link>
<guid>https://arxiv.org/abs/2404.02174</guid>
<content:encoded><![CDATA[
<div> PinFi, decentralized pricing, dissipative assets, liquidity providers, game-theoretic approach<br /><br />Summary:<br /><br />This paper introduces PinFi, a novel protocol designed for decentralized pricing of dissipative assets—assets whose values naturally decline over time. The protocol relies heavily on liquidity providers (LPs) for its efficient market functioning. The study identifies and tackles key challenges related to protocol stability and sustainability: LPs' tendency to sell assets on external markets instead of participating within the protocol, a similar preference for internal selling rather than contributing as LPs, and scenarios where LPs are reluctant to sell through the protocol. Utilizing a game-theoretic framework, the research analyzes PinFi's internal mechanisms and explores its broader economic implications. The results indicate that, assuming participants act with integrity, PinFi can achieve a dynamic equilibrium balancing the interests of LPs, sellers, and buyers. This balance is maintained through the implementation of optimally calibrated block rewards incentivizing LP engagement and selling behaviors. These rewards are critical to sustaining the protocol’s long-term stability and enhancing its utility. Consequently, PinFi presents a promising decentralized solution for pricing and trading dissipative assets by aligning participant incentives through carefully designed economic mechanisms. <div>
arXiv:2404.02174v2 Announce Type: replace-cross 
Abstract: PinFi is a class of novel protocols for decentralized pricing of dissipative assets, whose value naturally declines over time. Central to the protocol's functionality and its market efficiency is the role of liquidity providers (LPs). This study addresses critical stability and sustainability challenges within the protocol, namely: the propensity of LPs to prefer selling in external markets over participation in the protocol; a similar inclination towards selling within the PinFi system rather than contributing as LPs; and a scenario where LPs are disinclined to sell within the protocol. Employing a game-theoretic approach, we explore PinFi's mechanisms and its broader ramifications. Our findings reveal that, under a variety of common conditions and with an assumption of participant integrity, PinFi is capable of fostering a dynamic equilibrium among LPs, sellers, and buyers. This balance is maintained through a carefully calibrated range of block rewards for LPs, ensuring the protocol's long-term stability and utility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LooPIN: A PinFi protocol for decentralized computing</title>
<link>https://arxiv.org/abs/2406.09422</link>
<guid>https://arxiv.org/abs/2406.09422</guid>
<content:encoded><![CDATA[
<div> Keywords: Networked computing, PinFi protocol, decentralized, dynamic pricing, computing power 

<br /><br />Summary: The paper introduces the Physical Infrastructure Finance (PinFi) protocol, aimed at enhancing the distribution of computing power in decentralized networks. In the context of increased demand for networked computing due to advancements in artificial intelligence, PinFi addresses significant challenges like coordination, pricing, and liquidity intrinsic to decentralized physical infrastructure networks (DePIN). A key feature of the protocol is its innovative dynamic pricing mechanism, which allows providers to contribute surplus computing resources to a unique "dissipative" PinFi liquidity pool. This is contrasted with conventional decentralized finance (DeFi) liquidity pools. The PinFi protocol ensures clients have seamless access to computing power at fair, market-driven prices. By optimizing resource allocation, this protocol can decrease the costs of accessing computing power to as low as 1% compared to current alternatives, while also boosting security and reliability. Overall, the PinFi protocol has the potential to revolutionize the supply-demand dynamics in computing power networks, establishing a new benchmark for efficiency and accessibility in the market. <div>
arXiv:2406.09422v2 Announce Type: replace-cross 
Abstract: Networked computing power is a critical utility in the era of artificial intelligence. This paper presents a novel Physical Infrastructure Finance (PinFi) protocol designed to facilitate the distribution of computing power within networks in a decentralized manner. Addressing the core challenges of coordination, pricing, and liquidity in decentralized physical infrastructure networks (DePIN), the PinFi protocol introduces a distinctive dynamic pricing mechanism. It enables providers to allocate excess computing resources to a "dissipative" PinFi liquidity pool, distinct from traditional DeFi liquidity pools, ensuring seamless access for clients at equitable, market-based prices. This approach significantly reduces the costs of accessing computing power, potentially to as low as 1% compared to existing services, while simultaneously enhancing security and dependability. The PinFi protocol is poised to transform the dynamics of supply and demand in computing power networks, setting a new standard for efficiency and accessibility.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Urban Service Allocation with Time-Constrained Restless Bandits</title>
<link>https://arxiv.org/abs/2502.00045</link>
<guid>https://arxiv.org/abs/2502.00045</guid>
<content:encoded><![CDATA[
<div> Keywords: municipal inspections, food establishments, restless multi-armed bandit, optimization, Chicago Department of Public Health

<br /><br />Summary: This paper investigates the scheduling of municipal inspections, focusing on food establishment inspections in Chicago. The Chicago Department of Public Health (CDPH) annually inspects thousands of establishments, with over 3,000 failing inspections in 2023. The study's goal is to balance the need for adherence to food safety guidelines, minimize disruption to businesses, and reduce inspection costs. CDPH assigns each establishment a specific inspection window, ensuring they are inspected once within that period while also accommodating surprise inspections for emergencies. This scenario presents challenges for implementing a restless multi-armed bandit (RMAB) approach, which lacks existing methodologies. The authors introduce an extension of Whittle index-based systems for RMABs, ensuring compliance with inspection constraints and optimizing assignment schedules. To model state transitions of establishments, a neural network-based supervised learning model was developed, leading to a 10% improvement in AUC over traditional predictive methods. Experiments indicate objective improvements of up to 24% in simulations and 33% using real CDPH data, demonstrating the effectiveness of their approach and its robustness against unexpected inspections, while providing insights into the implications of scheduling constraints. <div>
arXiv:2502.00045v2 Announce Type: replace-cross 
Abstract: Municipal inspections are an important part of maintaining the quality of goods and services. In this paper, we approach the problem of intelligently scheduling service inspections to maximize their impact, using the case of food establishment inspections in Chicago as a case study. The Chicago Department of Public Health (CDPH) inspects thousands of establishments each year, with a substantial fail rate (over 3,000 failed inspection reports in 2023). To balance the objectives of ensuring adherence to guidelines, minimizing disruption to establishments, and minimizing inspection costs, CDPH assigns each establishment an inspection window every year and guarantees that they will be inspected exactly once during that window. Meanwhile, CDPH also promises surprise public health inspections for unexpected food safety emergencies or complaints. These constraints create a challenge for a restless multi-armed bandit (RMAB) approach, for which there are no existing methods. We develop an extension to Whittle index-based systems for RMABs that can guarantee action window constraints and frequencies, and furthermore can be leveraged to optimize action window assignments themselves. Briefly, we combine MDP reformulation and integer programming-based lookahead to maximize the impact of inspections subject to constraints. A neural network-based supervised learning model is developed to model state transitions of real Chicago establishments using public CDPH inspection records, which demonstrates 10% AUC improvements compared with directly predicting establishments' failures. Our experiments not only show up to 24% (in simulation) or 33% (on real data) objective improvements resulting from our approach and robustness to surprise inspections, but also give insight into the impact of scheduling constraints.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph topology estimation of power grids using pairwise mutual information of time series data</title>
<link>https://arxiv.org/abs/2505.11517</link>
<guid>https://arxiv.org/abs/2505.11517</guid>
<content:encoded><![CDATA[
<div> Keywords: power grid, topology, mutual information, Chow-Liu algorithm, IEEE networks  

<br /><br />Summary: The paper presents an information theoretic approach to estimate the topology of power grids by modeling them as graphs. It utilizes voltage magnitude data from individual nodes and computes the mutual information between node pairs using various approximation methods. The Chow-Liu algorithm is employed to construct a maximum spanning tree based on mutual information, which helps in estimating the power grid topology. The researchers apply this methodology to multiple datasets to gauge its effectiveness and explore its domain of applicability. They analyze factors such as data quality, precision, time windows, frequency, and mutual information calculation methods to understand their impact on successful graph reconstruction and the identification of leaf nodes. The findings indicate successful application on IEEE networks generated via MATPOWER and datasets produced through GridLAB-D. Lastly, the algorithm's performance is validated through cross-validation techniques on IEEE networks, highlighting its robustness in estimating power grid structures under varying conditions. <div>
arXiv:2505.11517v3 Announce Type: replace-cross 
Abstract: The topology of a power grid is estimated using an information theoretic approach. By modeling the grid as a graph and using voltage magnitude data of individual nodes in the grid, the mutual information between pairs of nodes is computed using different approximation methods. Using the well-known Chow-Liu algorithm, a maximum spanning tree based on mutual information is computed to estimate the power grid topology. This manuscript explores the application of this method to different datasets and explores the domain of applicability. The data quality, precision, time windows, frequency and the method for calculating the mutual information are varied to see the effect on the successful reconstruction of the graph and it's leaf nodes. Success is shown for IEEE networks generated with MATPOWER and data generated using GridLAB-D. The algorithm is then cross-validated on IEEE networks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surrogate-Based Differentiable Pipeline for Shape Optimization</title>
<link>https://arxiv.org/abs/2511.10761</link>
<guid>https://arxiv.org/abs/2511.10761</guid>
<content:encoded><![CDATA[
<div> Gradient-based optimization, non-differentiable components, surrogate models, aerodynamic shape optimization, 3D U-Net<br /><br />Summary:<br /><br />This paper addresses the challenge of using gradient-based optimization in engineering design workflows, which are often hindered by non-differentiable components such as meshing and physical simulation steps. These components prevent direct calculation of gradients necessary for optimization, despite the underlying physics often being differentiable. To overcome this, the authors propose replacing these non-differentiable parts with surrogate models that are inherently differentiable. They demonstrate their approach using a toy example in aerodynamic shape optimization, where a 3D U-Net model serves as a full-field surrogate for both meshing and simulation. This surrogate is trained to map from the signed distance field (SDF) of the shape to the desired fields of interest. By doing so, the entire optimization pipeline becomes end-to-end differentiable, enabling gradient-based shape optimization without relying on differentiable solvers or adjoint methods, which can be complex to implement or unavailable in many cases. This method promises to speed up high-dimensional optimization problems in engineering design by leveraging neural network surrogates to bypass traditional bottlenecks in CAE workflows. <div>
arXiv:2511.10761v1 Announce Type: new 
Abstract: Gradient-based optimization of engineering designs is limited by non-differentiable components in the typical computer-aided engineering (CAE) workflow, which calculates performance metrics from design parameters. While gradient-based methods could provide noticeable speed-ups in high-dimensional design spaces, codes for meshing, physical simulations, and other common components are not differentiable even if the math or physics underneath them is. We propose replacing non-differentiable pipeline components with surrogate models which are inherently differentiable. Using a toy example of aerodynamic shape optimization, we demonstrate an end-to-end differentiable pipeline where a 3D U-Net full-field surrogate replaces both meshing and simulation steps by training it on the mapping between the signed distance field (SDF) of the shape and the fields of interest. This approach enables gradient-based shape optimization without the need for differentiable solvers, which can be useful in situations where adjoint methods are unavailable and/or hard to implement.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments</title>
<link>https://arxiv.org/abs/2511.10716</link>
<guid>https://arxiv.org/abs/2511.10716</guid>
<content:encoded><![CDATA[
<div> Keywords: Pareto pruning, multi-objective optimization, quality measures, directed coverage, computational complexity  

<br /><br />Summary:  
This paper addresses the challenge of selecting a representative subset from the full set of Pareto optimal solutions in multi-objective optimization problems, where decision makers face cognitive overload due to multiple viable candidates. It reframes the Pareto pruning problem as a multiwinner voting issue and performs an axiomatic analysis on existing quality measures, revealing several unintuitive properties. Motivated by these insights, the authors propose a new quality measure called directed coverage, designed to better capture desirable representation characteristics. The study also explores the computational complexity of optimizing various quality measures, identifying new boundaries that distinguish tractable from intractable cases based on the number and structure of objectives involved. An experimental evaluation compares the performance of different quality measures, showing that the choice of measure significantly influences the nature of the selected subset of solutions. The proposed directed coverage measure demonstrates competitive or superior performance across various tested scenarios, suggesting it as a promising tool for practitioners aiming to simplify decision making in multi-objective optimization by effectively summarizing Pareto frontiers. <div>
arXiv:2511.10716v1 Announce Type: cross 
Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MECHBench: A Set of Black-Box Optimization Benchmarks originated from Structural Mechanics</title>
<link>https://arxiv.org/abs/2511.10821</link>
<guid>https://arxiv.org/abs/2511.10821</guid>
<content:encoded><![CDATA[
<div> Benchmarking, black-box optimization, structural mechanics, vehicle crashworthiness, gradient-free algorithms<br /><br />Summary:<br /><br />1. This paper introduces a new benchmarking suite specifically designed for black-box optimization algorithms, focusing on real-world applications rather than abstract or synthetic problems. <br />2. Existing benchmark suites often fail to fully represent the complexities encountered in practical engineering optimization, limiting their usefulness for application-oriented scenarios. <br />3. The proposed benchmark suite is rooted in structural mechanics, providing a more realistic and relevant set of test problems. <br />4. Current benchmarks are derived from vehicle crashworthiness problems, which are characterized by non-smooth, highly non-linear models requiring gradient-free optimization algorithms. <br />5. The paper offers detailed descriptions of each benchmark’s physical context, formal optimization problem statements, and clear instructions for using the suite, facilitating its adoption for evaluating black-box optimization methods in engineering domains. <div>
arXiv:2511.10821v1 Announce Type: cross 
Abstract: Benchmarking is essential for developing and evaluating black-box optimization algorithms, providing a structured means to analyze their search behavior. Its effectiveness relies on carefully selected problem sets used for evaluation. To date, most established benchmark suites for black-box optimization consist of abstract or synthetic problems that only partially capture the complexities of real-world engineering applications, thereby severely limiting the insights that can be gained for application-oriented optimization scenarios and reducing their practical impact. To close this gap, we propose a new benchmarking suite that addresses it by presenting a curated set of optimization benchmarks rooted in structural mechanics. The current implemented benchmarks are derived from vehicle crashworthiness scenarios, which inherently require the use of gradient-free algorithms due to the non-smooth, highly non-linear nature of the underlying models. Within this paper, the reader will find descriptions of the physical context of each case, the corresponding optimization problem formulations, and clear guidelines on how to employ the suite.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery</title>
<link>https://arxiv.org/abs/2511.11257</link>
<guid>https://arxiv.org/abs/2511.11257</guid>
<content:encoded><![CDATA[
<div> Keywords: Ionic Liquids, Large Language Models, AIonopedia, property prediction, molecular design  

<br /><br />Summary: The article presents AIonopedia, the first Large Language Model (LLM) agent designed specifically for the discovery of Ionic Liquids (ILs). AIonopedia tackles critical challenges in IL property prediction, such as limited data availability, model accuracy issues, and fragmented design workflows. The system is powered by an LLM-augmented multimodal domain foundation model tailored for ILs, which enhances the accuracy of property predictions. Its hierarchical search architecture allows efficient molecular screening and design, optimizing the discovery process. The model was trained and evaluated on a newly curated, comprehensive dataset of ILs, where it demonstrated superior performance compared to existing methods. Additional tests on literature-reported IL systems showed that AIonopedia can effectively modify IL structures to meet desired criteria. Beyond offline validations, the agent’s real-world efficacy was confirmed through wet-lab experiments, validating its ability to generalize and perform well on out-of-distribution tasks. These results highlight AIonopedia’s potential to significantly accelerate the practical discovery and optimization of Ionic Liquids, bridging the gap between computational predictions and experimental outcomes. <div>
arXiv:2511.11257v1 Announce Type: cross 
Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models</title>
<link>https://arxiv.org/abs/2511.11315</link>
<guid>https://arxiv.org/abs/2511.11315</guid>
<content:encoded><![CDATA[
<div> Natural Language Processing, financial NLP, large language models, Layer-wise Adaptive Ensemble Tuning, computational efficiency  

<br /><br />Summary:  
This paper addresses challenges in applying large language models (LLMs) to financial natural language processing (NLP) tasks, which currently require substantial computational resources, limiting their accessibility. The authors highlight the effectiveness of existing financial LLMs such as BloombergGPT, FinMA, and the bilingual FinMA-ES, which achieve strong results on benchmarks like FLARE and FLARE-ES in tasks including sentiment analysis, stock movement prediction, and credit risk assessment. To overcome computational bottlenecks, they propose a novel method called Layer-wise Adaptive Ensemble Tuning (LAET), which selectively fine-tunes only the most impactful layers of pre-trained LLMs by analyzing hidden state representations and freezing less critical layers. This selective tuning significantly reduces computational overhead while improving task-specific performance. Experimental results demonstrate that models using LAET not only outperform current benchmarks but also surpass state-of-the-art LLMs such as GPT-4 on various financial NLP tasks, despite using smaller models (~3 billion parameters). Overall, the work presents an efficient and scalable approach, bridging cutting-edge financial NLP research with practical real-world deployment for organizations with limited computing resources. <div>
arXiv:2511.11315v1 Announce Type: cross 
Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization</title>
<link>https://arxiv.org/abs/2511.11415</link>
<guid>https://arxiv.org/abs/2511.11415</guid>
<content:encoded><![CDATA[
<div> Keywords: differentiable programming, acoustic inverse problems, automatic differentiation, shape optimization, finite element method<br /><br />Summary:<br /><br />This article presents a practical differentiable programming approach tailored for acoustic inverse problems with two main applications: admittance estimation and shape optimization for resonance damping. First, the authors utilize JAX-FEM’s automatic differentiation (AD) capabilities to directly estimate complex boundary admittance from sparse pressure measurements. This method achieves three-digit precision without needing manual derivation of adjoint equations, simplifying the gradient-based parameter estimation process. Second, the work applies randomized finite differences combined with JAX-FEM for forward simulations and PyTorch3D for geometry manipulation through AD. This hybrid approach separates physics-driven boundary optimization from interior mesh adaptation, resulting in a significant 48.1% reduction in energy at target frequencies. Compared to standard finite difference methods applied on the full mesh, their method requires approximately 30 times fewer finite element method (FEM) solutions, indicating enhanced computational efficiency. Overall, the study showcases how leveraging modern differentiable software stacks can expedite the prototyping of optimization workflows in physics-based inverse problems. It highlights a seamless integration of automatic differentiation for parameter estimation alongside finite difference methods and AD for geometric design, thus advancing both precision and efficiency in acoustic optimization tasks. <div>
arXiv:2511.11415v1 Announce Type: cross 
Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Risk-Aware Deep Reinforcement Learning for Dynamic Portfolio Optimization</title>
<link>https://arxiv.org/abs/2511.11481</link>
<guid>https://arxiv.org/abs/2511.11481</guid>
<content:encoded><![CDATA[
<div> Keywords: deep reinforcement learning, portfolio optimization, risk control, Proximal Policy Optimization, Sharpe ratio<br /><br />Summary:<br /><br />This paper introduces a deep reinforcement learning (DRL) framework designed for dynamic portfolio optimization amidst market uncertainty and risk. The model incorporates a Sharpe ratio-based reward function augmented by direct risk control measures such as maximum drawdown and volatility constraints to better manage risk. Proximal Policy Optimization (PPO) is utilized to train the agent to adaptively allocate assets using historical financial time series data. The performance of the DRL agent is benchmarked against traditional portfolio strategies, notably mean-variance optimization and equal-weight allocation. Backtesting on historically high-performing equities demonstrates that the DRL approach effectively stabilizes volatility, indicating successful risk control. However, this success comes at a cost: the risk-adjusted returns deteriorate due to the agent converging on an overly conservative policy. This signifies the difficulty in achieving a balance between exploration, maximizing returns, and mitigating risk within the DRL framework. The findings highlight the limitations of the current reward design and suggest that reward shaping, alongside hybrid risk-aware strategies, could improve the efficacy and practical deployment of DRL-based portfolio management models in real-world scenarios. <div>
arXiv:2511.11481v1 Announce Type: cross 
Abstract: This paper presents a deep reinforcement learning (DRL) framework for dynamic portfolio optimization under market uncertainty and risk. The proposed model integrates a Sharpe ratio-based reward function with direct risk control mechanisms, including maximum drawdown and volatility constraints. Proximal Policy Optimization (PPO) is employed to learn adaptive asset allocation strategies over historical financial time series. Model performance is benchmarked against mean-variance and equal-weight portfolio strategies using backtesting on high-performing equities. Results indicate that the DRL agent stabilizes volatility successfully but suffers from degraded risk-adjusted returns due to over-conservative policy convergence, highlighting the challenge of balancing exploration, return maximization, and risk mitigation. The study underscores the need for improved reward shaping and hybrid risk-aware strategies to enhance the practical deployment of DRL-based portfolio allocation models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Localized kernel gradient correction for SPH simulations of water wave propagation</title>
<link>https://arxiv.org/abs/2511.10064</link>
<guid>https://arxiv.org/abs/2511.10064</guid>
<content:encoded><![CDATA[
<div> Keywords: Smoothed Particle Hydrodynamics, numerical dissipation, kernel gradient correction, water wave simulation, free surface treatment<br /><br />Summary:  
Basic Smoothed Particle Hydrodynamics (SPH) models often suffer from excessive numerical dissipation when simulating water wave propagation, which degrades the quality of the results. To address this, higher-order methods such as kernel gradient correction can be applied to improve accuracy, but these methods require significantly more computational resources. This work demonstrates that the higher-order kernel gradient correction scheme is only necessary in specific regions of the simulated water wave, rather than throughout the entire fluid domain, thus saving computational effort. The criterion for identifying particles that require special numerical treatment is based on fundamental principles of water wave mechanics, particularly effective for deep water wave conditions. Additionally, the paper introduces a solution to the challenges posed by kernel gradient correction near the free surface, enhancing the robustness of the simulation in this critical region. Validation of the proposed approach is presented through satisfactory results for two test cases: a standing wave in a basin and a progressive wave train in a long wave tank. These results illustrate that the selective application of higher-order corrections maintains accuracy while improving computational efficiency in SPH water wave simulations. <div>
arXiv:2511.10064v1 Announce Type: new 
Abstract: Basic Smoothed Particle Hydrodynamics (SPH) models exhibit excessive, numerical dissipation in the simulation of water wave propagation. This can be remedied using higher-order approaches such as kernel gradient correction, which introduce additional computational effort. The present work demonstrates, that the higher-order scheme is only required in a limited part of the water wave in order to obtain satisfying results. The criterion for distinguishing particles in need of special treatment from those that do not is motivated by water wave mechanics. Especially for deep water waves, the approach potentially spares large amounts of computational effort. The present paper also proposes a remedy for issues of the kernel gradient correction occurring at the free surface. Satisfying results for the proposed approach are shown for a standing wave in a basin and a progressive wave train in a long wave tank.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase field modelling of cracking and capacity fade in core-shell cathode particles for lithium-ion batteries</title>
<link>https://arxiv.org/abs/2511.10355</link>
<guid>https://arxiv.org/abs/2511.10355</guid>
<content:encoded><![CDATA[
<div> Core-shell electrode particles, mechanical failure, phase field method, chemo-mechano-damage model, capacity loss<br /><br />Summary:<br /><br />This work introduces a comprehensive computational framework to predict mechanical failure and performance degradation in core-shell electrode particles used in lithium-ion batteries. The model couples chemical, mechanical, and damage processes to simulate particle cracking and capacity fade. It captures both bulk material fracture and interface debonding via the phase field method. Using a representative core-shell system (NMC811@NMC532), the study quantifies cracking severity and capacity loss. Surface cracks initiate due to higher lithium concentration gradients between the core and the shell. Interface debonding is driven by localized hoop stresses near the core-shell boundary caused by differential expansion, leading to rapid debonding that blocks lithium-ion transport and can cause over 10% capacity loss after just one discharge cycle. Larger particles are prone to crack branching from tensile stress zones, which may fragment entire particles. This framework advances understanding of cracking mechanisms and their impact on battery degradation. It also offers a tool to design electrode materials with enhanced durability and improved electrochemical performance, potentially extending battery lifetime. <div>
arXiv:2511.10355v1 Announce Type: new 
Abstract: Core-shell electrode particles are a promising morphology control strategy for high-performance lithium-ion batteries. However, experimental observations reveal that these structures remain prone to mechanical failure, with shell fractures and core-shell debonding occurring after a single charge. In this work, we present a novel, comprehensive computational framework to predict and gain insight into the failure of core-shell morphologies and the associated degradation in battery performance. The fully coupled chemo-mechano-damage model presented captures the interplay between mechanical damage and electrochemical behaviours, enabling the quantification of particle cracking and capacity fade. Both bulk material fracture and interface debonding are captured by utilising the phase field method. We quantify the severity of particle cracking and capacity loss through case studies on a representative core-shell system (NMC811@NMC532). The results bring valuable insights into cracking patterns, underlying mechanisms, and their impact on capacity loss. Surface cracks are found to initiate when a significantly higher lithium concentration accumulates in the core compared to the shell. Interfacial debonding is shown to arise from localised hoop stresses near the core-shell interface, due to greater shell expansion. This debonding develops rapidly, impedes lithium-ion transport, and can lead to more than 10\% capacity loss after a single discharge. Furthermore, larger particles may experience crack branching driven by extensive tensile zones, potentially fragmenting the entire particle. The framework developed can not only bring new insight into the degradation mechanisms of core-shell particles but also be used to design electrode materials with improved performance and extended lifetime.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search</title>
<link>https://arxiv.org/abs/2511.09900</link>
<guid>https://arxiv.org/abs/2511.09900</guid>
<content:encoded><![CDATA[
<div> Protein evolution, protein language models, AlphaDE, Monte Carlo tree search, directed evolution<br /><br />Summary: Protein evolution through amino acid mutations is fundamental in life sciences, yet current in-silico directed evolution algorithms mainly focus on search strategies without fully leveraging protein language models that capture evolutionary patterns. To address this, the authors propose AlphaDE, a novel framework that integrates large protein language models into the evolution process. AlphaDE first fine-tunes pretrained protein language models via masked language modeling using homologous sequences to enhance evolutionary plausibility specific to the targeted protein class. Then, it employs a test-time inference strategy based on Monte Carlo tree search, guided by the fine-tuned language model, to efficiently navigate protein sequence evolution. Extensive benchmark experiments demonstrate that AlphaDE significantly outperforms existing state-of-the-art methods, even with only few-shot fine-tuning data. Additionally, a case study reveals AlphaDE’s capacity to compress and refine the protein sequence space through computational evolution, highlighting its potential to streamline protein design and understanding. Overall, AlphaDE represents an innovative integration of evolutionary biology and deep learning techniques, offering a powerful tool for protein engineering applications. <div>
arXiv:2511.09900v1 Announce Type: cross 
Abstract: Protein evolution through amino acid sequence mutations is a cornerstone of life sciences. While current in-silicon directed evolution algorithms focus on designing search strategies, they overlook how to utilize the transformative protein language models, which encode rich evolutionary patterns, to guide search. To bridge this gap, we propose AlphaDE, a novel framework to evolve protein sequences by harnessing the innovative paradigms of large language models. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility for the interested protein class. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. An interesting case study further shows that AlphaDE supports condensing the protein sequence space through computational evolution.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Efficient Quantum Algorithms for Solving Nonlinear Differential Equations Using Multiple Bosonic Modes Coupled with Qubits</title>
<link>https://arxiv.org/abs/2511.09939</link>
<guid>https://arxiv.org/abs/2511.09939</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum computing, nonlinear PDEs, continuous-variable quantum algorithms, Koopman-von Neumann formalism, analog quantum simulation<br /><br />Summary:<br /><br />This article presents a novel analog, continuous-variable quantum algorithm to solve nonlinear classical partial differential equations (PDEs) efficiently without digitizing the Hilbert space. Unlike conventional digital approaches relying on Carleman linearization and quantum linear-system solvers, which require fault-tolerant quantum computers and truncation, the proposed method encodes classical fields as coherent states and uses adaptive qubit-based measurements. The algorithm leverages the Koopman-von Neumann formalism to convert nonlinear PDE evolution into linear dynamics via Kraus-channel composition. The computational cost is shown to scale as \(\mathcal{O}(T(\log L + d r \log K))\) for advancing the solution over \(T\) time steps on a \(d\)-dimensional grid with \(L\) points, spatial-derivative order \(K\), and polynomial nonlinearity degree \(r\), demonstrating provable efficiency. The method is validated through simulations of the one-dimensional Burgers' equation and the two-dimensional Fisher-KPP equation, showcasing its practical applicability. Importantly, the algorithm exhibits robustness against photon loss under conditions of strong dissipation, and the authors derive an analytic counterterm to mitigate dominant experimental noise. This framework offers a promising route towards practical quantum speedup in simulating nonlinear systems on near-term analog quantum hardware. <div>
arXiv:2511.09939v1 Announce Type: cross 
Abstract: Quantum computers have long been expected to efficiently solve complex classical differential equations. Most digital, fault-tolerant approaches use Carleman linearization to map nonlinear systems to linear ones and then apply quantum linear-system solvers. However, provable speedups typically require digital truncation and full fault tolerance, rendering such linearization approaches challenging to implement on current hardware. Here we present an analog, continuous-variable algorithm based on coupled bosonic modes with qubit-based adaptive measurements that avoids Hilbert-space digitization. This method encodes classical fields as coherent states and, via Kraus-channel composition derived from the Koopman-von Neumann (KvN) formalism, maps nonlinear evolution to linear dynamics. Unlike many analog schemes, the algorithm is provably efficient: advancing a first-order, $L$-grid point, $d$-dimensional, order-$K$ spatial-derivative, degree-$r$ polynomial-nonlinearity, strongly dissipative partial differential equations (PDEs) for $T$ time steps costs $\mathcal{O}\left(T(\log L + d r \log K)\right)$. The capability of the scheme is demonstrated by using it to simulate the one-dimensional Burgers' equation and two-dimensional Fisher-KPP equation. The resilience of the method to photon loss is shown under strong-dissipation conditions and an analytic counterterm is derived that systematically cancels the dominant, experimentally calibrated noise. This work establishes a continuous-variable framework for simulating nonlinear systems and identifies a viable pathway toward practical quantum speedup on near-term analog hardware.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fourier Neural Operators for Structural Dynamics Models: Challenges, Limitations and Advantages of Using a Spectrogram Loss</title>
<link>https://arxiv.org/abs/2511.08753</link>
<guid>https://arxiv.org/abs/2511.08753</guid>
<content:encoded><![CDATA[
<div> Fourier Neural Operators, non-linear systems, artificial energy dissipation, frequency-aware training, LSTM <br />
<br /> 
Summary: Fourier Neural Operators (FNOs) have been found effective in modeling linear systems but fail on non-linear systems due to artificial energy dissipation and manipulated frequency content. This limitation persists regardless of training dataset size, attributed to discretization error analysis. Comparison with LSTM shows FNOs' superiority for both linear and non-linear systems. A spectrogram-based loss function addressing frequency biases improves FNO performance on both system types. The IEA 15MW turbine model validation confirms FNO accuracy in a predominantly linear regime. System non-linearity, rather than complexity, determines FNO success. These findings offer practical guidelines and challenge assumptions of FNO universality. <br /> <div>
arXiv:2511.08753v1 Announce Type: new 
Abstract: Fourier Neural Operators (FNOs) have emerged as promising surrogates for partial differential equation solvers. In this work, we extensively tested FNOs on a variety of systems with non-linear and non-stationary properties, using a wide range of forcing functions to isolate failure mechanisms. FNOs stand out in modeling linear systems, regardless of complexity, while achieving near-perfect energy preservation and accurate spectral representation for linear dynamics. However, they fail on non-linear systems, where the failure manifests as artificial energy dissipation and manipulated frequency content. This limitation persists regardless of training dataset size, and we discuss the root cause through discretization error analysis. Comparison with LSTM as the baseline shows FNOs are superior for both linear and non-linear systems, independent of the training dataset size. We develop a spectrogram-based loss function that combines time-domain Mean Squared Error (MSE) with frequency-domain magnitude and phase errors, addressing the low-frequency bias of FNOs. This frequency-aware training eliminates artificial dissipation in linear systems and enhances the energy ratios of non-linear systems. The IEA 15MW turbine model validates our findings. Despite hundreds of degrees of freedom, FNO predictions remain accurate because the turbine behaves in a predominantly linear regime. Our findings establish that system non-linearity, rather than dimensionality or complexity, determines the success of FNO. These results provide clear guidelines for practitioners and challenge assumptions about FNOs' universality.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Control PDEs with Differentiable Predictive Control and Time-Integrated Neural Operators</title>
<link>https://arxiv.org/abs/2511.08992</link>
<guid>https://arxiv.org/abs/2511.08992</guid>
<content:encoded><![CDATA[
<div> Keywords: PDE control, TI-DeepONet, Differentiable Predictive Control, neural policy optimization, operator learning<br /><br />Summary:<br /><br />This article proposes an end-to-end framework for controlling systems governed by partial differential equations (PDEs) using machine learning. The core innovation is integrating Time-Integrated Deep Operator Networks (TI-DeepONets), which serve as differentiable surrogate models capturing temporal dynamics of PDEs while maintaining temporal causality and reducing error accumulation in long-term predictions. These TI-DeepONets are embedded within the Differentiable Predictive Control (DPC) framework, enabling simultaneous learning of neural control policies and PDE dynamics in a self-supervised manner without requiring online optimization or external supervisory controllers. The approach uses automatic differentiation to backpropagate control loss gradients through the operator network, facilitating efficient offline optimization of control policies that satisfy constraints. Empirical validation is performed on a variety of PDE systems including heat, nonlinear Burgers’, and reaction-diffusion equations. The learned policies successfully achieve objectives such as target tracking, constraint adherence, and curvature minimization while showing robust generalization across diverse initial conditions and parameters. Overall, this work demonstrates the promise of combining operator learning with differentiable predictive control for scalable, model-based optimal control of PDE-constrained problems, offering a novel pathway for self-supervised neural control in infinite-dimensional dynamical systems. <div>
arXiv:2511.08992v1 Announce Type: new 
Abstract: We present an end-to-end learning to control framework for partial differential equations (PDEs). Our approach integrates Time-Integrated Deep Operator Networks (TI-DeepONets) as differentiable PDE surrogate models within the Differentiable Predictive Control (DPC)-a self-supervised learning framework for constrained neural control policies. The TI-DeepONet architecture learns temporal derivatives and couples them with numerical integrators, thus preserving the temporal causality of infinite-dimensional PDEs while reducing error accumulation in long-horizon predictions. Within DPC, we leverage automatic differentiation to compute policy gradients by backpropagating the expectations of optimal control loss through the learned TI-DeepONet, enabling efficient offline optimization of neural policies without the need for online optimization or supervisory controllers. We empirically demonstrate that the proposed method learns feasible parametric policies across diverse PDE systems, including the heat, the nonlinear Burgers', and the reaction-diffusion equations. The learned policies achieve target tracking, constraint satisfaction, and curvature minimization objectives, while generalizing across distributions of initial conditions and problem parameters. These results highlight the promise of combining operator learning with DPC for scalable, model-based self-supervised learning in PDE-constrained optimal control.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CENIC: Convex Error-controlled Numerical Integration for Contact</title>
<link>https://arxiv.org/abs/2511.08771</link>
<guid>https://arxiv.org/abs/2511.08771</guid>
<content:encoded><![CDATA[
<div> Continuous-time integration, error-controlled integration, robotics simulators, CENIC, stiff dynamics-contact

Summary: 
CENIC introduces a novel continuous-time integrator that addresses the challenges of time stepping in robotics simulations. By combining convex time-stepping and error-controlled integration, CENIC offers enhanced accuracy and convergence while maintaining fast real-time performance comparable to popular discrete-time simulators such as MuJoCo, Drake, and Isaac Sim. This integrator automates time step adjustments to achieve the desired accuracy, eliminating non-physical artifacts associated with large time steps and the sluggishness of small time steps. In particular, CENIC excels in handling the stiff dynamics of contact interactions, which pose a significant challenge for existing error-controlled integrators. With CENIC, users can benefit from reliable simulation results, increased scalability, and improved workflow efficiency in robotic systems development. <div>
arXiv:2511.08771v1 Announce Type: cross 
Abstract: State-of-the-art robotics simulators operate in discrete time. This requires users to choose a time step, which is both critical and challenging: large steps can produce non-physical artifacts, while small steps force the simulation to run slowly. Continuous-time error-controlled integration avoids such issues by automatically adjusting the time step to achieve a desired accuracy. But existing error-controlled integrators struggle with the stiff dynamics of contact, and cannot meet the speed and scalability requirements of modern robotics workflows. We introduce CENIC, a new continuous-time integrator that brings together recent advances in convex time-stepping and error-controlled integration, inheriting benefits from both continuous integration and discrete time-stepping. CENIC runs at fast real-time rates comparable to discrete-time robotics simulators like MuJoCo, Drake and Isaac Sim, while also providing guarantees on accuracy and convergence.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A coupled finite element-virtual element method for thermomechanical analysis of electronic packaging structures</title>
<link>https://arxiv.org/abs/2511.09348</link>
<guid>https://arxiv.org/abs/2511.09348</guid>
<content:encoded><![CDATA[
<div> Finite Element Method, Virtual Element Method, Thermomechanical Analysis, Electronic Packaging, Multi-material Systems<br /><br />Summary:<br /><br />This study introduces a coupled finite element (FEM) and virtual element method (VEM) tailored for thermomechanical analysis in electronic packaging structures. The computational domain is partitioned strategically, using FEM for regular-shaped regions to maximize efficiency and VEM for complex geometries to enhance flexibility. The methodology maintains interface compatibility by employing coincident nodal correspondence, ensuring continuous solutions across domain boundaries. This approach reduces meshing complexity and computational overhead. Validation of the FE-VE coupled method is demonstrated through various electronic packaging applications, showing good agreement with reference solutions. The method exhibits acceptable convergence behavior with varying mesh densities. It effectively captures both thermal distribution and stress concentrations in multi-material systems, which is critical for reliable electronic packaging design. The study establishes a practical computational framework that balances accuracy, efficiency, and geometric adaptability. Additionally, the authors provide source codes freely accessible on GitHub, encouraging further development and application of the method in related engineering problems. <div>
arXiv:2511.09348v1 Announce Type: cross 
Abstract: This study presents a finite element and virtual element (FE-VE) coupled method for thermomechanical analysis in electronic packaging structures. The approach partitions computational domains strategically, employing FEM for regular geometries to maximize computational efficiency and VEM for complex shapes to enhance geometric flexibility. Interface compatibility is maintained through coincident nodal correspondence, ensuring solution continuity across domain boundaries while reducing meshing complexity and computational overhead. Validation through electronic packaging applications demonstrates reasonable agreement with reference solutions and acceptable convergence characteristics across varying mesh densities. The method effectively captures thermal distributions and stress concentrations in multi-material systems, establishing a practical computational framework for electronic packaging analysis involving complex geometries. Source codes are available at https://github.com/yanpeng-gong/FeVeCoupled-ElectronicPackaging.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ray-trax: Fast, Time-Dependent, and Differentiable Ray Tracing for On-the-fly Radiative Transfer in Turbulent Astrophysical Flows</title>
<link>https://arxiv.org/abs/2511.09389</link>
<guid>https://arxiv.org/abs/2511.09389</guid>
<content:encoded><![CDATA[
<div> Radiative transfer, GPU, differentiable programming, hydrodynamics, ray tracing<br /><br />Summary:<br /><br />1. The article presents Ray-trax, a GPU-accelerated, fully differentiable 3D ray tracer implemented in JAX designed to address the challenges of radiative transfer in computational astrophysics. <br /><br />2. Ray-trax focuses on solving the time-dependent emission--absorption radiative transfer problem, particularly suitable for turbulent gas fields generated by hydrodynamic simulations, using the on-the-fly emission--absorption approximation prevalent in current hydro codes with isotropic scattering.<br /><br />3. The method is highly vectorized, efficiently supporting multiple rays and radiation sources simultaneously, and can handle an arbitrary number of frequency bins without requiring changes to its architecture.<br /><br />4. A significant feature is its end-to-end differentiability, enabling seamless integration with differentiable hydrodynamic solvers and facilitating gradient-based optimization approaches to inverse problems.<br /><br />5. The performance and accuracy are validated against analytical solutions, with applications demonstrated in turbulent media, maintaining a memory complexity scaling as \(\mathcal{O}(N_{\text{src}}\,N_{\text{cells}})\), while remaining efficient on modern hardware accelerators. <div>
arXiv:2511.09389v1 Announce Type: cross 
Abstract: Radiative transfer is a key bottleneck in computational astrophysics: it is nonlocal, stiff, and tightly coupled to hydrodynamics. We introduce Ray-trax, a GPU-oriented, fully differentiable 3D ray tracer written in JAX that solves the time-dependent emission--absorption problem and runs directly on turbulent gas fields produced by hydrodynamic simulations. The method favors the widely used on-the-fly emission--absorption approximation, which is state of the art in many production hydro codes when scattering is isotropic. Ray-trax vectorizes across rays and sources, supports arbitrarily many frequency bins without architectural changes, and exposes end-to-end gradients, making it straightforward to couple with differentiable hydro solvers while preserving differentiability. We validate against analytical solutions, demonstrate propagation in turbulent media, and perform a simple inverse problem via gradient-based optimization. In practice, the memory footprint scales as $\mathcal{O}(N_{\text{src}}\,N_{\text{cells}})$ while remaining highly efficient on accelerators.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TeVAE: A Variational Autoencoder Approach for Discrete Online Anomaly Detection in Variable-state Multivariate Time-series Data</title>
<link>https://arxiv.org/abs/2407.06849</link>
<guid>https://arxiv.org/abs/2407.06849</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, temporal variational autoencoder, automotive testing, unlabelled data, detection delay<br /><br />Summary:<br /><br />1. The article addresses the increasing need for automatic online anomaly detection in automotive testing due to the limitations of manual evaluation and the complexity of recorded real-world data. <br />2. It proposes a Temporal Variational Autoencoder (TeVAE) designed to model testee behavior and detect anomalies effectively while minimizing false positives, even when trained on unlabelled data. <br />3. The approach avoids the bypass phenomenon common in anomaly detection models and introduces a novel method to convert individual anomaly detection windows into a continuous time series, enhancing temporal coherence. <br />4. New evaluation metrics are proposed to measure both the detection delay and the root-cause identification capability, providing a comprehensive assessment of the method's practical performance.<br />5. Experimental results on an industrial real-world data set demonstrate that TeVAE achieves a low false positive rate of 6% and detects 65% of actual anomalies. Additionally, the model shows promise when trained with smaller datasets but requires advanced threshold estimation techniques to maintain performance. <div>
arXiv:2407.06849v2 Announce Type: replace-cross 
Abstract: As attention to recorded data grows in the realm of automotive testing and manual evaluation reaches its limits, there is a growing need for automatic online anomaly detection. This real-world data is complex in many ways and requires the modelling of testee behaviour. To address this, we propose a temporal variational autoencoder (TeVAE) that can detect anomalies with minimal false positives when trained on unlabelled data. Our approach also avoids the bypass phenomenon and introduces a new method to remap individual windows to a continuous time series. Furthermore, we propose metrics to evaluate the detection delay and root-cause capability of our approach and present results from experiments on a real-world industrial data set. When properly configured, TeVAE flags anomalies only 6% of the time wrongly and detects 65% of anomalies present. It also has the potential to perform well with a smaller training and validation subset but requires a more sophisticated threshold estimation method.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated, Memory-Efficient Far-Field Scattering Computation with Monte Carlo SBR</title>
<link>https://arxiv.org/abs/2511.07586</link>
<guid>https://arxiv.org/abs/2511.07586</guid>
<content:encoded><![CDATA[
<div> Monte Carlo integration, Shooting and Bouncing Ray, electromagnetic scattering, complex dielectric materials, variance reduction strategies <br />
<br />
Summary: 
The article introduces a Monte Carlo integration-based Shooting and Bouncing Ray (SBR) algorithm for electromagnetic scattering, specifically targeting complex dielectric materials. This innovative approach reformulates the SBR integral equations using Monte Carlo techniques and advanced variance reduction strategies. The method enables efficient, massively parallel computation on modern GPUs, resulting in reduced memory usage and faster runtime, especially for multilayer dielectric structures. Emphasizing high-energy propagation paths, the algorithm efficiently captures long multipath and interreflection effects. Verification on canonical 3D geometries and ISAR imaging of conducting and dielectric representative aircraft models shows that the Monte Carlo SBR achieves high accuracy with low noise, making it suitable for downstream imaging and analysis tasks. <div>
arXiv:2511.07586v1 Announce Type: new 
Abstract: We introduce a Monte Carlo integration-based Shooting and Bouncing Ray (SBR) algorithm for electromagnetic scattering, specifically targeting complex dielectric materials. Unlike traditional deterministic SBR methods, our approach is the first to reformulate the SBR integral equations using Monte Carlo techniques and advanced variance reduction strategies adapted from photorealistic rendering. This enables efficient, massively parallel computation on modern GPUs, resulting in up to a 10-15x reduction in memory usage and a 4x speed up in runtime, particularly for multilayer dielectric structures. Our method emphasizes high-energy propagation paths, efficiently capturing long multipath and interreflection effects. Verification on canonical 3D geometries and ISAR imaging of both conducting and dielectric representative aircraft models demonstrates that our Monte Carlo SBR achieves high accuracy while maintaining low noise, making it suitable for downstream imaging and analysis tasks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CometNet: Contextual Motif-guided Long-term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.08049</link>
<guid>https://arxiv.org/abs/2511.08049</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-term Time Series Forecasting, Transformer, Multi-layer Perceptron, Contextual Motif, Forecasting Performance

Summary: 
CometNet introduces a novel framework for Long-term Time Series Forecasting, addressing the limitations of existing models. It utilizes a Contextual Motif Extraction module to identify recurrent contextual motifs in historical sequences, allowing for modeling of extensive temporal dependencies. The Motif-guided Forecasting module integrates these dominant motifs into forecasting, improving long-term forecasting capability. By dynamically mapping the look-back window to relevant motifs, CometNet effectively leverages contextual information for forecasting. Experimental results on real-world datasets show that CometNet outperforms current state-of-the-art methods, particularly on extended forecast horizons. <div>
arXiv:2511.08049v1 Announce Type: new 
Abstract: Long-term Time Series Forecasting is crucial across numerous critical domains, yet its accuracy remains fundamentally constrained by the receptive field bottleneck in existing models. Mainstream Transformer- and Multi-layer Perceptron (MLP)-based methods mainly rely on finite look-back windows, limiting their ability to model long-term dependencies and hurting forecasting performance. Naively extending the look-back window proves ineffective, as it not only introduces prohibitive computational complexity, but also drowns vital long-term dependencies in historical noise. To address these challenges, we propose CometNet, a novel Contextual Motif-guided Long-term Time Series Forecasting framework. CometNet first introduces a Contextual Motif Extraction module that identifies recurrent, dominant contextual motifs from complex historical sequences, providing extensive temporal dependencies far exceeding limited look-back windows; Subsequently, a Motif-guided Forecasting module is proposed, which integrates the extracted dominant motifs into forecasting. By dynamically mapping the look-back window to its relevant motifs, CometNet effectively harnesses their contextual information to strengthen long-term forecasting capability. Extensive experimental results on eight real-world datasets have demonstrated that CometNet significantly outperforms current state-of-the-art (SOTA) methods, particularly on extended forecast horizons.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs</title>
<link>https://arxiv.org/abs/2511.07484</link>
<guid>https://arxiv.org/abs/2511.07484</guid>
<content:encoded><![CDATA[
<div> Framework, Counterfactual user behavior forecasting, Structural causal models, Transformer-based generative AI, Causal graphs 

Summary: 
This study introduces a new framework for counterfactual user behavior forecasting by combining structural causal models with transformer-based generative artificial intelligence. The framework utilizes causal graphs to map connections between user interactions, adoption metrics, and product features, generating realistic behavioral trajectories under counterfactual conditions using generative models conditioned on causal variables. Tested on various datasets, including web interactions and e-commerce, the framework outperforms traditional forecasting methods and uplift modeling techniques. It offers improved interpretability through causal path visualization, allowing product teams to simulate and evaluate potential interventions before implementation. <div>
arXiv:2511.07484v1 Announce Type: cross 
Abstract: This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that map the connections between user interactions, adoption metrics, and product features. The framework generates realistic behavioral trajectories under counterfactual conditions by using generative models that are conditioned on causal variables. Tested on datasets from web interactions, mobile applications, and e-commerce, the methodology outperforms conventional forecasting and uplift modeling techniques. Product teams can effectively simulate and assess possible interventions prior to deployment thanks to the framework improved interpretability through causal path visualization.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physical regularized Hierarchical Generative Model for Metallic Glass Structural Generation and Energy Prediction</title>
<link>https://arxiv.org/abs/2505.09977</link>
<guid>https://arxiv.org/abs/2505.09977</guid>
<content:encoded><![CDATA[
<div> Graph Variational Autoencoder, Disordered Materials, Atomic Configurations, Physics Informed Regularizers, Energy Landscape
Summary: 
Disordered materials like glasses pose challenges for accurate property predictions and structure generation due to their lack of atomic order. GlassVAE, a hierarchical graph variational autoencoder, addresses this by learning rotation, translation, and permutation invariant embeddings of atomic configurations. The structured latent space allows for efficient generation of realistic structures and exploration of the glass energy landscape. By incorporating physics informed regularizers - a radial distribution function loss for ordering and an energy regression loss for energetics - GlassVAE ensures structural realism and physical fidelity. The regularizers play a crucial role in enhancing model performance. Encoding high dimensional atomistic data into compact vectors and decoding accurate energy predictions, GlassVAE offers a fast, physics-aware approach for modeling and designing disordered materials. <br /><br />Summary: <div>
arXiv:2505.09977v2 Announce Type: replace 
Abstract: Disordered materials such as glasses, unlike crystals, lack long range atomic order and have no periodic unit cells, yielding a high dimensional configuration space with widely varying properties. The complexity not only increases computational costs for atomistic simulations but also makes it difficult for generative AI models to deliver accurate property predictions and realistic structure generation. In this work, we introduce GlassVAE, a hierarchical graph variational autoencoder that uses graph representations to learn compact, rotation, translation, and permutation invariant embeddings of atomic configurations. The resulting structured latent space not only enables efficient generation of novel, physically plausible structures but also supports exploration of the glass energy landscape. To enforce structural realism and physical fidelity, we augment GlassVAE with two physics informed regularizers, a radial distribution function (RDF) loss that captures characteristic short and medium range ordering and an energy regression loss that reflects the broad configurational energetics. Both theoretical analysis and experimental results highlight the critical impact of these regularizers. By encoding high dimensional atomistic data into a compact latent vector and decoding it into structures with accurate energy predictions, GlassVAE provides a fast, physics aware path for modeling and designing disordered materials.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Semantic Meta-Learning Framework for Long-Tail Motion Forecasting in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.06649</link>
<guid>https://arxiv.org/abs/2511.06649</guid>
<content:encoded><![CDATA[
<div> Keywords: long-tail motion forecasting, autonomous driving, SAML, semantic meta-learning, rare events <br />
Summary: 
The article introduces SAML, a Semantic-Aware Meta-Learning framework for improving motion forecasting in autonomous driving. SAML addresses the challenge of predicting rare yet critical events by defining a differentiable notion of "tailness" based on semantic properties. By incorporating intrinsic and interactive factors into a Bayesian Tail Perceiver, SAML quantifies motion rarity and uncertainty, enabling rapid adaptation to rare or evolving patterns. The framework utilizes a meta-memory adaptation module that combines a dynamic prototype memory with an MAML-based cognitive set mechanism to enhance forecasting accuracy and efficiency. Experimental results on various datasets demonstrate that SAML outperforms existing approaches in overall accuracy and achieves significant improvements in predicting top 1-5% worst-case events. The study underscores the potential of semantic meta-learning in enhancing the robustness and safety of motion forecasting in autonomous driving. <br /><br />Summary: <div>
arXiv:2511.06649v1 Announce Type: new 
Abstract: Long-tail motion forecasting is a core challenge for autonomous driving, where rare yet safety-critical events-such as abrupt maneuvers and dense multi-agent interactions-dominate real-world risk. Existing approaches struggle in these scenarios because they rely on either non-interpretable clustering or model-dependent error heuristics, providing neither a differentiable notion of "tailness" nor a mechanism for rapid adaptation. We propose SAML, a Semantic-Aware Meta-Learning framework that introduces the first differentiable definition of tailness for motion forecasting. SAML quantifies motion rarity via semantically meaningful intrinsic (kinematic, geometric, temporal) and interactive (local and global risk) properties, which are fused by a Bayesian Tail Perceiver into a continuous, uncertainty-aware Tail Index. This Tail Index drives a meta-memory adaptation module that couples a dynamic prototype memory with an MAML-based cognitive set mechanism, enabling fast adaptation to rare or evolving patterns. Experiments on nuScenes, NGSIM, and HighD show that SAML achieves state-of-the-art overall accuracy and substantial gains on top 1-5% worst-case events, while maintaining high efficiency. Our findings highlight semantic meta-learning as a pathway toward robust and safety-critical motion forecasting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction</title>
<link>https://arxiv.org/abs/2511.07014</link>
<guid>https://arxiv.org/abs/2511.07014</guid>
<content:encoded><![CDATA[
<div> Keywords: Probabilistic forecasting, multivariate financial time-series, portfolio construction, diffusion model, attention architecture <br />
<br />
Summary: 
Diffolio is a diffusion model designed for multivariate financial time-series forecasting and portfolio construction. It uses a denoising network with a hierarchical attention architecture, incorporating asset-level and market-level layers. A correlation-guided regularizer, informed by a stable estimate of the target correlation matrix, is introduced to reflect cross-sectional correlations accurately. The model leverages historical returns, asset-specific, and systematic covariates to enhance forecast and portfolio performance. Experimental results on industry portfolios show that Diffolio outperforms probabilistic forecasting baselines in accuracy and portfolio performance. Portfolios based on Diffolio's forecasts exhibit robust performance, outperforming benchmarks in terms of Sharpe ratios and growth-optimal portfolios. These findings highlight the superior performance of Diffolio in terms of statistical accuracy and economic significance. <br /> <div>
arXiv:2511.07014v1 Announce Type: new 
Abstract: Probabilistic forecasting is crucial in multivariate financial time-series for constructing efficient portfolios that account for complex cross-sectional dependencies. In this paper, we propose Diffolio, a diffusion model designed for multivariate financial time-series forecasting and portfolio construction. Diffolio employs a denoising network with a hierarchical attention architecture, comprising both asset-level and market-level layers. Furthermore, to better reflect cross-sectional correlations, we introduce a correlation-guided regularizer informed by a stable estimate of the target correlation matrix. This structure effectively extracts salient features not only from historical returns but also from asset-specific and systematic covariates, significantly enhancing the performance of forecasts and portfolios. Experimental results on the daily excess returns of 12 industry portfolios show that Diffolio outperforms various probabilistic forecasting baselines in multivariate forecasting accuracy and portfolio performance. Moreover, in portfolio experiments, portfolios constructed from Diffolio's forecasts show consistently robust performance, thereby outperforming those from benchmarks by achieving higher Sharpe ratios for the mean-variance tangency portfolio and higher certainty equivalents for the growth-optimal portfolio. These results demonstrate the superiority of our proposed Diffolio in terms of not only statistical accuracy but also economic significance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical Modeling and Error Estimation for the Thermal Dunking Problem: A Hierarchical Approach</title>
<link>https://arxiv.org/abs/2511.07138</link>
<guid>https://arxiv.org/abs/2511.07138</guid>
<content:encoded><![CDATA[
<div> thermal dunking problem, Biot number, conjugate heat transfer, lumped capacitance model, time scale separation <br />
<br />
Summary: 
The study investigates the thermal dunking problem by analyzing the temporal evolution of a solid body immersed in a fluid with a different temperature, focusing on the small Biot number regime. The research explores the coupling of Navier-Stokes and energy equations in the fluid with the heat equation in the solid through the conjugate heat transfer formulation. By reducing the coupled equations to the lumped capacitance model, the researchers derive a closed-form solution, considering time scale separation and uniform solid temperature assumptions. They also analyze the error bounds for the lumping error and propose a data-driven framework to extend empirical correlations to various geometries. The study also addresses the estimation of Biot numbers from empirical correlations and validates the results through direct numerical simulations up to Reynolds numbers of 10,000. <div>
arXiv:2511.07138v1 Announce Type: new 
Abstract: We consider the thermal dunking problem, in which a solid body is suddenly immersed in a fluid of different temperature, and study both the temporal evolution of the solid and the associated Biot number -- a non-dimensional heat transfer coefficient characterizing heat exchange across the solid-fluid interface. We focus on the small-Biot-number regime. The problem is accurately described by the conjugate heat transfer (CHT) formulation, which couples the Navier-Stokes and energy equations in the fluid with the heat equation in the solid through interfacial continuity conditions. Because full CHT simulations are computationally expensive, simplified models are often used in practice. Starting from the coupled equations, we systematically reduce the formulation to the lumped-capacitance model, a single ordinary differential equation with a closed-form solution, based on two assumptions: time scale separation and a spatially uniform solid temperature. The total modeling error is decomposed into time homogenization and lumping contributions. We derive an asymptotic error bound for the lumping error, valid for general heterogeneous solids and spatially varying heat transfer coefficients. Building on this theoretical result, we introduce a computable upper bound expressed in measurable quantities for practical evaluation. Time scale separation is analyzed theoretically and supported by physical arguments and simulations, showing that large separation yields small time homogenization errors. In practice, the Biot number must be estimated from so-called empirical correlations, which are typically limited to specific canonical geometries. We propose a data-driven framework that extends empirical correlations to a broader range of geometries through learned characteristic length scales. All results are validated by direct numerical simulations up to Reynolds numbers of 10,000.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Chain-of-Thought Summarization of Financial News for Investor Decision Support</title>
<link>https://arxiv.org/abs/2511.05508</link>
<guid>https://arxiv.org/abs/2511.05508</guid>
<content:encoded><![CDATA[
<div> Keywords: financial news, information overload, investment decisions, Chain-of-Thought framework, personalized summaries 

Summary: 
Financial advisors and investors face challenges due to information overload from financial news, leading to difficulty in making timely investment decisions. In response, a Chain-of-Thought (CoT) summarization framework has been proposed to condense financial news into concise, event-driven summaries. This framework incorporates user-specified keywords to generate personalized summaries, ensuring that only the most relevant information is highlighted. By providing personalized summaries, the CoT framework acts as an intermediary layer that assists language models in creating investor-focused narratives. This approach aims to bridge the gap between raw news and actionable insights, ultimately aiding financial professionals and investors in extracting key market signals from the noise of financial news. 

<br /><br />Summary: <div>
arXiv:2511.05508v1 Announce Type: cross 
Abstract: Financial advisors and investors struggle with information overload from financial news, where irrelevant content and noise obscure key market signals and hinder timely investment decisions. To address this, we propose a novel Chain-of-Thought (CoT) summarization framework that condenses financial news into concise, event-driven summaries. The framework integrates user-specified keywords to generate personalized outputs, ensuring that only the most relevant contexts are highlighted. These personalized summaries provide an intermediate layer that supports language models in producing investor-focused narratives, bridging the gap between raw news and actionable insights.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgriTrust: a Federated Semantic Governance Framework for Trusted Agricultural Data Sharing</title>
<link>https://arxiv.org/abs/2511.05572</link>
<guid>https://arxiv.org/abs/2511.05572</guid>
<content:encoded><![CDATA[
<div> trust, interoperability, AgriTrust, governance, ontology

Summary:
The paper addresses the "AgData Paradox" in agricultural data management, proposing a solution called AgriTrust. This framework combines a multi-stakeholder governance model with a semantic digital layer based on the AgriTrust Core Ontology. The ontology provides a shared vocabulary for tokenization, traceability, and certification, enabling interoperability across platforms. A blockchain-agnostic architecture prevents vendor lock-in. Case studies in Brazilian supply chains (coffee, soy, beef) demonstrate AgriTrust's effectiveness in ensuring provenance, automating compliance, and creating new revenue streams for data producers. Overall, AgriTrust transforms data sharing into a governed, automated process, fostering transparency, efficiency, and equity in the agricultural data economy. <br /><br />Summary: <div>
arXiv:2511.05572v1 Announce Type: cross 
Abstract: The potential of agricultural data (AgData) to drive efficiency and sustainability is stifled by the "AgData Paradox": a pervasive lack of trust and interoperability that locks data in silos, despite its recognized value. This paper introduces AgriTrust, a federated semantic governance framework designed to resolve this paradox. AgriTrust integrates a multi-stakeholder governance model, built on pillars of Data Sovereignty, Transparent Data Contracts, Equitable Value Sharing, and Regulatory Compliance, with a semantic digital layer. This layer is realized through the AgriTrust Core Ontology, a formal OWL ontology that provides a shared vocabulary for tokenization, traceability, and certification, enabling true semantic interoperability across independent platforms. A key innovation is a blockchain-agnostic, multi-provider architecture that prevents vendor lock-in. The framework's viability is demonstrated through case studies across three critical Brazilian supply chains: coffee (for EUDR compliance), soy (for mass balance), and beef (for animal tracking). The results show that AgriTrust successfully enables verifiable provenance, automates compliance, and creates new revenue streams for data producers, thereby transforming data sharing from a trust-based dilemma into a governed, automated operation. This work provides a foundational blueprint for a more transparent, efficient, and equitable agricultural data economy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Predictive Maintenance in Intelligent Manufacturing: An Integrated FNO-DAE-GNN-PPO MDP Framework</title>
<link>https://arxiv.org/abs/2511.05594</link>
<guid>https://arxiv.org/abs/2511.05594</guid>
<content:encoded><![CDATA[
<div> Keywords: Predictive maintenance, Markov Decision Process, Soft computing, Manufacturing systems, Data-driven strategies 

Summary: 
The paper proposes a novel framework for predictive maintenance in smart manufacturing systems. It integrates advanced soft computing techniques - Fourier Neural Operator (FNO), Denoising Autoencoder (DAE), Graph Neural Network (GNN), and Proximal Policy Optimisation (PPO) - to address complex challenges. The framework leverages FNOs for temporal pattern capture, DAEs for noise-resistant state embedding, and GNNs for inter-device dependencies. By using PPO, it ensures stable optimisation of long-term maintenance strategies. Experimental validation shows superior performance with cost reduction and strong convergence. The framework's industrial potential lies in reducing downtime and operating expenses through data-driven strategies. <br /><br />Summary: <div>
arXiv:2511.05594v1 Announce Type: cross 
Abstract: In the era of smart manufacturing, predictive maintenance (PdM) plays a pivotal role in improving equipment reliability and reducing operating costs. In this paper, we propose a novel Markov Decision Process (MDP) framework that integrates advanced soft computing techniques - Fourier Neural Operator (FNO), Denoising Autoencoder (DAE), Graph Neural Network (GNN), and Proximal Policy Optimisation (PPO) - to address the multidimensional challenges of predictive maintenance in complex manufacturing systems. Specifically, the proposed framework innovatively combines the powerful frequency-domain representation capability of FNOs to capture high-dimensional temporal patterns; DAEs to achieve robust, noise-resistant latent state embedding from complex non-Gaussian sensor data; and GNNs to accurately represent inter-device dependencies for coordinated system-wide maintenance decisions. Furthermore, by exploiting PPO, the framework ensures stable and efficient optimisation of long-term maintenance strategies to effectively handle uncertainty and non-stationary dynamics. Experimental validation demonstrates that the approach significantly outperforms multiple deep learning baseline models with up to 13% cost reduction, as well as strong convergence and inter-module synergy. The framework has considerable industrial potential to effectively reduce downtime and operating expenses through data-driven strategies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning solutions of parameterized stiff ODEs using Gaussian processes</title>
<link>https://arxiv.org/abs/2511.05990</link>
<guid>https://arxiv.org/abs/2511.05990</guid>
<content:encoded><![CDATA[
<div> Gaussian processes, surrogate models, stiff ordinary differential equations, reparameterization, nonstationary functions <br />
Summary:<br /> 
This article discusses the use of Gaussian processes (GPs) as surrogate models for stiff ordinary differential equations (ODEs) to study the dependence of solutions on additional parameters. GPs are effective for approximating stationary functions but struggle with nonstationary functions, common in stiff ODE solutions. To address this, the authors propose reparameterizing stiff ODE solutions based on available data to make them appear more stationary. This reparameterization process aims to improve GP performance without altering the internal GP implementation, serving as a preprocessing step. By making solutions appear more stationary, the study shows improved GP performance in various examples. Ultimately, this approach offers a practical and computationally efficient method for dealing with the complexity of stiff ODE solutions in scientific and engineering applications.
<br /> <div>
arXiv:2511.05990v1 Announce Type: cross 
Abstract: Stiff ordinary differential equations (ODEs) play an important role in many scientific and engineering applications. Often, the dependence of the solution of the ODE on additional parameters is of interest, e.g.\ when dealing with uncertainty quantification or design optimization. Directly studying this dependence can quickly become too computationally expensive, such that cheaper surrogate models approximating the solution are of interest. One popular class of surrogate models are Gaussian processes (GPs). They perform well when approximating stationary functions, functions which have a similar level of variation along any given parameter direction, however solutions to stiff ODEs are often characterized by a mixture of regions of rapid and slow variation along the time axis and when dealing with such nonstationary functions, GP performance frequently degrades drastically. We therefore aim to reparameterize stiff ODE solutions based on the available data, to make them appear more stationary and hence recover good GP performance. This approach comes with minimal computational overhead and requires no internal changes to the GP implementation, as it can be seen as a separate preprocessing step. We illustrate the achieved benefits using multiple examples.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A computational framework for evaluating an edge-integrated, multi-ramp construction model of the Great Pyramid of Giza</title>
<link>https://arxiv.org/abs/2511.06112</link>
<guid>https://arxiv.org/abs/2511.06112</guid>
<content:encoded><![CDATA[
<div> Keywords: Integrated Edge-Ramp, Khufu's pyramid, quantitative framework, ancient megastructures, finite-element analysis

Summary: 
The study introduces the Integrated Edge-Ramp (IER) model as a quantitative framework to evaluate the construction of Khufu's pyramid. Through a unified pipeline coupling parametric geometry, discrete-event logistics, and staged finite-element analysis (FEA), the IER model is shown to sustain minutescale throughput, geometric control, and zero external footprint. An adaptive multiramp strategy enables rapid dispatches with a median on-site duration of 13.8-20.6 years. The model's geometry is consistent with internal voids identified by muon imaging and predicts edge-fill signatures and corner wear. Finite-element analysis indicates that the construction process aligns with plausible limits of stress and settlement for Old Kingdom limestone. The study provides a transferable, open-data/code framework for testing construction hypotheses for ancient megastructures. <br /><br />Summary: <div>
arXiv:2511.06112v1 Announce Type: cross 
Abstract: Despite decades of study, a quantitative, integrated framework to evaluate minutescale throughput, geometric control, and a zero external footprint for Khufu's pyramid has been lacking. We test the Integrated Edge-Ramp (IER) model-a helical path formed by omitting and backfilling perimeter courses-using a unified, end-to-end pipeline coupling parametric geometry, discrete-event logistics, and staged finite-element analysis (FEA). An adaptive multiramp strategy can sustain 4-6-minute dispatches and yields a median on-site duration of 13.8-20.6 years (95% CI); including quarrying, river transport, and seasonal pauses gives 20-27 years. FEA indicates that stresses and settlements remain within plausible limits for Old Kingdom limestone under self-weight. The model's geometry is also consistent with internal voids identified by muon imaging (a hypothesis-generating result). The IER helps reconcile throughput, survey access, and zero-footprint closure, and produces falsifiable predictions (edge-fill signatures, corner wear). Our study provides a transferable, open-data/code framework for testing construction hypotheses for ancient megastructures.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assertion-Aware Test Code Summarization with Large Language Models</title>
<link>https://arxiv.org/abs/2511.06227</link>
<guid>https://arxiv.org/abs/2511.06227</guid>
<content:encoded><![CDATA[
<div> Unit tests, Java, Large Language Models, test-code summarization, code LLMs <br />
<br />
Summary: 
Large Language Models (LLMs) are explored for generating concise summaries of Java unit tests, a common challenge in poorly documented codebases. A benchmark of 91 real-world Java test cases with developer-written summaries is used to evaluate different LLMs (Codex, Codestral, DeepSeek, Qwen-Coder) and various prompt configurations. The study focuses on the impact of components like the method under test (MUT), assertion messages, and assertion semantics on the generated summaries' quality. Findings suggest that prompting with assertion semantics yields higher-quality summaries with fewer input tokens. Codex and Qwen-Coder perform well in alignment with human-written summaries, while DeepSeek lags despite lexical overlap. This research provides insights into improving test-code summarization with LLMs and offers a valuable benchmark for future studies. <div>
arXiv:2511.06227v1 Announce Type: cross 
Abstract: Unit tests often lack concise summaries that convey test intent, especially in auto-generated or poorly documented codebases. Large Language Models (LLMs) offer a promising solution, but their effectiveness depends heavily on how they are prompted. Unlike generic code summarization, test-code summarization poses distinct challenges because test methods validate expected behavior through assertions rather than im- plementing functionality. This paper presents a new benchmark of 91 real-world Java test cases paired with developer-written summaries and conducts a controlled ablation study to investigate how test code-related components-such as the method under test (MUT), assertion messages, and assertion semantics-affect the performance of LLM-generated test summaries. We evaluate four code LLMs (Codex, Codestral, DeepSeek, and Qwen-Coder) across seven prompt configurations using n-gram metrics (BLEU, ROUGE-L, METEOR), semantic similarity (BERTScore), and LLM-based evaluation. Results show that prompting with as- sertion semantics improves summary quality by an average of 0.10 points (2.3%) over full MUT context (4.45 vs. 4.35) while requiring fewer input tokens. Codex and Qwen-Coder achieve the highest alignment with human-written summaries, while DeepSeek underperforms despite high lexical overlap. The replication package is publicly available at https://doi.org/10. 5281/zenodo.17067550
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Discrete Fine-Scale Mechanical Models with Rotational Degrees of Freedom Homogenize Into a Cosserat or a Cauchy Continuum?</title>
<link>https://arxiv.org/abs/2511.06279</link>
<guid>https://arxiv.org/abs/2511.06279</guid>
<content:encoded><![CDATA[
<div> homogenization, discrete mechanical models, Cauchy-type, Cosserat-type, asymptotic expansion<br />
Summary:<br />
This article explores whether homogenization of discrete fine-scale mechanical models, such as particle or lattice models, leads to an equivalent continuum of Cauchy-type or Cosserat-type. Using asymptotic expansion homogenization, the study examines mechanical models with rotational degrees of freedom used to simulate heterogeneous solids. Results indicate that the unit cell problem is always stationary, with the only inertia term in the linear momentum balance equation at the coarse scale. The study identifies two limiting conditions - Cauchy continuum and Cosserat continuum - based on local bending stiffness and provides a heuristic combination for accurate results. Significant Cosserat character in the homogenized response is linked to high fine-scale bending stiffness, which is deemed impractical. <div>
arXiv:2511.06279v1 Announce Type: cross 
Abstract: This article answers the question of whether homogenization of discrete fine-scale mechanical models, such as particle or lattice models, gives rise to an equivalent continuum that is of Cauchy-type or Cosserat-type. The study employs the machinery of asymptotic expansion homogenization to analyze discrete mechanical models with rotational degrees of freedom commonly used to simulate the mechanical behavior of heterogeneous solids. The proposed derivation has general validity in both stationary (steady-state) and transient conditions (assuming wavelength much larger that particle size) and for arbitrary nonlinear, inelastic fine-scale constitutive equations. The results show that the unit cell problem is always stationary, and the only inertia term appears in the linear momentum balance equation at the coarse scale. Depending on the magnitude of the local bending stiffness, mathematical homogenization rigorously identifies two limiting conditions that correspond to the Cauchy continuum and the Cosserat continuum. A heuristic combination of these two limiting conditions provides very accurate results also in the transition from one limiting case to the other. Finally, the study demonstrates that cases for which the Cosserat character of the homogenized response is significant are associated with non-physically high fine-scale bending stiffness and, as such, are of no interest in practice.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A GPU-boosted high-performance multi-working condition joint analysis framework for predicting dynamics of textured axial piston pump</title>
<link>https://arxiv.org/abs/2511.06824</link>
<guid>https://arxiv.org/abs/2511.06824</guid>
<content:encoded><![CDATA[
<div> accelerate, dynamics, axial piston pump, GPU, simulation
<br />
Summary: 
The study presents a novel framework, GMAF, to accelerate the Picard iteration process for predicting the dynamics of axial piston pumps (APP) by utilizing GPU devices. The framework employs the Preconditioned Conjugate Gradient method with an Approximate Symmetric Successive Over-Relaxation preconditioner. By utilizing GPU for parallel computation, GMAF efficiently analyzes the dynamics of both smooth and textured APPs, handling multiple periods of simulation. The study reveals that the oil force and moment in axial direction and circumferential direction respond directly to the input pressure, while other components evolve in sinusoidal patterns. The pressure field shows 'steps' corresponding to textures, indicating promotion of pressure capacity and torsion resistance due to textured surfaces. The synchronized convergence strategy in the PCG solver enhances global convergence, providing a more accurate simulation of APP dynamics. <div>
arXiv:2511.06824v1 Announce Type: cross 
Abstract: Accurate simulation to dynamics of axial piston pump (APP) is essential for its design, manufacture and maintenance. However, limited by computation capacity of CPU device and traditional solvers, conventional iteration methods are inefficient in complicated case with textured surface requiring refined mesh, and could not handle simulation during multiple periods. To accelerate Picard iteration for predicting dynamics of APP, a GPU-boosted high-performance Multi-working condition joint Analysis Framework (GMAF) is designed, which adopts Preconditioned Conjugate Gradient method (PCG) using Approximate Symmetric Successive Over-Relaxation preconditioner (ASSOR). GMAF abundantly utilizes GPU device via elevating computational intensity and expanding scale of massive parallel computation. Therefore, it possesses novel performance in analyzing dynamics of both smooth and textured APPs during multiple periods, as the establishment and solution to joint algebraic system for pressure field are accelerated magnificently, as well as numerical integral for force and moment due to oil flow. Compared with asynchronized convergence strategy pursuing local convergence, synchronized convergence strategy targeting global convergence is adopted in PCG solver for the joint system. Revealed by corresponding results, oil force in axial direction and moment in circumferential directly respond to input pressure, while other components evolve in sinusoidal patterns. Specifically, force and moment due to normal pressure instantly reach their steady state initially, while ones due to viscous shear stress evolve during periods. After simulating dynamics of APP and pressure distribution via GMAF, the promotion of pressure capacity and torsion resistance due to textured surface is revealed numerically, as several 'steps' exist in the pressure field corresponding to textures.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and Dual-Channel Reasoning</title>
<link>https://arxiv.org/abs/2511.07166</link>
<guid>https://arxiv.org/abs/2511.07166</guid>
<content:encoded><![CDATA[
<div> few-shot learning, adaptive recommendation, large language model, personalized profiling, cross-task adaptation <br />
<br />
Summary: AdaRec is a few-shot in-context learning framework for personalized recommendations that utilizes large language models. It incorporates narrative profiling to transform user-item interactions into natural language representations, enhancing task handling and readability. The framework employs a bivariate reasoning paradigm with a dual-channel architecture that combines horizontal behavioral alignment and vertical causal attribution to identify user preferences. AdaRec eliminates manual feature engineering through semantic representations and supports rapid cross-task adaptation with minimal supervision. Experimental results on ecommerce datasets show that AdaRec outperforms traditional machine learning models and LLM-based baselines in few-shot settings by up to eight percent and in zero-shot scenarios by up to nineteen percent. Additionally, lightweight fine-tuning on synthetic data generated by AdaRec achieves performance comparable to fully fine-tuned models, demonstrating efficiency and generalization across various tasks. <div>
arXiv:2511.07166v1 Announce Type: cross 
Abstract: We propose AdaRec, a few-shot in-context learning framework that leverages large language models for an adaptive personalized recommendation. AdaRec introduces narrative profiling, transforming user-item interactions into natural language representations to enable unified task handling and enhance human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a dual-channel architecture that integrates horizontal behavioral alignment, discovering peer-driven patterns, with vertical causal attribution, highlighting decisive factors behind user preferences. Unlike existing LLM-based approaches, AdaRec eliminates manual feature engineering through semantic representations and supports rapid cross-task adaptation with minimal supervision. Experiments on real ecommerce datasets demonstrate that AdaRec outperforms both machine learning models and LLM-based baselines by up to eight percent in few-shot settings. In zero-shot scenarios, it achieves up to a nineteen percent improvement over expert-crafted profiling, showing effectiveness for long-tail personalization with minimal interaction data. Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec matches the performance of fully fine-tuned models, highlighting its efficiency and generalization across diverse tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning</title>
<link>https://arxiv.org/abs/2511.07262</link>
<guid>https://arxiv.org/abs/2511.07262</guid>
<content:encoded><![CDATA[
<div> AI agents, structured reasoning, SciML architectures, collaborative, method memory <br />
<br />
Summary: The article introduces AgenticSciML, a collaborative multi-agent system comprising over 10 specialized AI agents that aim to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. By integrating structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, the framework enables the agents to generate and evaluate new hypotheses on architectures and optimization procedures. In tasks related to physics-informed learning and operator learning, the framework discovers methods that surpass single-agent and human-designed baselines, achieving up to four orders of magnitude in error reduction. The AI agents develop innovative strategies such as adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models that were not explicitly present in the knowledge base. These results demonstrate the potential of collaborative reasoning among AI agents in yielding emergent methodological innovation and hint at scalable, transparent, and autonomous discovery in scientific computing. <br /> <div>
arXiv:2511.07262v1 Announce Type: cross 
Abstract: Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here we introduce AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated machine learning for physics-informed convolutional neural networks</title>
<link>https://arxiv.org/abs/2407.06151</link>
<guid>https://arxiv.org/abs/2407.06151</guid>
<content:encoded><![CDATA[
<div> physics-informed neural networks, deep learning, convolutional neural networks, automated machine learning, partial differential equations
Summary:
This study focuses on advancing the field of deep learning for solving partial differential equations (PDEs) by introducing physics-informed convolutional neural networks (PICNNs). Current PICNNs rely on manual design, which can be ineffective for solving diverse physical problems. To address this limitation, the researchers propose using automated machine learning (AutoML) to automatically search for optimal network architectures and loss functions for PICNNs. They introduce novel search spaces for these components and develop a two-stage search strategy. The first stage focuses on identifying factors and residual adjustment operations that influence the loss function, while the second stage aims to find the best CNN architecture. Experimental results demonstrate that the automatic searching method outperforms manually-designed models on multiple datasets. Such advancements in automated machine learning for PICNNs have the potential to enhance generalization and efficiency in solving PDEs. 
<br /><br />Summary: <div>
arXiv:2407.06151v2 Announce Type: replace 
Abstract: Recent advances in deep learning for solving partial differential equations (PDEs) have introduced physics-informed neural networks (PINNs), which integrate machine learning with physical laws. Physics-informed convolutional neural networks (PICNNs) extend PINNs by leveraging CNNs for enhanced generalization and efficiency. However, current PICNNs depend on manual design, and inappropriate designs may not effectively solve PDEs. Furthermore, due to the diversity of physical problems, the ideal network architectures and loss functions vary across different PDEs. It is impractical to find the optimal PICNN architecture and loss function for each specific physical problem through extensive manual experimentation. To surmount these challenges, this paper uses automated machine learning (AutoML) to automatically and efficiently search for the loss functions and network architectures of PICNNs. We introduce novel search spaces for loss functions and network architectures and propose a two-stage search strategy. The first stage focuses on searching for factors and residual adjustment operations that influence the loss function, while the second stage aims to find the best CNN architecture. Experimental results show that our automatic searching method significantly outperforms the manually-designed model on multiple datasets.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards personalised assessment of abdominal aortic aneurysm structural integrity</title>
<link>https://arxiv.org/abs/2502.09905</link>
<guid>https://arxiv.org/abs/2502.09905</guid>
<content:encoded><![CDATA[
<div> Keywords: Abdominal aortic aneurysm, biomechanical assessment, 4D-CTA, wall strain, wall tension

Summary: 
The study focuses on a new method for assessing the structural integrity of abdominal aortic aneurysms (AAAs) using non-invasive techniques. By analyzing wall strain and wall tension through 4D-CTA images and blood pressure data, the Relative Structural Integrity Index (RSII) is calculated to quantify local wall stiffness. Results from twenty patients show consistent RSII values across individuals, indicating the reliability of the method. Furthermore, the study reveals that AAA walls have higher stiffness compared to healthy aortic walls, with localized low-stiffness zones primarily present in the most dilated regions. This approach provides a promising way to evaluate AAA biomechanics without the need for invasive procedures, offering potential for improved risk assessment and treatment strategies. 

<br /><br />Summary: <div>
arXiv:2502.09905v3 Announce Type: replace 
Abstract: Abdominal aortic aneurysm (AAA) is a life-threatening condition characterized by the progressive dilation of the aorta, which can lead to rupture if undetected or untreated. Stress-based rupture risk estimation using computational biomechanics has been widely studied; however, it requires wall strength data that cannot be measured in humans in vivo. To overcome this limitation, the goal of this study is to present a new method for biomechanical assessment of AAA via simultaneous consideration of tension and strain in AAA wall. We present a patient-specific, non-invasive method for assessing the structural integrity of the AAA wall using only time-resolved 3D computed tomography angiography (4D-CTA) images and blood pressure data. The proposed approach integrates wall strain (throughout the cardiac cycle) and wall tension analysis to compute a novel index, the Relative Structural Integrity Index (RSII), which quantifies local wall stiffness independently of wall thickness, wall material properties, and blood pressure measurement conditions. We applied our method to twenty patients from three different hospitals to extract visual RSII maps over the AAA wall of each individual patient and to compare the RSII values between aneurysmal and non-aneurysmal aortas in one patient. Our results primarily show similar RSII values across all patients, indicating the consistency of the method. Additionally, we observed patterns consistent with experimental findings reported in the literature: AAA walls exhibited higher stiffness than healthy aortic walls, while localized low-stiffness zones in the AAA wall were predominantly found in the most dilated regions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing Expert Opinion from Social Media for Stock Prediction</title>
<link>https://arxiv.org/abs/2504.10078</link>
<guid>https://arxiv.org/abs/2504.10078</guid>
<content:encoded><![CDATA[
<div> Algorithm, social media, expert tracing, stock prediction, neural network

Summary:
This article introduces a novel approach to stock prediction using social media sentiment data from platforms like StockTwits. The proposed dynamic expert tracing algorithm filters out noise from social media posts to identify true and inverse experts whose predictions can serve as valuable trading signals. This approach outperforms existing methods in stock trend prediction but faces the challenge of signal sparsity. To address this, a dual graph attention neural network is proposed to propagate expert signals across related stocks, increasing signal coverage and predicting return ratios accurately. Empirical results show that the combined signals from the expert-based approach and traditional financial features outperform baseline models in various metrics, leading to more robust investment strategies. This research aims to inspire further exploration of social media data for enhancing quantitative investment strategies. The code for the proposed method can be accessed on GitHub. 

<br /><br />Summary: <div>
arXiv:2504.10078v2 Announce Type: replace 
Abstract: While stock prediction task traditionally relies on volume-price and fundamental data to predict the return ratio or price movement trend, sentiment factors derived from social media platforms such as StockTwits offer a complementary and useful source of real-time market information. However, we find that most social media posts, along with the public sentiment they reflect, provide limited value for trading predictions due to their noisy nature. To tackle this, we propose a novel dynamic expert tracing algorithm that filters out non-informative posts and identifies both true and inverse experts whose consistent predictions can serve as valuable trading signals. Our approach achieves significant improvements over existing expert identification methods in stock trend prediction. However, when using binary expert predictions to predict the return ratio, similar to all other expert identification methods, our approach faces a common challenge of signal sparsity with expert signals cover only about 4% of all stock-day combinations in our dataset. To address this challenge, we propose a dual graph attention neural network that effectively propagates expert signals across related stocks, enabling accurate prediction of return ratios and significantly increasing signal coverage. Empirical results show that our propagated expert-based signals not only exhibit strong predictive power independently but also work synergistically with traditional financial features. These combined signals significantly outperform representative baseline models in all quant-related metrics including predictive accuracy, return metrics, and correlation metrics, resulting in more robust investment strategies. We hope this work inspires further research into leveraging social media data for enhancing quantitative investment strategies. The code can be seen in https://github.com/wanyunzh/DualGAT.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Doubly Stochastic Transformers</title>
<link>https://arxiv.org/abs/2504.16275</link>
<guid>https://arxiv.org/abs/2504.16275</guid>
<content:encoded><![CDATA[
<div> doubly stochastic Transformer, quantum circuit, self-attention layer, variational quantum circuit, object recognition tasks <br />
Summary:<br />
The study introduces a novel hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that incorporates a variational quantum circuit in place of the softmax in the self-attention layer. The research explores the expressive capabilities of the quantum circuit, demonstrating that it generates more diverse doubly stochastic matrices (DSMs) that effectively preserve information compared to classical operators. Through experiments on small-scale object recognition tasks, the QDSFormer consistently outperforms a standard Vision Transformer (ViT) and other doubly stochastic Transformers, including a quantum-inspired model based on QR decomposition. Additionally, the QDSFormer exhibits improved training stability and reduced performance variance, potentially addressing the issue of unstable training observed in ViTs when working with limited data. <div>
arXiv:2504.16275v2 Announce Type: replace-cross 
Abstract: At the core of the Transformer, the softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often de-stabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard ViT and other doubly stochastic Transformers. Beyond the Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. Our QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Connectomics Informed by Large Language Models</title>
<link>https://arxiv.org/abs/2511.05383</link>
<guid>https://arxiv.org/abs/2511.05383</guid>
<content:encoded><![CDATA[
<div> Tractography, white matter connections, prior knowledge, large language models, connectomics <br />
<br />Summary:
Tractography, a method for mapping white matter connections in the brain, faces accuracy limitations due to a trade-off between sensitivity and specificity. This study develops a pipeline using large language models (LLMs) to generate quantitative priors for connectomics based on neuroanatomy knowledge. By benchmarking against a gold-standard tractography atlas, accurate connectivity information is obtained from the LLMs using specific prompting techniques. Incorporating external knowledge sources enhances accuracy by grounding the LLM. The LLM-derived priors augment existing tractography filtering approaches by identifying true-positive connections for retention, improving the accuracy of a connectome-based model of pathology spread. This study demonstrates that the connections retained by the LLM are valid, providing valuable insights for improving tractography algorithms and connectomics research. <br /> <div>
arXiv:2511.05383v1 Announce Type: new 
Abstract: Tractography is a unique method for mapping white matter connections in the brain, but tractography algorithms suffer from an inherent trade-off between sensitivity and specificity that limits accuracy. Incorporating prior knowledge of white matter anatomy is an effective strategy for improving accuracy and has been successful for reducing false positives and false negatives in bundle-mapping protocols. However, it is challenging to scale this approach for connectomics due to the difficulty in synthesising information relating to many thousands of possible connections. In this work, we develop and evaluate a pipeline using large language models (LLMs) to generate quantitative priors for connectomics, based on their knowledge of neuroanatomy. We benchmark our approach against an evaluation set derived from a gold-standard tractography atlas, identifying prompting techniques to elicit accurate connectivity information from the LLMs. We further identify strategies for incorporating external knowledge sources into the pipeline, which can provide grounding for the LLM and improve accuracy. Finally, we demonstrate how the LLM-derived priors can augment existing tractography filtering approaches by identifying true-positive connections to retain during the filtering process. We show that these additional connections can improve the accuracy of a connectome-based model of pathology spread, which provides supporting evidence that the connections preserved by the LLM are valid.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block-structured Operator Inference for coupled multiphysics model reduction</title>
<link>https://arxiv.org/abs/2511.05389</link>
<guid>https://arxiv.org/abs/2511.05389</guid>
<content:encoded><![CDATA[
<div> Operator Inference, Structured Reduced-Order Models, Multiphysics Systems, Block-Structured Formulation, Aeroelastic Analysis<br />
<br />Summary: This paper introduces a block-structured approach to Operator Inference for learning structured reduced-order models in multiphysics systems. By specifying the governing equation and coupling term structures for each physics component, it preserves the physical system structure while reducing the overall dimensionality of the learning problem. The method allows tailored regularization for each physics component and maintains system properties like stability and second-order structure. In aeroelastic analysis, the block-structured formulation outperforms monolithic Operator Inference by providing a 20% online prediction speedup for the AGARD 445.6 wing case study. It retains accuracy while improving computational efficiency, making it a promising approach for modeling complex multiphysics systems. <br /><br /> <div>
arXiv:2511.05389v1 Announce Type: new 
Abstract: This paper presents a block-structured formulation of Operator Inference as a way to learn structured reduced-order models for multiphysics systems. The approach specifies the governing equation structure for each physics component and the structure of the coupling terms. Once the multiphysics structure is specified, the reduced-order model is learned from snapshot data following the nonintrusive Operator Inference methodology. In addition to preserving physical system structure, which in turn permits preservation of system properties such as stability and second-order structure, the block-structured approach has the advantages of reducing the overall dimensionality of the learning problem and admitting tailored regularization for each physics component. The numerical advantages of the block-structured formulation over a monolithic Operator Inference formulation are demonstrated for aeroelastic analysis, which couples aerodynamic and structural models. For the benchmark test case of the AGARD 445.6 wing, block-structured Operator Inference provides an average 20% online prediction speedup over monolithic Operator Inference across subsonic and supersonic flow conditions in both the stable and fluttering parameter regimes while preserving the accuracy achieved with monolithic Operator Inference.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Alignment between Visual Stimuli and Neural Responses in the Visual Cortex</title>
<link>https://arxiv.org/abs/2511.04096</link>
<guid>https://arxiv.org/abs/2511.04096</guid>
<content:encoded><![CDATA[
<div> Keywords: visual stimuli, neural responses, visual cortex, discriminative encoding, discriminative decoding

Summary: 
This paper explores the mapping between visual stimuli and neural responses in the visual cortex through discriminative encoding and decoding tasks, introducing a novel approach called Visual-Neural Alignment (VNA). The study shifts from traditional direct encoding and decoding methods to address challenges related to neural response variability and recording limitations. Experiments conducted on invasive visual cortex datasets involving mice and macaques demonstrate that VNA outperforms direct encoding and decoding approaches. The results indicate that VNA offers a more precise characterization of the visual-neural mapping, highlighting its potential for advancing understanding of biological visual processing mechanisms. The proposed approach provides a valuable contribution to the field of visual neuroscience research by offering a more robust and generalized methodology for studying the relationship between visual stimuli and neural responses in the visual cortex. 

<br /><br />Summary: <div>
arXiv:2511.04096v1 Announce Type: new 
Abstract: Investigating the mapping between visual stimuli and neural responses in the visual cortex contributes to a deeper understanding of biological visual processing mechanisms. Most existing studies characterize this mapping by training models to directly encode visual stimuli into neural responses or decode neural responses into visual stimuli. However, due to neural response variability and limited neural recording techniques, these studies suffer from overfitting and lack generalizability. Motivated by this challenge, in this paper we shift the tasks from conventional direct encoding and decoding to discriminative encoding and decoding, which are more reasonable. And on top of this we propose a cross-modal alignment approach, named Visual-Neural Alignment (VNA). To thoroughly test the performance of the three methods (direct encoding, direct decoding, and our proposed VNA) on discriminative encoding and decoding tasks, we conduct extensive experiments on three invasive visual cortex datasets, involving two types of subject mammals (mice and macaques). The results demonstrate that our VNA generally outperforms direct encoding and direct decoding, indicating our VNA can most precisely characterize the above visual-neural mapping among the three methods.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fitting Reinforcement Learning Model to Behavioral Data under Bandits</title>
<link>https://arxiv.org/abs/2511.04454</link>
<guid>https://arxiv.org/abs/2511.04454</guid>
<content:encoded><![CDATA[
<div> optimization, reinforcement learning, multi-armed bandit, convexity, Python package

Summary:
The paper addresses the problem of fitting reinforcement learning (RL) models to behavioral data in multi-armed bandit environments. It introduces a mathematical optimization formulation for a wide range of RL models and analyzes their convexity properties. A novel solution method based on convex relaxation and optimization is proposed, showing comparable performance to benchmark methods with reduced computation time. The method is evaluated in simulated bandit environments and an open-source Python package is provided for easy implementation by researchers. <div>
arXiv:2511.04454v1 Announce Type: new 
Abstract: We consider the problem of fitting a reinforcement learning (RL) model to some given behavioral data under a multi-armed bandit environment. These models have received much attention in recent years for characterizing human and animal decision making behavior. We provide a generic mathematical optimization problem formulation for the fitting problem of a wide range of RL models that appear frequently in scientific research applications, followed by a detailed theoretical analysis of its convexity properties. Based on the theoretical results, we introduce a novel solution method for the fitting problem of RL models based on convex relaxation and optimization. Our method is then evaluated in several simulated bandit environments to compare with some benchmark methods that appear in the literature. Numerical results indicate that our method achieves comparable performance to the state-of-the-art, while significantly reducing computation time. We also provide an open-source Python package for our proposed method to empower researchers to apply it in the analysis of their datasets directly, without prior knowledge of convex optimization.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvSim algorithm for pre-computing airplane flight controls in limited-range autonomous missions, and demonstration via double-roll maneuver of Mirage III fighters</title>
<link>https://arxiv.org/abs/2511.03745</link>
<guid>https://arxiv.org/abs/2511.03745</guid>
<content:encoded><![CDATA[
<div> mathematical framework, flight mechanics, inverse simulation, numerical procedure, fixed-wing aircraft
Summary:
- The article presents a mathematical framework for the equations of motion (EOM) in flight mechanics with six degrees of freedom for a fixed-wing aircraft.
- The framework incorporates body, inertial, and wind axes, spherical flight path and flight angles.
- A customized version for inverse simulation flight mechanics is derived, predicting necessary flight controls to achieve a target trajectory.
- A numerical procedure for integrating the inverse simulation system in time is presented, utilizing symbolic mathematics and numerical integration techniques.
- The calculated control values enable the aircraft to achieve the desired flight trajectory specified by inertial Cartesian coordinates and Euler's roll angle. 
Summary: <div>
arXiv:2511.03745v1 Announce Type: cross 
Abstract: In this work, we start with a generic mathematical framework for the equations of motion (EOM) in flight mechanics with six degrees of freedom (6-DOF) for a general (not necessarily symmetric) fixed-wing aircraft. This mathematical framework incorporates (1) body axes (fixed in the airplane at its center of gravity), (2) inertial axes (fixed in the earth/ground at the take-off point), wind axes (aligned with the flight path/course), (3) spherical flight path angles (azimuth angle measured clockwise from the geographic north, and elevation angle measured above the horizon plane), and (4) spherical flight angles (angle of attack and sideslip angle). We then manipulate these equations of motion to derive a customized version suitable for inverse simulation flight mechanics, where a target flight trajectory is specified while a set of corresponding necessary flight controls to achieve that maneuver are predicted. We then present a numerical procedure for integrating the developed inverse simulation (InvSim) system in time; utilizing (1) symbolic mathematics, (2) explicit fourth-order Runge-Kutta (RK4) numerical integration technique, and (3) expressions based on the finite difference method (FDM); such that the four necessary control variables (engine thrust force, ailerons' deflection angle, elevators' deflection angle, and rudder's deflection angle) are computed as discrete values over the entire maneuver time, and these calculated control values enable the airplane to achieve the desired flight trajectory, which is specified by three inertial Cartesian coordinates of the airplane, in addition to the Euler's roll angle. We finally demonstrate the proposed numerical procedure of flight mechanics inverse simulation (InvSim).
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices</title>
<link>https://arxiv.org/abs/2511.03753</link>
<guid>https://arxiv.org/abs/2511.03753</guid>
<content:encoded><![CDATA[
<div> framework, privacy-preserving, electrocardiogram, federated learning, IoT
Summary:<br />
This study introduces a federated learning framework for ECG classification in IoT healthcare settings, using 2D GAF images for feature extraction with CNNs while keeping sensitive data local. The experimental validation shows high accuracy of 95.18% across diverse IoT devices. The deployment on server, laptop, and Raspberry Pi 4 demonstrates edge-cloud integration in IoT. The FL-GAF model outperforms single-client setups in accuracy and training time. Despite the added complexity, the framework maintains efficiency in resource utilization and communication overhead. This research showcases the potential of lightweight, secure AI for monitoring in IoT-based healthcare, supporting scalable and secure edge deployments in smart health systems.<br /> <div>
arXiv:2511.03753v1 Announce Type: cross 
Abstract: This study presents a federated learning (FL) framework for privacy-preserving electrocardiogram (ECG) classification in Internet of Things (IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian Angular Field (GAF) images, the proposed approach enables efficient feature extraction through Convolutional Neural Networks (CNNs) while ensuring that sensitive medical data remain local to each device. This work is among the first to experimentally validate GAF-based federated ECG classification across heterogeneous IoT devices, quantifying both performance and communication efficiency. To evaluate feasibility in realistic IoT settings, we deployed the framework across a server, a laptop, and a resource-constrained Raspberry Pi 4, reflecting edge-cloud integration in IoT ecosystems. Experimental results demonstrate that the FL-GAF model achieves a high classification accuracy of 95.18% in a multi-client setup, significantly outperforming a single-client baseline in both accuracy and training time. Despite the added computational complexity of GAF transformations, the framework maintains efficient resource utilization and communication overhead. These findings highlight the potential of lightweight, privacy-preserving AI for IoT-based healthcare monitoring, supporting scalable and secure edge deployments in smart health systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure Code Generation at Scale with Reflexion</title>
<link>https://arxiv.org/abs/2511.03898</link>
<guid>https://arxiv.org/abs/2511.03898</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, secure code generation, zero-shot baseline, reflexion prompting, security metrics

Summary: 
Large language models (LLMs) are utilized for code drafting and refactoring, but ensuring secure code is crucial. This study assesses the generation of secure code using Instruct Prime, evaluating five instruction-tuned code LLMs through a zero-shot baseline and reflexion prompting approach. Insecurity is prevalent initially, with around 25-33% of programs being insecure at baseline (t0). Weak cryptography/config-dependent bugs pose challenges, while templated vulnerabilities such as XSS and code injection are handled more effectively. Python exhibits the highest secure rates, while C and C# show the lowest. Reflexion prompting enhances security across all models, with significant improvements in the first round. Applying one to two rounds yields the most benefits in terms of Repair, Regression, and NetGain metrics. The study's findings highlight the importance of prompt design in improving code security. <div>
arXiv:2511.03898v1 Announce Type: cross 
Abstract: Large language models (LLMs) are now widely used to draft and refactor code, but code that works is not necessarily secure. We evaluate secure code generation using the Instruct Prime, which eliminated compliance-required prompts and cue contamination, and evaluate five instruction-tuned code LLMs using a zero-shot baseline and a three-round reflexion prompting approach. Security is measured using the Insecure Code Detector (ICD), and results are reported by measuring Repair, Regression, and NetGain metrics, considering the programming language and CWE family. Our findings show that insecurity remains common at the first round: roughly 25-33% of programs are insecure at a zero-shot baseline (t0 ). Weak cryptography/config-dependent bugs are the hardest to avoid while templated ones like XSS, code injection, and hard-coded secrets are handled more reliably. Python yields the highest secure rates; C and C# are the lowest, with Java, JS, PHP, and C++ in the middle. Reflexion prompting improves security for all models, improving average accuracy from 70.74% at t0 to 79.43% at t3 , with the largest gains in the first round followed by diminishing returns. The trends with Repair, Regression, and NetGain metrics show that applying one to two rounds produces most of the benefits. A replication package is available at https://doi.org/10.5281/zenodo.17065846.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating scientific discovery with the common task framework</title>
<link>https://arxiv.org/abs/2511.04001</link>
<guid>https://arxiv.org/abs/2511.04001</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, artificial intelligence, dynamic systems, common task framework, objective metrics

Summary:
Machine learning and artificial intelligence algorithms are revolutionizing the characterization and control of dynamic systems in various scientific disciplines. The need for comparative metrics to evaluate these algorithms in tasks such as forecasting, state reconstruction, generalization, and control, considering limited data scenarios and noisy measurements, is crucial. The introduction of a common task framework (CTF) for science and engineering aims to provide a standardized platform with challenge datasets that address practical objectives. This framework facilitates the comparison of diverse algorithms rapidly being developed and deployed across science and engineering domains. By leveraging the CTF, researchers and practitioners can objectively assess the performance of ML/AI algorithms and drive advancements in traditional applications and emerging areas of research. <div>
arXiv:2511.04001v1 Announce Type: cross 
Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are transforming and empowering the characterization and control of dynamic systems in the engineering, physical, and biological sciences. These emerging modeling paradigms require comparative metrics to evaluate a diverse set of scientific objectives, including forecasting, state reconstruction, generalization, and control, while also considering limited data scenarios and noisy measurements. We introduce a common task framework (CTF) for science and engineering, which features a growing collection of challenge data sets with a diverse set of practical and common objectives. The CTF is a critically enabling technology that has contributed to the rapid advance of ML/AI algorithms in traditional applications such as speech recognition, language processing, and computer vision. There is a critical need for the objective metrics of a CTF to compare the diverse algorithms being rapidly developed and deployed in practice today across science and engineering.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared Spatial Memory Through Predictive Coding</title>
<link>https://arxiv.org/abs/2511.04235</link>
<guid>https://arxiv.org/abs/2511.04235</guid>
<content:encoded><![CDATA[
<div> Framework, Multi-agent, Coordination, Spatial memory, Social representations

Summary:
The article introduces a multi-agent predictive coding framework for enhancing coordination in multi-agent systems. The framework minimizes mutual uncertainty among agents by formulating coordination as an information bottleneck objective. Agents learn when, what, and who to communicate, with a focus on self-localization using an internal spatial coding similar to grid cells. Through a hierarchical reinforcement learning policy, agents develop a bandwidth-efficient communication mechanism and artificial social place cells that encode partners' locations. The approach demonstrates remarkable resilience to bandwidth constraints on the Memory-Maze benchmark, outperforming a full-broadcast baseline. The findings offer insights into how complex social representations can emerge from a unified predictive drive, leading to improved social collective intelligence. 

<br /><br />Summary: <div>
arXiv:2511.04235v1 Announce Type: cross 
Abstract: Sharing and reconstructing a consistent spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulate coordination as the minimization of mutual uncertainty among agents. Instantiated as an information bottleneck objective, it prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners' locations: an artificial analogue of hippocampal social place cells (SPCs). These social representations are further enacted by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to social collective intelligence.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the relationship between MESP and 0/1 D-Opt and their upper bounds</title>
<link>https://arxiv.org/abs/2511.04350</link>
<guid>https://arxiv.org/abs/2511.04350</guid>
<content:encoded><![CDATA[
<div> maximum entropy sampling, 0/1 optimization, experimental design, branch-and-bound, domination results

Summary: 
This article explores the relationship between two key nonlinear 0/1 optimization problems in experimental design: maximum entropy sampling (MESP) and 0/1 D-optimality. By establishing mappings between instances of these problems, the study investigates how upper-bounding methods can be transferred between the two, leading to the discovery of new domination results and inequalities. The research also delves into the comparison of different branch-and-bound schemes based on these mappings. Surprisingly, numerical experiments reveal that bounding methods previously deemed ineffective for real-data MESP instances can prove useful when applied to MESP instances derived from 0/1 D-Optimality. This study sheds light on the interconnected nature of these optimization problems and demonstrates the potential for cross-fertilization of techniques in the field of experimental design. 

<br /><br />Summary: <div>
arXiv:2511.04350v1 Announce Type: cross 
Abstract: We establish strong connections between two fundamental nonlinear 0/1 optimization problems coming from the area of experimental design, namely maximum entropy sampling and 0/1 D-Optimality. The connections are based on maps between instances, and we analyze the behavior of these maps. Using these maps, we transport basic upper-bounding methods between these two problems, and we are able to establish new domination results and other inequalities relating various basic upper bounds. Further, we establish results relating how different branch-and-bound schemes based on these maps compare. Additionally, we observe some surprising numerical results, where bounding methods that did not seem promising in their direct application to real-data MESP instances, are now useful for MESP instances that come from 0/1 D-Optimality.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition</title>
<link>https://arxiv.org/abs/2511.04461</link>
<guid>https://arxiv.org/abs/2511.04461</guid>
<content:encoded><![CDATA[
<div> ensemble, Hankel Dynamic Mode Decomposition, uncertainty-aware, seakeeping predictions, high-speed catamaran

Summary:
The study introduces Hankel Dynamic Mode Decomposition with control (HDMDc) for seakeeping predictions of a high-speed catamaran by creating a linear reduced-order model. Experimental wave basin test data from the Delft 372 model were used to train and test the HDMDc algorithm. Two ensembling strategies, Bayesian HDMDc (BHDMDc) and Frequentist HDMDc (FHDMDc), were compared for seakeeping prediction and uncertainty quantification. FHDMDc showed improved prediction accuracy and robust uncertainty estimation compared to the deterministic model. However, BHDMDc did not provide significant benefits in this case. FHDMDc-derived probability density functions closely matched experimental and computational results, demonstrating reliable seakeeping prediction for design and operational support. The study showcases the effectiveness of HDMDc in capturing nonlinear and memory effects for uncertainty-aware seakeeping predictions. 

<br /><br />Summary: <div>
arXiv:2511.04461v1 Announce Type: cross 
Abstract: In this study, we present and validate an ensemble-based Hankel Dynamic Mode Decomposition with control (HDMDc) for uncertainty-aware seakeeping predictions of a high-speed catamaran, namely the Delft 372 model. Experimental measurements (time histories) of wave elevation at the longitudinal center of gravity, heave, pitch, notional flight-deck velocity, notional bridge acceleration, and total resistance were collected from irregular wave basin tests on a 1:33.3 scale replica of the Delft 372 model under sea state 5 conditions at Fr = 0.425, and organized into training, validation, and test sets. The HDMDc algorithm constructs an equation-free linear reduced-order model of the seakeeping vessel by augmenting states and inputs with their time-lagged copies to capture nonlinear and memory effects. Two ensembling strategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparameters considered stochastic variables with prior distribution to produce posterior mean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), which aggregates multiple model obtained over data subsets, are compared in providing seakeeping prediction and uncertainty quantification. The FHDMDc approach is found to improve the accuracy of the predictions compared to the deterministic counterpart, also providing robust uncertainty estimation; whereas the application of BHDMDc to the present test case is not found beneficial in comparison to the deterministic model. FHDMDc-derived probability density functions for the motions closely match both experimental data and URANS results, demonstrating reliable and computationally efficient seakeeping prediction for design and operational support.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach</title>
<link>https://arxiv.org/abs/2511.04556</link>
<guid>https://arxiv.org/abs/2511.04556</guid>
<content:encoded><![CDATA[
<div> optimization, urban surface water flooding, sensor placement, peak flowrates, data-driven

Summary:
The study presents a data-driven sparse sensing (DSS) framework integrated with EPA-SWMM to optimize sensor placement and reconstruct peak flowrates in urban drainage networks. Using the Woodland Avenue catchment in Duluth, Minnesota, as a case study, the framework leverages singular value decomposition and QR factorization to identify optimal monitoring nodes based on a SWMM-generated training dataset. Three optimally placed sensors among 77 nodes achieved satisfactory peak flowrate reconstruction performance. The model exhibited robustness to uncertainty in measurements and sensor failures, improving with an increased number of sensors. Balancing computational efficiency and physical interpretability, the DSS framework enables high-accuracy flow reconstruction with minimal sensors. Integration of this framework with predictive models can facilitate flood early warning and real-time control in urban areas with limited sensing and monitoring resources.<br /><br />Summary: <div>
arXiv:2511.04556v1 Announce Type: cross 
Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming drainage systems, is increasingly frequent and widespread. While flood prediction and monitoring in high spatial-temporal resolution are desired, practical constraints in time, budget, and technology hinder its full implementation. How to monitor urban drainage networks and predict flow conditions under constrained resource is a major challenge. This study presents a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to optimize sensor placement and reconstruct peak flowrates in a stormwater system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case study. We utilized a SWMM model to generate a training dataset of peak flowrate profiles across the stormwater network. Furthermore, we applied DSS - leveraging singular value decomposition for dimensionality reduction and QR factorization for sensor allocation - to identify the optimal monitoring nodes based on the simulated training dataset. We then validated the representativeness of these identified monitoring nodes by comparing the DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three optimally placed sensors among 77 nodes achieved satisfactory reconstruction performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to 75th percentiles). In addition, the model showed good robustness to uncertainty in measurements. Its robustness to sensor failures is location-dependent and improves with the number of sensors deployed. The framework balances computational efficiency and physical interpretability, enabling high-accuracy flow reconstruction with minimal sensors. This DSS framework can be further integrated with predictive models to realize flood early warning and real-time control under limited sensing and monitoring resource.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Electron-Scale Turbulence Modeling in W7-X</title>
<link>https://arxiv.org/abs/2511.04567</link>
<guid>https://arxiv.org/abs/2511.04567</guid>
<content:encoded><![CDATA[
<div> machine-learning, reduced models, turbulent transport, ETG turbulence, W7-X stellarator

Summary:
- This paper presents machine-learning-driven reduced models for Electron Temperature Gradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator.
- The models predict ETG heat flux based on three plasma parameters: $\omega_{T_e}$, $\eta_e$, and $\tau.
- The models are constructed using regression and an active machine-learning-based procedure across seven radial locations.
- Evaluation using out-of-sample datasets shows robust performance with prediction intervals estimated via bootstrapping.
- Generalized reduced models are also developed and demonstrate accurate heat flux predictions even beyond the training domain.<br /><br />Summary: <div>
arXiv:2511.04567v1 Announce Type: cross 
Abstract: Constructing reduced models for turbulent transport is essential for accelerating profile predictions and enabling many-query tasks such as uncertainty quantification, parameter scans, and design optimization. This paper presents machine-learning-driven reduced models for Electron Temperature Gradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator. Each model predicts the ETG heat flux as a function of three plasma parameters: the normalized electron temperature radial gradient ($\omega_{T_e}$), the ratio of normalized electron temperature and density radial gradients ($\eta_e$), and the electron-to-ion temperature ratio ($\tau$). We first construct models across seven radial locations using regression and an active machine-learning-based procedure. This process initializes models using low-cardinality sparse-grid training data and then iteratively refines their training sets by selecting the most informative points from a pre-existing simulation database. We evaluate the prediction capabilities of our models using out-of-sample datasets with over $393$ points per location, and $95\%$ prediction intervals are estimated via bootstrapping to assess prediction uncertainty. We then investigate the construction of generalized reduced models, including a generic, position-independent model, and assess their heat flux prediction capabilities at three additional locations. Our models demonstrate robust performance and predictive accuracy comparable to the original reference simulations, even when applied beyond the training domain.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic causal discovery in Alzheimer's disease through latent pseudotime modelling</title>
<link>https://arxiv.org/abs/2511.04619</link>
<guid>https://arxiv.org/abs/2511.04619</guid>
<content:encoded><![CDATA[
<div> Keywords: causal discovery, Alzheimer's disease, latent variable model, pseudotime, dynamic interactions

Summary:
Causal discovery methods face limitations when applied to diseases like Alzheimer's due to static graph assumptions. A proposed solution involves using a latent variable model on real-world AD data to infer pseudotime, ordering patients along a disease trajectory independent of age. Pseudotime outperformed age in predicting diagnosis, demonstrating the model's efficacy. Incorporating minimal background knowledge improved graph accuracy and orientation. The framework unveiled dynamic interactions between novel and established AD markers, enabling practical causal discovery despite assumptions being violated. This study sheds light on the evolving pathophysiology of AD and provides valuable insights for future research and potential clinical applications. 

<br /><br />Summary: <div>
arXiv:2511.04619v1 Announce Type: cross 
Abstract: The application of causal discovery to diseases like Alzheimer's (AD) is limited by the static graph assumptions of most methods; such models cannot account for an evolving pathophysiology, modulated by a latent disease pseudotime. We propose to apply an existing latent variable model to real-world AD data, inferring a pseudotime that orders patients along a data-driven disease trajectory independent of chronological age, then learning how causal relationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC 0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge substantially improved graph accuracy and orientation. Our framework reveals dynamic interactions between novel (NfL, GFAP) and established AD markers, enabling practical causal discovery despite violated assumptions.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaboration Dynamics and Reliability Challenges of Multi-Agent LLM Systems in Finite Element Analysis</title>
<link>https://arxiv.org/abs/2408.13406</link>
<guid>https://arxiv.org/abs/2408.13406</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, multi-agent systems, Finite Element Analysis, collaboration effectiveness, verification reliability<br />
Summary:<br />
- The study evaluates the impact of inter-agent dynamics on reasoning quality and verification reliability in Large Language Model-based multi-agent systems for Finite Element Analysis.<br />
- Collaboration effectiveness is found to be more dependent on functional complementarity than team size.<br />
- Three-agent configurations like Coder-Executor-Critic showed the best performance in producing physically and visually correct solutions, while redundant reviewers reduced success rates.<br />
- Systematic failure modes like affirmation bias, premature consensus, and a verification-validation gap were identified in the study.<br />
- Design principles proposed include assigning complementary agent roles, enforcing multi-level validation, and preventing early consensus through interaction controls. <br /> <div>
arXiv:2408.13406v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based multi-agent systems are increasingly applied to automate computational workflows in science and engineering. However, how inter-agent dynamics influence reasoning quality and verification reliability remains unclear. We study these mechanisms using an AutoGen-based multi-agent framework for linear-elastic Finite Element Analysis (FEA), evaluating seven role configurations across four tasks under a fixed 12-turn conversation limit. From 1,120 controlled trials, we find that collaboration effectiveness depends more on functional complementarity than team size: the three-agent Coder-Executor-Critic configuration uniquely produced physically and visually correct solutions, while adding redundant reviewers reduced success rates. Yet three systematic failure modes persist: (1) affirmation bias, where the Rebuttal agent endorsed rather than challenged outputs (85-92% agreement, including errors); (2) premature consensus caused by redundant reviewers; and (3) a verification-validation gap where executable but physically incorrect code passed undetected. No agent combination successfully validated constitutive relations in complex tasks. Building on theories of functional diversity, role differentiation, and computational validation, we propose actionable design principles: (i) assign complementary agent roles, (ii) enforce multi-level validation (execution, specification, physics), and (iii) prevent early consensus through adversarial or trigger-based interaction control. These findings establish a principled foundation for designing trustworthy LLM collaborations in engineering workflows.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Workday's Approach to Secure and Compliant Cloud ERP Systems</title>
<link>https://arxiv.org/abs/2511.02856</link>
<guid>https://arxiv.org/abs/2511.02856</guid>
<content:encoded><![CDATA[
<div> GDPR, SOC 2, HIPAA, ISO 27001, FedRAMP <br />
Summary:<br />
The paper discusses Workday's compliance with global standards such as GDPR, SOC 2, HIPAA, ISO 27001, and FedRAMP, highlighting its ability to protect critical financial, healthcare, and government data. Automated compliance attributes like audit trails and behavioral analytics enhance risk management and operational flexibility, while reducing manual effort. The paper also explores the use of AI, ML, and blockchain technologies for enhanced attack detection and data integrity. Overall, Workday emerges as a secure, compliant, and future-ready ERP solution. The integration of emerging technologies like AI, machine learning, and blockchain further strengthens threat detection capabilities, positioning Workday as a reliable choice for secure enterprise cloud management. <br /> <div>
arXiv:2511.02856v1 Announce Type: new 
Abstract: Workday's compliance with global standards -- such as GDPR, SOC 2, HIPAA, ISO 27001, and FedRAMP -- shows its ability to best protect critical financial, healthcare, and government data.Automated compliance attributes like audit trails, behavioral analytics, and continuous reporting improve automation of the process and cut down on the manual effort to audit. A comparative review demonstrates enhanced risk management, operational flexibility, and breach mitigation. The paper also discusses potential future solutions with AI, ML and blockchain, to enhance attackdetection and data integrity. Overall, Workday turns out to be a secure, compliant and future-ready ERP solution. The paper also explores emerging trends, including the integration of AI, machine learning, and blockchain technologies to enhance next-generation threat detection and data integrity. The findings position Workday as a reliable, compliant, and future-ready ERP solution, setting a new benchmark for secure enterprise cloud management.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Conditional Diffusion Model for Building Energy Modeling Workflows</title>
<link>https://arxiv.org/abs/2511.02930</link>
<guid>https://arxiv.org/abs/2511.02930</guid>
<content:encoded><![CDATA[
<div> generate modeling, energy consumption behavior, urban energy models, building attributes, conditional diffusion<br />
<br />
Summary: 
This study focuses on understanding energy consumption behavior in communities by using generative modeling to fill in missing building characteristics for urban energy models. The researchers develop a tabular diffusion-based framework to handle heterogeneous building data and create a conditional diffusion model for imputing missing attributes. They validate the model by comparing generated distributions with real data and conducting a case study in a Baltimore residential region. The results demonstrate the potential of generative modeling to enhance building energy modeling workflows. <div>
arXiv:2511.02930v1 Announce Type: new 
Abstract: Understanding current energy consumption behavior in communities is critical for informing future energy use decisions and enabling efficient energy management. Urban energy models, which are used to simulate these energy use patterns, require large datasets with detailed building characteristics for accurate outcomes. However, such detailed characteristics at the individual building level are often unknown and costly to acquire, or unavailable. Through this work, we propose using a generative modeling approach to generate realistic building attributes to fill in the data gaps and finally provide complete characteristics as inputs to energy models. Our model learns complex, building-level patterns from training on a large-scale residential building stock model containing 2.2 million buildings. We employ a tabular diffusion-based framework that is designed to handle heterogeneous (discrete and continuous) features in tabular building data, such as occupancy, floor area, heating, cooling, and other equipment details. We develop a capability for conditional diffusion, enabling the imputation of missing building characteristics conditioned on known attributes. We conduct a comprehensive validation of our conditional diffusion model, firstly by comparing the generated conditional distributions against the underlying data distribution, and secondly, by performing a case study for a Baltimore residential region, showing the practical utility of our approach. Our work is one of the first to demonstrate the potential of generative modeling to accelerate building energy modeling workflows.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A physics-augmented neural network framework for finite strain incompressible viscoelasticity</title>
<link>https://arxiv.org/abs/2511.02959</link>
<guid>https://arxiv.org/abs/2511.02959</guid>
<content:encoded><![CDATA[
<div> Keywords: physics-augmented neural network, finite strain incompressible viscoelasticity, generalized standard materials theory, thermodynamic consistency, internal variables

Summary: 
A new framework, Physics-Augmented Neural Network (PANN), is proposed for modeling finite strain incompressible viscoelasticity within the Generalized Standard Materials Theory. The formulation of PANN involves the decomposition of the deformation gradient and ensures unimodularity of the inelastic deformation part. The neural networks representing the free energy and dual dissipation potential are constructed to be thermodynamically consistent, objective, and material symmetric. The evolution of internal variables is handled using an implicit exponential time integrator during training. A trainable gate layer with lp regularization automatically determines the number of internal variables needed. The model is calibrated using synthetic and experimental data, showing excellent agreement across various deformation rates and load paths, achieving both interpolation and extrapolation accuracy. Furthermore, the PANN demonstrates consistency with linear viscoelasticity through linearization of the full model. <br /><br />Summary: <div>
arXiv:2511.02959v1 Announce Type: new 
Abstract: We propose a physics-augmented neural network (PANN) framework for finite strain incompressible viscoelasticity within the generalized standard materials theory. The formulation is based on the multiplicative decomposition of the deformation gradient and enforces unimodularity of the inelastic deformation part throughout the evolution. Invariant-based representations of the free energy and the dual dissipation potential by monotonic and fully input-convex neural networks ensure thermodynamic consistency, objectivity, and material symmetry by construction. The evolution of the internal variables during training is handled by solving the evolution equations using an implicit exponential time integrator. In addition, a trainable gate layer combined with lp regularization automatically identifies the required number of internal variables during training. The PANN is calibrated with synthetic and experimental data, showing excellent agreement for a wide range of deformation rates and different load paths. We also show that the proposed model achieves excellent interpolation as well as plausible and accurate extrapolation behaviors. In addition, we demonstrate consistency of the PANN with linear viscoelasticity by linearization of the full model.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid DeepONet Surrogates for Multiphase Flow in Porous Media</title>
<link>https://arxiv.org/abs/2511.02962</link>
<guid>https://arxiv.org/abs/2511.02962</guid>
<content:encoded><![CDATA[
<div> surrogate models, deep learning, porous media, multiphase flow, PDE solutions<br />
<br />
Summary: <br />
The article discusses the development of hybrid neural operator surrogates for solving partial differential equations (PDEs) in complex porous media flows. These surrogates combine Fourier Neural Operators, Multi-Layer Perceptrons (MLPs), and Kolmogorov-Arnold Networks (KANs) to address challenges such as high memory requirements and handling the time dimension. The framework splits spatial and temporal learning tasks into branch and trunk networks, respectively, leading to accurate surrogate modeling with fewer parameters. The hybrid models are evaluated on various multiphase flow problems, showing strong predictive performance on large-scale reservoir simulations. This approach offers a promising solution for efficiently solving complex PDEs in real-world applications involving multiphase flow in porous media. <br /> <div>
arXiv:2511.02962v1 Announce Type: new 
Abstract: The solution of partial differential equations (PDEs) plays a central role in numerous applications in science and engineering, particularly those involving multiphase flow in porous media. Complex, nonlinear systems govern these problems and are notoriously computationally intensive, especially in real-world applications and reservoirs. Recent advances in deep learning have spurred the development of data-driven surrogate models that approximate PDE solutions with reduced computational cost. Among these, Neural Operators such as Fourier Neural Operator (FNO) and Deep Operator Networks (DeepONet) have shown strong potential for learning parameter-to-solution mappings, enabling the generalization across families of PDEs. However, both methods face challenges when applied independently to complex porous media flows, including high memory requirements and difficulty handling the time dimension. To address these limitations, this work introduces hybrid neural operator surrogates based on DeepONet models that integrate Fourier Neural Operators, Multi-Layer Perceptrons (MLPs), and Kolmogorov-Arnold Networks (KANs) within their branch and trunk networks. The proposed framework decouples spatial and temporal learning tasks by splitting these structures into the branch and trunk networks, respectively. We evaluate these hybrid models on multiphase flow in porous media problems ranging in complexity from the steady 2D Darcy flow to the 2D and 3D problems belonging to the $10$th Comparative Solution Project from the Society of Petroleum Engineers. Results demonstrate that hybrid schemes achieve accurate surrogate modeling with significantly fewer parameters while maintaining strong predictive performance on large-scale reservoir simulations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes</title>
<link>https://arxiv.org/abs/2511.03170</link>
<guid>https://arxiv.org/abs/2511.03170</guid>
<content:encoded><![CDATA[
<div> Keywords: quantitative structure-activity relationship, activity cliffs, machine learning models, molecular graph structures, GraphCliff <br />
Summary: 
Quantitative structure-activity relationship (QSAR) is based on the assumption of a smooth relationship between molecular structure and biological activity, but activity cliffs, where structurally similar compounds have large potency differences, challenge this notion. Recent benchmarks have shown that traditional machine learning models with extended connectivity fingerprints outperform graph neural networks in addressing activity cliffs. Graph embeddings fail to effectively differentiate between structurally similar molecules, limiting their utility. To address this, a new model called GraphCliff is proposed, which integrates short- and long-range information through a gating mechanism. Experimental results demonstrate GraphCliff's ability to improve performance on both cliff and non-cliff compounds. Layer-wise node embedding analyses show reduced over-smoothing and enhanced discriminative power compared to baseline models. This approach preserves the expressive power of molecular graph structures while addressing the limitations of existing methods. <br /> <br />Summary: <div>
arXiv:2511.03170v1 Announce Type: new 
Abstract: Quantitative structure-activity relationship assumes a smooth relationship between molecular structure and biological activity. However, activity cliffs defined as pairs of structurally similar compounds with large potency differences break this continuity. Recent benchmarks targeting activity cliffs have revealed that classical machine learning models with extended connectivity fingerprints outperform graph neural networks. Our analysis shows that graph embeddings fail to adequately separate structurally similar molecules in the embedding space, making it difficult to distinguish between structurally similar but functionally different molecules. Despite this limitation, molecular graph structures are inherently expressive and attractive, as they preserve molecular topology. To preserve the structural representation of molecules as graphs, we propose a new model, GraphCliff, which integrates short- and long-range information through a gating mechanism. Experimental results demonstrate that GraphCliff consistently improves performance on both non-cliff and cliff compounds. Furthermore, layer-wise node embedding analyses reveal reduced over-smoothing and enhanced discriminative power relative to strong baseline graph models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Framework for Environmental Similarity and Vessel Mobility as Coupled Predictors of Marine Invasive Species Pathways</title>
<link>https://arxiv.org/abs/2511.03499</link>
<guid>https://arxiv.org/abs/2511.03499</guid>
<content:encoded><![CDATA[
<div> Keywords: marine invasive species, global shipping, risk assessment, maritime mobility, climate-based feature representations

Summary: 
This study introduces a novel theoretical framework for assessing invasion risk posed by marine invasive species spread through global shipping. By combining environmental similarity across ports with observed and forecasted maritime mobility, the framework offers a comprehensive approach to estimate exposure levels at both the port and voyage levels. Climate-based feature representations capture marine conditions at each port, while mobility networks derived from Automatic Identification System data track vessel flows and potential transfer pathways. Clustering and metric learning techniques identify climate analogues and assess species survival likelihood along shipping routes. A temporal link prediction model anticipates changes in traffic patterns under evolving environmental conditions. This integrated approach enables targeted monitoring, routing adjustments, and management interventions to mitigate the ecological and economic impacts of marine invasive species. 

<br /><br />Summary: <div>
arXiv:2511.03499v1 Announce Type: new 
Abstract: Marine invasive species spread through global shipping and generate substantial ecological and economic impacts. Traditional risk assessments require detailed records of ballast water and traffic patterns, which are often incomplete, limiting global coverage. This work advances a theoretical framework that quantifies invasion risk by combining environmental similarity across ports with observed and forecasted maritime mobility. Climate-based feature representations characterize each port's marine conditions, while mobility networks derived from Automatic Identification System data capture vessel flows and potential transfer pathways. Clustering and metric learning reveal climate analogues and enable the estimation of species survival likelihood along shipping routes. A temporal link prediction model captures how traffic patterns may change under shifting environmental conditions. The resulting fusion of environmental similarity and predicted mobility provides exposure estimates at the port and voyage levels, supporting targeted monitoring, routing adjustments, and management interventions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation-Based Validation of an Integrated 4D/5D Digital-Twin Framework for Predictive Construction Control</title>
<link>https://arxiv.org/abs/2511.03684</link>
<guid>https://arxiv.org/abs/2511.03684</guid>
<content:encoded><![CDATA[
<div> Keywords: construction industry, 4D/5D digital twin framework, AI-based analytics, probabilistic CPM, deep reinforcement learning

Summary:
This study introduces an integrated 4D/5D digital twin framework that combines Building Information Modeling (BIM) with natural-language processing (NLP), computer-vision progress measurement, Bayesian probabilistic CPM updating, and deep-reinforcement learning resource-leveling. The framework was implemented on a mid-rise project in Dallas-Fort Worth, resulting in significant improvements in accuracy and efficiency. The project saw reductions in estimating labor and overtime, as well as improved project-buffer utilization, while maintaining an on-time finish. The digital twin sandbox allowed for real-time "what-if" forecasting and traceable cost-schedule alignment through a 5D knowledge graph. The findings confirm that integrating AI-based analytics with probabilistic CPM and deep reinforcement learning enhances forecasting precision, transparency, and control resilience. This validated workflow paves the way for predictive, adaptive, and auditable construction management. 

<br /><br />Summary: <div>
arXiv:2511.03684v1 Announce Type: new 
Abstract: Persistent cost and schedule deviations remain a major challenge in the U.S. construction industry, revealing the limitations of deterministic CPM and static document-based estimating. This study presents an integrated 4D/5D digital-twin framework that couples Building Information Modeling (BIM) with natural-language processing (NLP)-based cost mapping, computer-vision (CV)-driven progress measurement, Bayesian probabilistic CPM updating, and deep-reinforcement-learning (DRL) resource-leveling. A nine-month case implementation on a Dallas-Fort Worth mid-rise project demonstrated measurable gains in accuracy and efficiency: 43% reduction in estimating labor, 6% reduction in overtime, and 30% project-buffer utilization, while maintaining an on-time finish at 128 days within P50-P80 confidence bounds. The digital-twin sandbox also enabled real-time "what-if" forecasting and traceable cost-schedule alignment through a 5D knowledge graph. Findings confirm that integrating AI-based analytics with probabilistic CPM and DRL enhances forecasting precision, transparency, and control resilience. The validated workflow establishes a practical pathway toward predictive, adaptive, and auditable construction management.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Region Matrix Interpolation for Dynamic Analysis of Aperiodic Structures under Large Model Parameter Perturbations</title>
<link>https://arxiv.org/abs/2511.03711</link>
<guid>https://arxiv.org/abs/2511.03711</guid>
<content:encoded><![CDATA[
<div> surrogate-based model, dynamic mechanical metamaterials, frequency response, parametric perturbations, interpolation <br />
<br />Summary: This work introduces a surrogate-based model for estimating the frequency response of dynamic mechanical metamaterials efficiently. It addresses limitations of existing methods by providing insight into common modal projection restrictions, proposing a sampling-based procedure for identifying usable parameter space boundaries, and enhancing the surrogate model with a multi-region interpolation strategy. The research demonstrates the effectiveness of the framework through two examples: validating the approach for a single well-conditioned projection region with a unit cell structure and showcasing the advantage of the multi-region approach with a beam-like structure. The proposed method maintains high accuracy across different perturbation levels, unlike traditional Lagrange interpolation, which deviates significantly with increasing perturbations. <div>
arXiv:2511.03711v1 Announce Type: new 
Abstract: This work introduces a surrogate-based model for efficiently estimating the frequency response of dynamic mechanical metamaterials, particularly when dealing with large parametric perturbations and aperiodic substructures. The research builds upon a previous matrix interpolation method applied on top of a Craig-Bampton modal reduction, allowing the variations of geometrical features without the need to remesh and recompute Finite Element matrices. This existing procedure has significant limitations since it requires a common modal projection, which inherently restricts the allowable perturbation size of the model parameters, thereby limiting the model parameter space where matrices can be effectively interpolated. The present work offers three contributions: (1) It provides structural dynamic insight into the restrictions imposed by the common modal projection, demonstrating that ill-conditioning can be controlled, (2) it proposes an efficient, sampling-based procedure to identify the non-regular boundaries of the usable region in the model parameter space, and (3) it enhances the surrogate model to accommodate larger model parameter perturbations by proposing a multi-region interpolation strategy. The efficacy of this proposed framework is verified through two illustrative examples. The first example, involving a unit cell with a square plate and circular core, validates the approach for a single well-conditioned projection region. The second example, using a beam-like structure with vibration attenuation bands, demonstrates the true advantage of the multi-region approach, where predictions from traditional Lagrange interpolation deviated significantly with increasing perturbations, while the proposed method maintained high accuracy across different perturbation levels.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.02957</link>
<guid>https://arxiv.org/abs/2511.02957</guid>
<content:encoded><![CDATA[
<div> Keywords: Pavement infrastructure monitoring, Digital Twin, Graph Neural Network, predictive maintenance, proactive interventions

Summary:<br />
The article proposes a unified framework that combines Digital Twin and Graph Neural Network for monitoring pavement health and predictive maintenance. It models pavement segments and spatial relations as nodes and edges on a graph, incorporating real-time data from UAVs, sensors, and LiDAR. The Graph Neural Network learns deterioration patterns from this data, enabling proactive interventions to prevent failures. The model, trained on a realistic dataset, achieves a high R2 value, outperforming baseline regressors and capturing non-linear degradation effectively. An interactive dashboard and reinforcement learning module are developed for simulation, visualization, and adaptive maintenance planning. This integration enhances forecasting precision, establishing a closed feedback loop for continuous improvement. The approach sets the foundation for intelligent and sustainable pavement management, with potential extensions for real-world deployment, multi-agent coordination, and smart-city integration.<br /> <div>
arXiv:2511.02957v1 Announce Type: cross 
Abstract: Pavement infrastructure monitoring is challenged by complex spatial dependencies, changing environmental conditions, and non-linear deterioration across road networks. Traditional Pavement Management Systems (PMS) remain largely reactive, lacking real-time intelligence for failure prevention and optimal maintenance planning. To address this, we propose a unified Digital Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven pavement health monitoring and predictive maintenance. Pavement segments and spatial relations are modeled as graph nodes and edges, while real-time UAV, sensor, and LiDAR data stream into the DT. The inductive GNN learns deterioration patterns from graph-structured inputs to forecast distress and enable proactive interventions. Trained on a real-world-inspired dataset with segment attributes and dynamic connectivity, our model achieves an R2 of 0.3798, outperforming baseline regressors and effectively capturing non-linear degradation. We also develop an interactive dashboard and reinforcement learning module for simulation, visualization, and adaptive maintenance planning. This DT-GNN integration enhances forecasting precision and establishes a closed feedback loop for continuous improvement, positioning the approach as a foundation for proactive, intelligent, and sustainable pavement management, with future extensions toward real-world deployment, multi-agent coordination, and smart-city integration.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive-Sensorless Monitoring of Shipping Containers</title>
<link>https://arxiv.org/abs/2511.03022</link>
<guid>https://arxiv.org/abs/2511.03022</guid>
<content:encoded><![CDATA[
<div> sensorless monitoring, machine learning, residual correction, adaptive-sensorless models, cargo transportation <br />
<br />
Summary: 
The paper introduces the concept of adaptive-sensorless monitoring for shipping containers, which utilizes machine learning models to predict internal conditions while incorporating live telemetry data to correct for systematic errors. By training and evaluating these adaptive-sensorless models on a large dataset of 3.48 million sensor readings, the study demonstrates consistent improvements over traditional sensorless models. When tested on a holdout set, the adaptive-sensorless models achieve lower mean absolute errors and root mean-squared errors for temperature and relative humidity monitoring. These models offer more accurate cargo monitoring, early risk detection, and reduce the reliance on full connectivity in global shipping, making them a valuable tool for enhancing the quality and efficiency of cargo transportation. <br /> <div>
arXiv:2511.03022v1 Announce Type: cross 
Abstract: Monitoring the internal temperature and humidity of shipping containers is essential to preventing quality degradation during cargo transportation. Sensorless monitoring -- machine learning models that predict the internal conditions of the containers using exogenous factors -- shows promise as an alternative to monitoring using sensors. However, it does not incorporate telemetry information and correct for systematic errors, causing the predictions to differ significantly from the live data and confusing the users. In this paper, we introduce the residual correction method, a general framework for correcting for systematic biases in sensorless models after observing live telemetry data. We call this class of models ``adaptive-sensorless'' monitoring. We train and evaluate adaptive-sensorless models on the 3.48 million data points -- the largest dataset of container sensor readings ever used in academic research -- and show that they produce consistent improvements over the baseline sensorless models. When evaluated on the holdout set of the simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$ 2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$ 7.09% for relative humidity (vs 7.99% by sensorless) and average root mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs 3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs 10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and less dependence on full connectivity in global shipping.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tsallis-Entropy Lens on Genetic Variation</title>
<link>https://arxiv.org/abs/2511.03063</link>
<guid>https://arxiv.org/abs/2511.03063</guid>
<content:encoded><![CDATA[
<div> Tsallis-order q F-statistic, fixation statistic, variance-based fixation index, mutual information, allele-frequency spectra<br />
<br />
Summary: <br />
The article introduces a new statistic, the Tsallis-order q F-statistic, Fq, which measures the fraction of Tsallis q-entropy lost within subpopulations. This statistic generalizes the fixation statistic and provides a more fine-grained view of population differentiation compared to traditional methods. By varying q, Fq can emphasize rare or common variants, offering a spectral differentiation of population structure. The study, conducted on Oceanian genomes and simulated data from diverse populations, demonstrates that Fq in One-vs-Rest and Leave-One-Out modes can pinpoint influential subpopulations and reveal isolation-migration events and founder effects. Fq serves as a valuable tool for simulation audits and summarizing population structure with higher resolution. <div>
arXiv:2511.03063v1 Announce Type: cross 
Abstract: We introduce an information-theoretic generalization of the fixation statistic, the Tsallis-order $q$ F-statistic, $F_q$, which measures the fraction of Tsallis $q$-entropy lost within subpopulations relative to the pooled population. The family nests the classical variance-based fixation index $F_{\textbf{ST}}$ at $q{=}2$ and a Shannon-entropy analogue at $q{=}1$, whose absolute form equals the mutual information between alleles and population labels. By varying $q$, $F_q$ acts as a spectral differentiator that up-weights rare variants at low $q$, while $q{>}1$ increasingly emphasizes common variants, providing a more fine-grained view of differentiation than $F_{\textbf{ST}}$ when allele-frequency spectra are skewed. On real data (865 Oceanian genomes with 1,823,000 sites) and controlled genealogical simulations (seeded from 1,432 founders from HGDP and 1000 Genomes panels, with 322,216 sites), we show that $F_q$ in One-vs-Rest (OVR) and Leave-One-Out (LOO) modes provides clear attribution of which subpopulations drive regional structure, and sensitively timestamps isolation-migration events and founder effects. $F_q$ serves as finer-resolution complement for simulation audits and population-structure summaries.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>System Identification of a Moored ASV with Recessed Moon Pool via Deterministic and Bayesian Hankel-DMDc</title>
<link>https://arxiv.org/abs/2511.03482</link>
<guid>https://arxiv.org/abs/2511.03482</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous surface vehicle, system identification, dynamic mode decomposition, Bayesian extension, mooring loads

Summary:
This study focuses on system identification of a small autonomous surface vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were conducted on a Codevintec CK-14e ASV in varying wave conditions, including irregular and regular head-sea waves. The ASV has a recessed moon pool, leading to nonlinear responses due to sloshing. Data-driven reduced-order models were constructed based on measurements of vessel motions and mooring loads. The HDMDc framework provided accurate deterministic predictions of the vessel's dynamics, while the Bayesian formulation accounted for uncertainty in model response. Validation against experimental data demonstrated the models' capability to predict the vessel's response to both regular and irregular wave excitations. Overall, HDMDc-based ROMs offer a reliable data-driven approach for system identification, showcasing their generalization capability and accuracy in reproducing vessel dynamics in different sea conditions. 

<br /><br />Summary: <div>
arXiv:2511.03482v1 Announce Type: cross 
Abstract: This study addresses the system identification of a small autonomous surface vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were carried out on a Codevintec CK-14e ASV in the towing tank of CNR-INM, under both irregular and regular head-sea wave conditions. The ASV under investigation features a recessed moon pool, which induces nonlinear responses due to sloshing, thereby increasing the modelling challenge. Data-driven reduced-order models were built from measurements of vessel motions and mooring loads. The HDMDc framework provided accurate deterministic predictions of vessel dynamics, while the Bayesian formulation enabled uncertainty-aware characterization of the model response by accounting for variability in hyperparameter selection. Validation against experimental data demonstrated that both HDMDc and BHDMDc can predict the vessel's response to unseen regular and irregular wave excitations. In conclusion, the study shows that HDMDc-based ROMs are a viable data-driven alternative for system identification, demonstrating for the first time their generalization capability for a sea condition different from the training set, achieving high accuracy in reproducing vessel dynamics.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveTradeBench: Seeking Real-World Alpha with Large Language Models</title>
<link>https://arxiv.org/abs/2511.03628</link>
<guid>https://arxiv.org/abs/2511.03628</guid>
<content:encoded><![CDATA[
<div> LiveTradeBench, large language models, evaluation, trading, real-time uncertainty <br />
Summary:<br />
1. The study introduces LiveTradeBench, a live trading environment for evaluating Large Language Models (LLMs) in real and evolving markets, focusing on decision-making under uncertainty.
2. LiveTradeBench follows design principles including live data streaming of market prices and news, multi-asset portfolio management, and multi-market evaluation across different environments.
3. Results from 50-day live evaluations of 21 LLMs show that high LMArena scores do not guarantee superior trading outcomes; models exhibit distinct portfolio styles based on risk appetite and decision-making dynamics.
4. Some LLMs effectively adapt decisions using live signals, highlighting the need for benchmarks that assess sequential decision-making and consistency under live uncertainty.
5. The findings emphasize the disparity between static evaluations and real-world trading competence, underscoring the importance of evaluating models in dynamic and uncertain environments. <br /> <div>
arXiv:2511.03628v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Gene Trees without more data</title>
<link>https://arxiv.org/abs/2511.03692</link>
<guid>https://arxiv.org/abs/2511.03692</guid>
<content:encoded><![CDATA[
<div> Keywords: gene tree estimation, species tree estimation, incomplete lineage sorting, Statistical Binning, BestML

Summary: 
The study addresses the challenges of estimating species and gene trees from sequence data. Gene tree estimation is often inaccurate due to low phylogenetic signal in alignments, while species tree estimation is complicated by incomplete lineage sorting (ILS). Existing methods like MP-EST, ASTRAL2, and ASTRID suffer from low gene tree accuracy. The study proposes a novel pipeline, WSB+WQMC, which improves gene tree estimation and species tree accuracy under the GTR+MSC model. Evaluations using BestML analysis on simulated datasets showed that WSB+WQMC significantly enhances gene tree and species tree accuracy, particularly in the presence of low to high ILS levels. While WSB+WQMC may perform slightly less accurately than WSB+CAML under certain conditions, it excels in estimating gene trees and species trees in datasets with moderately high and high ILS levels. Overall, WSB+WQMC shows promise as a reliable alternative for phylogenetic estimation, especially when dealing with low phylogenetic signal. 

<br /><br />Summary: <div>
arXiv:2511.03692v1 Announce Type: cross 
Abstract: Estimating species and gene trees from sequence data is challenging. Gene tree estimation is often hampered by low phylogenetic signal in alignments, leading to inaccurate trees. Species tree estimation is complicated by incomplete lineage sorting (ILS), where gene histories differ from the species' history. Summary methods like MP-EST, ASTRAL2, and ASTRID infer species trees from gene trees but suffer when gene tree accuracy is low. To address this, the Statistical Binning (SB) and Weighted Statistical Binning (WSB) pipelines were developed to improve gene tree estimation. However, previous studies only tested these pipelines using multi-locus bootstrapping (MLBS), not the BestML approach.
  This thesis proposes a novel pipeline, WSB+WQMC, which shares design features with the existing WSB+CAML pipeline but has other desirable properties and is statistically consistent under the GTR+MSC model. This study evaluated WSB+WQMC against WSB+CAML using BestML analysis on various simulated datasets. The results confirmed many trends seen in prior MLBS analyses. WSB+WQMC substantially improved gene tree and species tree accuracy (using ASTRAL2 and ASTRID) on most datasets with low, medium, and moderately high ILS levels. In a direct comparison, WSB+WQMC computed less accurate trees than WSB+CAML under certain low and medium ILS conditions. However, WSB+WQMC performed better or at least as accurately as WSB+CAML on all datasets with moderately high and high ILS. It also proved better for estimating gene trees on some medium and low ILS datasets. Thus, WSB+WQMC is a promising alternative to WSB+CAML for phylogenetic estimation, especially in the presence of low phylogenetic signal.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChemFM as a Scaling Law Guided Foundation Model Pre-trained on Informative Chemicals</title>
<link>https://arxiv.org/abs/2410.21422</link>
<guid>https://arxiv.org/abs/2410.21422</guid>
<content:encoded><![CDATA[
<div> ChemFM, foundation model, chemicals, scalability, generalization <br />
Summary:<br />
ChemFM is introduced as a large foundation model for chemicals, pre-trained on 178 million molecules using self-supervised causal language modeling. It outperforms task-specific AI models in various chemical tasks, showing up to 67.48% performance improvement in property prediction and 3.7% top-1 accuracy improvement in reaction prediction. ChemFM also excels in predicting antibiotic activity and cytotoxicity, showcasing its potential in discovering novel antibiotics. Additionally, it exhibits strong data efficiency, requiring fewer labeled training samples to achieve state-of-the-art performance. The model's scalability and generalizability make it a valuable tool for advancing chemistry research across a wide range of tasks with minimal additional training. <br />Summary: <div>
arXiv:2410.21422v3 Announce Type: replace 
Abstract: Traditional AI methods often rely on task-specific model designs and training, which constrain both the scalability of model size and generalization across different tasks. Here, we introduce ChemFM, a large foundation model specifically developed for chemicals. By conducting a series of scaling experiments, we identify UniChem as the informative molecular database for pre-training the foundation model. ChemFM comprises 3 billion parameters and is pre-trained on 178 million molecules using self-supervised causal language modeling to extract generalizable molecular representations. This model can be adapted to diverse downstream chemical applications using either full-parameter or parameter-efficient fine-tuning methods. ChemFM consistently outperforms state-of-the-art task-specific AI models across all tested tasks. Notably, it achieves up to 67.48% performance improvement across 34 property prediction benchmarks, up to 33.80% reduction in mean average deviation between conditioned and actual properties of generated molecules in conditional molecular generation tasks, and up to 3.7% top-1 accuracy improvement across 4 reaction prediction datasets. Moreover, ChemFM demonstrates its superior performance in predicting antibiotic activity and cytotoxicity, highlighting its potential to advance the discovery of novel antibiotics. Furthermore, we demonstrate that, as a foundation model, ChemFM exhibits strong data efficiency, requiring significantly fewer labeled training samples to achieve state-of-the-art performance. We anticipate that ChemFM will significantly advance chemistry research by providing a foundation model capable of effectively generalizing across a broad range of tasks with minimal additional training.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-driven framework for team selection in Fantasy Premier League</title>
<link>https://arxiv.org/abs/2505.02170</link>
<guid>https://arxiv.org/abs/2505.02170</guid>
<content:encoded><![CDATA[
<div> Fantasy football, optimization, linear programming, data-driven, Premier League
<br />
Summary: This study presents a data-driven approach to optimize fantasy football lineup selection in the Fantasy Premier League (FPL). Using deterministic and robust mixed-integer linear programs, the study formulates the selection of starting eleven, bench players, and captain under various constraints. An objective function combining realized FPL points with predictions from a regression model is used, with benchmarks including ARIMA, weighted averages, and Monte Carlo simulation. Experiments on the 2023/24 Premier League season show that ARIMA with a constrained budget performs consistently well. The framework also extends to other FPL strategies like multi-week transfer planning and dynamic captaincy decisions. <div>
arXiv:2505.02170v2 Announce Type: replace 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Under a fixed budget, managers select squads to maximize future Fantasy Premier League (FPL) points. This study formulates lineup selection as data-driven optimization and develops deterministic and robust mixed-integer linear programs that choose the starting eleven, bench, and captain under budget, formation, and club-quota constraints (maximum three players per club). The objective is parameterized by a hybrid scoring metric that combines realized FPL points with predictions from a linear regression model trained on match-performance features identified using exploratory data analysis techniques. The study benchmarks alternative objectives and cost estimators, including simple and recency-weighted averages, exponential smoothing, autoregressive integrated moving average (ARIMA), and Monte Carlo simulation. Experiments on the 2023/24 Premier League season show that ARIMA with a constrained budget and a rolling window yields the most consistent out-of-sample performance; weighted averages and Monte Carlo are also competitive. Robust variants improve some objectives but are not uniformly superior. The framework provides transparent decision support for fantasy roster construction and extends to FPL chips, multi-week rolling-horizon transfer planning, and week-by-week dynamic captaincy.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Fidelity Global Search Framework for Hotspot Prevention in 3D Thermal Design Space</title>
<link>https://arxiv.org/abs/2511.02211</link>
<guid>https://arxiv.org/abs/2511.02211</guid>
<content:encoded><![CDATA[
<div> Bezier-based, Multi-Fidelity, Thermal Optimization, Framework, Heat Sinks <br />
<br />
Summary: 
The article presents a Bezier-based Multi-Fidelity Thermal Optimization Framework for global optimization of 3D heat sinks. The framework utilizes flexible Bezier-parameterized fin geometries and a multi-fidelity pseudo-3D thermal modeling strategy to balance accuracy and computational cost efficiently. By representing fins using smooth Bezier curves, the design space can generate diverse topologies with minimal variables. The Covariance Matrix Adaptation Evolution Strategy is employed as a global optimizer to minimize pressure drop while respecting surface-average temperature constraints. The pseudo-3D model couples thermally interacting 2D layers representing fluid flow through fins and conductive base plates. Validation against full 3D simulations shows good agreement in temperature distribution and pressure drops but at significantly reduced computational costs. Optimization results demonstrate up to a 50% reduction in pressure loss compared to conventional fin configurations, indicating a trade-off between thermal performance and hydraulic efficiency. The framework enables fast, geometry-flexible, and optimized heat sink design, facilitating efficient exploration of complex geometries. <br /> <div>
arXiv:2511.02211v1 Announce Type: new 
Abstract: We present a B\'ezier-based Multi-Fidelity Thermal Optimization Framework, which is a computationally efficient methodology for the global optimization of 3D heat sinks. The flexible B\'ezier-parameterized fin geometries and the adopted multi-fidelity pseudo-3D thermal modeling strategy meet at a balance between accuracy and computational cost. In this method, the smooth and compact B\'ezier representation of fins defines the design space from which diverse topologies can be generated with minimal design variables. A global optimizer, the Covariance Matrix Adaptation Evolution Strategy, minimizes the pressure drop with respect to a given surface-average temperature constraint to achieve improvement in the pressure loss. In the framework, the pseudo-3D model couples two thermally interacting 2D layers: a thermofluid layer representing the fluid domain passing through the fins, and a conductive base plate representing the surface where excessive average temperature is to be avoided. Both layers are coupled with calibrated heat transfer coefficients obtained from high-fidelity 3D simulations. For several fin geometries, the proposed framework has been validated by comparing the pseudo-3D results with those of full 3D simulations, which yielded good agreement in terms of temperature distribution and pressure drops when the computational cost was reduced by several orders of magnitude. Optimization results show that it attains up to 50\% pressure loss reduction compared to conventional straight-fin configurations, and it reveals a clear trade-off between thermal performance and hydraulic efficiency. Thus, the proposed method forms a new basis for fast, geometry-flexible, and optimized heat sink design, enabling efficient exploration of complex geometries.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wavelet-Optimized Motion Artifact Correction in 3D MRI Using Pre-trained 2D Score Priors</title>
<link>https://arxiv.org/abs/2511.02256</link>
<guid>https://arxiv.org/abs/2511.02256</guid>
<content:encoded><![CDATA[
<div> Keywords: MRI, motion artifacts, generative models, wavelet-optimized, image quality<br />
Summary:<br />
Motion artifacts in MRI are a significant challenge, affecting image quality and diagnosis. Existing 3D generative models struggle with known operators and slow speed. To address this, a new 3D-WMoCo framework combines 2D priors and a mean-reverting SDE for motion artifact correction. Wavelet diffusion and convolution techniques enhance speed and feature extraction. The method is validated using simulated and real clinical data, showing improved performance. The approach is available on GitHub for implementation. <div>
arXiv:2511.02256v1 Announce Type: new 
Abstract: Motion artifacts in magnetic resonance imaging (MRI) remain a major challenge, as they degrade image quality and compromise diagnostic reliability. Score-based generative models (SGMs) have recently shown promise for artifact removal. However, existing 3D SGM-based approaches are limited in two key aspects: (1) their strong dependence on known forward operators makes them ineffective for correcting MRI motion artifacts, and (2) their slow inference speed hinders clinical translation. To overcome these challenges, we propose a wavelet-optimized end-to-end framework for 3D MRI motion correct using pre-trained 2D score priors (3D-WMoCo). Specifically, two orthogonal 2D score priors are leveraged to guide the 3D distribution prior, while a mean-reverting stochastic differential equation (SDE) is employed to model the restoration process of motion-corrupted 3D volumes to motion-free 3D distribution. Furthermore, wavelet diffusion is introduced to accelerate inference, and wavelet convolution is applied to enhance feature extraction. We validate the effectiveness of our approach through both simulated motion artifact experiments and real-world clinical motion artifact correction tests. The proposed method achieves robust performance improvements over existing techniques. Implementation details and source code are available at: https://github.com/ZG-yuan/3D-WMoCo.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas</title>
<link>https://arxiv.org/abs/2511.02458</link>
<guid>https://arxiv.org/abs/2511.02458</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, macroeconomic forecasting, persona-based prompting, GPT-4o, ECB Survey

Summary:
The study evaluates the impact of persona-based prompting on the performance of the Large Language Model (LLM) in macroeconomic forecasting tasks. Using economics-related personas, GPT-4o is prompted to replicate the ECB Survey of Professional Forecasters across multiple rounds. The comparison between persona-prompted forecasts and human experts reveals similar accuracy levels, with GPT-4o maintaining competitiveness in out-of-sample forecasting for 2024-2025. Ablation experiments show that persona descriptions do not provide a measurable advantage in forecasting accuracy, suggesting they can be omitted to reduce computational costs. The results indicate that GPT-4o can achieve competitive forecasting accuracy even on unseen macroeconomic events with relevant context data, but diverse prompts lead to homogeneous forecasts compared to human panels.<br /><br />Summary: <div>
arXiv:2511.02458v1 Announce Type: cross 
Abstract: We evaluate whether persona-based prompting improves Large Language Model (LLM) performance on macroeconomic forecasting tasks. Using 2,368 economics-related personas from the PersonaHub corpus, we prompt GPT-4o to replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds (2013-2025). We compare the persona-prompted forecasts against the human experts panel, across four target variables (HICP, core HICP, GDP growth, unemployment) and four forecast horizons. We also compare the results against 100 baseline forecasts without persona descriptions to isolate its effect. We report two main findings. Firstly, GPT-4o and human forecasters achieve remarkably similar accuracy levels, with differences that are statistically significant yet practically modest. Our out-of-sample evaluation on 2024-2025 data demonstrates that GPT-4o can maintain competitive forecasting performance on unseen events, though with notable differences compared to the in-sample period. Secondly, our ablation experiment reveals no measurable forecasting advantage from persona descriptions, suggesting these prompt components can be omitted to reduce computational costs without sacrificing accuracy. Our results provide evidence that GPT-4o can achieve competitive forecasting accuracy even on out-of-sample macroeconomic events, if provided with relevant context data, while revealing that diverse prompts produce remarkably homogeneous forecasts compared to human panels.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural-gas storage modelling by deep reinforcement learning</title>
<link>https://arxiv.org/abs/2511.02646</link>
<guid>https://arxiv.org/abs/2511.02646</guid>
<content:encoded><![CDATA[
<div> simulator, GasRL, deep reinforcement learning, natural gas market, equilibrium prices <br />
Summary:
GasRL is introduced as a simulator integrating a representation of the natural gas market with storage-operator policies trained using deep reinforcement learning (RL). Soft Actor Critic (SAC) emerges as the most effective RL algorithm in achieving various storage operator objectives, including profitability and price stabilization. The equilibrium price dynamics produced by SAC-derived policies closely mirror real-world price characteristics. The simulator demonstrates the impact of EU-mandated minimum storage thresholds, highlighting their positive effect on market resilience against supply shocks. Specifically, the presence of thresholds reduces market disruptions in response to unexpectedly large shocks. Overall, GasRL provides insights into the effects of optimal stockpile management on market dynamics and the benefits of policy interventions in ensuring market stability. <br /><br />Summary: <div>
arXiv:2511.02646v1 Announce Type: cross 
Abstract: We introduce GasRL, a simulator that couples a calibrated representation of the natural gas market with a model of storage-operator policies trained with deep reinforcement learning (RL). We use it to analyse how optimal stockpile management affects equilibrium prices and the dynamics of demand and supply. We test various RL algorithms and find that Soft Actor Critic (SAC) exhibits superior performance in the GasRL environment: multiple objectives of storage operators - including profitability, robust market clearing and price stabilisation - are successfully achieved. Moreover, the equilibrium price dynamics induced by SAC-derived optimal policies have characteristics, such as volatility and seasonality, that closely match those of real-world prices. Remarkably, this adherence to the historical distribution of prices is obtained without explicitly calibrating the model to price data. We show how the simulator can be used to assess the effects of EU-mandated minimum storage thresholds. We find that such thresholds have a positive effect on market resilience against unanticipated shifts in the distribution of supply shocks. For example, with unusually large shocks, market disruptions are averted more often if a threshold is in place.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization</title>
<link>https://arxiv.org/abs/2511.02659</link>
<guid>https://arxiv.org/abs/2511.02659</guid>
<content:encoded><![CDATA[
<div> implicit neural representations, in situ training protocol, sketching, continual learning, implicit neural compression<br />
<br />
Summary: 
The article introduces a novel in situ training protocol for implicit neural representations, utilizing limited memory buffers of full and sketched data samples to prevent catastrophic forgetting. The use of sketching as a regularizer is theoretically supported by a Johnson-Lindenstrauss-informed result. The focus is on in situ neural compression with implicit neural representation-based hypernetworks. The method is evaluated on complex simulation data in two and three dimensions, showcasing strong reconstruction performance at high compression rates. The study demonstrates that sketching allows the in situ scheme to closely approximate the performance of the offline method on various tasks, including long time horizons and unstructured grids. <div>
arXiv:2511.02659v1 Announce Type: cross 
Abstract: Focusing on implicit neural representations, we present a novel in situ training protocol that employs limited memory buffers of full and sketched data samples, where the sketched data are leveraged to prevent catastrophic forgetting. The theoretical motivation for our use of sketching as a regularizer is presented via a simple Johnson-Lindenstrauss-informed result. While our methods may be of wider interest in the field of continual learning, we specifically target in situ neural compression using implicit neural representation-based hypernetworks. We evaluate our method on a variety of complex simulation data in two and three dimensions, over long time horizons, and across unstructured grids and non-Cartesian geometries. On these tasks, we show strong reconstruction performance at high compression rates. Most importantly, we demonstrate that sketching enables the presented in situ scheme to approximately match the performance of the equivalent offline method.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient GA: Gradient Genetic Algorithm for Drug Molecular Design</title>
<link>https://arxiv.org/abs/2502.09860</link>
<guid>https://arxiv.org/abs/2502.09860</guid>
<content:encoded><![CDATA[
<div> Keywords: Molecular discovery, molecule design, genetic algorithms, gradient information, neural network

Summary:
The article introduces the Gradient Genetic Algorithm (Gradient GA) as a novel approach to enhance molecular design techniques. Traditional optimization methods like genetic algorithms have limitations in random exploration, impacting solution quality and convergence speed. The Gradient GA incorporates gradient information from the objective function by utilizing a differentiable objective function parameterized with a neural network. This allows for iterative progress towards optimal solutions by following the gradient direction, instead of random exploration. The Discrete Langevin Proposal is used to enable gradient guidance in discrete molecular spaces. Experimental results show significant improvements in both convergence speed and solution quality, surpassing state-of-the-art techniques with up to a 25% enhancement in the top-10 score over the vanilla genetic algorithm. The code for the Gradient GA is openly accessible on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2502.09860v2 Announce Type: replace-cross 
Abstract: Molecular discovery has brought great benefits to the chemical industry. Various molecule design techniques are developed to identify molecules with desirable properties. Traditional optimization methods, such as genetic algorithms, continue to achieve state-of-the-art results across multiple molecular design benchmarks. However, these techniques rely solely on random walk exploration, which hinders both the quality of the final solution and the convergence speed. To address this limitation, we propose a novel approach called Gradient Genetic Algorithm (Gradient GA), which incorporates gradient information from the objective function into genetic algorithms. Instead of random exploration, each proposed sample iteratively progresses toward an optimal solution by following the gradient direction. We achieve this by designing a differentiable objective function parameterized by a neural network and utilizing the Discrete Langevin Proposal to enable gradient guidance in discrete molecular spaces. Experimental results demonstrate that our method significantly improves both convergence speed and solution quality, outperforming cutting-edge techniques. For example, it achieves up to a 25% improvement in the top-10 score over the vanilla genetic algorithm. The code is publicly available at https://github.com/debadyuti23/GradientGA.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEDICorrect: A Scalable Python Tool for Orbit-, Beam-, and Footprint-Level GEDI Geolocation Correction</title>
<link>https://arxiv.org/abs/2511.00319</link>
<guid>https://arxiv.org/abs/2511.00319</guid>
<content:encoded><![CDATA[
<div> geolocation, GEDI data, LiDAR, accuracy, framework

Summary:
- The study introduces a framework called GEDICorrect for geolocation correction of GEDI LiDAR data at orbit, beam, and footprint levels.
- GEDICorrect integrates existing GEDI Simulator modules and enhances functionality with flexible correction logic, multiple similarity metrics, adaptive clustering, and optimized I/O handling.
- By using the Kullback--Leibler divergence as a waveform similarity metric, GEDICorrect significantly improves canopy height and terrain elevation accuracy.
- Canopy height accuracy is enhanced from $R^2 = 0.61$ to 0.78 with footprint-level correction, reducing RMSE and rRMSE.
- Terrain elevation accuracy also sees improvements, with GEDICorrect reducing RMSE compared to uncorrected data and the GEDI Simulator baseline.
- The framework demonstrates computational efficiency, achieving a speedup over the GEDI Simulator and scaling efficiently to 24 cores for a significant improvement in runtime.
<br /><br />Summary: <div>
arXiv:2511.00319v1 Announce Type: new 
Abstract: Accurate geolocation is essential for the reliable use of GEDI LiDAR data in footprint-scale applications such as aboveground biomass modeling, data fusion, and ecosystem monitoring. However, residual geolocation errors arising from both systematic biases and random ISS-induced jitter can significantly affect the accuracy of derived vegetation and terrain metrics. The main goal of this study is to develop and evaluate a flexible, computationally efficient framework (GEDICorrect) that enables geolocation correction of GEDI data at the orbit, beam, and footprint levels. The framework integrates existing GEDI Simulator modules (gediRat and gediMetrics) and extends their functionality with flexible correction logic, multiple similarity metrics, adaptive footprint clustering, and optimized I/O handling. Using the Kullback--Leibler divergence as the waveform similarity metric, GEDICorrect improved canopy height (RH95) accuracy from $R^2 = 0.61$ (uncorrected) to 0.74 with the orbit-level correction, and up to $R^2 = 0.78$ with the footprint-level correction, reducing RMSE from 2.62~m ($rRMSE = 43.13\%$) to 2.12~m ($rRMSE = 34.97\%$) at the orbit level, and 2.01~m ($rRMSE = 33.05\%$) at the footprint level. Terrain elevation accuracy also improved, decreasing RMSE by 0.34~m relative to uncorrected data and by 0.37~m compared to the GEDI Simulator baseline. In terms of computational efficiency, GEDICorrect achieved a $\sim2.4\times$ speedup over the GEDI Simulator in single-process mode (reducing runtime from $\sim84$~h to $\sim35$~h) and scaled efficiently to 24 cores, completing the same task in $\sim4.3$~h -- an overall $\sim19.5\times$ improvement. GEDICorrect offers a robust and scalable solution for improving GEDI geolocation accuracy while maintaining full compatibility with standard GEDI data products.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STARC-9: A Large-scale Dataset for Multi-Class Tissue Classification for CRC Histopathology</title>
<link>https://arxiv.org/abs/2511.00383</link>
<guid>https://arxiv.org/abs/2511.00383</guid>
<content:encoded><![CDATA[
<div> Dataset, tissue classification, colorectal cancer, histopathologic images, DeepCluster++

Summary:
The article introduces STARC-9, a large-scale dataset for multi-class tissue classification in colorectal cancer (CRC) histopathologic images. The dataset consists of 630,000 image tiles across nine tissue classes from 200 CRC patients. STARC-9 was created using the DeepCluster++ framework, which ensures intra-class diversity and reduces manual curation. The process involves extracting feature vectors, clustering similar tiles, and sampling diverse morphologic patterns within each class. Expert pathologists verify the selected tiles, resulting in high-quality data. Models trained on STARC-9 show superior generalizability compared to existing datasets, demonstrating its utility in CRC tissue classification and segmentation tasks. DeepCluster++ is highlighted as a flexible framework that can be applied to construct high-quality datasets from large WSI repositories for various cancer and non-cancer applications. 

<br /><br />Summary: <div>
arXiv:2511.00383v1 Announce Type: new 
Abstract: Multi-class tissue-type classification of colorectal cancer (CRC) histopathologic images is a significant step in the development of downstream machine learning models for diagnosis and treatment planning. However, existing public CRC datasets often lack morphologic diversity, suffer from class imbalance, and contain low-quality image tiles, limiting model performance and generalizability. To address these issues, we introduce STARC-9 (STAnford coloRectal Cancer), a large-scale dataset for multi-class tissue classification. STARC-9 contains 630,000 hematoxylin and eosin-stained image tiles uniformly sampled across nine clinically relevant tissue classes (70,000 tiles per class) from 200 CRC patients at the Stanford University School of Medicine. The dataset was built using a novel framework, DeepCluster++, designed to ensure intra-class diversity and reduce manual curation. First, an encoder from a histopathology-specific autoencoder extracts feature vectors from tiles within each whole-slide image. Then, K-means clustering groups morphologically similar tiles, followed by equal-frequency binning to sample diverse morphologic patterns within each class. The selected tiles are subsequently verified by expert gastrointestinal pathologists to ensure accuracy. This semi-automated process significantly reduces manual effort while producing high-quality, diverse tiles. To evaluate STARC-9, we benchmarked convolutional neural networks, transformers, and pathology-specific foundation models on multi-class CRC tissue classification and segmentation tasks, showing superior generalizability compared to models trained on existing datasets. Although we demonstrate the utility of DeepCluster++ on CRC as a pilot use-case, it is a flexible framework that can be used for constructing high-quality datasets from large WSI repositories across a wide range of cancer and non-cancer applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeltaLag: Learning Dynamic Lead-Lag Patterns in Financial Markets</title>
<link>https://arxiv.org/abs/2511.00390</link>
<guid>https://arxiv.org/abs/2511.00390</guid>
<content:encoded><![CDATA[
<div> deep learning, lead-lag effect, financial markets, portfolio construction, predictive signals
<br />
Summary:
DeltaLag is introduced as an innovative deep learning approach for detecting and exploiting dynamic lead-lag structures in financial markets. It utilizes a sparsified cross-attention mechanism to identify relevant lead-lag pairs and extract lag-aligned raw features for predicting future returns of lagger stocks. The method outperforms traditional baselines and precomputed lead-lag graphs, demonstrating superior adaptability in dynamic market conditions. DeltaLag surpasses temporal and spatio-temporal deep learning models in stock prediction accuracy and interpretability, offering enhanced trading performance. <div>
arXiv:2511.00390v1 Announce Type: new 
Abstract: The lead-lag effect, where the price movement of one asset systematically precedes that of another, has been widely observed in financial markets and conveys valuable predictive signals for trading. However, traditional lead-lag detection methods are limited by their reliance on statistical analysis methods and by the assumption of persistent lead-lag patterns, which are often invalid in dynamic market conditions. In this paper, we propose \textbf{DeltaLag}, the first end-to-end deep learning method that discovers and exploits dynamic lead-lag structures with pair-specific lag values in financial markets for portfolio construction. Specifically, DeltaLag employs a sparsified cross-attention mechanism to identify relevant lead-lag pairs. These lead-lag signals are then leveraged to extract lag-aligned raw features from the leading stocks for predicting the lagger stock's future return. Empirical evaluations show that DeltaLag substantially outperforms both fixed-lag and self-lead-lag baselines. In addition, its adaptive mechanism for identifying lead-lag relationships consistently surpasses precomputed lead-lag graphs based on statistical methods. Furthermore, DeltaLag outperforms a wide range of temporal and spatio-temporal deep learning models designed for stock prediction or time series forecasting, offering both better trading performance and enhanced interpretability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Meta-Cognitive Swarm Intelligence Framework for Resilient UAV Navigation in GPS-Denied and Cluttered Environments</title>
<link>https://arxiv.org/abs/2511.00884</link>
<guid>https://arxiv.org/abs/2511.00884</guid>
<content:encoded><![CDATA[
<div> framework, UAV swarms, autonomous navigation, meta-cognition, Self-Learning Slime Mould Algorithm

Summary:
The paper introduces a new framework for autonomous navigation of UAV swarms in challenging environments. The framework, called Self-Learning Slime Mould Algorithm (SLSMA), incorporates meta-cognitive principles to enable real-time adaptation and recovery from planning failures. It includes a situation-aware search strategy, a collective memory mechanism, and an adaptive recovery behavior. The multi-UAV trajectory problem is formulated as a resilient planning challenge, considering path length, collisions, navigational uncertainty, and proximity to failure states. Extensive simulations demonstrate that the SLSMA outperforms existing metaheuristics in terms of mission success rate, recovery speed, and solution reliability. This work marks a crucial advancement towards the development of truly autonomous UAV swarms capable of operating persistently in complex and dynamic environments. 

Summary:<br /><br />framework: new framework for autonomous UAV swarm navigation introduced<br />UAV swarms: multiple UAVs working together autonomously<br />autonomous navigation: navigating without human intervention<br />meta-cognition: incorporating cognitive principles in algorithms<br />Self-Learning Slime Mould Algorithm: algorithm integrating meta-cognition for real-time adaptation and recovery of UAV swarms <div>
arXiv:2511.00884v1 Announce Type: new 
Abstract: Autonomous navigation of UAV swarms in perceptually-degraded environments, where GPS is unavailable and terrain is densely cluttered, presents a critical bottleneck for real-world deployment. Existing optimization-based planners lack the resilience to avoid catastrophic convergence to local optima under such uncertainty. Inspired by principles of computational meta-cognition, this paper introduces a novel swarm intelligence framework that enables a fleet of UAVs to autonomously sense, adapt, and recover from planning failures in real-time. At its core is the Self-Learning Slime Mould Algorithm (SLSMA), which integrates three meta-cognitive layers: a situation-aware search strategy that dynamically selects between exploration and exploitation based on perceived search stagnation; a collective memory mechanism that allows the swarm to learn from and avoid previously failed trajectories; and an adaptive recovery behavior that triggers global re-exploration upon entrapment. We formulate the multi-UAV trajectory problem as a resilient planning challenge, with a cost function that penalizes not only path length and collisions but also navigational uncertainty and proximity to failure states. Extensive simulations in synthetically complex 3D worlds and against the CEC 2017 benchmark suite demonstrate the framework's superior performance. The SLSMA does not merely optimize paths; it generates resilient trajectories, demonstrating a 99.5% mission success rate and significantly outperforming state-of-the-art metaheuristics in recovery speed and solution reliability. This work provides a foundational step towards truly autonomous swarms capable of persistent operation in denied and dynamic environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigation of Performance and Scalability of a Quantum-Inspired Evolutionary Optimizer (QIEO) on NVIDIA GPU</title>
<link>https://arxiv.org/abs/2511.01298</link>
<guid>https://arxiv.org/abs/2511.01298</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum Inspired Evolutionary Optimization, GPU acceleration, 01 Knapsack problems, CUDA, parallel processing

Summary:
This study explores the performance and scalability of GPU-accelerated Quantum Inspired Evolutionary Optimization (QIEO) for solving large-scale 01 Knapsack problems. By leveraging CUDA's parallel processing capabilities on NVIDIA Tesla V100 GPUs, the researchers systematically analyze various problem sizes, kernel configurations, and memory models. They find that optimizing memory strategies and kernel configurations is crucial for maximizing efficiency, with constant memory outperforming other options up to hardware limits. Beyond these limits, global memory and tiling strategies offer trade-offs in performance. The study highlights both the potential and practical constraints of implementing QIEO on GPUs for complex combinatorial optimization problems, providing valuable insights for future large-scale metaheuristic implementations. 

<br /><br />Summary: <div>
arXiv:2511.01298v1 Announce Type: new 
Abstract: Quantum inspired evolutionary optimization leverages quantum computing principles like superposition, interference, and probabilistic representation to enhance classical evolutionary algorithms with improved exploration and exploitation capabilities. Implemented on NVIDIA Tesla V100 SXM2 GPUs, this study systematically investigates the performance and scalability of a GPU-accelerated Quantum Inspired Evolutionary Optimizer applied to large scale 01 Knapsack problems. By exploiting CUDA`s parallel processing capabilities, particularly through optimized memory management and thread configuration, significant speedups and efficient utilization of GPU resources is demonstrated. The analysis covers various problem sizes, kernel launch configurations, and memory models including constant, shared, global, and pinned memory, alongside extensive scaling studies. The results reveal that careful tuning of memory strategies and kernel configurations is essential for maximizing throughput and efficiency, with constant memory providing superior performance up to hardware limits. Beyond these limits, global memory and strategic tiling become necessary, albeit with some performance trade offs. The findings highlight both the promise and the practical constraints of applying QIEO on GPUs for complex combinatorial optimization, offering actionable insights for future large scale metaheuristic implementations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSMD: Curated Multimodal Dataset for Chinese Stock Analysis</title>
<link>https://arxiv.org/abs/2511.01318</link>
<guid>https://arxiv.org/abs/2511.01318</guid>
<content:encoded><![CDATA[
<div> Keywords: stock market analysis, Chinese stock market, multimodal dataset, LightQuant framework, financial domains<br />
Summary:<br />
The article introduces a new multimodal dataset, CSMD, specifically curated for analyzing the Chinese stock market. The dataset is meticulously processed to ensure validated quality and addresses the limitations of existing resources mainly focused on the U.S. stock market in English. The authors also present a lightweight and user-friendly framework called LightQuant for researchers and practitioners with expertise in financial domains. Experimental results using various backbone models demonstrate the effectiveness of the dataset and framework compared to existing datasets. The datasets and code are publicly available on GitHub, providing a valuable resource for researchers and practitioners interested in stock market analysis in the Chinese market. <br /><br />Summary: <div>
arXiv:2511.01318v1 Announce Type: new 
Abstract: The stock market is a complex and dynamic system, where it is non-trivial for researchers and practitioners to uncover underlying patterns and forecast stock movements. The existing studies for stock market analysis rely on leveraging various types of information to extract useful factors, which are highly conditional on the quality of the data used. However, the currently available resources are mainly based on the U.S. stock market in English, which is inapplicable to adapt to other countries. To address these issues, we propose CSMD, a multimodal dataset curated specifically for analyzing the Chinese stock market with meticulous processing for validated quality. In addition, we develop a lightweight and user-friendly framework LightQuant for researchers and practitioners with expertise in financial domains. Experimental results on top of our datasets and framework with various backbone models demonstrate their effectiveness compared with using existing datasets. The datasets and code are publicly available at the link: https://github.com/ECNU-CILAB/LightQuant.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solution Space Topology Guides CMTS Search</title>
<link>https://arxiv.org/abs/2511.01701</link>
<guid>https://arxiv.org/abs/2511.01701</guid>
<content:encoded><![CDATA[
<div> Keywords: Monte Carlo Tree Search, puzzle solving, solution space topology, compatibility graphs, algebraic connectivity <br />
Summary: 
- The study focuses on determining the most effective topology to guide Monte Carlo Tree Search (MCTS) in puzzle solving tasks.
- The research identifies that grid topology, commonly used in prior work, is not suitable for guiding MCTS due to its constant nature across all instances.
- The proposed approach involves measuring solution space topology, which determines the structure of valid color assignments based on detected pattern rules.
- The automatic detection of pattern rules is achieved with 100% accuracy for five types, and compatibility graphs are constructed to encode solution space structure.
- Topological features such as algebraic connectivity play a crucial role in task difficulty and are integrated into MCTS node selection to improve search efficiency. <br /><br />Summary: <div>
arXiv:2511.01701v1 Announce Type: new 
Abstract: A fundamental question in search-guided AI: what topology should guide Monte Carlo Tree Search (MCTS) in puzzle solving? Prior work applied topological features to guide MCTS in ARC-style tasks using grid topology -- the Laplacian spectral properties of cell connectivity -- and found no benefit. We identify the root cause: grid topology is constant across all instances. We propose measuring \emph{solution space topology} instead: the structure of valid color assignments constrained by detected pattern rules. We build this via compatibility graphs where nodes are $(cell, color)$ pairs and edges represent compatible assignments under pattern constraints.
  Our method: (1) detect pattern rules automatically with 100\% accuracy on 5 types, (2) construct compatibility graphs encoding solution space structure, (3) extract topological features (algebraic connectivity, rigidity, color structure) that vary with task difficulty, (4) integrate these features into MCTS node selection via sibling-normalized scores.
  We provide formal definitions, a rigorous selection formula, and comprehensive ablations showing that algebraic connectivity is the dominant signal. The work demonstrates that topology matters for search -- but only the \emph{right} topology. For puzzle solving, this is solution space structure, not problem space structure.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Synthesizability-Guided Pipeline for Materials Discovery</title>
<link>https://arxiv.org/abs/2511.01790</link>
<guid>https://arxiv.org/abs/2511.01790</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational materials discovery, crystal structures, density functional theory, synthesizability prediction, experimental synthesis <br />
Summary: 
Computational materials discovery involves generating crystal structures judged for plausibility with density functional theory, but these structures may not be experimentally accessible. A new combined compositional and structural synthesizability score has been developed to predict which compounds can be synthesized in a laboratory. Using this score, non-synthesized structures from various databases were evaluated, leading to the identification of several highly synthesizable candidates. Predicted synthesis pathways were tested experimentally across 16 targets, resulting in successful synthesis of 7 out of 16 compounds within just three days. This process not only filled gaps in known synthesized structures but also provided valuable insights into the practical usability of current materials databases. The study underscores the importance of synthesizability prediction in accelerating materials discovery. <br /><br />Summary: <div>
arXiv:2511.01790v1 Announce Type: new 
Abstract: Computational materials discovery relies on the generation of plausible crystal structures. The plausibility is typically judged through density functional theory methods which, while typically accurate at zero Kelvin, often favor low-energy structures that are not experimentally accessible. We develop a combined compositional and structural synthesizability score which provides an accurate way of predicting which compounds can actually be synthesized in a laboratory. We use it to evaluate non-synthesized structures from the Materials Project, GNoME, and Alexandria, and identified several hundred highly synthesizable candidates. We then predict synthesis pathways, conduct corresponding experiments, and characterize the products across 16 targets, successfully synthesizing 7 of 16. The entire experimental process was completed in only three days. Our results highlight omissions in lists of known synthesized structures, deliver insights into the practical utility of current materials databases, and showcase the central role synthesizability prediction can play in materials discovery.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection</title>
<link>https://arxiv.org/abs/2511.00047</link>
<guid>https://arxiv.org/abs/2511.00047</guid>
<content:encoded><![CDATA[
<div> Graph-BERT, financial fraud detection, dynamic financial transaction analysis, DynBERG, Gated Recurrent Unit (GRU) <br />
<br />
Summary: 
The study introduces DynBERG, a novel model that combines Graph-BERT with a GRU layer to analyze dynamic financial transaction networks. The model is designed to capture temporal evolution over multiple time steps and support directed edges, making it suitable for analyzing evolving structures in financial data. Tested on the Elliptic dataset containing Bitcoin transactions, including those during the Dark Market Shutdown event, DynBERG shows superior performance compared to other dynamic graph classification models like EvolveGCN and GCN. Its ability to adapt to significant market shifts is evident, with GRU playing a crucial role in modeling the temporal dynamics of financial transactions. DynBERG outperforms EvolveGCN before the market shutdown and surpasses GCN post-event, highlighting its effectiveness in detecting financial fraud and adapting to changing market conditions. <div>
arXiv:2511.00047v1 Announce Type: cross 
Abstract: Financial fraud detection is critical for maintaining the integrity of financial systems, particularly in decentralised environments such as cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are widely used for financial fraud detection, graph Transformer models such as Graph-BERT are gaining prominence due to their Transformer-based architecture, which mitigates issues such as over-smoothing. Graph-BERT is designed for static graphs and primarily evaluated on citation networks with undirected edges. However, financial transaction networks are inherently dynamic, with evolving structures and directed edges representing the flow of money. To address these challenges, we introduce DynBERG, a novel architecture that integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture temporal evolution over multiple time steps. Additionally, we modify the underlying algorithm to support directed edges, making DynBERG well-suited for dynamic financial transaction analysis. We evaluate our model on the Elliptic dataset, which includes Bitcoin transactions, including all transactions during a major cryptocurrency market event, the Dark Market Shutdown. By assessing DynBERG's resilience before and after this event, we analyse its ability to adapt to significant market shifts that impact transaction behaviours. Our model is benchmarked against state-of-the-art dynamic graph classification approaches, such as EvolveGCN and GCN, demonstrating superior performance, outperforming EvolveGCN before the market shutdown and surpassing GCN after the event. Additionally, an ablation study highlights the critical role of incorporating a time-series deep learning component, showcasing the effectiveness of GRU in modelling the temporal dynamics of financial transactions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications</title>
<link>https://arxiv.org/abs/2511.00366</link>
<guid>https://arxiv.org/abs/2511.00366</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital twins, Gaussian process models, Derivative data, Dynamic updating, Sparse GP approximation

Summary: 
Digital twins are crucial for modeling the behavior of physical assets, utilizing high-fidelity models or surrogates for accurate real-time forecasting. This study extends Gaussian process models by incorporating derivative data for improved accuracy, enabling dynamic updating with in-service data from the physical twin. Though derivative data enhances accuracy, it increases the covariance matrix dimension significantly. To address this issue, a sparse GP approximation is used, with developed extensions for derivative inclusion. Numerical experiments show that the derivative-enhanced sparse GP method produces more accurate predictions with dynamic data additions. The algorithm is applied in a digital twin framework to model fatigue crack growth in an aerospace vehicle, demonstrating practical utility in real-world applications. <div>
arXiv:2511.00366v1 Announce Type: cross 
Abstract: Digital twins are developed to model the behavior of a specific physical asset (or twin), and they can consist of high-fidelity physics-based models or surrogates. A highly accurate surrogate is often preferred over multi-physics models as they enable forecasting the physical twin future state in real-time. To adapt to a specific physical twin, the digital twin model must be updated using in-service data from that physical twin. Here, we extend Gaussian process (GP) models to include derivative data, for improved accuracy, with dynamic updating to ingest physical twin data during service. Including derivative data, however, comes at a prohibitive cost of increased covariance matrix dimension. We circumvent this issue by using a sparse GP approximation, for which we develop extensions to incorporate derivatives. Numerical experiments demonstrate that the prediction accuracy of the derivative-enhanced sparse GP method produces improved models upon dynamic data additions. Lastly, we apply the developed algorithm within a DT framework to model fatigue crack growth in an aerospace vehicle.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights</title>
<link>https://arxiv.org/abs/2511.01019</link>
<guid>https://arxiv.org/abs/2511.01019</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, oceanography, data integration, conversational platform, NOAA.

Summary:<br /><br />Artificial intelligence is revolutionizing scientific research, but conventional conversational AI systems often produce unreliable information, posing a threat to scientific integrity. In response to this challenge, the OceanAI platform has been developed to combine the linguistic proficiency of large language models with direct access to real-time oceanographic data from NOAA. By leveraging APIs to access and synthesize authoritative data, OceanAI can provide accurate and reproducible answers to queries, such as historical water levels in specific locations. In a comparison with other AI chat interfaces, OceanAI was the only platform to consistently produce accurate values supported by original data sources. The platform's extensibility allows it to connect with various NOAA datasets, enabling applications in marine hazard prediction, ecosystem evaluation, and water quality monitoring. By prioritizing transparency and verifiability, OceanAI enhances the trustworthiness and credibility of AI-driven decision-making processes in ocean-related domains. Visit https://oceanai.ai4ocean.xyz for a live demonstration of OceanAI. <div>
arXiv:2511.01019v1 Announce Type: cross 
Abstract: Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified "hallucinations" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as "What was Boston Harbor's highest water level in 2024?" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at https://oceanai.ai4ocean.xyz.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novelty and Impact of Economics Papers</title>
<link>https://arxiv.org/abs/2511.01211</link>
<guid>https://arxiv.org/abs/2511.01211</guid>
<content:encoded><![CDATA[
<div> framework, scientific novelty, spatial novelty, temporal novelty, Large Language Models

Summary:
The article introduces a framework for understanding scientific novelty as a reflection of a paper's position within the evolving intellectual landscape. It decomposes novelty into spatial and temporal dimensions, using semantic isolation metrics derived from Large Language Models to quantify a paper's location relative to the literature. The study on a corpus of economics articles reveals a trade-off between spatial and temporal novelty: temporal novelty predicts citation counts, while spatial novelty predicts disruptive impact. This distinction helps identify four archetypes of semantic neighborhoods with distinct impact profiles. The research highlights that novelty is a multidimensional construct with different forms that have measurable and distinct consequences for scientific progress. <div>
arXiv:2511.01211v1 Announce Type: cross 
Abstract: We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper's intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Modeling of Precipitation in Electrolyte Systems</title>
<link>https://arxiv.org/abs/2511.01519</link>
<guid>https://arxiv.org/abs/2511.01519</guid>
<content:encoded><![CDATA[
<div> modeling, precipitation, electrolyte systems, continuous processes, crystallization 

Summary: 
The study proposes a dynamic modeling approach for precipitation in electrolyte systems, specifically focusing on the crystallization of an aromatic amine in continuous processes. The novel model integrates equilibrium and crystallization kinetics, assuming rapid equilibrium establishment and formulating as a set of differential algebraic equations. Key features include a population balance equation model for particle size distribution and modeling dynamically changing equilibria. The dynamic model's predictions align well with experimental measurements, aiming to assist the transition from batch to continuous processes by providing a foundation for numerical optimization and advanced control. <div>
arXiv:2511.01519v1 Announce Type: cross 
Abstract: This study presents a dynamic modeling approach for precipitation in electrolyte systems, focusing on the crystallization of an aromatic amine through continuous processes. A novel model, integrating equilibrium and crystallization kinetics, is formulated and applied to a continuous oscillatory baffled reactor. The approach assumes rapid equilibrium establishment and is formulated as a set of differential algebraic equations. Key features include a population balance equation model to describe the particle size distribution and the modeling of dynamically changing equilibria. The predictions of the dynamic model show good agreement with the available experimental measurements. The model is aimed at aiding the transition from a batch process to continuous process by forming the basis for numerical optimization and advanced control.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disciplined Biconvex Programming</title>
<link>https://arxiv.org/abs/2511.01813</link>
<guid>https://arxiv.org/abs/2511.01813</guid>
<content:encoded><![CDATA[
<div> Keywords: disciplined biconvex programming, biconvex optimization, alternate convex search, convex optimization, Python package<br />
Summary:<br />
Disciplined biconvex programming (DBCP) is proposed as a framework for solving biconvex optimization problems in various fields such as machine learning and signal processing. Typically, these problems are tackled using heuristics like alternate convex search (ACS) methods. DBCP simplifies the process by extending disciplined convex programming principles to biconvex problems, enabling users to specify problems with minimal syntax rules. The framework automatically splits and transforms problems into convex subproblems, generating customized ACS solvers for efficient solutions. Implemented in the Python package dbcp, DBCP integrates seamlessly with the popular CVXPY language for convex optimization. This enhancement allows users to experiment with diverse biconvex formulations without requiring specialized knowledge in convex optimization. <div>
arXiv:2511.01813v1 Announce Type: cross 
Abstract: We introduce disciplined biconvex programming (DBCP), a modeling framework for specifying and solving biconvex optimization problems. Biconvex optimization problems arise in various applications, including machine learning, signal processing, computational science, and control. Solving a biconvex optimization problem in practice usually resolves to heuristic methods based on alternate convex search (ACS), which iteratively optimizes over one block of variables while keeping the other fixed, so that the resulting subproblems are convex and can be efficiently solved. However, designing and implementing an ACS solver for a specific biconvex optimization problem usually requires significant effort from the user, which can be tedious and error-prone. DBCP extends the principles of disciplined convex programming to biconvex problems, allowing users to specify biconvex optimization problems in a natural way based on a small number of syntax rules. The resulting problem can then be automatically split and transformed into convex subproblems, for which a customized ACS solver is then generated and applied. DBCP allows users to quickly experiment with different biconvex problem formulations, without expertise in convex optimization. We implement DBCP into the open source Python package dbcp, as an extension to the famous domain specific language CVXPY for convex optimization.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning</title>
<link>https://arxiv.org/abs/2505.17050</link>
<guid>https://arxiv.org/abs/2505.17050</guid>
<content:encoded><![CDATA[
<div> multimodal data, Project-Based Learning, large language models, educational tasks, benchmark

Summary:
The article introduces PBLBench, a benchmark designed to evaluate complex reasoning in Project-Based Learning using large language models. It addresses the limitations of existing benchmarks by providing a free-form output structure and rigorous human expert validation process. Automated pipelines are lacking due to model hallucination and instability. PBLBench challenges models with tasks that resemble those handled by human experts, grounded in domain-specific knowledge and long-context understanding. The benchmark utilizes the Analytic Hierarchy Process for reliable ground truth, resulting in structured and weighted evaluation criteria. Performance of 15 MLLMs/LLMs on PBLBench shows that even advanced models achieve only 59% rank accuracy, highlighting the challenges it presents. PBLBench aims to facilitate the development of more capable AI agents to assist teachers and enhance educational productivity.<br /><br />Summary: <div>
arXiv:2505.17050v2 Announce Type: replace-cross 
Abstract: Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMint-SDE: A Multimodal Foundation Model for Accelerating Numerical Simulation of SDEs via Error Correction</title>
<link>https://arxiv.org/abs/2510.27173</link>
<guid>https://arxiv.org/abs/2510.27173</guid>
<content:encoded><![CDATA[
<div> decoder-only transformer, in-context learning, error-correction scheme, universal model, simulation tool

Summary:
FMint-SDE is a new multi-modal foundation model for large-scale simulations of differential equations, combining numerical and textual modalities to improve accuracy and efficiency. Unlike traditional numerical integrators, FMint-SDE uses a decoder-only transformer with in-context learning to learn a universal error-correction scheme. By training the model on prompted sequences of coarse solutions, it achieves superior accuracy and efficiency tradeoff compared to classical solvers. The model shows promise in various applications such as molecular dynamics, mechanical systems, finance, and biology, highlighting its potential as a general-purpose simulation tool for dynamical systems. <div>
arXiv:2510.27173v1 Announce Type: new 
Abstract: Fast and accurate simulation of dynamical systems is a fundamental challenge across scientific and engineering domains. Traditional numerical integrators often face a trade-off between accuracy and computational efficiency, while existing neural network-based approaches typically require training a separate model for each case. To overcome these limitations, we introduce a novel multi-modal foundation model for large-scale simulations of differential equations: FMint-SDE (Foundation Model based on Initialization for stochastic differential equations). Based on a decoder-only transformer with in-context learning, FMint-SDE leverages numerical and textual modalities to learn a universal error-correction scheme. It is trained using prompted sequences of coarse solutions generated by conventional solvers, enabling broad generalization across diverse systems. We evaluate our models on a suite of challenging SDE benchmarks spanning applications in molecular dynamics, mechanical systems, finance, and biology. Experimental results show that our approach achieves a superior accuracy-efficiency tradeoff compared to classical solvers, underscoring the potential of FMint-SDE as a general-purpose simulation tool for dynamical systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Control for a Physics-Informed Model of a Thermal Energy Distribution System: Qualitative Analysis</title>
<link>https://arxiv.org/abs/2510.26959</link>
<guid>https://arxiv.org/abs/2510.26959</guid>
<content:encoded><![CDATA[
<div> optimal control, adaptive control, integrated energy systems, uncertainties, heat exchangers

Summary:
- Integrated energy systems (IES) are complex architectures that require operating control strategy optimization but are hindered by uncertainties.
- Adaptive control (AC) is proposed as a methodology to accommodate uncertainties in real-time for linear systems where all states are observable.
- A specific application of AC to a glycol heat exchanger (GHX) in an IES showed significant reduction in error metrics with minimal computing overhead.
- The study found that AC can reduce mean absolute error and integral time absolute error by 30%-75% compared to a nominal control approach.
- The control effort induced by AC is significant, requiring further study to estimate its impact on physical systems. 
- The formulation of AC is being enhanced to address challenges such as partially observable and non-linear dynamics. 

<br /><br />Summary: <div>
arXiv:2510.26959v1 Announce Type: cross 
Abstract: Integrated energy systems (IES) are complex heterogeneous architectures that typically encompass power sources, hydrogen electrolyzers, energy storage, and heat exchangers. This integration is achieved through operating control strategy optimization. However, the lack of physical understanding as to how these systems evolve over time introduces uncertainties that hinder reliable application thereof. Techniques that can accommodate such uncertainties are fundamental for ensuring proper operation of these systems. Unfortunately, no unifying methodology exists for accommodating uncertainties in this regard. That being said, adaptive control (AC) is a discipline that may allow for accommodating such uncertainties in real-time. In the present work, we derive an AC formulation for linear systems in which all states are observable and apply it to the control of a glycol heat exchanger (GHX) in an IES. Based on prior research in which we quantified the uncertainties of the GHXs system dynamics, we introduced an error of 50% on four terms of the nominal model. In the case where a linear quadratic regulator is used as the nominal control for the reference system, we found that employing AC can reduce the mean absolute error and integral time absolute error by a factor of 30%-75%. This reduction is achieved with minimal computing overhead and control infrastructure, thus underscoring the strength of AC. However, the control effort induced is significant, therefore warranting further study in order to estimate its impact on a physical system. To address further challenges, including partially observable and non-linear dynamics, enhancements of the linear formulation are currently being developed.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Black-Scholes Model, comparison between Analytical Solution and Numerical Analysis</title>
<link>https://arxiv.org/abs/2510.27277</link>
<guid>https://arxiv.org/abs/2510.27277</guid>
<content:encoded><![CDATA[
<div> Keywords: option-pricing model, Black-Scholes, calculus, analytical method, numerical method

Summary: <br /><br />
This article provides an overview of the Black-Scholes option-pricing model, discussing its history, context, and significance in economics. The model's foundation is based on calculus concepts, with explanations on how to derive and solve the equation using both analytical and numerical methods. The article concludes by discussing the practical applications of the Black-Scholes model in today's financial markets. Additionally, two appendices are included to enhance the reader's understanding, one with essential economics concepts and another with code scripts for practical implementation. <div>
arXiv:2510.27277v1 Announce Type: cross 
Abstract: The main purpose of this article is to give a general overview and understanding of the first widely used option-pricing model, the Black-Scholes model. The history and context are presented, with the usefulness and implications in the economics world. A brief review of fundamental calculus concepts is introduced to derive and solve the model. The equation is then resolved using both an analytical (variable separation) and a numerical method (finite differences). Conclusions are drawn in order to understand how Black-Scholes is employed nowadays. At the end a handy appendix (A) is written with some economics notions to ease the reader's comprehension of the paper; furthermore a second appendix (B) is given with some code scripts, to allow the reader to put in practice some concepts.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reduced order modelling of Hopf bifurcations for the Navier-Stokes equations through invariant manifolds</title>
<link>https://arxiv.org/abs/2510.26542</link>
<guid>https://arxiv.org/abs/2510.26542</guid>
<content:encoded><![CDATA[
<div> parametric simulation-free reduced order model, incompressible flows, Hopf bifurcation, parametrisation method, invariant manifolds<br />
<br />
parametric simulation-free reduced order model for incompressible flows undergoing a Hopf bifurcation is introduced. This method directly operates on the governing equations, eliminating the need for full-order simulations. The model, computed at a specific bifurcation parameter value, remains valid over a range of values. It systematically constructs an invariant manifold and embedded dynamics, providing an efficient reduction of the original system. The proposed model accurately captures pre-critical steady states, the bifurcation point, and post-critical limit cycle oscillations, demonstrating strong agreement with full order simulations and significant computational speed-up. <br /><br />Summary: <div>
arXiv:2510.26542v1 Announce Type: new 
Abstract: This work introduces a parametric simulation-free reduced order model for incompressible flows undergoing a Hopf bifurcation, leveraging the parametrisation method for invariant manifolds. Unlike data-driven approaches, this method operates directly on the governing equations, eliminating the need for full-order simulations. The proposed model is computed at a single value of the bifurcation parameter yet remains valid over a range of values. The approach systematically constructs an invariant manifold and embedded dynamics, providing an accurate and efficient reduction of the original system. The ability to capture pre-critical steady states, the bifurcation point, and post-critical limit cycle oscillations is demonstrated by a strong agreement between the reduced order model and full order simulations, while achieving significant computational speed-up.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fast spectral overlapping domain decomposition method with discretization-independent conditioning bounds</title>
<link>https://arxiv.org/abs/2510.25991</link>
<guid>https://arxiv.org/abs/2510.25991</guid>
<content:encoded><![CDATA[
<div> domain decomposition method, variable-coefficient elliptic PDEs, H-matrix structure, reduced linear system, black-box randomized compression

Summary:
A new domain decomposition method is presented for solving variable-coefficient elliptic partial differential equations on regular domains. The method involves tessellating the domain into thin slabs or shells and creating a reduced linear system connecting the domains. By utilizing H-matrix structure, the method efficiently handles large dense blocks in the system. The formulation ensures well-conditioning, converging to a second kind Fredholm equation with refined local solves. Data sparsity in dense blocks leads to faster H-matrix arithmetic. The reduced linear system is formed using black-box randomized compression, leveraging the efficiency of sparse direct solvers on sub-domains. The solver successfully handles oscillatory 2D and 3D problems with up to 28 million degrees of freedom. <br /><br />Summary: <div>
arXiv:2510.25991v1 Announce Type: cross 
Abstract: A domain decomposition method for the solution of general variable-coefficient elliptic partial differential equations on regular domains is introduced. The method is based on tessellating the domain into overlapping thin slabs or shells, and then explicitly forming a reduced linear system that connects the different domains. Rank-structure ('H-matrix structure') is exploited to handle the large dense blocks that arise in the reduced linear system. Importantly, the formulation used is well-conditioned, as it converges to a second kind Fredholm equation as the precision in the local solves is refined. Moreover, the dense blocks that arise are far more data-sparse than in existing formulations, leading to faster and more efficient H-matrix arithmetic. To form the reduced linear system, black-box randomized compression is used, taking full advantage of the fact that sparse direct solvers are highly efficient on the thin sub-domains. Numerical experiments demonstrate that our solver can handle oscillatory 2D and 3D problems with as many as 28 million degrees of freedom.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Manage Investment Portfolios beyond Simple Utility Functions</title>
<link>https://arxiv.org/abs/2510.26165</link>
<guid>https://arxiv.org/abs/2510.26165</guid>
<content:encoded><![CDATA[
<div> Keywords: investment funds, generative framework, latent representations, U.S. equity mutual funds, market simulation

Summary: 
A generative framework is proposed to learn latent representations of investment fund manager strategies without explicit utility specification. The framework models the conditional probability of a fund's portfolio weights based on various factors. It successfully captures known investment styles and reveals implicit manager objectives. The framework is validated on a dataset of U.S. equity mutual funds, showing heterogeneous realizations for turnover, concentration, and latent factors. Tests are developed to explain and interpret the model, demonstrating that expert labeling is contained in a linear interpretable way. The framework provides a data-driven approach for characterizing investment strategies, with applications in market simulation, strategy attribution, and regulatory oversight. <div>
arXiv:2510.26165v1 Announce Type: cross 
Abstract: While investment funds publicly disclose their objectives in broad terms, their managers optimize for complex combinations of competing goals that go beyond simple risk-return trade-offs. Traditional approaches attempt to model this through multi-objective utility functions, but face fundamental challenges in specification and parameterization. We propose a generative framework that learns latent representations of fund manager strategies without requiring explicit utility specification.
  Our approach directly models the conditional probability of a fund's portfolio weights, given stock characteristics, historical returns, previous weights, and a latent variable representing the fund's strategy. Unlike methods based on reinforcement learning or imitation learning, which require specified rewards or labeled expert objectives, our GAN-based architecture learns directly from the joint distribution of observed holdings and market data.
  We validate our framework on a dataset of 1436 U.S. equity mutual funds. The learned representations successfully capture known investment styles, such as "growth" and "value," while also revealing implicit manager objectives. For instance, we find that while many funds exhibit characteristics of Markowitz-like optimization, they do so with heterogeneous realizations for turnover, concentration, and latent factors.
  To analyze and interpret the end-to-end model, we develop a series of tests that explain the model, and we show that the benchmark's expert labeling are contained in our model's encoding in a linear interpretable way.
  Our framework provides a data-driven approach for characterizing investment strategies for applications in market simulation, strategy attribution, and regulatory oversight.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stiff Circuit System Modeling via Transformer</title>
<link>https://arxiv.org/abs/2510.24727</link>
<guid>https://arxiv.org/abs/2510.24727</guid>
<content:encoded><![CDATA[
<div> Transformer model, time-series prediction, Kolmogorov-Arnold Networks, stiff circuit, circuit behavior modeling <br />
Summary: <br />
The article introduces a new approach for modeling stiff circuit transient behavior using a combination of Crossformer and Kolmogorov-Arnold Networks (KANs). The proposed method leverages the temporal representation capabilities of Crossformer and the enhanced feature extraction of KANs to predict circuit responses accurately. Experimental evaluations conducted on datasets generated through SPICE simulations of analog-to-digital converter (ADC) circuits demonstrate the effectiveness of the approach. The results show improved fidelity in predicting circuit responses to various input conditions, along with significant reductions in training time and error rates. This new approach presents a promising solution for accurately and efficiently modeling stiff circuits in electronic design automation. <div>
arXiv:2510.24727v1 Announce Type: new 
Abstract: Accurate and efficient circuit behavior modeling is a cornerstone of modern electronic design automation. Among different types of circuits, stiff circuits are challenging to model using previous frameworks. In this work, we propose a new approach using Crossformer, which is a current state-of-the-art Transformer model for time-series prediction tasks, combined with Kolmogorov-Arnold Networks (KANs), to model stiff circuit transient behavior. By leveraging the Crossformer's temporal representation capabilities and the enhanced feature extraction of KANs, our method achieves improved fidelity in predicting circuit responses to a wide range of input conditions. Experimental evaluations on datasets generated through SPICE simulations of analog-to-digital converter (ADC) circuits demonstrate the effectiveness of our approach, with significant reductions in training time and error rates.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Black Box Variational Inference Scheme for Inverse Problems with Demanding Physics-Based Models</title>
<link>https://arxiv.org/abs/2510.25038</link>
<guid>https://arxiv.org/abs/2510.25038</guid>
<content:encoded><![CDATA[
<div> black box variational inference, importance sampling, Bayesian inference, computational costs, non-differentiable models

Summary:
The article introduces a novel Bayesian inference approach based on black box variational inference for addressing inverse problems with costly forward models. The method utilizes importance sampling to estimate the variational objective gradient without the need for forward model gradients, reducing computational costs. A batch-sequential sampling procedure is implemented to determine when new model evaluations are required, optimizing variational parameter updates. This approach is particularly suitable for inverse problems involving non-differentiable, physics-based models. Efficiency gains of the proposed method are demonstrated through benchmarks, including density matching and Bayesian calibration of a nonlinear electro-chemo-mechanical model for solid-state batteries. This method outperforms sequential Monte Carlo and Markov-Chain Monte Carlo, showcasing its effectiveness in managing uncertainties and optimizing computational resources. 

<br /><br />Summary: <div>
arXiv:2510.25038v1 Announce Type: new 
Abstract: Bayesian methods are particularly effective for addressing inverse problems due to their ability to manage uncertainties inherent in the inference process. However, employing these methods with costly forward models poses significant challenges, especially in the context of non-differentiable models, where the absence of likelihood model gradient information can result in high computational costs. To tackle this issue, we develop a novel Bayesian inference approach based on black box variational inference, utilizing importance sampling to reuse existing simulation model calls in the variational objective gradient estimation, without relying on forward model gradients. The novelty lies in a new batch-sequential sampling procedure, which only requires new model evaluations if the currently available model evaluations fail to yield a suitable approximation of the objective gradient. The resulting approach reduces computational costs by leading to variational parameter updates without requiring new model evaluations when possible, while adaptively increasing the number of model calls per iteration as needed. In combination with its black box nature, this new approach is suitable for inverse problems involving demanding physics-based models that lack model gradients. We demonstrate the efficiency gains of the proposed method compared to its baseline version, sequential Monte Carlo, and Markov-Chain Monte Carlo in diverse benchmarks, ranging from density matching to the Bayesian calibration of a nonlinear electro-chemo-mechanical model for solid-state batteries.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Financial Decision-Making: Machine Learning and AI-Powered Predictions and Analysis</title>
<link>https://arxiv.org/abs/2510.25201</link>
<guid>https://arxiv.org/abs/2510.25201</guid>
<content:encoded><![CDATA[
<div> Machine learning algorithms, financial prediction, AI-driven platform, inflation analysis, stock market prediction<br />
Summary:<br />
The proposed system utilizes machine learning algorithms to improve financial prediction accuracy. It includes an AI-driven platform offering inflation analysis, stock market prediction, and an E-learning module with a chatbot. Achieving high accuracy, it demonstrates 0.8% MAE for inflation analysis and 98% and 96% accuracy for Apple and Google stock price predictions, respectively. Key features comprise historical price trends, inflation rates, and short-term stock prediction using real-world financial datasets. The E-learning component helps bridge financial knowledge gaps and aids in making informed decisions. Implemented algorithms include linear regression, ARIMA, and LSTM, with accuracy evaluated using metrics like MAE and RMSE. <br /><br />Summary: <div>
arXiv:2510.25201v1 Announce Type: new 
Abstract: The proposed system aims to use various machine learning algorithms to enhance financial prediction and generate highly accurate analyses. It introduces an AI-driven platform which offers inflation-analysis, stock market prediction, and E-learning module powered by a chatbot. It has achieved high accuracy where the Inflation Analysis depicts 0.8% MAE, 1.2% RMSE and the Stock Prediction shows 98% and 96% accuracy for Apple and Google stock prices respectively. Key features include historical price trends, inflation rates, short-term future stock prediction, where the data has been extracted using real-world financial datasets. Additionally, the E-learning feature contributes to bridging financial gaps and promoting informed decisions. We have implemented algorithms like linear regression, ARIMA, LSTM where the accuracy has been evaluated using metrics such as MAE, RMSE and the like.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related to Metal Casting</title>
<link>https://arxiv.org/abs/2510.25697</link>
<guid>https://arxiv.org/abs/2510.25697</guid>
<content:encoded><![CDATA[
<div> Reframing, Metal casting, 2D operator learning, Surrogate, CFD simulations <br />
Summary:<br /> 
This work presents a novel approach to mold filling in metal casting by utilizing a 2D operator learning surrogate to replace costly transient CFD simulations. The method combines a graph-based encoder for aggregating neighborhood information, a Fourier spectral core for capturing global interactions, and a graph-based decoder for mapping latent fields. The model predicts velocities, pressure, and volume fraction efficiently across various ingate locations and process settings, with 5% mean relative L2 errors. Inference is significantly faster than traditional CFD simulations, enabling rapid design exploration. Ablation studies demonstrate the model's robustness to spatial and temporal subsampling, while reducing training data only slightly affects accuracy. Overall, this approach showcases the effectiveness of neural operators as surrogates for mold filling and enables quick optimization of gating system designs in casting workflows. <br /> <div>
arXiv:2510.25697v1 Announce Type: new 
Abstract: This work reframes mold filling in metal casting as a simplified 2D operator learning surrogate to avoid costly transient CFD simulations. The method combines a graph based encoder that aggregates neighborhood information on an unstructured input mesh to encode geometry and boundary data, a Fourier spectral core that operates on a regular latent grid to capture global interactions, and a graph based decoder that maps latent fields back to a target mesh. The model jointly predicts velocities, pressure, and volume fraction over a fixed horizon and generalizes across varied ingate locations and process settings. On held out geometries and inlet conditions it reproduces large scale advection and the fluid air interface with errors concentrated near steep gradients. Mean relative L2 errors are about 5 percent across all fields. Inference is roughly 100 to 1000 times faster than conventional CFD simulations, thereby enabling rapid in-the-loop design exploration. Ablation studies show accuracy drops monotonically with stronger spatial subsampling of input vertices while temporal subsampling causes a gentler decline. Cutting the training data by 50 percent yields only small error growth. Overall the results demonstrate neural operators as efficient surrogates for 2D mold filling and related filling problems and enable fast exploration and optimization of gating system designs in casting workflows.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework</title>
<link>https://arxiv.org/abs/2510.25402</link>
<guid>https://arxiv.org/abs/2510.25402</guid>
<content:encoded><![CDATA[
<div> quality evaluation, patent content, AI drafting tools, systematic evaluation, improvement suggestions

Summary: 
A proposed framework for evaluating patent quality addresses the lack of research in this area, using modules to assess regulatory compliance, technical coherence, and figure-reference consistency. Results on a dataset of human-authored and AI-generated patents demonstrate high accuracies in detection modules, pointing to the need for attention to figure-text consistency and technical detail precision. Analysis reveals specific issues in Mechanical Engineering and Construction patents, with AI-generated patents showing more structural defects compared to human-authored ones. The latter primarily contain surface-level errors, while the former exhibit issues in figure-text alignment and cross-references. This study highlights the differences between human and AI-generated patents and emphasizes the importance of enhancing the quality of patent content through systematic evaluation and improvement suggestions. 

<br /><br /> <div>
arXiv:2510.25402v1 Announce Type: cross 
Abstract: Despite the surge in patent applications and emergence of AI drafting tools, systematic evaluation of patent content quality has received limited research attention. To address this gap, We propose to evaluate patents using regulatory compliance, technical coherence, and figure-reference consistency detection modules, and then generate improvement suggestions via an integration module. The framework is validated on a comprehensive dataset comprising 80 human-authored and 80 AI-generated patents from two patent drafting tools. Experimental results show balanced accuracies of 99.74\%, 82.12\%, and 91.2\% respectively across the three detection modules when validated against expert annotations. Additional analysis was conducted to examine defect distributions across patent sections, technical domains, and authoring sources. Section-based analysis indicates that figure-text consistency and technical detail precision require particular attention. Mechanical Engineering and Construction show more claim-specification inconsistencies due to complex technical documentation requirements. AI-generated patents show a significant gap compared to human-authored ones. While human-authored patents primarily contain surface-level errors like typos, AI-generated patents exhibit more structural defects in figure-text alignment and cross-references.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics</title>
<link>https://arxiv.org/abs/2510.25683</link>
<guid>https://arxiv.org/abs/2510.25683</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Structural Simulator, Dynamic simulations, Node kinematics, Connectivity radius<br />
Summary:<br />
The article introduces the Graph Network-based Structural Simulator (GNSS) for dynamic structural problems, utilizing GNNs for surrogate modeling in a novel way. GNSS incorporates node kinematics in local frames, employs a sign-aware regression loss, and utilizes a wavelength-informed connectivity radius. In a case study of a beam, GNSS accurately reproduces physics over numerous timesteps and generalizes to new loading conditions. Compared to finite element methods, GNSS offers faster inference speeds while maintaining spatial and temporal accuracy. These results showcase the effectiveness of using locality-preserving GNNs with physics-consistent update rules for dynamic, wave-dominated structural simulations. <br /> <div>
arXiv:2510.25683v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models for numerical simulations. While their applications in computational fluid dynamics have been investigated, little attention has been given to structural problems, especially for dynamic cases. To address this gap, we introduce the Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine learning models, and its design makes it particularly suited for dynamic simulations thanks to three key features: (i) expressing node kinematics in node-fixed local frames, which avoids catastrophic cancellation in finite-difference velocities; (ii) employing a sign-aware regression loss, which reduces phase errors in long rollouts; and (iii) using a wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz Hanning-modulated pulse. The results show that GNSS accurately reproduces the physics of the problem over hundreds of timesteps and generalizes to unseen loading conditions, where existing GNNs fail to converge or deliver meaningful predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial inference speedups while preserving spatial and temporal fidelity. These findings demonstrate that locality-preserving GNNs with physics-consistent update rules are a competitive alternative for dynamic, wave-dominated structural simulations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMP-Val: Large Language Models Empower Personalized Valuation in Auction</title>
<link>https://arxiv.org/abs/2410.15817</link>
<guid>https://arxiv.org/abs/2410.15817</guid>
<content:encoded><![CDATA[
<div> auctions, personalized valuation, Large Language Models, bidding algorithms, valuation errors

Summary:
The article discusses the importance of incorporating individual users' preferences into auction mechanisms to improve valuation accuracy. It introduces a personalized valuation framework called LaMP-Val, which utilizes Large Language Models to integrate personalized semantic preferences into the valuation process. The framework consists of three components: data, learning, and evaluation, which collectively aim to enhance the accuracy of personalized valuation. Data component focuses on building a novel dataset for fine-tuning LLMs in personalized valuation modeling. Learning component introduces diversity template to improve LLMs' capacity for capturing personal valuation patterns. Evaluation component establishes a closed-loop system for assessing the accuracy of LLM-generated valuations in personalized scenarios. Experimental results demonstrate that LaMP-Val outperforms baseline approaches in capturing personalized values and maximizing profits in auction settings. <div>
arXiv:2410.15817v2 Announce Type: replace 
Abstract: Auctions are a vital economic mechanism used to determine the market value of goods or services through competitive bidding within a specific framework. However, much of the current research primarily focuses on the bidding algorithms used within auction mechanisms. This often neglects the potential benefits of incorporating individual users' unique preferences into the valuation process. Our theoretical and empirical analysis demonstrates that valuation errors can significantly impact the overall utility. To bridge this gap, we propose a personalized valuation framework, namely Large \underline{La}nguage \underline{M}odels-powered \underline{P}ersonalized \underline{Val}uation (LaMP-Val), which integrates Large Language Models to incorporate personalized semantic preference into users valuation process. LaMP-Val integrating three components: data, learning, and evaluation. The data component tackles the challenge of building a novel dataset specifically for LLMs fine-tuning in personalized valuation modeling. The learning component introduces a diversity template to enhance LLMs' capacity for modeling fine-grained personal valuation patterns. The evaluation component establishes a closed-loop system where LLM-generated valuations interact with bidding strategies and auction. It proposes two novel metrics to quantify valuation precision and bidding intention accuracy in personalized scenarios. Extensive experiments show that LaMP-Val more accurately captures personalized values and achieves greater profits than baseline approaches.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
<link>https://arxiv.org/abs/2510.24106</link>
<guid>https://arxiv.org/abs/2510.24106</guid>
<content:encoded><![CDATA[
<div> Transformer Module, Aerodynamic Simulation, Deep Neural Networks, Joint Training, UniField<br />
Summary:<br />
The article discusses the importance of aerodynamic simulation in engineering and the use of deep neural networks as a cost-effective alternative to traditional CFD simulations for modeling surface pressure fields. Data scarcity is a significant challenge in this field, limiting the application of neural networks. To address this limitation, the authors propose UniField, a model that integrates aerodynamic data from multiple subfields and conducts joint training to learn more general field representations. By consolidating different datasets covering various domains, including automobiles, trains, aircraft, and general shapes, UniField uses a domain-agnostic Transformer module to extract general point cloud features and domain-specific flow-conditioned adapters to adapt to different flow conditions. Results show that jointly training on all data leads to better performance, indicating that the data from different subfields complement each other and help the model learn better flow field representations. This research lays the foundation for broader applications of neural networks in aerodynamic analysis. <br /> <div>
arXiv:2510.24106v1 Announce Type: new 
Abstract: Aerodynamic simulation of the surface pressure field around objects is crucial for many engineering problems. In recent years, deep neural networks have emerged as an efficient alternative to traditional, computationally expensive CFD simulations for modeling surface pressure fields. However, data scarcity remains a fundamental challenge, limiting the application of neural networks. To address this limitation, we propose to integrate aerodynamic data from multiple subfields and conduct joint training to learn more general field representations. We consolidate five different datasets covering various fields, including automobiles, trains, aircraft, and general shapes. Facing significant data differences across different domains, we propose UniField, which employs a domain-agnostic Transformer module to extract general point cloud features and customizes domain-specific flow-conditioned adapters to adapt to the flow information in different subfields. Despite the fact that aerodynamic data from different subfields are typically governed by different equations, we compare models trained jointly on all data with those trained separately on individual datasets and find that the jointly-trained model commonly demonstrates better performance. This indicates that these data complement each other to help the model learn better flow field representations. These results highlight the potential of UniField as a universal flow field representation model and lay the foundation for broader applications of neural networks in aerodynamic analysis.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A data-driven multiscale scheme for anisotropic finite strain magneto-elasticity</title>
<link>https://arxiv.org/abs/2510.24197</link>
<guid>https://arxiv.org/abs/2510.24197</guid>
<content:encoded><![CDATA[
<div> neural network-based, multiscale scheme, structured magnetically soft magnetorheological elastomers, data-driven, decoupled <br />
Summary: 
This work presents a novel approach for modeling structured magnetically soft magnetorheological elastomers using a neural network-based, data-driven, decoupled multiscale scheme. The microscale responses are computed using a mixed finite element formulation and homogenized to create a database for training a macroscopic physics-augmented neural network model. The model enforces physical principles and accurately predicts magnetization, mechanical stress, and total stress within the training data range. For larger magnetic fields, plausible results are obtained. The model is applied to analyze the magnetostrictive behavior of a macroscopic spherical MRE sample, showing contraction along the magnetic field direction when aligned with the material's preferred direction. <div>
arXiv:2510.24197v1 Announce Type: new 
Abstract: In this work, we develop a neural network-based, data-driven, decoupled multiscale scheme for the modeling of structured magnetically soft magnetorheological elastomers (MREs). On the microscale, sampled magneto-mechanical loading paths are imposed on a representative volume element containing spherical particles and an elastomer matrix, and the resulting boundary value problem is solved using a mixed finite element formulation. The computed microscale responses are homogenized to construct a database for the training and testing of a macroscopic physics-augmented neural network model. The proposed model automatically detects the material's preferred direction during training and enforces key physical principles, including objectivity, material symmetry, thermodynamic consistency, and the normalization of free energy, stress, and magnetization. Within the range of the training data, the model enables accurate predictions of magnetization, mechanical stress, and total stress. For larger magnetic fields, the model yields plausible results. Finally, we apply the model to investigate the magnetostrictive behavior of a macroscopic spherical MRE sample, which exhibits contraction along the magnetic field direction when aligned with the material's preferred direction.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization</title>
<link>https://arxiv.org/abs/2510.23667</link>
<guid>https://arxiv.org/abs/2510.23667</guid>
<content:encoded><![CDATA[
<div> predictive modeling, structural topology optimization, deep learning, physics-aware design, generative modeling

Summary: 
The article introduces Optimize Any Topology (OAT), a framework for structural topology optimization that can predict minimum-compliance layouts for various configurations. OAT combines an autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on a large corpus of optimized structures. It outperforms prior models by reducing mean compliance up to 90% and provides fast inference across different resolutions and aspect ratios. OAT is resolution-agnostic and delivers results in less than one second on a single GPU. The framework is general, fast, and supports physics-aware topology optimization. The article also introduces the OpenTO corpus, a dataset of 2.2 million optimized structures, to facilitate further research in generative modeling for inverse design. <div>
arXiv:2510.23667v1 Announce Type: cross 
Abstract: Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales</title>
<link>https://arxiv.org/abs/2510.23993</link>
<guid>https://arxiv.org/abs/2510.23993</guid>
<content:encoded><![CDATA[
<div> GPU, compressible reacting flow solver, AMReX framework, multi-GPU, high-performance <br />
Summary:
A high-speed chemically active flow solver for GPUs targets stiff chemistry challenges by optimizing memory access, workload variability, and multi-GPU load distribution. Utilizing the AMReX framework, it achieves significant performance enhancements for combustion simulations. By optimizing memory access patterns and workload variability, the solver exhibits $2-5\times$ performance gains over initial GPU implementations. It efficiently distributes computational workload across multiple GPUs for adaptive mesh refinement applications. The solver adapts matrix-based chemical kinetics to multigrid contexts, improving arithmetic intensity for convection and chemistry routines. With near-ideal weak scaling across $1-96$ NVIDIA H100 GPUs, the solver demonstrates efficient utilization of GPU resources and memory bandwidth, indicating improved performance for high-speed reacting flow simulations. <div>
arXiv:2510.23993v1 Announce Type: cross 
Abstract: High-speed chemically active flows present significant computational challenges due to their disparate space and time scales, where stiff chemistry often dominates simulation time. While modern supercomputing scientific codes achieve exascale performance by leveraging graphics processing units (GPUs), existing GPU-based compressible combustion solvers face critical limitations in memory management, load balancing, and handling the highly localized nature of chemical reactions. To this end, we present a high-performance compressible reacting flow solver built on the AMReX framework and optimized for multi-GPU settings. Our approach addresses three GPU performance bottlenecks: memory access patterns through column-major storage optimization, computational workload variability via a bulk-sparse integration strategy for chemical kinetics, and multi-GPU load distribution for adaptive mesh refinement applications. The solver adapts existing matrix-based chemical kinetics formulations to multigrid contexts. Using representative combustion applications including hydrogen-air detonations and jet in supersonic crossflow configurations, we demonstrate $2-5\times$ performance improvements over initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4 \times$) routines, confirming efficient utilization of GPU memory bandwidth and computational resources.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HergNet: a Fast Neural Surrogate Model for Sound Field Predictions via Superposition of Plane Waves</title>
<link>https://arxiv.org/abs/2510.24279</link>
<guid>https://arxiv.org/abs/2510.24279</guid>
<content:encoded><![CDATA[
<div> neural network, sound fields, Helmholtz equation, wave phenomena, room acoustics simulation <br />
<br />
Summary: 
A novel neural network architecture is introduced for predicting sound fields efficiently in two and three dimensions. This neural network is uniquely designed to automatically satisfy the Helmholtz equation, ensuring that the predicted sound fields are physically valid. As a result, the neural network can effectively learn solutions to boundary-value problems in various wave phenomena like acoustics, optics, and electromagnetism. Numerical experiments indicate that this approach shows promise in outperforming existing methods, particularly in room acoustics simulation at mid to high frequencies. <div>
arXiv:2510.24279v1 Announce Type: cross 
Abstract: We present a novel neural network architecture for the efficient prediction of sound fields in two and three dimensions. The network is designed to automatically satisfy the Helmholtz equation, ensuring that the outputs are physically valid. Therefore, the method can effectively learn solutions to boundary-value problems in various wave phenomena, such as acoustics, optics, and electromagnetism. Numerical experiments show that the proposed strategy can potentially outperform state-of-the-art methods in room acoustics simulation, in particular in the range of mid to high frequencies.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering</title>
<link>https://arxiv.org/abs/2510.24402</link>
<guid>https://arxiv.org/abs/2510.24402</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, metadata-driven, financial filings, document analysis, RAG architecture

Summary: 
This study explores advanced metadata-driven techniques for Retrieval-Augmented Generation (RAG) in handling long and structured financial filings. The researchers propose a multi-stage RAG architecture that utilizes metadata generated by Large Language Models (LLMs) to improve performance on the FinanceBench dataset. By incorporating contextual embeddings with text ("contextual chunks"), the study shows significant performance gains in RAG. The research highlights the importance of a powerful reranker for precision and the effectiveness of embedding chunk metadata directly with text. The proposed optimal architecture combines LLM-driven pre-retrieval optimizations with contextual embeddings to achieve superior performance. Additionally, a custom metadata reranker is introduced as a cost-effective alternative to commercial solutions, emphasizing the balance between peak performance and operational efficiency in RAG systems for financial document analysis. 

<br /><br />Summary: <div>
arXiv:2510.24402v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) struggles on long, structured financial filings where relevant evidence is sparse and cross-referenced. This paper presents a systematic investigation of advanced metadata-driven Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a novel, multi-stage RAG architecture that leverages LLM-generated metadata. We introduce a sophisticated indexing pipeline to create contextually rich document chunks and benchmark a spectrum of enhancements, including pre-retrieval filtering, post-retrieval reranking, and enriched embeddings, benchmarked on the FinanceBench dataset. Our results reveal that while a powerful reranker is essential for precision, the most significant performance gains come from embedding chunk metadata directly with text ("contextual chunks"). Our proposed optimal architecture combines LLM-driven pre-retrieval optimizations with these contextual embeddings to achieve superior performance. Additionally, we present a custom metadata reranker that offers a compelling, cost-effective alternative to commercial solutions, highlighting a practical trade-off between peak performance and operational efficiency. This study provides a blueprint for building robust, metadata-aware RAG systems for financial document analysis.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-supervised and unsupervised learning for health indicator extraction from guided waves in aerospace composite structures</title>
<link>https://arxiv.org/abs/2510.24614</link>
<guid>https://arxiv.org/abs/2510.24614</guid>
<content:encoded><![CDATA[
<div> deep learning, aerospace structures, health indicators, composite materials, signal processing

Summary: 
- Health indicators (HIs) are crucial for assessing aerospace composite structures' condition.
- Traditional methods face challenges due to material variability, damage evolution, and incidents like bird strikes.
- A data-driven framework with two learning approaches and multi-domain signal processing is proposed.
- The approaches include a diversity deep semi-supervised anomaly detection (Diversity-DeepSAD) and a degradation-trend-constrained variational autoencoder (DTC-VAE).
- Guided waves with multiple excitation frequencies are used to monitor structures under fatigue loading, with results showing improved performance in extracting reliable HIs. 

<br /><br /> <div>
arXiv:2510.24614v1 Announce Type: cross 
Abstract: Health indicators (HIs) are central to diagnosing and prognosing the condition of aerospace composite structures, enabling efficient maintenance and operational safety. However, extracting reliable HIs remains challenging due to variability in material properties, stochastic damage evolution, and diverse damage modes. Manufacturing defects (e.g., disbonds) and in-service incidents (e.g., bird strikes) further complicate this process. This study presents a comprehensive data-driven framework that learns HIs via two learning approaches integrated with multi-domain signal processing. Because ground-truth HIs are unavailable, a semi-supervised and an unsupervised approach are proposed: (i) a diversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach augmented with continuous auxiliary labels used as hypothetical damage proxies, which overcomes the limitation of prior binary labels that only distinguish healthy and failed states while neglecting intermediate degradation, and (ii) a degradation-trend-constrained variational autoencoder (DTC-VAE), in which the monotonicity criterion is embedded via an explicit trend constraint. Guided waves with multiple excitation frequencies are used to monitor single-stiffener composite structures under fatigue loading. Time, frequency, and time-frequency representations are explored, and per-frequency HIs are fused via unsupervised ensemble learning to mitigate frequency dependence and reduce variance. Using fast Fourier transform features, the augmented Diversity-DeepSAD model achieved 81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3% performance, outperforming existing baselines.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative study of Bitcoin and Ripple cryptocurrencies trading using Deep Reinforcement Learning algorithms</title>
<link>https://arxiv.org/abs/2505.07660</link>
<guid>https://arxiv.org/abs/2505.07660</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, automated trading, Deep Reinforcement Learning, Bitcoin, Ripple <br />
<br />
Summary: 
The article explores the use of innovative rule-based strategies combined with Deep Reinforcement Learning (DRL) to address the challenges in forecasting prices of financial assets like Bitcoin and Ripple. Different algorithms such as Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, and Advantage Actor-Critic are applied to train the DRL model to formulate optimal trading policies. The evaluation metrics used are portfolio wealth and trade signals. Results show that Dueling and Double Deep Q-Network perform better in increasing portfolio wealth when trading Ripple. All codes for the project are available on Github for reference and further study. <div>
arXiv:2505.07660v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) has demonstrated remarkable success across various applications. In light of this trend, the field of automated trading has developed a keen interest in leveraging AI techniques to forecast the future prices of financial assets. This interest stems from the need to address trading challenges posed by the inherent volatility and dynamic nature of asset prices. However, crafting a flawless strategy becomes a formidable task when dealing with assets characterized by intricate and ever-changing price dynamics. To surmount these formidable challenges, this research employs an innovative rule-based strategy approach to train Deep Reinforcement Learning (DRL). This application is carried out specifically in the context of trading Bitcoin (BTC) and Ripple (XRP). Our proposed approach hinges on the integration of Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, alongside the Advantage Actor-Critic algorithms. Each of them aims to yield an optimal policy for our application. To evaluate the effectiveness of our Deep Reinforcement Learning (DRL) approach, we rely on portfolio wealth and the trade signal as performance metrics. The experimental outcomes highlight that Duelling and Double Deep Q-Network outperformed when using XRP with the increasing of the portfolio wealth. All codes are available in this \href{https://github.com/VerlonRoelMBINGUI/RL_Final_Projects_AMMI2023}{\color{blue}Github link}.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data</title>
<link>https://arxiv.org/abs/2505.12638</link>
<guid>https://arxiv.org/abs/2505.12638</guid>
<content:encoded><![CDATA[
<div> scATAC-seq, foundation model, ChromFound, single-cell chromatin accessibility, regulatory mechanisms<br />
<br />
Summary:<br />
The article introduces ChromFound, a foundation model designed for single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) data analysis. It addresses the challenges of high dimensionality and sparsity in scATAC-seq data by using a hybrid architecture and genome-aware tokenization. ChromFound, pretrained on a vast dataset, demonstrates strong performance in various tasks such as cell identification, cell type annotation, and cross-omics prediction. It uncovers enhancer-gene links that were previously undetected, providing insights into disease risk variants in the noncoding genome. ChromFound's ability to generate universal cell representations and its transferability make it a promising tool for comprehensive multi-omics analysis in the field of chromatin accessibility research. <div>
arXiv:2505.12638v3 Announce Type: replace-cross 
Abstract: The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present ChromFound, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error bounded compression for weather and climate applications</title>
<link>https://arxiv.org/abs/2510.22265</link>
<guid>https://arxiv.org/abs/2510.22265</guid>
<content:encoded><![CDATA[
<div> JPEG2000, wavelet transform, SPIHT encoding, error-bounded compression, weather and climate simulations

Summary: 
EBCC (Error Bounded Climate-data Compressor) addresses the challenge of rapidly growing data in weather and climate simulations by utilizing a two-layer compression approach. The base compression layer employs JPEG2000, while the residual compression layer utilizes wavelet transform and SPIHT encoding to eliminate extreme errors. A feedback rate-control mechanism adjusts compression ratios to meet specified error targets. EBCC outperforms other compression methods in benchmarks related to weather and climate science, concentrating errors near zero and achieving compression ratios ranging from 15x to over 300x. In benchmarks such as energy budget closure and Lagrangian trajectory simulation, EBCC maintains errors within natural variability while achieving more than 100x compression. The source code for EBCC is available on github.com/spcl/EBCC. <br /><br />Summary: <div>
arXiv:2510.22265v1 Announce Type: new 
Abstract: As the resolution of weather and climate simulations increases, the amount of data produced is growing rapidly from hundreds of terabytes to tens of petabytes. The huge size becomes a limiting factor for broader adoption, and its fast growth rate will soon exhaust all the available storage devices. To address these issues, we present EBCC (Error Bounded Climate-data Compressor). It follows a two-layer approach: a base compression layer using JPEG2000 to capture the bulk of the data with a high compression ratio, and a residual compression layer using wavelet transform and SPIHT encoding to efficiently eliminate long-tail extreme errors introduced by the base compression layer. It incorporates a feedback rate-control mechanism for both layers that adjusts compression ratios to achieve the specified maximum error target. We evaluate EBCC alongside other established compression methods on benchmarks related to weather and climate science including error statistics, a case study on primitive and derived variables near a hurricane, evaluation of the closure of the global energy budget, and a Lagrangian air parcel trajectory simulation. This is the first time that trajectory simulation is used to benchmark compression methods. Our method concentrates most errors near zero, while others tend to distribute errors uniformly within the error bound. EBCC outperforms other methods in the benchmarks at relative error targets ranging from 0.1% to 10% and achieves compression ratios from 15x to more than 300x. In the energy budget closure and Lagrangian trajectory benchmarks, it can achieve more than 100x compression while keeping errors within natural variability derived from ERA5 uncertainty members. This verifies the effectiveness of EBCC in creating heavily compressed weather and climate datasets suitable for downstream applications. The source code of EBCC is available in github.com/spcl/EBCC.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Sensor Placement: A Correlation-Aware Attribution Framework (CAAF) for Real-world Data Modeling</title>
<link>https://arxiv.org/abs/2510.22517</link>
<guid>https://arxiv.org/abs/2510.22517</guid>
<content:encoded><![CDATA[
<div> sensor placement, machine learning, feature attribution, optimal, real-world

Summary:
The article introduces a Correlation-Aware Attribution Framework (CAAF) for optimal sensor placement (OSP) in complex systems. OSP is crucial for accurate monitoring and control. CAAF uses machine learning to identify OSP by quantifying input contributions through feature attribution. It addresses challenges of highly correlated input data by introducing a clustering step before feature attribution to reduce redundancy and improve generalizability. The framework is validated through various cases and applied to real-world systems like structural health monitoring and airfoil lift prediction. Results demonstrate the effectiveness of CAAF in identifying OSP in systems with nonlinear dynamics, chaotic behavior, and multi-scale interactions. The proposed framework outperforms traditional approaches and enables efficient application of feature attribution in real-world environments.<br /><br />Summary: <div>
arXiv:2510.22517v1 Announce Type: new 
Abstract: Optimal sensor placement (OSP) is critical for efficient, accurate monitoring, control, and inference in complex real-world systems. We propose a machine-learning-based feature attribution framework to identify OSP for the prediction of quantities of interest. Feature attribution quantifies input contributions to a model's output; however, it struggles with highly correlated input data often encountered in real-world applications. To address this, we propose a Correlation-Aware Attribution Framework (CAAF), which introduces a clustering step before performing feature attribution to reduce redundancy and enhance generalizability. We first illustrate the core principles of the proposed framework through a series of validation cases, then demonstrate its effectiveness in real-world dynamical systems, such as structural health monitoring, airfoil lift prediction, and wall-normal velocity estimation for turbulent channel flow. The results show that the CAAF outperforms alternative approaches that typically struggle due to the presence of nonlinear dynamics, chaotic behavior, and multi-scale interactions, and enables the effective application of feature attribution for identifying OSP in real-world environments.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capsule Network-Based Multimodal Fusion for Mortgage Risk Assessment from Unstructured Data Sources</title>
<link>https://arxiv.org/abs/2510.22987</link>
<guid>https://arxiv.org/abs/2510.22987</guid>
<content:encoded><![CDATA[
<div> deep learning; multimodal fusion; mortgage risk assessment; interpretability; sentiment analysis

Summary:
The study proposes a novel multimodal deep learning framework for mortgage risk assessment using publicly available unstructured data sources. The framework leverages BERT for text, VGG for images, and a multilayer perceptron for sentiment-based features in the unimodal phase. In the fusion phase, FusionCapsNet, a capsule-based fusion network, integrates modalities while preserving contextual information. Sentiment analysis across news categories and GradCAM-based visualizations are key components for interpretability. Results show that the FusionCapsNet framework outperforms individual models and benchmark fusion strategies, enhancing both predictive accuracy and interpretability for mortgage risk assessment. <div>
arXiv:2510.22987v1 Announce Type: new 
Abstract: Mortgage risk assessment traditionally relies on structured financial data, which is often proprietary, confidential, and costly. In this study, we propose a novel multimodal deep learning framework that uses cost-free, publicly available, unstructured data sources, including textual information, images, and sentiment scores, to generate credit scores that approximate commercial scorecards. Our framework adopts a two-phase approach. In the unimodal phase, we identify the best-performing models for each modality, i.e. BERT for text, VGG for image data, and a multilayer perceptron for sentiment-based features. In the fusion phase, we introduce the capsule-based fusion network (FusionCapsNet), a novel fusion strategy inspired by capsule networks, but fundamentally redesigned for multimodal integration. Unlike standard capsule networks, our method adapts a specific mechanism in capsule networks to each modality and restructures the fusion process to preserve spatial, contextual, and modality-specific information. It also enables adaptive weighting so that stronger modalities dominate without ignoring complementary signals.
  Our framework incorporates sentiment analysis across distinct news categories to capture borrower and market dynamics and employs GradCAM-based visualizations as an interpretability tool. These components are designed features of the framework, while our results later demonstrate that they effectively enrich contextual understanding and highlight the influential factors driving mortgage risk predictions. Our results show that our multimodal FusionCapsNet framework not only exceeds individual unimodal models but also outperforms benchmark fusion strategies such as addition, concatenation, and cross attention in terms of AUC, partial AUC, and F1 score, demonstrating clear gains in both predictive accuracy and interpretability for mortgage risk assessment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P1GPT: a multi-agent LLM workflow module for multi-modal financial information analysis</title>
<link>https://arxiv.org/abs/2510.23032</link>
<guid>https://arxiv.org/abs/2510.23032</guid>
<content:encoded><![CDATA[
<div> framework, multi-agent, financial analysis, reasoning workflow, interpretability 

Summary: 
The article introduces P1GPT, a new multi-agent framework for financial analysis that incorporates various data modalities. Unlike existing systems, P1GPT focuses on structured reasoning and communication among agents to provide interpretable trading decision support. Through backtesting on U.S. equities data, P1GPT demonstrates superior cumulative returns, low drawdowns, and transparent causal rationales. The framework systematically integrates technical, fundamental, and news-based insights, showcasing the effectiveness of structured reasoning workflows in achieving explainable and trustworthy financial AI systems. <div>
arXiv:2510.23032v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled multi-agent reasoning systems capable of collaborative decision-making. However, in financial analysis, most frameworks remain narrowly focused on either isolated single-agent predictors or loosely connected analyst ensembles, and they lack a coherent reasoning workflow that unifies diverse data modalities. We introduce P1GPT, a layered multi-agent LLM framework for multi-modal financial information analysis and interpretable trading decision support. Unlike prior systems that emulate trading teams through role simulation, P1GPT implements a structured reasoning pipeline that systematically fuses technical, fundamental, and news-based insights through coordinated agent communication and integration-time synthesis. Backtesting on multi-modal datasets across major U.S. equities demonstrates that P1GPT achieves superior cumulative and risk-adjusted returns, maintains low drawdowns, and provides transparent causal rationales. These findings suggest that structured reasoning workflows, rather than agent role imitation, offer a scalable path toward explainable and trustworthy financial AI systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroupSHAP-Guided Integration of Financial News Keywords and Technical Indicators for Stock Price Prediction</title>
<link>https://arxiv.org/abs/2510.23112</link>
<guid>https://arxiv.org/abs/2510.23112</guid>
<content:encoded><![CDATA[
<div> Keywords: FinBERT, SHAP, GroupSHAP, financial forecasting, news articles

Summary:
- Recent advances in finance-specific language models, such as FinBERT, have allowed for the quantification of public sentiment into index-based measures.
- Using explainable AI techniques like SHAP, researchers have identified influential features, but its computational cost grows exponentially with input features.
- This study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which quantifies contributions of semantically related keyword groups, reducing computational burden.
- News articles from 2015 to 2024 were embedded using FinBERT, clustered into semantic groups, and GroupSHAP was applied to measure each group's contribution to stock price movements.
- GroupSHAP variables across multiple topics were used for one-day-ahead forecasting of the S&amp;P 500 index in 2024, resulting in a 32.2% reduction in MAE and a 40.5% reduction in RMSE compared to benchmark models.
<br /><br />Summary: 
Recent advancements in finance-focused language models have enabled the translation of public sentiment into index-based measures. However, compressing complex language signals into single metrics can overlook contextual nuances. To address this limitation, this study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which identifies influential semantic groupings instead of individual tokens in financial news articles. By employing this approach, the research achieves significant improvements in predictive performance for the S&amp;P 500 index, highlighting the potential for grouped sentiment representations to enhance both interpretability and forecasting accuracy in finance. <div>
arXiv:2510.23112v1 Announce Type: new 
Abstract: Recent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability. To address this limitation, explainable AI techniques, particularly SHAP (SHapley Additive Explanations), have been employed to identify influential features. However, SHAP's computational cost grows exponentially with input features, making it impractical for large-scale text-based financial data. This study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which quantifies contributions of semantically related keyword groups rather than individual tokens, substantially reducing computational burden while preserving interpretability. We employed FinBERT to embed news articles from 2015 to 2024, clustered them into coherent semantic groups, and applied GroupSHAP to measure each group's contribution to stock price movements. The resulting group-level SHAP variables across multiple topics were used as input features for the prediction model. Empirical results from one-day-ahead forecasting of the S&amp;P 500 index throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE and a 40.5% reduction in RMSE compared with benchmark models without the GroupSHAP mechanism. This research presents the first application of GroupSHAP in news-driven financial forecasting, showing that grouped sentiment representations simultaneously enhance interpretability and predictive performance.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms</title>
<link>https://arxiv.org/abs/2510.23166</link>
<guid>https://arxiv.org/abs/2510.23166</guid>
<content:encoded><![CDATA[
<div> Machine learning, scientific machine learning, Common Task Framework, benchmarks, reproducibility<br />
<br />
Summary: 
Machine learning is revolutionizing modeling and control in various scientific fields, but the lack of standardized benchmarks hinders progress. To address this issue, a Common Task Framework (CTF) is proposed, offering curated datasets and metrics for evaluating algorithms. By benchmarking methods on nonlinear systems like Kuramoto-Sivashinsky and Lorenz, the CTF reveals strengths, limitations, and suitability for different problems. A global sea surface temperature dataset competition with a true holdout dataset will foster community engagement. The long-term goal is to replace ad hoc comparisons with standardized evaluations on hidden test sets, enhancing rigor and reproducibility in scientific machine learning. <br /><br /> <div>
arXiv:2510.23166v1 Announce Type: new 
Abstract: Machine learning (ML) is transforming modeling and control in the physical, engineering, and biological sciences. However, rapid development has outpaced the creation of standardized, objective benchmarks - leading to weak baselines, reporting bias, and inconsistent evaluations across methods. This undermines reproducibility, misguides resource allocation, and obscures scientific progress. To address this, we propose a Common Task Framework (CTF) for scientific machine learning. The CTF features a curated set of datasets and task-specific metrics spanning forecasting, state reconstruction, and generalization under realistic constraints, including noise and limited data. Inspired by the success of CTFs in fields like natural language processing and computer vision, our framework provides a structured, rigorous foundation for head-to-head evaluation of diverse algorithms. As a first step, we benchmark methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz. These results illustrate the utility of the CTF in revealing method strengths, limitations, and suitability for specific classes of problems and diverse objectives. Next, we are launching a competition around a global real world sea surface temperature dataset with a true holdout dataset to foster community engagement. Our long-term vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets that raise the bar for rigor and reproducibility in scientific ML.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRO-Based Computation Offloading and Trajectory Design for Low-Altitude Networks</title>
<link>https://arxiv.org/abs/2510.23202</link>
<guid>https://arxiv.org/abs/2510.23202</guid>
<content:encoded><![CDATA[
<div> architecture, computation offloading, UAVs, HAPs, distributionally robust optimization

Summary:<br />
An architecture for Low-altitude networks (LANs) that integrates unmanned aerial vehicles (UAVs) and high-altitude platforms (HAPs) is proposed to meet increasing computation demands. Uncertain task sizes and UAV mobility present challenges in ensuring quality of service. To tackle this, a distributionally robust optimization problem is formulated to minimize worst-case delays by optimizing offloading decisions and UAV trajectories. An algorithm is designed to solve the mixed-integer min-max optimization problem, utilizing an outer-inner layer approach. The algorithm employs Benders decomposition to optimize offloading decisions and UAV trajectories iteratively. Simulation results indicate the proposed algorithm excels over traditional methods in balancing worst-case delay and robustness. <div>
arXiv:2510.23202v1 Announce Type: new 
Abstract: The low-altitude networks (LANs) integrating unmanned aerial vehicles (UAVs) and high-altitude platforms (HAPs) have become a promising solution for the rising computation demands. However, the uncertain task sizes and high mobility of UAVs pose great challenges to guarantee the quality of service. To address these issues, we propose an LAN architecture where UAVs and HAPs collaboratively provide computation offloading for ground users. Moreover, the uncertainty sets are constructed to characterize the uncertain task size, and a distributionally robust optimization problem is formulated to minimize the worst-case delay by jointly optimizing the offloading decisions and UAV trajectories. To solve the mixed-integer min-max optimization problem, we design the distributionally robust computation offloading and trajectories optimization algorithm. Specifically, the original problem is figured out by iteratively solving the outerlayer and inner-layer problems. The convex outer-layer problem with probability distributions is solved by the optimization toolkit. As for the inner-layer mixed-integer problem, we employ the Benders decomposition. The decoupled master problem concerning the binary offloading decisions is solved by the integer solver, and UAV trajectories in the sub-problem are optimized via the successive convex approximation. Simulation results show the proposed algorithm outperforms traditional optimization methods in balancing the worst-case delay and robustness.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the PTM Code through a Coarse-to-Fine, Mechanism-Aware Framework</title>
<link>https://arxiv.org/abs/2510.23492</link>
<guid>https://arxiv.org/abs/2510.23492</guid>
<content:encoded><![CDATA[
<div> Keywords: Post-translational modifications, COMPASS-PTM, enzyme-substrate assignment, protein language models, cellular signaling.

Summary: 
The article introduces COMPASS-PTM, a novel learning framework that combines residue-level PTM profiling with enzyme-substrate assignment. By integrating evolutionary representations, physicochemical priors, and a crosstalk-aware prompting mechanism, COMPASS-PTM effectively deciphers the complex code of PTMs in cellular signaling. The model achieves state-of-the-art performance in multi-label site prediction and enzyme assignment, demonstrating a significant improvement in accuracy. It also shows interpretability by identifying kinase motifs and predicting disease-related PTM alterations due to missense variants. COMPASS-PTM bridges statistical learning with biochemical mechanisms, allowing it to learn the grammar underlying protein regulation and signaling. This unified framework provides a comprehensive understanding of PTMs, shedding light on the intricate regulatory mechanisms within cells.<br /><br />Summary: <div>
arXiv:2510.23492v1 Announce Type: new 
Abstract: Post-translational modifications (PTMs) form a combinatorial "code" that regulates protein function, yet deciphering this code - linking modified sites to their catalytic enzymes - remains a central unsolved problem in understanding cellular signaling and disease. We introduce COMPASS-PTM, a mechanism-aware, coarse-to-fine learning framework that unifies residue-level PTM profiling with enzyme-substrate assignment. COMPASS-PTM integrates evolutionary representations from protein language models with physicochemical priors and a crosstalk-aware prompting mechanism that explicitly models inter-PTM dependencies. This design allows the model to learn biologically coherent patterns of cooperative and antagonistic modifications while addressing the dual long-tail distribution of PTM data. Across multiple proteome-scale benchmarks, COMPASS-PTM establishes new state-of-the-art performance, including a 122% relative F1 improvement in multi-label site prediction and a 54% gain in zero-shot enzyme assignment. Beyond accuracy, the model demonstrates interpretable generalization, recovering canonical kinase motifs and predicting disease-associated PTM rewiring caused by missense variants. By bridging statistical learning with biochemical mechanism, COMPASS-PTM unifies site-level and enzyme-level prediction into a single framework that learns the grammar underlying protein regulation and signaling.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vertex and front-tracking methods for the modeling of microstructure evolution at the solid state: a brief review</title>
<link>https://arxiv.org/abs/2510.21818</link>
<guid>https://arxiv.org/abs/2510.21818</guid>
<content:encoded><![CDATA[
<div> Front-Capturing, Front-Tracking, interface properties, mesoscopic scale, microstructure evolution<br />
Summary:<br />
In mesoscopic scale microstructure evolution modeling, two main numerical frameworks are used: Front-Capturing (FC) and Front-Tracking (FT). FC models indirectly define interfaces by tracking field variable changes, while FT models explicitly define interfaces using interconnected segments or surfaces. FT-type approaches, associated with Lagrangian movement, provide enhanced spatial resolution in 3D and 2D problems. They efficiently handle physical mechanisms related to interface properties and geometries but face challenges with complex topological events in 3D. Recent advancements show their potential in computational efficiency and analyzing mobility and energy properties for potential applications in intragranular phenomena. <div>
arXiv:2510.21818v1 Announce Type: cross 
Abstract: In mesoscopic scale microstructure evolution modeling, two primary numerical frameworks are used: Front-Capturing (FC) and Front-Tracking (FT) ones. FC models, like phase-field or level-set methods, indirectly define interfaces by tracking field variable changes. On the contrary, FT models explicitly define interfaces using interconnected segments or surfaces. In historical FT methodologies, Vertex models were first developed and consider the description of the evolution of polygonal structures in terms of the motion of points where multiple boundaries meet. Globally, FT-type approaches, often associated with Lagrangian movement, enhance spatial resolution in 3D surfacic and 2D lineic problems using techniques derived from finite element meshing and remeshing algorithms. These efficient approaches, by nature, are well adapted to physical mechanisms correlated to interface properties and geometries. They also face challenges in managing complex topological events, especially in 3D. However, recent advances highlight their potential in computational efficiency and analysis of mobility and energy properties, with possible applications in intragranular phenomena.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreditXAI: A Multi-Agent System for Explainable Corporate Credit Rating</title>
<link>https://arxiv.org/abs/2510.22222</link>
<guid>https://arxiv.org/abs/2510.22222</guid>
<content:encoded><![CDATA[
<div> Keywords: corporate credit rating, deep learning, interpretability, Multi-Agent System, credit risk assessment

Summary:
The study introduces CreditXAI, a Multi-Agent System framework that mimics collaborative decision-making in credit analysis. Traditional deep learning methods in corporate credit rating have limitations in interpretability and 'black-box' issues. By incorporating business, financial, and governance risk dimensions, CreditXAI enhances interpretability and accuracy in credit assessments. Experimental results show a 7% improvement in predictive accuracy compared to single-agent models. The framework offers a new approach to building intelligent and transparent credit rating models, addressing the shortcomings of existing methods. <div>
arXiv:2510.22222v1 Announce Type: cross 
Abstract: In the domain of corporate credit rating, traditional deep learning methods have improved predictive accuracy but still suffer from the inherent 'black-box' problem and limited interpretability. While incorporating non-financial information enriches the data and provides partial interpretability, the models still lack hierarchical reasoning mechanisms, limiting their comprehensive analytical capabilities. To address these challenges, we propose CreditXAI, a Multi-Agent System (MAS) framework that simulates the collaborative decision-making process of professional credit analysts. The framework focuses on business, financial, and governance risk dimensions to generate consistent and interpretable credit assessments. Experimental results demonstrate that multi-agent collaboration improves predictive accuracy by more than 7% over the best single-agent baseline, confirming its significant synergistic advantage in corporate credit risk evaluation. This study provides a new technical pathway to build intelligent and interpretable credit rating models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAMP: Data-Efficient Linear Affine Weight-Space Models for Parameter-Controlled 3D Shape Generation and Extrapolation</title>
<link>https://arxiv.org/abs/2510.22491</link>
<guid>https://arxiv.org/abs/2510.22491</guid>
<content:encoded><![CDATA[
<div> align, signed distance function, generation, data-efficient, controllable

Summary: 
LAMP (Linear Affine Mixing of Parametric shapes) is introduced as a data-efficient framework for controllable and interpretable 3D generation, with a focus on generating high-fidelity 3D geometries that satisfy specific parameter constraints. The approach aligns signed distance function (SDF) decoders to overfit exemplars from a shared initialization and synthesizes new geometries by solving a parameter-constrained mixing problem in the aligned weight space. A safety metric is also proposed to detect geometry validity via linearity mismatch. LAMP enables controlled interpolation within bounds with a limited number of samples, safe extrapolation beyond training ranges by up to 100% parameter difference, and physics performance-guided optimization under fixed parameters. The results show LAMP outperforms conditional autoencoder and Deep Network Interpolation (DNI) baselines in both extrapolation and data efficiency, showcasing its advancements in controllable, data-efficient, and safe 3D generation for various applications in design exploration, dataset generation, and performance-driven optimization. <div>
arXiv:2510.22491v1 Announce Type: cross 
Abstract: Generating high-fidelity 3D geometries that satisfy specific parameter constraints has broad applications in design and engineering. However, current methods typically rely on large training datasets and struggle with controllability and generalization beyond the training distributions. To overcome these limitations, we introduce LAMP (Linear Affine Mixing of Parametric shapes), a data-efficient framework for controllable and interpretable 3D generation. LAMP first aligns signed distance function (SDF) decoders by overfitting each exemplar from a shared initialization, then synthesizes new geometries by solving a parameter-constrained mixing problem in the aligned weight space. To ensure robustness, we further propose a safety metric that detects geometry validity via linearity mismatch. We evaluate LAMP on two 3D parametric benchmarks: DrivAerNet++ and BlendedNet. We found that LAMP enables (i) controlled interpolation within bounds with as few as 100 samples, (ii) safe extrapolation by up to 100% parameter difference beyond training ranges, (iii) physics performance-guided optimization under fixed parameters. LAMP significantly outperforms conditional autoencoder and Deep Network Interpolation (DNI) baselines in both extrapolation and data efficiency. Our results demonstrate that LAMP advances controllable, data-efficient, and safe 3D generation for design exploration, dataset generation, and performance-driven optimization.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization</title>
<link>https://arxiv.org/abs/2510.22839</link>
<guid>https://arxiv.org/abs/2510.22839</guid>
<content:encoded><![CDATA[
<div> Framework, Structural parameters, Optimization, Genetic Algorithm, Machine learning<br />
<br />
Summary: 
This study proposes a hybrid data-driven framework that combines a Graph Neural Network (GNN) surrogate model with a Genetic Algorithm (GA) optimizer for optimizing structural parameters in design. The GNN is trained to predict dynamic displacement responses based on mass, stiffness, and damping configurations, reducing computational cost compared to conventional simulations. A dataset of single-degree-of-freedom system responses is used to train the GNN, which enables rapid predictions without solving the system equations repeatedly. The GA then searches for globally optimal parameter sets by minimizing displacements and enhancing dynamic stability. The framework demonstrates strong convergence, robust generalization, and efficient optimization, showcasing the potential of combining machine learning surrogates with evolutionary algorithms for intelligent structural design. <div>
arXiv:2510.22839v1 Announce Type: cross 
Abstract: The optimization of structural parameters, such as mass(m), stiffness(k), and damping coefficient(c), is critical for designing efficient, resilient, and stable structures. Conventional numerical approaches, including Finite Element Method (FEM) and Computational Fluid Dynamics (CFD) simulations, provide high-fidelity results but are computationally expensive for iterative optimization tasks, as each evaluation requires solving the governing equations for every parameter combination. This study proposes a hybrid data-driven framework that integrates a Graph Neural Network (GNN) surrogate model with a Genetic Algorithm (GA) optimizer to overcome these challenges. The GNN is trained to accurately learn the nonlinear mapping between structural parameters and dynamic displacement responses, enabling rapid predictions without repeatedly solving the system equations. A dataset of single-degree-of-freedom (SDOF) system responses is generated using the Newmark Beta method across diverse mass, stiffness, and damping configurations. The GA then searches for globally optimal parameter sets by minimizing predicted displacements and enhancing dynamic stability. Results demonstrate that the GNN and GA framework achieves strong convergence, robust generalization, and significantly reduced computational cost compared to conventional simulations. This approach highlights the effectiveness of combining machine learning surrogates with evolutionary optimization for automated and intelligent structural design.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Generalizable AI for Materials Discovery: Validation through Immersion Coolant Screening</title>
<link>https://arxiv.org/abs/2510.23371</link>
<guid>https://arxiv.org/abs/2510.23371</guid>
<content:encoded><![CDATA[
<div> Framework, AI, Materials discovery, Generalizable, GATE

Summary:
- GATE is a generalizable AI framework that can learn multiple physicochemical properties simultaneously.
- It aligns these properties in a shared geometric space to capture cross-property correlations and reduce bias in multi-criteria screening.
- GATE was applied to the discovery of immersion cooling fluids for data centers without reconfiguration and screened billions of candidates to identify promising molecules.
- Experimentally validated results showed agreement with wet-lab measurements and comparable or superior performance to a commercial coolant.
- GATE is established as a versatile AI platform applicable across various materials discovery tasks. 

<br /><br />Summary: <div>
arXiv:2510.23371v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has emerged as a powerful accelerator of materials discovery, yet most existing models remain problem-specific, requiring additional data collection and retraining for each new property. Here we introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a generalizable AI framework that jointly learns 34 physicochemical properties spanning thermal, electrical, mechanical, and optical domains. By aligning these properties within a shared geometric space, GATE captures cross-property correlations that reduce disjoint-property bias -- a key factor causing false negatives in multi-criteria screening. To demonstrate its generalizability, GATE -- without any problem-specific reconfiguration -- was directly applied to the discovery of immersion cooling fluids for data centers, a stringent real-world challenge defined by the Open Compute Project (OCP). Screening billions of candidates, GATE identified 92,861 molecules as promising for practical deployment. Four were experimentally or literarily validated, showing strong agreement with wet-lab measurements and performance comparable to or exceeding a commercial coolant. These results establish GATE as a ready-to-use, generalizable AI platform readily applicable across diverse materials discovery tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree-Cotree-Based IETI-DP for Eddy Current Problems in Time-Domain</title>
<link>https://arxiv.org/abs/2510.23446</link>
<guid>https://arxiv.org/abs/2510.23446</guid>
<content:encoded><![CDATA[
<div> Keywords: low-frequency electromagnetic problems, eddy current formulations, time-domain simulations, tearing and interconnecting approach, scalability

Summary:
For low-frequency electromagnetic problems, eddy current formulations are commonly used for simplification. However, time-domain simulations can be computationally expensive. This study introduces a tearing and interconnecting approach for eddy currents in the time domain to improve efficiency and scalability. Wave-propagation effects are neglected in this setup, making it suitable for capturing transient startup responses and nonlinear behavior. The proposed method aims to reduce computational costs while maintaining accuracy in simulations. Overall, this approach offers a promising solution for tackling time-domain simulations of eddy current problems in electromagnetic analysis. <div>
arXiv:2510.23446v1 Announce Type: cross 
Abstract: For low-frequency electromagnetic problems, where wave-propagation effects can be neglected, eddy current formulations are commonly used as a simplification of the full Maxwell's equations. In this setup, time-domain simulations, needed to capture transient startup responses or nonlinear behavior, are often computationally expensive. We propose a novel tearing and interconnecting approach for eddy currents in time-domain and investigate its scalability.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error estimation and step size control with minimal subsystem interfaces</title>
<link>https://arxiv.org/abs/2406.17353</link>
<guid>https://arxiv.org/abs/2406.17353</guid>
<content:encoded><![CDATA[
<div> methods, co-simulation, error estimation, black-box subsystems, step size control <br />
Summary: 
This article reviews error estimation methods for co-simulation, focusing on scenarios where subsystems have minimal interfaces. These "black-box" subsystems lack time step rollback capabilities and provide limited internal information, presenting challenges for integration in large-system simulations. The article discusses using error indicators to automatically adjust macro time step sizes for optimal simulation speed and accuracy. It includes pseudocode for implementing the step size control algorithm and offers practical advice on assessing co-simulation quality, avoiding common errors, and configuring the control algorithm effectively. By highlighting the nuances of error estimation in co-simulation, the article aims to guide readers in implementing and testing these methods in their industrial applications. <div>
arXiv:2406.17353v2 Announce Type: replace 
Abstract: We review error estimation methods for co-simulation, in particular methods that are applicable when the subsystems provide minimal interfaces. By this, we mean that subsystems do not support rollback of time steps, do not output derivatives, and do not provide any other information about their internals besides the output variables that are required for coupling with other subsystems. Such "black-box" subsystems are common in industrial applications, and the ability to couple them and run large-system simulations is one of the major attractions of the co-simulation paradigm. We also describe how the resulting error indicators may be used to automatically control macro time step sizes to strike a good balance between simulation speed and accuracy. The various elements of the step size control algorithm are presented in pseudocode so that readers may implement them and test them in their own applications. We provide practicable advice on how to use error indicators to judge the quality of a co-simulation, how to avoid common pitfalls, and how to configure the step size control algorithm.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional Hierarchical Deep Learning Neural Networks-Tensor Decomposition (C-HiDeNN-TD): a scalable surrogate modeling approach for large-scale physical systems</title>
<link>https://arxiv.org/abs/2409.00329</link>
<guid>https://arxiv.org/abs/2409.00329</guid>
<content:encoded><![CDATA[
<div> artificial intelligence, deep learning, neural network, tensor decomposition, large-scale systems

Summary:<br />
- Simulation-driven engineering applications are becoming increasingly large and complex, leading to significant computational time and memory cost issues with classical numerical methods.
- Data-driven surrogates have been explored to accelerate PDE solvers, but often require extensive training data.
- The C-HiDeNN-TD method is proposed in this paper, which can directly obtain surrogate models for large-scale space-time PDE without the need for offline training data.
- The performance of this method is compared to classical numerical methods for extremely large-scale systems.
- The C-HiDeNN-TD method shows promise in providing efficient solutions for complex PDE problems without the need for massive amounts of training data. 

<br /><br />Summary: <div>
arXiv:2409.00329v2 Announce Type: replace 
Abstract: A common trend in simulation-driven engineering applications is the ever-increasing size and complexity of the problem, where classical numerical methods typically suffer from significant computational time and huge memory cost. Methods based on artificial intelligence have been extensively investigated to accelerate partial differential equations (PDE) solvers using data-driven surrogates. However, most data-driven surrogates require an extremely large amount of training data. In this paper, we propose the Convolutional Hierarchical Deep Learning Neural Network-Tensor Decomposition (C-HiDeNN-TD) method, which can directly obtain surrogate models by solving large-scale space-time PDE without generating any offline training data. We compare the performance of the proposed method against classical numerical methods for extremely large-scale systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Magneto-thermally Coupled Field Simulation of Homogenized Foil Winding Models</title>
<link>https://arxiv.org/abs/2503.13010</link>
<guid>https://arxiv.org/abs/2503.13010</guid>
<content:encoded><![CDATA[
<div> homogenization technique, foil windings, high frequency applications, magneto-thermal simulation, Joule losses<br />
Summary:
Homogenization techniques are used in the simulation of foil windings to capture their unique properties for high frequency applications. The layered structure of foil windings results in different electromagnetic and thermal characteristics compared to traditional wire windings. A coupled magneto-thermal simulation approach is employed, considering the interaction between electromagnetic and thermal effects through Joule losses and temperature-dependent material properties. Utilizing different time step sizes for electromagnetic and thermal simulations allows for efficient analysis of foil windings. The method is validated using a simple geometry and applied to a pot transformer with both foil and wire windings, demonstrating its effectiveness in capturing the behavior of complex winding structures. <br /><br />Summary: <div>
arXiv:2503.13010v2 Announce Type: replace 
Abstract: Foil windings have, due to their layered structure, different properties than conventional wire windings, which make them advantageous for high frequency applications. Both electromagnetic and thermal analyses are relevant for foil windings. These two physical areas are coupled through Joule losses and temperature dependent material properties. For an efficient simulation of foil windings, homogenization techniques are used to avoid resolving the single turns. Therefore, this paper comprises a coupled magneto-thermal simulation that uses a homogenization method in the electromagnetic and thermal part. A weak coupling with different time step sizes for both parts is presented. The method is verified on a simple geometry and showcased for a pot transformer that uses a foil and a wire winding.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cohort-attention Evaluation Metric against Tied Data: Studying Performance of Classification Models in Cancer Detection</title>
<link>https://arxiv.org/abs/2503.12755</link>
<guid>https://arxiv.org/abs/2503.12755</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, medical screening, imbalanced data, fairness, evaluation metrics

Summary:
The article discusses the limitations of traditional classification metrics in evaluating the accuracy of AI-driven medical screening models, particularly in handling imbalanced data and varying performance across different patient cohorts. To address these challenges, the authors propose the Cohort-Attention Evaluation Metrics (CAT) framework. This framework introduces patient-level assessment, entropy-based distribution weighting, and cohort-weighted sensitivity and specificity metrics to ensure fair and balanced evaluations across diverse populations. Key metrics such as CATSensitivity (CATSen), CATSpecificity (CATSpe), and CATMean are introduced to enhance predictive reliability and interpretability of AI models in medical screening. By incorporating these metrics, the CAT framework aims to provide a more robust evaluation method that takes into account patient-level inconsistencies and ensures fairness and reliability in assessing the performance of AI-driven medical screening models. <div>
arXiv:2503.12755v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) has significantly improved medical screening accuracy, particularly in cancer detection and risk assessment. However, traditional classification metrics often fail to account for imbalanced data, varying performance across cohorts, and patient-level inconsistencies, leading to biased evaluations. We propose the Cohort-Attention Evaluation Metrics (CAT) framework to address these challenges. CAT introduces patient-level assessment, entropy-based distribution weighting, and cohort-weighted sensitivity and specificity. Key metrics like CATSensitivity (CATSen), CATSpecificity (CATSpe), and CATMean ensure balanced and fair evaluation across diverse populations. This approach enhances predictive reliability, fairness, and interpretability, providing a robust evaluation method for AI-driven medical screening models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive Alignment</title>
<link>https://arxiv.org/abs/2505.11194</link>
<guid>https://arxiv.org/abs/2505.11194</guid>
<content:encoded><![CDATA[
<div> Keywords: protein function, sequence-to-text model, natural language generation, multimodal learning, contrastive alignment

Summary:
Prot2Text-V2 is a novel multimodal sequence-to-text model that generates natural language descriptions of protein function from amino acid sequences. It combines a protein language model and a decoder-only language model through a modality projector, and includes Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE) to enhance cross-modal learning. The model is trained on curated entries from SwissProt and excels in low-homology conditions, outperforming traditional and LLM-based baselines. The innovative approach of Prot2Text-V2 showcases its ability to generate accurate protein function descriptions directly from protein sequences, showcasing the potential of multimodal learning in predicting protein function.<br /><br />Summary: <div>
arXiv:2505.11194v3 Announce Type: replace 
Abstract: Predicting protein function from sequence is a central challenge in computational biology. While existing methods rely heavily on structured ontologies or similarity-based techniques, they often lack the flexibility to express structure-free functional descriptions and novel biological functions. In this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text model that generates free-form natural language descriptions of protein function directly from amino acid sequences. Our method combines a protein language model as a sequence encoder (ESM-3B) and a decoder-only language model (LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A key innovation is our Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE), which improves cross-modal learning by matching mean- and std-pooled protein embeddings with text representations via contrastive loss. After the alignment phase, we apply instruction-based fine-tuning using LoRA on the decoder to teach the model how to generate accurate protein function descriptions conditioned on the protein sequence. We train Prot2Text-V2 on about 250K curated entries from SwissProt and evaluate it under low-homology conditions, where test sequences have low similarity with training samples. Prot2Text-V2 consistently outperforms traditional and LLM-based baselines across various metrics.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated physics-informed learning and resonance process signature for the prediction of fatigue crack growth for laser-fused alloys</title>
<link>https://arxiv.org/abs/2510.21018</link>
<guid>https://arxiv.org/abs/2510.21018</guid>
<content:encoded><![CDATA[
<div> machine learning, fatigue crack growth, laser-fused components, SS-316L, physics-informed modeling

Summary: 
This study addresses the challenges in predicting fatigue crack growth of laser-fused SS-316L components with random geometrical defects. Traditional statistics-based models struggle to account for the scattering in fatigue behaviors caused by these defects. The proposed approach, a physics-informed machine learning (PIML) model, integrates fatigue laws and constraints with small data to provide a realistic and interpretable prediction. By leveraging resonance process signature data and Paris's law, the model successfully learns the constants and predicts crack growth rates without the need for experimental crack growth data. The results demonstrate that the model can accurately predict crack sizes and provide insights into the fatigue behavior of laser-fused materials. The combination of machine learning and physics-based modeling offers a promising approach for predicting fatigue crack growth in metal components with inherent scattering. <br /><br />Summary: <div>
arXiv:2510.21018v1 Announce Type: new 
Abstract: Fatigue behaviors of metal components by laser fusion suffer from scattering due to random geometrical defects (e.g., porosity, lack of fusion). Monitoring fatigue crack initiation and growth is critical, especially for laser-fused components with significant inherent fatigue scattering. Conventional statistics-based curve-fitting fatigue models have difficulty incorporating significant scattering in their fatigue life due to the random geometrical defects. A scattering-informed predictive method is needed for laser-fused materials' crack size and growth. Current data-driven machine learning could circumvent the issue of deterministic modeling, but results in a black-box function that lacks interpretability. To address these challenges, this study explores a novel nondimensionalized physics-informed machine learning (PIML) model to predict fatigue crack growth of laser-fused SS-316L by integrating fatigue laws and constraints with small data to ensure a realistic and interpretable prediction. Resonance process signature data were leveraged with Paris's law to train the PIML model without experimental crack growth data. The results show that Paris's law constants can be learned with good similarity to comparable data from the literature, and the crack growth rate can be predicted to compute crack sizes.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linked Cell Traversal Algorithms for Three-Body Interactions in Molecular Dynamics</title>
<link>https://arxiv.org/abs/2510.21230</link>
<guid>https://arxiv.org/abs/2510.21230</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular dynamics, three-body interactions, parallel computation, linked cells, Lennard-Jones fluid <br />
Summary: 
This work focuses on developing parallel computation algorithms for three-body interactions in molecular dynamics. The algorithms extend existing traversals for pair interactions to handle interactions between molecules stored across three cells. A general framework for computing three-body interactions in linked cells is described and implemented. The analysis includes considering cutoff conditions, which affect the workload of interaction computations. The algorithms are validated using the Lennard-Jones fluid, with case studies configured for homogeneous and inhomogeneous scenarios. The study evaluates strong scalability and performance in terms of molecule updates at the node level. The results demonstrate the effectiveness and efficiency of the developed algorithms for handling three-body interactions in molecular dynamics. <br /><br />Summary: <div>
arXiv:2510.21230v1 Announce Type: new 
Abstract: In this work, algorithms for the parallel computation of three-body interactions in molecular dynamics are developed. While traversals for the computation of pair interactions are readily available in the literature, here, such traversals are extended to allow for the computation between molecules stored across three cells. A general framework for the computation of three-body interactions in linked cells is described, and then used to implement the corresponding traversals. In addition, our analysis is combined with the commonly used cutoff conditions, because they influence the total workload of the computation of interactions. The combinations between traversals and truncation conditions are validated using the well-known Lennard-Jones fluid. Validation case studies are taken from the literature and configured into homogeneous and inhomogeneous scenarios. Finally, strong scalability and performance in terms of molecule updates are measured at node-level.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crisis-Resilient Portfolio Management via Graph-based Spatio-Temporal Learning</title>
<link>https://arxiv.org/abs/2510.20868</link>
<guid>https://arxiv.org/abs/2510.20868</guid>
<content:encoded><![CDATA[
<div> Keywords: financial time series forecasting, graph-based spatio-temporal learning, crisis-resilient investment, regime-specific predictions, adaptive portfolio allocation

Summary:
CRISP, a novel framework for financial time series forecasting, addresses the challenge of predicting optimal asset allocations during crisis periods. Unlike traditional methods, CRISP adapts to shifting market dynamics by leveraging Graph Convolutional Networks and BiLSTM with self-attention to learn sparse structures through Graph Attention Networks. By filtering out noise and capturing crisis-relevant dependencies, CRISP achieves accurate regime-specific predictions. Trained on data spanning credit and pandemic crises, CRISP demonstrates robust generalization to inflation-driven markets. This adaptive approach enables portfolio allocation that maintains profitability during downturns, outperforming equal-weight baselines and static graph methods. The learned attention weights provide interpretable regime detection, with defensive cluster attention strengthening significantly during crises, reflecting emergent behavior from learning to forecast rather than imposing assumptions.

<br /><br />Summary: <div>
arXiv:2510.20868v1 Announce Type: cross 
Abstract: Financial time series forecasting faces a fundamental challenge: predicting optimal asset allocations requires understanding regime-dependent correlation structures that transform during crisis periods. Existing graph-based spatio-temporal learning approaches rely on predetermined graph topologies--correlation thresholds, sector classifications--that fail to adapt when market dynamics shift across different crisis mechanisms: credit contagion, pandemic shocks, or inflation-driven selloffs.
  We present CRISP (Crisis-Resilient Investment through Spatio-temporal Patterns), a graph-based spatio-temporal learning framework that encodes spatial relationships via Graph Convolutional Networks and temporal dynamics via BiLSTM with self-attention, then learns sparse structures through multi-head Graph Attention Networks. Unlike fixed-topology methods, CRISP discovers which asset relationships matter through attention mechanisms, filtering 92.5% of connections as noise while preserving crisis-relevant dependencies for accurate regime-specific predictions.
  Trained on 2005--2021 data encompassing credit and pandemic crises, CRISP demonstrates robust generalization to 2022--2024 inflation-driven markets--a fundamentally different regime--by accurately forecasting regime-appropriate correlation structures. This enables adaptive portfolio allocation that maintains profitability during downturns, achieving Sharpe ratio 3.76: 707% improvement over equal-weight baselines and 94% improvement over static graph methods. Learned attention weights provide interpretable regime detection, with defensive cluster attention strengthening 49% during crises versus 31% market-wide--emergent behavior from learning to forecast rather than imposing assumptions.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Structural Validation of a Micro-UAV with On-Board Dynamic Route Planning</title>
<link>https://arxiv.org/abs/2510.21648</link>
<guid>https://arxiv.org/abs/2510.21648</guid>
<content:encoded><![CDATA[
<div> Keywords: Micro aerial vehicles, search and rescue operations, lightweight drone, structural durability, adaptive navigation<br />
Summary:<br />
Micro aerial vehicles are vital for search and rescue missions due to their agility and accessibility to hazardous areas. However, many low-cost aerial systems lack structural durability and dynamic path replanning capabilities. This study introduces a custom-built drone that addresses these limitations by using readily available components and materials, focusing on modularity, affordability, and ease of assembly. The drone's frame is reinforced for impact resistance, while the control system operates on free open-source software, enabling real-time perception and adaptive navigation without costly hardware accelerators. This approach offers a practical and cost-effective solution for search and rescue missions, showcasing the importance of lightweight yet durable drone design for navigating rough terrains and responding dynamically to new obstacles or victims.<br /><br />Summary: <div>
arXiv:2510.21648v1 Announce Type: cross 
Abstract: Micro aerial vehicles are becoming increasingly important in search and rescue operations due to their agility, speed, and ability to access confined spaces or hazardous areas. However, designing lightweight aerial systems presents significant structural, aerodynamic, and computational challenges. This work addresses two key limitations in many low-cost aerial systems under two kilograms: their lack of structural durability during flight through rough terrains and inability to replan paths dynamically when new victims or obstacles are detected. We present a fully customised drone built from scratch using only commonly available components and materials, emphasising modularity, low cost, and ease of assembly. The structural frame is reinforced with lightweight yet durable materials to withstand impact, while the onboard control system is powered entirely by free, open-source software solutions. The proposed system demonstrates real-time perception and adaptive navigation capabilities without relying on expensive hardware accelerators, offering an affordable and practical solution for real-world search and rescue missions.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A hybrid framework integrating classical computers and quantum annealers for optimisation of truss structures</title>
<link>https://arxiv.org/abs/2502.19570</link>
<guid>https://arxiv.org/abs/2502.19570</guid>
<content:encoded><![CDATA[
<div> optimisation, quantum computing, hybrid framework, structural engineering, truss design

Summary:
The proposed work introduces a hybrid framework that combines classical computers with quantum annealers for structural optimization. This approach involves formulating two minimization problems at each iteration: one for the mechanical boundary value problem and one for updating the design variables. By leveraging quantum computing, specifically quantum annealing-assisted sequential programming, the framework can efficiently solve these minimization problems. Several case studies on truss optimization demonstrate the framework's effectiveness in utilizing quantum computers for optimization tasks. The framework shows promise for future structural optimization applications, especially in scenarios where classical computers face limitations due to problem complexities. This advancement opens up new possibilities in the field of structural engineering by harnessing the power of quantum computing for design optimization. 

<br /><br />Summary: <div>
arXiv:2502.19570v2 Announce Type: replace 
Abstract: This work proposes a hybrid framework combining classical computers with quantum annealers for structural optimisation. At each optimisation iteration of an iterative process, two minimisation problems are formulated one for the underlying mechanical boundary value problem through the minimisation potential energy principle and one for the minimisation problem to update the design variables. Our hybrid approach leverages the strength of quantum computing to solve these two minimisation problems at each step, thanks to the developed quantum annealing-assisted sequential programming strategy introduced in [Nguyen, Wu, Remacle, and Noels. A quantum annealing-sequential quadratic programming assisted finite element simulation for non-linear and history-dependent mechanical problems. European Journal of Mechanics-A/Solids 105 (2024): 105254]. The applicability of the proposed framework is demonstrated through several case studies of truss optimisation, highlighting its capability to perform optimisation with quantum computers. The proposed framework offers a promising direction for future structural optimisation applications, particularly in scenarios where the quantum computer could resolve the size limitations of the classical computers due to problem complexities.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VENI, VINDy, VICI: a generative reduced-order modeling framework with uncertainty quantification</title>
<link>https://arxiv.org/abs/2405.20905</link>
<guid>https://arxiv.org/abs/2405.20905</guid>
<content:encoded><![CDATA[
<div> machine learning, reduced-order models, variational autoencoders, sparse identification of nonlinear dynamics, uncertainty quantification

Summary:
The article proposes a data-driven framework for building interpretable reduced-order models (ROMs) to solve complex systems of partial differential equations. The framework combines variational autoencoders for dimensionality reduction and a new method called Variational Identification of Nonlinear Dynamics (VINDy) to identify latent variables and dynamics in an interpretable manner. The method, named VENI, VINDy, VICI, uses Variational Encoding of Noisy Inputs (VENI) to determine reduced coordinates' distribution and VINDy to learn the coefficients of candidate functions. The trained model allows for querying new parameter instances and initial conditions to compute full-time solutions. The probabilistic setup enables uncertainty quantification through Variational Inference, providing Certainty Intervals (VICI) during online testing. The effectiveness of the VINDy method is demonstrated on the Roessler system under different noise intensities, followed by testing on PDE benchmarks in structural mechanics and fluid dynamics. <div>
arXiv:2405.20905v2 Announce Type: replace-cross 
Abstract: The simulation of many complex phenomena in engineering and science requires solving expensive, high-dimensional systems of partial differential equations (PDEs). To circumvent this, reduced-order models (ROMs) have been developed to speed up computations. However, when governing equations are unknown or partially known, typically ROMs lack interpretability and reliability of the predicted solutions.
  In this work we present a data-driven, non-intrusive framework for building ROMs where the latent variables and dynamics are identified in an interpretable manner and uncertainty is quantified. Starting from a limited amount of high-dimensional, noisy data the proposed framework constructs an efficient ROM by leveraging variational autoencoders for dimensionality reduction along with a newly introduced, variational version of sparse identification of nonlinear dynamics (SINDy), which we refer to as Variational Identification of Nonlinear Dynamics (VINDy).
  In detail, the method consists of Variational Encoding of Noisy Inputs (VENI) to identify the distribution of reduced coordinates. Simultaneously, we learn the distribution of the coefficients of a pre-determined set of candidate functions by VINDy. Once trained offline, the identified model can be queried for new parameter instances and new initial conditions to compute the corresponding full-time solutions. The probabilistic setup enables uncertainty quantification as the online testing consists of Variational Inference naturally providing Certainty Intervals (VICI). In this work we showcase the effectiveness of the newly proposed VINDy method in identifying interpretable and accurate dynamical system for the Roessler system with different noise intensities and sources. Then the performance of the overall method - named VENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics and fluid dynamics.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite Element and Machine Learning Modeling of Autogenous Self-Healing Concrete</title>
<link>https://arxiv.org/abs/2510.19839</link>
<guid>https://arxiv.org/abs/2510.19839</guid>
<content:encoded><![CDATA[
<div> Keywords: self-healing concrete, moisture diffusion, damage evolution, finite element modeling, machine learning <br />
Summary: A time-dependent modeling framework was developed for autogenous self-healing concrete, considering moisture diffusion and damage evolution. Two finite element variants, Crack Diffusion Model (CDM) and Crack Membrane Model (CMM), were implemented to simulate healing processes. The orientation and size of initial cracks, diffusion coefficients, healing rate constant, and cement availability smoothing parameter were identified as key parameters affecting healing time. The simulations showed non-monotonic variations in healing time based on crack orientation and size. The CMM model replicated staged moisture penetration but extended total healing time compared to the CDM model. Machine learning classifiers achieved high accuracy in predicting healing outcomes. The framework serves as a valuable tool for guiding experimental studies and practical applications of self-healing concrete. <br /><br />Summary: <div>
arXiv:2510.19839v1 Announce Type: new 
Abstract: A time-dependent modeling framework for autogenous self-healing concrete that couples moisture diffusion with damage evolution was developed. Water transport follows Fick's second law with a damage-dependent diffusivity obtained by power-law interpolation between intact concrete and crack space. Healing reduces damage in proportion to local moisture and a smoothed cement availability field computed via a Helmholtz filter. Two finite element variants were implemented in FEniCSx over time horizons up to $5\times10^6$ seconds: a Crack Diffusion Model (CDM) with standard diffusion and a Crack Membrane Model (CMM) that gates cross-crack transport until a critical moisture threshold is reached. Key control parameters are the initial crack orientation and size, the diffusion coefficients of intact and cracked concrete, the healing rate constant, and the cement availability smoothing parameter. Simulations on a unit square domain show that healing time varies non-monotonically with crack orientation, peaking near $45^\circ$ and $135^\circ$ and minimizing near $90^\circ$, consistent with diffusion distance to crack endpoints dominating the process. The dependence on crack width reverses with material parameters: healing time increases when $D_{\text{cracked}}D_{\text{intact}}$. The CMM reproduces staged moisture penetration with delayed gate activation but lengthens total healing time, whereas the CDM is efficient for parametric sweeps. Machine learning classifiers trained on one million simulation samples predict binary healing outcomes $H(\sigma,\gamma,t)$ (healed or not) with high accuracy (up to 0.998 for neural networks). Although experimental calibration is still required, the framework provides a versatile tool for guiding laboratory studies and implementations of self-healing concrete.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A polygonal Reissner-Mindlin plate element based on the scaled boundary finite element method</title>
<link>https://arxiv.org/abs/2510.20044</link>
<guid>https://arxiv.org/abs/2510.20044</guid>
<content:encoded><![CDATA[
<div> Keywords: polygonal Reissner-Mindlin plate element, scaled boundary finite element method, transverse shear locking, assumed natural strain approach, two-field variational formulation <br />
<br />
Summary: 
This work introduces a polygonal Reissner-Mindlin plate element using a scaled boundary finite element method with linear shape functions. This approach allows the use of non-star-convex-polygonal elements, simplifying meshing. To address transverse shear locking, an assumed natural strain approach is applied. A two-field variational formulation is developed to incorporate three-dimensional material laws while enforcing plane stress assumptions. This enables the use of material models defined in three-dimensional continuum, considering Poisson's thickness locking. The effectiveness of the formulation is demonstrated through various numerical examples. <div>
arXiv:2510.20044v1 Announce Type: new 
Abstract: In this work, a polygonal Reissner-Mindlin plate element is presented. The formulation is based on a scaled boundary finite element method, where in contrast to the original semi-analytical approach, linear shape functions are introduced for the parametrization of the scaling and the radial direction. This yields a fully discretized formulation, which enables the use of non-star-convex-polygonal elements with an arbitrary number of edges, simplifying the meshing process. To address the common effect of transverse shear locking for low-order Reissner-Mindlin elements in the thin-plate limit, an assumed natural strain approach for application on the polygonal scaled boundary finite elements is derived. Further, a two-field variational formulation is introduced to incorporate three-dimensional material laws. Here the plane stress assumptions are enforced on the weak formulation, facilitating the use of material models defined in three-dimensional continuum while considering the effect of Poisson's thickness locking. The effectiveness of the proposed formulation is demonstrated in various numerical examples.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparseEB-gMCR: A Generative Solver for Extreme Sparse Components with Application to Contamination Removal in GC-MS</title>
<link>https://arxiv.org/abs/2510.20364</link>
<guid>https://arxiv.org/abs/2510.20364</guid>
<content:encoded><![CDATA[
<div> decomposability, SparseEB-gMCR, chemical data analysis, contamination removal, analytical chemistry

Summary:
SparseEB-gMCR, a new extension of the EB-gMCR method, has been developed to handle extreme signal sparsity in analytical chemistry instruments. By introducing a fixed EB-select module and making minor adjustments to energy optimization, SparseEB-gMCR demonstrated comparable decomposability and scalability to dense-component EB-gMCR in synthetic datasets. When applied to real GC-MS chromatograms for unsupervised contamination removal, SparseEB-gMCR effectively eliminated siloxane-related pollution signals and improved compound identification reliability. This new extension allows the EB-gMCR family to be applied to a wider range of real-world chemical datasets, providing a general mathematical framework for signal unmixing and contamination elimination in analytical chemistry. <div>
arXiv:2510.20364v1 Announce Type: new 
Abstract: Analytical chemistry instruments provide physically meaningful signals for elucidating analyte composition and play important roles in material, biological, and food analysis. These instruments are valued for strong alignment with physical principles, enabling compound identification through pattern matching with chemical libraries. More reliable instruments generate sufficiently sparse signals for direct interpretation. Generative multivariate curve resolution (gMCR) and its energy-based solver (EB-gMCR) offer powerful tools for decomposing mixed signals suitable for chemical data analysis. However, extreme signal sparsity from instruments such as GC-MS or 1H-NMR can impair EB-gMCR decomposability. To address this, a fixed EB-select module inheriting EB-gMCR's design was introduced for handling extreme sparse components. Combined with minor adjustments to energy optimization, this led to SparseEB-gMCR. In synthetic datasets, SparseEB-gMCR exhibited comparable decomposability and graceful scalability to dense-component EB-gMCR. The sparse variant was applied to real GC-MS chromatograms for unsupervised contamination removal. Analysis showed siloxane-related pollution signals were effectively eliminated, improving compound identification reliability. Results demonstrate that SparseEB-gMCR preserves the decomposability and self-determining component capability of EB-gMCR while extending adaptability to sparse and irregular chemical data. With this sparse extension, the EB-gMCR family becomes applicable to wider ranges of real-world chemical datasets, providing a general mathematical framework for signal unmixing and contamination elimination in analytical chemistry.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers</title>
<link>https://arxiv.org/abs/2510.20066</link>
<guid>https://arxiv.org/abs/2510.20066</guid>
<content:encoded><![CDATA[
<div> liquidity, volatility, spillovers, risk, cryptoassets

Summary:
- The study examines the relationship between liquidity and volatility proxies of core cryptoassets and their impact on forecasting market-wide risk.
- Three statistical layers are integrated: interactions between liquidity and returns, principal-component relations, and volatility-factor projections.
- Various statistical techniques are utilized, including vector autoregression impulse responses, forecast error variance decompositions, and HAR-X models with exogenous regressors.
- The analysis shows significant Granger-causal relationships across layers and moderate predictive accuracy out-of-sample.
- The most informative figures, such as the pipeline overview, Layer A heatmap, Layer C robustness analysis, and precision-recall curve, are presented. Full data and figures are available in the artifact repository. 

Summary:<br /><br /> <div>
arXiv:2510.20066v1 Announce Type: cross 
Abstract: We study whether liquidity and volatility proxies of a core set of cryptoassets generate spillovers that forecast market-wide risk. Our empirical framework integrates three statistical layers: (A) interactions between core liquidity and returns, (B) principal-component relations linking liquidity and returns, and (C) volatility-factor projections that capture cross-sectional volatility crowding. The analysis is complemented by vector autoregression impulse responses and forecast error variance decompositions (see Granger 1969; Sims 1980), heterogeneous autoregressive models with exogenous regressors (HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using temporal splits, early stopping, validation-only thresholding, and SHAP-based interpretation. Using daily data from 2021 to 2025 (1462 observations across 74 assets), we document statistically significant Granger-causal relationships across layers and moderate out-of-sample predictive accuracy. We report the most informative figures, including the pipeline overview, Layer A heatmap, Layer C robustness analysis, vector autoregression variance decompositions, and the test-set precision-recall curve. Full data and figure outputs are provided in the artifact repository.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI PB: A Grounded Generative Agent for Personalized Investment Insights</title>
<link>https://arxiv.org/abs/2510.20099</link>
<guid>https://arxiv.org/abs/2510.20099</guid>
<content:encoded><![CDATA[
<div> Keywords: AI PB, generative agent, retail finance, investment insights, finance-domain embedding model

Summary: 
AI PB is a generative agent deployed in retail finance that proactively generates investment insights. It uses a deterministic orchestration layer to route between LLMs, a hybrid retrieval pipeline, and a multi-stage recommendation mechanism. The system operates on-premises under Korean financial regulations and utilizes Docker Swarm and vLLM on NVIDIA H100 GPUs. Grounded generation with explicit routing and layered safety ensures trustworthy AI insights in high-stakes finance. <div>
arXiv:2510.20099v1 Announce Type: cross 
Abstract: We present AI PB, a production-scale generative agent deployed in real retail finance. Unlike reactive chatbots that answer queries passively, AI PB proactively generates grounded, compliant, and user-specific investment insights. It integrates (i) a component-based orchestration layer that deterministically routes between internal and external LLMs based on data sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the finance-domain embedding model, and (iii) a multi-stage recommendation mechanism combining rule heuristics, sequential behavioral modeling, and contextual bandits. Operating fully on-premises under Korean financial regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100 GPUs. Through human QA and system metrics, we demonstrate that grounded generation with explicit routing and layered safety can deliver trustworthy AI insights in high-stakes finance.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Exchange that Mitigate a Bribery Attack</title>
<link>https://arxiv.org/abs/2510.20645</link>
<guid>https://arxiv.org/abs/2510.20645</guid>
<content:encoded><![CDATA[
<div> HTLC, bribery attack, MAD-HTLC, He-HTLC, Miner-collusion<br />
Summary:<br />
Despite the popularity of Hashed Time-Locked Contracts (HTLCs) in various applications, their use in exchanges is still questionable due to vulnerability to bribery attacks. State-of-the-art solutions like MAD-HTLC and He-HTLC address this issue but have limitations against collusion scenarios. This paper exposes vulnerabilities in these solutions and proposes a stronger attack, demonstrating the need for a more secure protocol. The proposed protocol, \prot, is designed to be game-theoretically secure and resistant to all bribery scenarios by employing a two-phase approach to prevent unauthorized token confiscation. Implementation on Bitcoin and Ethereum shows \prot’s efficiency in transaction cost and latency. <div>
arXiv:2510.20645v1 Announce Type: cross 
Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of their use in wide areas of applications such as payment channels, atomic swaps, etc, their use in exchange is still questionable. This is because of its incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC (NDSS'23) address this by leveraging miners' profit-driven behaviour to mitigate such attacks. The former is the mitigation against passive miners; however, the latter works against both active and passive miners. However, they consider only two bribing scenarios where either of the parties involved in the transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by presenting a miner-collusion bribery attack with implementation and game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC than He-HTLC, allowing the attacker to earn profits equivalent to attacking naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC protocol resistant to all bribery scenarios. \prot\ employs a two-phase approach, preventing unauthorized token confiscation by third parties, such as miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is executed without manipulation. We demonstrate \prot's efficiency in transaction cost and latency via implementations on Bitcoin and Ethereum.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter Estimation in River Transport Models With Immobile Phase Exchange Using Dimensional Analysis and Reduced-Order Models</title>
<link>https://arxiv.org/abs/2510.19664</link>
<guid>https://arxiv.org/abs/2510.19664</guid>
<content:encoded><![CDATA[
<div> framework, parameter estimation, river transport models, breakthrough curves, dimensionless

Summary:
The article introduces a novel framework, Dimensionless Synthetic Transport Estimation (DSTE), for parameter estimation in river transport models using breakthrough curve data. The framework incorporates immobile phase exchange through a memory function in a one-dimensional advection-dispersion equation model. By solving the governing equation analytically in the Laplace domain and numerically inverting it, synthetic breakthrough curves are generated for different memory functions and boundary conditions. Through a dimensionless formulation, the estimation of advection velocity is decoupled from other parameters, reducing the required forward solutions. Computational efficiency is improved using a Karhunen-Loeve (KL) expansion to transform the synthetic dataset into a reduced-order space. The DSTE method is benchmarked against various alternatives and proves to deliver accurate parameter estimates when applied to 295 breakthrough curves from 54 tracer tests in 25 rivers. The resulting labeled dataset provides valuable insights into linking transport parameters with hydraulic conditions, site characteristics, and measured concentrations, eliminating the need for additional forward simulations. <br /><br />Summary: <div>
arXiv:2510.19664v1 Announce Type: new 
Abstract: We propose a framework for parameter estimation in river transport models using breakthrough curve data, which we refer to as Dimensionless Synthetic Transport Estimation (DSTE). We utilize this framework to parameterize the one-dimensional advection-dispersion equation model, incorporating immobile phase exchange through a memory function. We solve the governing equation analytically in the Laplace domain and numerically invert it to generate synthetic breakthrough curves for different memory functions and boundary conditions. A dimensionless formulation enables decoupling the estimation of advection velocity from other parameters, significantly reducing the number of required forward solutions. To improve computational efficiency, we apply a Karhunen-Loeve (KL) expansion to transform the synthetic dataset into a reduced-order space. Given a measured breakthrough curve, we estimate the advection velocity by minimizing the distance from the measurement to the synthetic data in KL space, and infer the remaining dimensionless parameters by Projected Barycentric Interpolation (PBI). We benchmark our method against several alternatives, including Laplace domain fitting, moment matching, global random optimization, and variations of the DSTE framework using nearest-neighbor interpolation and neural network-based estimation. Applied to 295 breakthrough curves from 54 tracer tests in 25 rivers, DSTE delivers accurate parameter estimates. The resulting labeled dataset allows researchers to link transport parameters with hydraulic conditions, site characteristics, and measured concentrations. The synthetic dataset can be leveraged for the analysis of new breakthrough curves, eliminating the need for additional forward simulations.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Black Tuesday Attack: how to crash the stock market with adversarial examples to financial forecasting models</title>
<link>https://arxiv.org/abs/2510.18990</link>
<guid>https://arxiv.org/abs/2510.18990</guid>
<content:encoded><![CDATA[
<div> Keywords: stock market crash, adversarial example, financial forecasting models, hostile actors, economic damage <br />
Summary: <br />
The article explores the potential risk of triggering a stock market crash through manipulations of individual stock values to create adversarial examples that deceive financial forecasting models. These subtle interventions could lead to self-fulfilling predictions of a crash, posing a significant threat to financial stability and providing an opportunity for hostile actors to inflict economic harm on adversaries. The scheme, though difficult to detect, could be directed at an entire economy or a specific company. The theoretical basis and potential impacts of such attacks are outlined, emphasizing the urgent need for research on defense strategies. The underappreciated threat highlights the importance of understanding and mitigating vulnerabilities in financial systems. <br /> <div>
arXiv:2510.18990v1 Announce Type: cross 
Abstract: We investigate and defend the possibility of causing a stock market crash via small manipulations of individual stock values that together realize an adversarial example to financial forecasting models, causing these models to make the self-fulfilling prediction of a crash. Such a crash triggered by an adversarial example would likely be hard to detect, since the model's predictions would be accurate and the interventions that would cause it are minor. This possibility is a major risk to financial stability and an opportunity for hostile actors to cause great economic damage to an adversary. This threat also exists against individual stocks and the corresponding valuation of individual companies. We outline how such an attack might proceed, what its theoretical basis is, how it can be directed towards a whole economy or an individual company, and how one might defend against it. We conclude that this threat is vastly underappreciated and requires urgent research on how to defend against it.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wind Variability and Its Effect on Transmission Line Capacity Estimation</title>
<link>https://arxiv.org/abs/2510.19433</link>
<guid>https://arxiv.org/abs/2510.19433</guid>
<content:encoded><![CDATA[
<div> wind velocity averaging, Dynamic Thermal Rating, transmission line, Nusselt number, ampacity

Summary: 
This study examines the impact of wind velocity averaging on Dynamic Thermal Rating (DTR) calculations using high-temporal-resolution wind measurements from a transmission line in Slovenia. Two averaging methods, vector averaging and hybrid averaging, are compared. The study finds that averaging significantly affects Nusselt number and ampacity, with a strong angular dependency on wind direction relative to the line. In cases of parallel wind, averaged data underestimates ampacity, while perpendicular wind can lead to overestimation. The study highlights the importance of considering the averaging method in DTR calculations to ensure safe operation of transmission lines. <div>
arXiv:2510.19433v1 Announce Type: cross 
Abstract: This study investigates the impact of wind velocity averaging on Dynamic Thermal Rating (DTR) calculations. It is based on a high-temporal-resolution (1 second) wind measurements obtained from a transmission line in Slovenia, Europe. Wind speed and direction variability are analysed, and two averaging methods, namely vector averaging, where velocity is averaged as vector, and hybrid averaging, where speed is averaged as scalar, are employed. DTR calculations are performed on both high-resolution data and averaged data (5 minute averaging window). It is demonstrated that averaging has a significant effect on both Nusselt number and ampacity, and the effect exhibits a strong angular dependency on the relative angle of the wind to the line. Therefore, two limit cases are studied: in the case of parallel wind, averaged data underestimates the ampacity, and there is a significant amount of cases where the underestimation is larger than 10 %. In the case of perpendicular wind, the two averaging methods affect the results in different ways, but both result in a substantial amount of cases where ampacity is overestimated, potentially leading to unsafe operation. The main takeaway of the study is that averaging wind velocity has a significant impact on DTR results, and special emphasis should be given to the averaging method, as different methods affect the results in different ways.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regional heterogeneity in left atrial stiffness impacts passive deformation in a cohort of patient-specific models</title>
<link>https://arxiv.org/abs/2510.18642</link>
<guid>https://arxiv.org/abs/2510.18642</guid>
<content:encoded><![CDATA[
<div> patient-specific models, left atrium, atrial fibrillation, biomechanics, myocardial stiffness
Summary:
Patient-specific models were created from CT images to study the biomechanics of the left atrium (LA) in atrial fibrillation (AF). Regional myocardial stiffness variations in the LA were found to be significant factors in physiological deformation, while anatomical features such as wall thickness and adipose volume were less impactful. The models successfully replicated patient atrial motion and provided insights into the altered biomechanics of the LA in AF. This study highlights the importance of considering regional stiffness in understanding atrial biomechanics and suggests a complex interplay of factors influencing LA function. <div>
arXiv:2510.18642v1 Announce Type: new 
Abstract: The deformation of the left atrium (LA), or its biomechanical function, is closely linked to the health of this cardiac chamber. In atrial fibrillation (AF), atrial biomechanics are significantly altered but the underlying cause of this change is not always clear. Patient-specific models of the LA that replicate patient atrial motion can allow us to understand how factors such as atrial anatomy, myocardial stiffness and physiological constraints are linked to atrial biomechanics. We created patient-specific LA models from CT images. We fitted regional model stiffness to peak CT-derived deformation during the LA reservoir phase ($\pm0.90$ mm) and used the CT deformation transients through the reservoir and conduit phase for model validation (deformation transients fell within $\pm0.38$ mm per unit time of targets). We found that myocardial stiffness varies regionally across the LA. The regional stiffness values were significant factors contributing to regional physiological LA deformation ($p=0.023$) while features of LA anatomy, including regional wall thickness and adipose volume, were less important. These findings provide insight into the underlying causes of altered LA biomechanics in AF.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAISE: A Unified Framework for Responsible AI Scoring and Evaluation</title>
<link>https://arxiv.org/abs/2510.18559</link>
<guid>https://arxiv.org/abs/2510.18559</guid>
<content:encoded><![CDATA[
<div> framework, responsible AI, evaluation, deep learning models, multi-dimensional  

Summary:  
The article introduces RAISE, a framework for evaluating AI models based on four dimensions: explainability, fairness, robustness, and sustainability, creating a holistic Responsibility Score. Three deep learning models were tested on structured datasets from finance, healthcare, and socioeconomics. The Multilayer Perceptron (MLP) showed strong sustainability and robustness, the Feature Tokenizer Transformer excelled in explainability and fairness but at a high environmental cost, and the Tabular ResNet had a balanced profile. This study emphasizes the need for multi-dimensional evaluation in responsible model selection. The implementation of RAISE is available on GitHub. <div>
arXiv:2510.18559v1 Announce Type: cross 
Abstract: As AI systems enter high-stakes domains, evaluation must extend beyond predictive accuracy to include explainability, fairness, robustness, and sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a unified framework that quantifies model performance across these four dimensions and aggregates them into a single, holistic Responsibility Score. We evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular ResNet, and a Feature Tokenizer Transformer, on structured datasets from finance, healthcare, and socioeconomics. Our findings reveal critical trade-offs: the MLP demonstrated strong sustainability and robustness, the Transformer excelled in explainability and fairness at a very high environmental cost, and the Tabular ResNet offered a balanced profile. These results underscore that no single model dominates across all responsibility criteria, highlighting the necessity of multi-dimensional evaluation for responsible model selection. Our implementation is available at: https://github.com/raise-framework/raise.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competitive algorithms for calculating the ground state properties of Bose-Fermi mixtures</title>
<link>https://arxiv.org/abs/2503.13717</link>
<guid>https://arxiv.org/abs/2503.13717</guid>
<content:encoded><![CDATA[
<div> Gross-Pitaevskii equation, Hartree-Fock equation, numerical schemes, Bose-Fermi systems, quantum droplets
Summary:<br />
This work examines various numerical methods for studying the ground state properties of Bose-Fermi systems, specifically mixtures of different atomic species under external forces or self-bound quantum droplets. The bosonic atoms are modeled using the generalized Gross-Pitaevskii equation, while the fermionic atoms are treated individually with wave functions following the Hartree-Fock equation. The study solves the formulated equations using techniques like adiabatic switching of interactions and imaginary time propagation with Gram-Schmidt orthonormalization or Hamiltonian matrix diagonalization. By applying these algorithms to the mixture parameters, including those leading to self-bound quantum droplets, the research demonstrates how different numerical schemes perform in competition. <br /><br />Summary: <div>
arXiv:2503.13717v2 Announce Type: replace 
Abstract: In this work we define, analyze, and compare different numerical schemes that can be used to study the ground state properties of Bose-Fermi systems, such as mixtures of different atomic species under external forces or self-bound quantum droplets. The bosonic atoms are assumed to be condensed and are described by the generalized Gross-Pitaevskii equation. The fermionic atoms, on the other hand, are treated individually, and each atom is associated with a wave function whose evolution follows the Hartree-Fock equation. We solve such a formulated set of equations using a variety of methods, including those based on adiabatic switching of interactions and the imaginary time propagation technique combined with the Gram-Schmidt orthonormalization or the diagonalization of the Hamiltonian matrix. We show how different algorithms compete at the numerical level by studying the mixture in the range of parameters covering the formation of self-bound quantum Bose-Fermi droplets.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Population-Based Search Method Using Uncertainty-related Pareto Front for Robust Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2510.16386</link>
<guid>https://arxiv.org/abs/2510.16386</guid>
<content:encoded><![CDATA[
<div> framework, robustness, convergence, optimization, multi-objective  
<br />  
Summary:  
The article introduces a new Uncertainty-related Pareto Front (UPF) framework that prioritizes both robustness and convergence in multi-objective optimization. Traditional methods often focus on convergence at the expense of robustness, leading to suboptimal solutions in noisy environments. The UPF framework addresses this by quantifying the impact of noise perturbations on convergence and robustness equally. Building on UPF, the RMOEA-UPF algorithm is proposed, which efficiently calculates and optimizes the UPF during the evolutionary process. Experimental results on various benchmark problems and a real-world application show that RMOEA-UPF consistently outperforms existing methods, providing high-quality solutions for complex, uncertain optimization problems. The algorithm's performance highlights its general applicability and reliability in tackling challenging multi-objective optimization tasks.  
<br /><br />  
 <div>
arXiv:2510.16386v1 Announce Type: new 
Abstract: Traditional robust multi-objective optimization methods typically prioritize convergence while treating robustness as a secondary consideration. This approach can yield solutions that are not genuinely robust optimal under noise-affected scenarios. Furthermore, compared to population-based search methods, determining the robust optimal solution by evaluating the robustness of a single convergence-optimal solution is also inefficient. To address these two limitations,we propose a novel Uncertainty-related Pareto Front (UPF) framework that balances robustness and convergence as equal priorities. Unlike traditional Pareto Front, the UPF explicitly accounts for decision variable with noise perturbation by quantifying their effects on both convergence guarantees and robustness preservation equally within a theoretically grounded and general framework. Building upon UPF, we propose RMOEA-UPF--a population-based search robust multi-objective optimization algorithm. This method enables efficient search optimization by calculating and optimizing the UPF during the evolutionary process.Experiments on nine benchmark problems and a real-world application demonstrate that RMOEA-UPF consistently delivers high-quality results. Our method's consistent top-ranking performance indicates a more general and reliable approach for solving complex, uncertain multi-objective optimization problems. Code is available at: https://github.com/WenxiangJiang-me/RMOEA-UPF.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViT-Transformer: Self-attention mechanism based constitutive modeling for nonlinear heterogeneous materials</title>
<link>https://arxiv.org/abs/2510.16575</link>
<guid>https://arxiv.org/abs/2510.16575</guid>
<content:encoded><![CDATA[
<div> Transformer architectures, machine learning, heterogeneous materials, composites, surrogate model<br />
<br />
Summary: <br />
This study introduces a new surrogate model called ViT-Transformer for multi-scale simulations of nonlinear heterogeneous materials and composites. The model utilizes a Vision Transformer encoder and a Transformer-based decoder with attention mechanisms to capture long-range dependencies and generalization across microstructures. A random extract training algorithm is proposed to enhance training robustness to sequences of variable length. A compact and diverse dataset is constructed via data augmentation to validate the model. The ViT-Transformer model demonstrates effectiveness and accuracy in predicting stress responses from material images and loading scenarios. Multiple numerical examples showcase the model's performance and the effectiveness of the training algorithm. <div>
arXiv:2510.16575v1 Announce Type: new 
Abstract: Multi-scale simulations of nonlinear heterogeneous materials and composites are challenging due to the prohibitive computational costs of high-fidelity simulations. Recently, machine learning (ML) based approaches have emerged as promising alternatives to traditional multiscale methods. However, existing ML surrogate constitutive models struggle in capturing long-range dependencies and generalization across microstructures. The recent advancements in attention-based Transformer architectures open the door to a more powerful class of surrogate models. Attention mechanism has demonstrated remarkable capabilities in natural language processing and computer vision. In this work, we introduce a surrogate (meta) model, namely ViT-Transformer, using a Vision Transformer (ViT) encoder and a Transformer-based decoder which are both driven by the self-attention mechanism. The ViT encoder extracts microstructural features from material images, while the decoder is a masked Transformer encoder that combines the latent geometrical features with the macroscopic strain input sequence to predict the corresponding stress response. To enhance training, we propose a random extract training algorithm that improves robustness to sequences of variable length. We design and construct a compact yet diverse dataset via data augmentation, and validate the surrogate model using various composite material images and loading scenarios. Several numerical examples are provided to show the effectiveness and accuracy of the ViT-Transformer model and the training algorithm.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chem-R: Learning to Reason as a Chemist</title>
<link>https://arxiv.org/abs/2510.16880</link>
<guid>https://arxiv.org/abs/2510.16880</guid>
<content:encoded><![CDATA[
<div> Chem-R, large language models, chemical discovery, reasoning capabilities, molecular tasks, reaction tasks <br />
Summary: <br />
Chem-R is a novel Chemical Reasoning model designed to enhance chemical discovery by integrating core chemical knowledge and advanced reasoning capabilities. The model undergoes a three-phase training framework: Chemical Foundation Training, Chemical Reasoning Protocol Distillation, and Multi-task Group Relative Policy Optimization. This structured approach enables Chem-R to outperform leading large language models like Gemini-2.5-Pro and DeepSeek-R1 by up to 46% on molecular tasks and 66% on reaction tasks. Chem-R also surpasses existing chemical foundation models in performance on molecular and reaction level tasks, showcasing its robust generalization and interpretability. The results demonstrate Chem-R's potential as a cutting-edge AI-driven tool for advancing chemical discovery. <br /> <div>
arXiv:2510.16880v1 Announce Type: new 
Abstract: Although large language models (LLMs) have significant potential to advance chemical discovery, current LLMs lack core chemical knowledge, produce unreliable reasoning trajectories, and exhibit suboptimal performance across diverse chemical tasks. To address these challenges, we propose Chem-R, a generalizable Chemical Reasoning model designed to emulate the deliberative processes of chemists. Chem-R is trained through a three-phase framework that progressively builds advanced reasoning capabilities, including: 1) Chemical Foundation Training, which establishes core chemical knowledge. 2) Chemical Reasoning Protocol Distillation, incorporating structured, expert-like reasoning traces to guide systematic and reliable problem solving. 3) Multi-task Group Relative Policy Optimization that optimizes the model for balanced performance across diverse molecular- and reaction-level tasks. This structured pipeline enables Chem-R to achieve state-of-the-art performance on comprehensive benchmarks, surpassing leading large language models, including Gemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on reaction tasks. Meanwhile, Chem-R also consistently outperforms the existing chemical foundation models across both molecular and reaction level tasks. These results highlight Chem-R's robust generalization, interpretability, and potential as a foundation for next-generation AI-driven chemical discovery.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing data scarcity in structural health monitoring through generative augmentation</title>
<link>https://arxiv.org/abs/2510.16889</link>
<guid>https://arxiv.org/abs/2510.16889</guid>
<content:encoded><![CDATA[
<div> Generative Adversarial Network, Structural Health Monitoring, Bridge Infrastructures, Deep Learning, Data Augmentation

Summary:<br /><br />Structural Health Monitoring is important for bridge safety, but faces challenges like data scarcity and noise. A new study introduces STFTSynth, a custom Generative Adversarial Network model for generating spectrograms from acoustic event signals. STFTSynth outperforms baseline models by creating realistic spectrograms through dense residual blocks and bidirectional gated recurrent units. Evaluation metrics show its superiority in producing high-resolution and temporally consistent spectrograms, especially beneficial for scenarios with rare events like prestressing wire breakage. This approach demonstrates the potential of generative-based data augmentation for enhancing dataset diversity and robustness in Structural Health Monitoring applications. <div>
arXiv:2510.16889v1 Announce Type: new 
Abstract: Structural Health Monitoring plays a crucial role in ensuring the safety, reliability, and longevity of bridge infrastructures through early damage detection. Although recent advances in deep learning-based models have enabled automated event detection, their performance is often limited by data scarcity, environmental noise, and class imbalance. To address these challenges, this study introduces a customized Generative Adversarial Network model, STFTSynth, designed particularly for generating short-time Fourier transform spectrograms derived from acoustic event signals. In contrast to augmentation techniques such as MixUp, generative adversarial networks can synthesize high-quality spectrograms that mimic real-world events, enhancing dataset diversity and robustness. The proposed model integrates dense residual blocks for spatial consistency with bidirectional gated recurrent units for temporal dependency modeling. Model performance is evaluated against three baseline generative models using qualitative inspection and quantitative metrics, including Structural Similarity Index Measure, Peak Signal-to-Noise Ratio, and Fr\'echet Inception Distance. Results show that STFTSynth outperforms baseline models, producing high-resolution, temporally consistent spectrograms that align closely with real-world data. These findings indicate the potential of generative-based data augmentation as a scalable and cost-effective solution for bridge monitoring scenarios where rare events, such as prestressing wire breakage, suffer from data scarcity.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading with the Devil: Risk and Return in Foundation Model Strategies</title>
<link>https://arxiv.org/abs/2510.17165</link>
<guid>https://arxiv.org/abs/2510.17165</guid>
<content:encoded><![CDATA[
<div> financial market, time-series tasks, Foundation models, risk profiles, Capital Asset Pricing Model 

Summary:
The paper introduces an extension to the Capital Asset Pricing Model (CAPM) to analyze the risk profiles of trading strategies built on foundation models in finance. It distinguishes between systematic risk from the shared pretrained model and idiosyncratic risk from custom fine-tuning. By aligning this decomposition with uncertainty disentanglement, it categorizes systematic risk as epistemic uncertainty and idiosyncratic risk as aleatory uncertainty. The study shows how methods like Monte Carlo dropout can measure epistemic risk and provide insights into performance limits, model degradation, and refinements of foundation-model-based strategies in financial markets. The results emphasize the potential and challenges of using large pretrained models for trading strategies. 

<br /><br />Summary: <div>
arXiv:2510.17165v1 Announce Type: new 
Abstract: Foundation models - already transformative in domains such as natural language processing - are now starting to emerge for time-series tasks in finance. While these pretrained architectures promise versatile predictive signals, little is known about how they shape the risk profiles of the trading strategies built atop them, leaving practitioners reluctant to commit serious capital. In this paper, we propose an extension to the Capital Asset Pricing Model (CAPM) that disentangles the systematic risk introduced by a shared foundation model - potentially capable of generating alpha if the underlying model is genuinely predictive - from the idiosyncratic risk attributable to custom fine-tuning, which typically accrues no systematic premium. To enable a practical estimation of these separate risks, we align this decomposition with the concepts of uncertainty disentanglement, casting systematic risk as epistemic uncertainty (rooted in the pretrained model) and idiosyncratic risk as aleatory uncertainty (introduced during custom adaptations). Under the Aleatory Collapse Assumption, we illustrate how Monte Carlo dropout - among other methods in the uncertainty-quantization toolkit - can directly measure the epistemic risk, thereby mapping trading strategies to a more transparent risk-return plane. Our experiments show that isolating these distinct risk factors yields deeper insights into the performance limits of foundation-model-based strategies, their model degradation over time, and potential avenues for targeted refinements. Taken together, our results highlight both the promise and the pitfalls of deploying large pretrained models in competitive financial markets.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StrengthLawExtractor: A Fiji plugin for 3D morphological feature extraction from X-ray micro-CT data</title>
<link>https://arxiv.org/abs/2510.17279</link>
<guid>https://arxiv.org/abs/2510.17279</guid>
<content:encoded><![CDATA[
<div> Keywords: non-destructive methods, micro-computed tomography, morphometric measures, porous materials, predictive modeling  
Summary:  
- Non-destructive methods are crucial for understanding the relationship between porous material microstructure and mechanical behavior.  
- Micro-CT technology allows for high-resolution 3D reconstructions of porous materials, aiding in the quantification of geometric descriptors.  
- Recent advances in morphometric theory suggest that specific measures such as porosity, surface area, mean curvature, and Euler characteristic are essential for predicting strength.  
- A Fiji plugin has been developed to automatically extract these morphometric measures from micro-CT datasets, making analysis reproducible and user-friendly.  
- The extracted descriptors can be used in constitutive models and machine learning algorithms to predict stress-strain behavior and design microstructures.  
<br /><br />Summary: <div>
arXiv:2510.17279v1 Announce Type: new 
Abstract: Non-destructive methods are essential for linking the microstructural geometry of porous materials to their mechanical behavior, as destructive testing is often infeasible due to limited material availability or irreproducible conditions. Micro-computed tomography (micro-CT) provides high resolution three dimensional reconstructions of porous microstructures, enabling direct quantification of geometric descriptors. Recent advances in morphometric theory have demonstrated that four independent morphometric measures (porosity, surface area, mean curvature, and Euler characteristic) are required to capture the relationship between microstructure and strength, thereby forming the basis of generalized strength laws. To facilitate practical application of this framework, a Fiji plugin was developed to extract the four morphometric measures (porosity, surface area, mean curvature, Euler characteristic) from micro-CT datasets automatically. The plugin integrates within the Fiji platform to provide reproducible, accessible, and user friendly analysis. The application of the tool demonstrates that the extracted descriptors can be readily incorporated into constitutive models and machine learning workflows, enabling the forward prediction of stress-strain behavior as well as the inverse design of microstructures. This approach supports non-destructive evaluation, accelerates materials selection, and advances the integration of imaging with predictive modeling in porous media research.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling complexity in system safety: generalizing the D2T2 methodology</title>
<link>https://arxiv.org/abs/2510.17351</link>
<guid>https://arxiv.org/abs/2510.17351</guid>
<content:encoded><![CDATA[
<div> Fault Tree, Event Tree, system safety analysis, complex systems, Dynamic and Dependent Tree Theory 

Summary:
The article discusses the limitations of traditional Fault Tree and Event Tree analysis in capturing the dynamic behavior of complex systems. These techniques struggle to depict the dense network of dependencies within systems, leading to the use of conservative assumptions to compensate for oversimplifications. The proposed solution involves integrating dependency modeling within the conventional Fault and Event Tree framework through the Dynamic and Dependent Tree Theory. This approach combines the combinatorial nature of failure analysis with flexible modeling solutions, allowing for the comprehensive inclusion of any dependency type. By incorporating complex system features while maintaining the effectiveness of traditional safety modeling, this method offers a more versatile and accurate approach to system safety analysis. <div>
arXiv:2510.17351v1 Announce Type: new 
Abstract: Although Fault Tree and Event Tree analysis are still today the standard approach to system safety analysis for many engineering sectors, these techniques lack the capabilities of fully capturing the realistic, dynamic behaviour of complex systems, which results in a dense network of dependencies at any level, i.e. between components, trains of components or subsystems. While these limitations are well recognised across both industry and academia, the shortage of alternative tools able to tackle such challenges while retaining the computational feasibility of the analysis keeps fuelling the long-lived success of Fault Tree and Event Tree modelling. Analysts and regulators often rely on the use of conservative assumptions to mitigate the effect of oversimplifications associated with the use of such techniques. However, this results in the analysis output to be characterised by an unknown level of conservatism, with potential consequences on market competitiveness (i.e., over-conservatism) or safety (i.e., under-conservatism). This study proposes a generalization of the Dynamic and Dependent Tree Theory, which offers theoretical tools for the systematic integration of dependency modelling within the traditional Fault and Event Tree analysis framework. This is achieved by marrying the traditional combinatorial nature of failure analysis, formalised by the Fault and Event Tree language, with more flexible modelling solutions, which provide the flexibility required to capture complex system features. The main advantage of the proposed approach in comparison to existent solutions is the ability to take into account, under the same modelling framework, any type of dependency regardless of its nature and location, while retaining the familiarity and effectiveness of traditional safety modelling.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Volumetric Non-Invasive Cardiac Mapping for Accessible Global Arrhythmia Characterization</title>
<link>https://arxiv.org/abs/2510.17539</link>
<guid>https://arxiv.org/abs/2510.17539</guid>
<content:encoded><![CDATA[
<div> Cardiac arrhythmias; Imageless volumetric ECGI; Non-invasive mapping; 3D activation mapping; Arrhythmia localization<br />
<br />
Summary:<br />
Cardiac arrhythmias pose significant health risks, but traditional mapping techniques are limited to surface reconstructions. A new approach, imageless volumetric ECGI, offers non-invasive 3D activation mapping using Green's functions. By solving an inverse source problem, this method enhances localization accuracy, reducing geodesic errors by 59.3% compared to surface-only methods. Evaluation on patient cases, including ventricular tachycardia and Wolff-Parkinson-White syndrome, demonstrates precise activation pattern alignment with clinical diagnoses. This accessible technique shows promise in preprocedural planning, ablation target guidance, and optimizing cardiac resynchronization therapy. <div>
arXiv:2510.17539v1 Announce Type: new 
Abstract: Cardiac arrhythmias are a major cause of morbidity and mortality increasing the risk of stroke, heart failure, and sudden cardiac death. Imageless electrocardiographic imaging (ECGI) provides a non invasive alternative to electrical mapping from body surface potentials, but conventional ECGI is confined to epicardial reconstructions and can miss arrhythmias originating in deeper myocardium. We address this by reconstructing three dimensional cardiac activity with a volumetric formulation that solves an inverse source problem via Green's functions, enabling full volume activation mapping and improved localization in anatomically complex regions. We evaluate the approach on simulated premature ventricular beats and on four challenging patient cases, a right ventricular outflow tract premature ventricular contraction, a left bundle branch block, a ventricular tachycardia, and Wolff Parkinson White, and additionally assess performance on an open source myocardial infarction dataset. Results show that volumetric ECGI recovers 3D activation and sharpens arrhythmia origin localization, achieving a 59.3% reduction in geodesic error between estimated and simulated origins relative to surface only methods; in patient cases, activation patterns align with clinical diagnoses. Overall, imageless volumetric ECGI offers accessible, non invasive 3D activation mapping that overcomes a core limitation of surface restricted techniques and may improve preprocedural planning, ablation target guidance, and selection or optimization of cardiac resynchronization therapy.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter</title>
<link>https://arxiv.org/abs/2510.15954</link>
<guid>https://arxiv.org/abs/2510.15954</guid>
<content:encoded><![CDATA[
<div> diffusion model, Ensemble Score Filter, data assimilation, wildfire spread predictions, numerical investigations
<br />
Summary: 
This paper explores the use of the Ensemble Score Filter (EnSF) algorithm for data assimilation in real-time active wildfire spread predictions. By integrating observations like remote-sensing data with fire predictions from numerical models, EnSF enhances forecasting accuracy. The EnSF algorithm, based on a score-based generative diffusion model, offers superior accuracy for high-dimensional nonlinear filtering problems, making it well-suited for wildfire spread models. The study provides technical details and demonstrates through numerical investigations that EnSF outperforms other methods in terms of accuracy, stability, and computational efficiency. The robustness and practicality of EnSF make it a valuable tool for wildfire data assimilation. The public availability of the code used in this study encourages further research and application of EnSF in wildfire management. 
<br /><br />Summary: <div>
arXiv:2510.15954v1 Announce Type: cross 
Abstract: As wildfires become increasingly destructive and expensive to control, effective management of active wildfires requires accurate, real-time fire spread predictions. To enhance the forecasting accuracy of active fires, data assimilation plays a vital role by integrating observations (such as remote-sensing data) and fire predictions generated from numerical models. This paper provides a comprehensive investigation on the application of a recently proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter (EnSF) -- to the data assimilation problem for real-time active wildfire spread predictions. Leveraging a score-based generative diffusion model, EnSF has been shown to have superior accuracy for high-dimensional nonlinear filtering problems, making it an ideal candidate for the filtering problems of wildfire spread models. Technical details are provided, and our numerical investigations demonstrate that EnSF provides superior accuracy, stability, and computational efficiency, establishing it as a robust and practical method for wildfire data assimilation. Our code has been made publicly available.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia</title>
<link>https://arxiv.org/abs/2510.16066</link>
<guid>https://arxiv.org/abs/2510.16066</guid>
<content:encoded><![CDATA[
<div> Keywords: MSMEs, financing, bank statement data, credit assessment, financial inclusion<br />
<br />
Summary: 
This study addresses the challenge of financing faced by Micro, Small, and Medium Enterprises (MSMEs) in Malaysia through the use of bank statement data for credit assessment. The research proposes a cash flow-based underwriting pipeline that leverages machine learning credit scoring on bank transaction data. A novel dataset of 611 loan applicants from a Malaysian lending institution is introduced for model development and evaluation. The study demonstrates that incorporating bank transaction-derived features improves credit scoring models for new-to-lending MSMEs. The results highlight the potential of using alternative data sources for credit assessment to promote financial inclusion in emerging markets. The anonymised bank transaction dataset will be released to support further research on MSMEs' financial inclusion in Malaysia's economy. <div>
arXiv:2510.16066v1 Announce Type: cross 
Abstract: Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established or young businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. Firstly, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end to end data extraction and machine learning credit scoring. Secondly, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Thirdly, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Lastly, we intend to release the anonymised bank transaction dataset to facilitate further research on MSMEs financial inclusion within Malaysia's emerging economy.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A hierarchical Bayesian approach for population-based structural health monitoring in ship hull structures</title>
<link>https://arxiv.org/abs/2510.16316</link>
<guid>https://arxiv.org/abs/2510.16316</guid>
<content:encoded><![CDATA[
<div> Hierarchical Bayesian model, Structural health monitoring, Population-based SHM, Deflection amplitudes, Markov Chain Monte Carlo<br />
<br />
Summary: 
This work explores utilizing a hierarchical Bayesian model for assessing structural health monitoring data in a population of similar structures. The model aims to detect excessive initial deflections in plate elements, which can lead to unexpected events if not monitored. Using Finite Element modeling to generate strain response data, Bayesian inference with Markov Chain Monte Carlo is applied, with a surrogate model used to calculate the likelihood function. The hierarchical approach is compared to an independent model for a plate component with limited data, showing that the hierarchical model offers more robust results in uncertainty estimation under data sparsity conditions. This enhancement is crucial for decision-making tasks in structural health monitoring strategies. <div>
arXiv:2510.16316v1 Announce Type: cross 
Abstract: Structural health monitoring (SHM) strategies involve the processing of structural response data to indirectly assess an asset's condition. These strategies can be enhanced for a group of structures, especially when they are similar, since mutual underlying physics are expected to exist. The concept behind population-based SHM exploits the sharing of data among individuals, so that data-rich members can support data-scarce ones. One approach to population-level modeling is the hierarchical Bayesian method, where the model is structured hierarchically in terms of its parameters, and correlation among learning tasks is enabled by conditioning on shared latent variables.
  This work investigates the application of a hierarchical Bayesian model to infer expected distributions of deflection amplitudes at both the population and domain levels, with the aim of detecting excessive initial deflections in a population of plate elements. Although these damages are typically localized, they can trigger unexpected events, if not properly monitored. The work is conducted in a numerical setting using a Finite Element model to generate strain response data, which serve as the monitoring data. Bayesian inference was conducted using Markov Chain Monte Carlo (MCMC), with a surrogate model employed to calculate the likelihood function. The hierarchical approach was compared to an independent model for a plate component with few data. The results revealed that, under data sparsity conditions, the hierarchical model can offer more robust results in terms of uncertainty, which is essential for decision-making tasks.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2510.16658</link>
<guid>https://arxiv.org/abs/2510.16658</guid>
<content:encoded><![CDATA[
<div> neuroscience, artificial intelligence, large-scale models, computational challenges, clinical applications
Summary:
The paper discusses the transformative effects of large-scale artificial intelligence (AI) models on various neuroscience domains, including neuroimaging, brain-computer interfaces, molecular neuroscience, clinical assistance, and disease-specific applications. These models revolutionize traditional computational methods by enabling end-to-end learning from raw brain signals and neural data. They address key computational challenges in neuroscience, such as integrating multimodal neural data, interpreting spatiotemporal patterns, and developing translational frameworks for clinical deployment. The interaction between neuroscience and AI is becoming more reciprocal, with biologically inspired architectural constraints enhancing interpretability and efficiency of models. The review emphasizes the importance of rigorous evaluation frameworks, domain knowledge integration, and ethical guidelines for clinical use. The paper also provides a systematic listing of critical neuroscience datasets used to develop and validate large-scale AI models across diverse research applications. <div>
arXiv:2510.16658v1 Announce Type: cross 
Abstract: The advent of large-scale artificial intelligence (AI) models has a transformative effect on neuroscience research, which represents a paradigm shift from the traditional computational methods through the facilitation of end-to-end learning from raw brain signals and neural data. In this paper, we explore the transformative effects of large-scale AI models on five major neuroscience domains: neuroimaging and data processing, brain-computer interfaces and neural decoding, molecular neuroscience and genomic modeling, clinical assistance and translational frameworks, and disease-specific applications across neurological and psychiatric disorders. These models are demonstrated to address major computational neuroscience challenges, including multimodal neural data integration, spatiotemporal pattern interpretation, and the derivation of translational frameworks for clinical deployment. Moreover, the interaction between neuroscience and AI has become increasingly reciprocal, as biologically informed architectural constraints are now incorporated to develop more interpretable and computationally efficient models. This review highlights both the notable promise of such technologies and key implementation considerations, with particular emphasis on rigorous evaluation frameworks, effective domain knowledge integration, and comprehensive ethical guidelines for clinical use. Finally, a systematic listing of critical neuroscience datasets used to derive and validate large-scale AI models across diverse research applications is provided.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSight: Towards Real-World Financial Deep Research</title>
<link>https://arxiv.org/abs/2510.16844</link>
<guid>https://arxiv.org/abs/2510.16844</guid>
<content:encoded><![CDATA[
<div> agent, financial reports, AI systems, visualization, analysis

Summary: 
FinSight is a multi-agent framework designed to automate the generation of professional financial reports. The Code Agent with Variable Memory (CAVM) architecture integrates data, tools, and agents into a flexible space for data collection, analysis, and report generation through executable code. The Iterative Vision-Enhanced Mechanism refines visual outputs into high-quality financial charts. A two-stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports. Experiments demonstrate that FinSight outperforms existing deep research systems in terms of factual accuracy, analytical depth, and presentation quality. The framework shows promising results in generating reports that approach human-expert quality. <div>
arXiv:2510.16844v1 Announce Type: cross 
Abstract: Generating professional financial reports is a labor-intensive and intellectually demanding process that current AI systems struggle to fully automate. To address this challenge, we introduce FinSight (Financial InSight), a novel multi agent framework for producing high-quality, multimodal financial reports. The foundation of FinSight is the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, enabling flexible data collection, analysis and report generation through executable code. To ensure professional-grade visualization, we propose an Iterative Vision-Enhanced Mechanism that progressively refines raw visual outputs into polished financial charts. Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports, ensuring both analytical depth and structural consistency. Experiments on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing</title>
<link>https://arxiv.org/abs/2510.17088</link>
<guid>https://arxiv.org/abs/2510.17088</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial anomalies, adaptive graph learning, expert networks, interpretability, temporal evolution detection <br />
Summary: 
This article introduces a novel framework for detecting financial anomalies by addressing three main challenges. These challenges include static graph structures that cannot adapt to market correlations, uniform detection mechanisms missing type-specific signatures, and lack of actionable guidance on anomaly mechanisms. The proposed framework utilizes adaptive graph learning with specialized expert networks to provide interpretability. Multi-scale temporal dependencies are captured through BiLSTM with self-attention, combining temporal and spatial information with cross-modal attention. Dynamic graphs are learned through neural multi-source interpolation, balancing dynamics with structural priors via stress-modulated fusion. Anomalies are routed to mechanism-specific experts for detection. The framework achieved a high detection rate of major events with lead time and outperformed baseline methods. A case study demonstrated the tracking of anomaly evolution, showcasing automatic temporal mechanism identification without labeled supervision. <div>
arXiv:2510.17088v1 Announce Type: cross 
Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity freezes, contagion cascades, regime shifts), but existing detectors treat all anomalies uniformly, producing scalar scores without revealing which mechanism is failing, where risks concentrate, or how to intervene. This opacity prevents targeted regulatory responses. Three unsolved challenges persist: (1) static graph structures cannot adapt when market correlations shift during regime changes; (2) uniform detection mechanisms miss type-specific signatures across multiple temporal scales while failing to integrate individual behaviors with network contagion; (3) black-box outputs provide no actionable guidance on anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks that provide built-in interpretability. Our framework captures multi-scale temporal dependencies through BiLSTM with self-attention, fuses temporal and spatial information via cross-modal attention, learns dynamic graphs through neural multi-source interpolation, adaptively balances learned dynamics with structural priors via stress-modulated fusion, routes anomalies to four mechanism-specific experts, and produces dual-level interpretable attributions. Critically, interpretability is embedded architecturally rather than applied post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley Bank case study demonstrates anomaly evolution tracking: Price-Shock expert weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48 (66% above baseline) one week later, revealing automatic temporal mechanism identification without labeled supervision.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation</title>
<link>https://arxiv.org/abs/2510.17146</link>
<guid>https://arxiv.org/abs/2510.17146</guid>
<content:encoded><![CDATA[
<div> Keywords: HVAC systems, anomaly detection, Large Language Models, Physics-Informed LLM, smart building systems

Summary: 
PILLM is a novel framework for anomaly detection in HVAC systems that combines the benefits of Large Language Models (LLMs) with physics-based constraints. By incorporating thermodynamic and control-theoretic principles, PILLM generates adaptive and physically grounded rules for anomaly detection. The framework operates within an evolutionary loop to automatically refine detection rules, leading to improved performance on the Building Fault Detection dataset. PILLM bridges the gap between interpretability and predictive power, offering transparent and actionable diagnostic rules for smart building systems. This approach represents a significant advancement in the field of AI for HVAC systems, enabling efficient energy use and reduced emissions while maintaining reliability and trustworthiness. <div>
arXiv:2510.17146v1 Announce Type: cross 
Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a substantial share of global building energy use, making reliable anomaly detection essential for improving efficiency and reducing emissions. Classical rule-based approaches offer explainability but lack adaptability, while deep learning methods provide predictive power at the cost of transparency, efficiency, and physical plausibility. Recent attempts to use Large Language Models (LLMs) for anomaly detection improve interpretability but largely ignore the physical principles that govern HVAC operations. We present PILLM, a Physics-Informed LLM framework that operates within an evolutionary loop to automatically generate, evaluate, and refine anomaly detection rules. Our approach introduces physics-informed reflection and crossover operators that embed thermodynamic and control-theoretic constraints, enabling rules that are both adaptive and physically grounded. Experiments on the public Building Fault Detection dataset show that PILLM achieves state-of-the-art performance while producing diagnostic rules that are interpretable and actionable, advancing trustworthy and deployable AI for smart building systems.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Temporal Disturbance Estimations For Magnetic Anomaly Navigation and Mapping</title>
<link>https://arxiv.org/abs/2510.15113</link>
<guid>https://arxiv.org/abs/2510.15113</guid>
<content:encoded><![CDATA[
<div> Reference Station Model, Magnetic Anomaly Mapping, Long-range Aeromagnetic Surveys, Regression Models, Space Weather<br />
Summary:<br />
The article introduces the Extended Reference Station Model (ERSM) to address the challenges faced by slow-moving vehicles relying on crustal magnetic anomaly navigation. This model aims to eliminate the need for fixed ground stations within 100 km by utilizing an extended reference ground magnetometer station. By applying longitudinal correction and regression models, such as linear regression, k-nearest neighbors, and neural-network regression, ERSM successfully estimates the local temporal disturbance field with performance benefits. The results demonstrate low root mean square errors and median performance below 5nT, particularly with the kNN and neural-net models for longer distances. Factors such as space-weather events, water-body separation, and proximity to polar regions are also considered in determining the model's performance based on ERS selection. <div>
arXiv:2510.15113v1 Announce Type: new 
Abstract: Slow-moving vehicles relying on crustal magnetic anomaly navigation (MagNav) or vehicles revisiting the same location in a short time - such as those used for surveys in magnetic anomaly mapping - require fixed ground stations within 100 km of the vehicle's trajectory to measure and remove the geomagnetic disturbance field from magnetic readings. This approach is impractical due to the limited network of fixed-ground magnetometer stations, making long-range (several hundred kilometers long) aeromagnetic surveys for anomaly map-making infeasible. To address these challenges, we developed the Extended Reference Station Model (ERSM). ERSM applies a longitudinal correction and regression model to an extended reference ground magnetometer station (ERS) to produce an estimate of the local temporal disturbance field. ERSM is regression model-agnostic, so we implemented a linear regression, a k-nearest neighbors (kNN) regression, and a neural-network regression model to assess performance benefits. Our results show typical performance below 10nT root mean square error and median performance below 5nT for typical use with the kNN and neural-net model for farther distances and below 5nT performance using the linear regression model on stations with proximity. We also consider how space-weather events, water-body separation, and proximity to polar regions affect the model performance based on ERS selection.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Black Scholes for Prediction Markets: A Unified Kernel and Market Maker's Handbook</title>
<link>https://arxiv.org/abs/2510.15205</link>
<guid>https://arxiv.org/abs/2510.15205</guid>
<content:encoded><![CDATA[
<div> prediction markets, Polymarket, logit jump-diffusion, risk factors, calibration pipeline <br />
Summary:<br />
The article introduces a new stochastic kernel for prediction markets, specifically focusing on platforms like Polymarket. The proposed logit jump-diffusion model aims to provide a unified framework for market makers to manage belief volatility, jump, and cross-event risks. By treating traded probabilities as Q-martingales and incorporating risk factors such as belief volatility and jump intensity, the model enables quoting and hedging strategies. A calibration pipeline is developed to filter noise, separate diffusion from jumps, and establish a stable belief-volatility surface. Additionally, the model supports the creation of derivative instruments analogous to options, allowing for the transfer of belief risk. Experimental results show improved short-horizon belief-variance forecast accuracy compared to existing approaches, demonstrating the model's utility in enhancing market efficiency and risk management in prediction markets like Polymarket. <br /> <div>
arXiv:2510.15205v1 Announce Type: new 
Abstract: Prediction markets, such as Polymarket, aggregate dispersed information into tradable probabilities, but they still lack a unifying stochastic kernel comparable to the one options gained from Black-Scholes. As these markets scale with institutional participation, exchange integrations, and higher volumes around elections and macro prints, market makers face belief volatility, jump, and cross-event risks without standardized tools for quoting or hedging. We propose such a foundation: a logit jump-diffusion with risk-neutral drift that treats the traded probability p_t as a Q-martingale and exposes belief volatility, jump intensity, and dependence as quotable risk factors. On top, we build a calibration pipeline that filters microstructure noise, separates diffusion from jumps using expectation-maximization, enforces the risk-neutral drift, and yields a stable belief-volatility surface. We then define a coherent derivative layer (variance, correlation, corridor, and first-passage instruments) analogous to volatility and correlation products in option markets. In controlled experiments on synthetic risk-neutral paths and real event data, the model reduces short-horizon belief-variance forecast error relative to diffusion-only and probability-space baselines, supporting both causal calibration and economic interpretability. Conceptually, the logit jump-diffusion kernel supplies an implied-volatility analogue for prediction markets: a tractable, tradable language for quoting, hedging, and transferring belief risk across venues such as Polymarket.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Market Microstructure for Decentralized Prediction Markets (DePMs)</title>
<link>https://arxiv.org/abs/2510.15612</link>
<guid>https://arxiv.org/abs/2510.15612</guid>
<content:encoded><![CDATA[
<div> Keywords: DePMs, decentralized prediction markets, Polymarket, Truthcoin, Augur

Summary:
Decentralized prediction markets (DePMs) have evolved over the years, with modern platforms like Polymarket showcasing significant differences from earlier designs such as Truthcoin and Augur v1. The article reviews the history of DePMs, spanning back to 2011 and encompassing numerous proposals. A modular workflow consisting of seven stages is outlined, including underlying infrastructure, market topic, share structure and pricing, trading, market resolution, settlement, and archiving. Design variants for each module are discussed, with a focus on decentralization, expressiveness, and manipulation resistance trade-offs. The article also highlights open challenges for researchers in the DePM ecosystem. <div>
arXiv:2510.15612v1 Announce Type: new 
Abstract: Decentralized prediction markets (DePMs) allow open participation in event-based wagering without fully relying on centralized intermediaries. We review the history of DePMs which date back to 2011 and includes hundreds of proposals. Perhaps surprising, modern DePMs like Polymarket deviate materially from earlier designs like Truthcoin and Augur v1. We use our review to present a modular workflow comprising seven stages: underlying infrastructure, market topic, share structure and pricing, trading, market resolution, settlement, and archiving. For each module, we enumerate the design variants, analyzing trade-offs around decentralization, expressiveness, and manipulation resistance. We also identify open problems for researchers interested in this ecosystem.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepAries: Adaptive Rebalancing Interval Selection for Enhanced Portfolio Selection</title>
<link>https://arxiv.org/abs/2510.14985</link>
<guid>https://arxiv.org/abs/2510.14985</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, portfolio management, deep learning, financial markets, transaction costs
Summary:<br /><br />DeepAries is introduced as a novel deep reinforcement learning framework for dynamic portfolio management that optimizes both timing and allocation decisions. It addresses the limitation of fixed rebalancing intervals by adaptively selecting optimal intervals and portfolio weights based on market conditions. The framework combines a Transformer-based state encoder with Proximal Policy Optimization to generate simultaneous discrete and continuous actions for rebalancing and asset allocations. Extensive experiments on real-world financial markets show that DeepAries outperforms traditional strategies in terms of risk-adjusted returns, transaction costs, and drawdowns. A live demo and open-source code and dataset are provided, demonstrating the framework's interpretability and effectiveness in producing adaptive portfolio management decisions aligned with changing market conditions. Overall, DeepAries offers a new approach to portfolio management by integrating timing and allocation decisions in a unified framework. <div>
arXiv:2510.14985v1 Announce Type: cross 
Abstract: We propose DeepAries , a novel deep reinforcement learning framework for dynamic portfolio management that jointly optimizes the timing and allocation of rebalancing decisions. Unlike prior reinforcement learning methods that employ fixed rebalancing intervals regardless of market conditions, DeepAries adaptively selects optimal rebalancing intervals along with portfolio weights to reduce unnecessary transaction costs and maximize risk-adjusted returns. Our framework integrates a Transformer-based state encoder, which effectively captures complex long-term market dependencies, with Proximal Policy Optimization (PPO) to generate simultaneous discrete (rebalancing intervals) and continuous (asset allocations) actions. Extensive experiments on multiple real-world financial markets demonstrate that DeepAries significantly outperforms traditional fixed-frequency and full-rebalancing strategies in terms of risk-adjusted returns, transaction costs, and drawdowns. Additionally, we provide a live demo of DeepAries at https://deep-aries.github.io/, along with the source code and dataset at https://github.com/dmis-lab/DeepAries, illustrating DeepAries' capability to produce interpretable rebalancing and allocation decisions aligned with shifting market regimes. Overall, DeepAries introduces an innovative paradigm for adaptive and practical portfolio management by integrating both timing and allocation into a unified decision-making process.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning reveals key predictors of thermal conductivity in covalent organic frameworks</title>
<link>https://arxiv.org/abs/2409.06457</link>
<guid>https://arxiv.org/abs/2409.06457</guid>
<content:encoded><![CDATA[
<div> machine learning, thermal conductivity, covalent organic frameworks, dangling branches, molecular dynamics simulations

Summary:
The study explores the relationship between the structure and thermal conductivity of covalent organic frameworks (COFs), a class of nanoporous polymeric materials. Traditional features do not reliably predict thermal conductivity in COFs, prompting the development of an attention-based machine learning model that accurately predicts thermal conductivities even for structures beyond the training set. The model's analysis highlights dangling molecular branches as a significant predictor, leading to the definition of the dangling mass ratio (DMR) as a descriptor for thermal conductivity prediction. Feature importance assessments confirm the importance of DMR in predicting thermal conductivity. Molecular dynamics simulations support the observation that COFs with dangling functional groups have lower thermal transfer capabilities, attributed to significant mismatches in vibrational density of states due to the presence of dangling branches.<br /><br />Summary: <div>
arXiv:2409.06457v3 Announce Type: replace 
Abstract: The thermal conductivity of covalent organic frameworks (COFs), an emerging class of nanoporous polymeric materials, is crucial for many applications, yet the link between their structure and thermal properties remains poorly understood. Analysis of a dataset containing over 2,400 COFs reveals that conventional features such as density, pore size, void fraction, and surface area do not reliably predict thermal conductivity. To address this, an attention-based machine learning model was trained, accurately predicting thermal conductivities even for structures outside the training set. The attention mechanism was then utilized to investigate the model's success. The analysis identified dangling molecular branches as a key predictor of thermal conductivity, leading us to define the dangling mass ratio (DMR), a descriptor that quantifies the fraction of atomic mass in dangling branches relative to the total COF mass. Feature importance assessments on regression models confirm the significance of DMR in predicting thermal conductivity. These findings indicate that COFs with dangling functional groups exhibit lower thermal transfer capabilities. Molecular dynamics simulations support this observation, revealing significant mismatches in the vibrational density of states due to the presence of dangling branches.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Symmetry-Aware Efficient Simulation of Quantum Systems and Beyond</title>
<link>https://arxiv.org/abs/2303.11409</link>
<guid>https://arxiv.org/abs/2303.11409</guid>
<content:encoded><![CDATA[
<div> tensor networks, quantum simulation, symmetry, machine learning, scalable approaches <br />
<br />
Summary: Physics-informed tensor networks are key in efficiently simulating complex quantum systems. By incorporating symmetry, such as $U(1)$-symmetric tensor networks, computational costs are reduced, enabling larger simulations. These networks are accelerated on GPUs and scaled to supercomputers, making them essential for quantum simulation, computation, and machine learning. Physics-informed design extends beyond symmetry, including hybrid tensor networks and parallel sequential circuits, to enhance efficiency through different principles. This Perspective emphasizes the importance of physics-informed tensor networks, incorporating both symmetry and beyond-symmetry insights, as unifying strategies for scalable approaches in quantum simulation, computation, and machine learning. <br /><br /> <div>
arXiv:2303.11409v2 Announce Type: replace-cross 
Abstract: The efficient simulation of complex quantum systems remains a central challenge due to the exponential growth of Hilbert space with system size. Tensor network methods have long been established as powerful approximation schemes, and their efficiency can be further enhanced by incorporating physics-informed priors. A prominent example is symmetry: recent progress on $U(1)$-symmetric tensor networks, accelerated on GPUs and scaled to supercomputers, shows how conserved charges induce block-sparse structures that reduce computational cost and enable larger simulations. The same principle extends to general symmetries, inspiring equivariant neural networks in machine learning and guiding symmetry-preserving ansatze in variational quantum algorithms. Beyond symmetry, physics-informed design also includes strategies such as hybrid tensor networks and parallel sequential circuits, which pursue efficiency from complementary principles. This Perspective argues that physics-informed tensor networks, grounded in both symmetry and beyond-symmetry insights, provide unifying strategies for scalable approaches in quantum simulation, computation, and machine learning.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing trust in automated reasoning</title>
<link>https://arxiv.org/abs/2309.12351</link>
<guid>https://arxiv.org/abs/2309.12351</guid>
<content:encoded><![CDATA[
<div> automated reasoning, machine learning, trust, reviewability, science<br />
<br />
Summary: 
The article discusses the importance of automated reasoning in scientific research and the shift towards formulating rules through machine learning techniques. It highlights the need for trust in these systems and the results they produce, an issue often overlooked by practitioners. The focus is on independent reviewing as a key factor in building trust in automated reasoning systems. The article identifies the characteristics that affect the reviewability of these systems and proposes a combination of technical and social measures to enhance their trustworthiness. By emphasizing the importance of transparency and accountability in automated reasoning, the article aims to bridge the gap between the philosophical discussions on trust in science and practical considerations in research. <div>
arXiv:2309.12351v2 Announce Type: replace-cross 
Abstract: Since its beginnings in the 1940s, automated reasoning by computers has become a tool of ever growing importance in scientific research. So far, the rules underlying automated reasoning have mainly been formulated by humans, in the form of program source code. Rules derived from large amounts of data, via machine learning techniques, are a complementary approach currently under intense development. The question of why we should trust these systems, and the results obtained with their help, has been discussed by philosophers of science but has so far received little attention by practitioners. The present work focuses on independent reviewing, an important source of trust in science, and identifies the characteristics of automated reasoning systems that affect their reviewability. It also discusses possible steps towards increasing reviewability and trustworthiness via a combination of technical and social measures.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach</title>
<link>https://arxiv.org/abs/2510.13937</link>
<guid>https://arxiv.org/abs/2510.13937</guid>
<content:encoded><![CDATA[
<div> machine learning, rock classification, mineral composition, 1D-CNN, geological expertise

Summary:
A new study introduces a novel approach for automated rock classification based on mineral composition, addressing a significant challenge in geological applications. The study combines geological domain expertise with spectral analysis to enhance deep learning models. Evaluation of machine learning methods, including 1D-CNN and its uncertainty-aware variant, shows excellent performance in mineral classification. However, the classification of rock types from mineral assemblages poses challenges, especially for rocks with similar mineral compositions. Results vary across different lithologies, with limestone classification achieving optimal accuracy. The study underscores the complexity of automated geological classification systems and highlights the need for methodological advancements in material characterization and sorting technologies. <div>
arXiv:2510.13937v1 Announce Type: new 
Abstract: Automated rock classification from mineral composition presents a significant challenge in geological applications, with critical implications for material recycling, resource management, and industrial processing. While existing methods using One dimensional Convolutional Neural Network (1D-CNN) excel at mineral identification through Raman spectroscopy, the crucial step of determining rock types from mineral assemblages remains unsolved, particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance (98.37+-0.006% and 97.75+-0.010% respectively). The integrated system's evaluation on rock samples revealed variable performance across lithologies, with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading</title>
<link>https://arxiv.org/abs/2510.14264</link>
<guid>https://arxiv.org/abs/2510.14264</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model (LLM), automated trading, reinforcement learning (RL), transparency, interpretable reasoning

Summary:<br /><br />
The article introduces AlphaQuanter, a single-agent framework using reinforcement learning to learn a dynamic policy for automated trading. Unlike multi-agent frameworks, AlphaQuanter addresses inefficiency and inconsistency issues by enabling a single agent to orchestrate tools and acquire information proactively. The framework is built on a transparent, tool-augmented decision workflow, allowing for end-to-end optimization and coherent strategy learning from market feedback. Through extensive experiments, AlphaQuanter achieves state-of-the-art performance on financial metrics and its interpretable reasoning reveals sophisticated trading strategies. The framework provides novel insights for human traders and enhances transparency and audibility in the trading process. The code for data acquisition and agent training is publicly available on GitHub at: https://github.com/AlphaQuanter/AlphaQuanter. <div>
arXiv:2510.14264v1 Announce Type: new 
Abstract: While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Neural ODE Approach for Real Time Evaluation of AC Losses in 3D Superconducting Tapes</title>
<link>https://arxiv.org/abs/2510.14487</link>
<guid>https://arxiv.org/abs/2510.14487</guid>
<content:encoded><![CDATA[
<div> Keywords: High Temperature Superconductors, Reduced-order modeling, Integral Equation Method, Proper Orthogonal Decomposition, Neural Ordinary Differential Equation <br />
Summary: 
This article explores efficient modeling techniques for High Temperature Superconductors (HTS) to improve real-time quench monitoring. Conventional full-order electromagnetic simulations are costly due to strong nonlinearities. The study investigates reduced-order strategies for HTS systems using Integral Equation Method (IEM). The researchers apply Proper Orthogonal Decomposition (POD) and Discrete Empirical Interpolation Method (DEIM) to IEM-based HTS models. They also introduce a Structured Neural Ordinary Differential Equation (Neural ODE) approach to learn nonlinear dynamics in the reduced space. Benchmark results demonstrate that the Neural ODE outperforms POD-DEIM in efficiency and accuracy, making it a promising tool for real-time superconducting simulations. <br /><br />Summary: <div>
arXiv:2510.14487v1 Announce Type: new 
Abstract: Efficient modeling of High Temperature Superconductors (HTS) is crucial for real-time quench monitoring; however, full-order electromagnetic simulations remain prohibitively costly due to the strong nonlinearities. Conventional reduced-order methods, such as the Proper Orthogonal Decomposition (POD) and Discrete Empirical Interpolation Method (DEIM), alleviate this cost but are limited by intrusive implementation and by the need for many interpolation points. This work investigates reduced-order strategies for Integral Equation Method (IEM) of HTS systems. We present the first application of POD-DEIM to IEM-based HTS models, and introduce a Structured Neural Ordinary Differential Equation (Neural ODE) approach that learns nonlinear dynamics directly in the reduced space. Benchmark results show that the Neural ODE outperforms POD-DEIM in both efficiency and accuracy, highlighting its potential for real-time superconducting simulations.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics</title>
<link>https://arxiv.org/abs/2510.14944</link>
<guid>https://arxiv.org/abs/2510.14944</guid>
<content:encoded><![CDATA[
<div> benchmark, metabolomics, large language models, evaluation, AI systems

Summary:
MetaBench is introduced as the first benchmark for evaluating the capabilities of Large Language Models (LLMs) in the specialized scientific domain of metabolomics. The benchmark assesses five essential capabilities for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Results from evaluating 25 LLMs show that while models perform well on text generation tasks, they struggle with cross-database identifier grounding and long-tail metabolites with sparse annotations. The study highlights the challenges LLMs face in leveraging complex biochemical pathways, heterogeneous identifier systems, and fragmented databases in metabolomics research. MetaBench provides crucial infrastructure for the development and evaluation of AI systems in metabolomics, aiming to facilitate progress towards reliable computational tools for metabolomics research. 

<br /><br />Summary: <div>
arXiv:2510.14944v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Program of Thoughts for Financial Reasoning: Leveraging Dynamic In-Context Examples and Generative Retrieval</title>
<link>https://arxiv.org/abs/2510.13157</link>
<guid>https://arxiv.org/abs/2510.13157</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, financial numerical reasoning, FINDER, state-of-the-art, performance improvement

Summary:
FINDER is a new two-step framework designed to enhance large language models' (LLMs) performance in financial numerical reasoning tasks. The first step involves using a generative retriever to extract relevant information from unstructured data, including text and tables. In the second step, context-aware Program of Thought prompting is applied with dynamic selection of in-context examples. This approach significantly improves performance on financial numerical reasoning datasets like FinQA and ConvFinQA, surpassing previous benchmarks with execution accuracy improvements of 5.98% and 4.05%, respectively. FINDER's success demonstrates the effectiveness of combining information extraction with context-aware prompting techniques in enhancing LLMs' capabilities in dealing with complex numerical reasoning tasks. <br /><br />Summary: FINDER, a two-step framework, utilizes a generative retriever to extract relevant facts and context-aware prompting to improve LLMs' performance in financial numerical reasoning, achieving state-of-the-art results on FinQA and ConvFinQA datasets. <div>
arXiv:2510.13157v1 Announce Type: new 
Abstract: Despite continuous advancements in the capabilities of large language models (LLMs), numerical reasoning remains a challenging area. Techniques like chain-of-thought prompting, tree-of-thought prompting, and program-of-thought prompting guide LLMs through intermediate reasoning steps. Although in-context learning with few-shot prompting has improved performance, LLMs still lag behind state-of-the-art models on financial numerical reasoning datasets such as FinQA and ConvFinQA. In this work, we introduce FINDER, a novel two-step framework, to enhance LLMs' capabilities in financial numerical reasoning. The first step utilizes a generative retriever to extract relevant facts from unstructured data, including both text and tables. This is followed by context-aware Program of Thought prompting with dynamic selection of in-context examples. Our model FINDER achieves a new state-of-the-art performance on both the FinQA and ConvFinQA datasets, surpassing previous benchmarks with execution accuracy improvements of 5.98% and 4.05%, respectively.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Constitutive Model Discovery from Sparse and Noisy Data</title>
<link>https://arxiv.org/abs/2510.13559</link>
<guid>https://arxiv.org/abs/2510.13559</guid>
<content:encoded><![CDATA[
<div> Framework, Virtual Fields Method, statFEM, unsupervised constitutive model discovery, isotropic hyperelastic materials <br />
Summary: <br />
This article introduces a new framework, statFEM--EUCLID, that combines the statistical finite element method (statFEM) with the EUCLID approach for unsupervised constitutive model discovery. The aim is to address issues of measurement noise and data sparsity that affect VFM-based approaches. By integrating statFEM with EUCLID, the framework is able to reconstruct displacement fields while enforcing consistency with equilibrium and constitutive laws. The study focuses on isotropic hyperelastic materials and shows that the integration reduces sensitivity to noise and data sparsity. The results demonstrate that statFEM--EUCLID provides a more robust approach to constitutive model discovery in the presence of uncertainties, offering both reliable field reconstruction and interpretable constitutive models. <div>
arXiv:2510.13559v1 Announce Type: new 
Abstract: Recently, unsupervised constitutive model discovery has gained attention through frameworks based on the Virtual Fields Method (VFM), most prominently the EUCLID approach. However, the performance of VFM-based approaches, including EUCLID, is affected by measurement noise and data sparsity, which are unavoidable in practice. The statistical finite element method (statFEM) offers a complementary perspective by providing a Bayesian framework for assimilating noisy and sparse measurements to reconstruct the full-field displacement response, together with quantified uncertainty. While statFEM recovers displacement fields under uncertainty, it does not strictly enforce consistency with constitutive relations or aim to yield interpretable constitutive models. In this work, we couple statFEM with unsupervised constitutive model discovery in the EUCLID framework, yielding statFEM--EUCLID. The framework is demonstrated for isotropic hyperelastic materials. The results show that this integration reduces sensitivity to noise and data sparsity, while ensuring that the reconstructed fields remain consistent with both equilibrium and constitutive laws.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs</title>
<link>https://arxiv.org/abs/2510.12839</link>
<guid>https://arxiv.org/abs/2510.12839</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Factuality Evaluation, Claim Extraction, Evidence Collection, Verification <br />
Summary: <br />
The article introduces a new evaluation framework called FastFact to assess the factuality of long-form generations from Large Language Models (LLMs). It addresses the inefficiency and ineffectiveness of existing methods by implementing chunk-level claim extraction with pre-verification and document-level evidence collection. FastFact significantly reduces the cost of web searching and verification while ensuring reliability, making it the most efficient and accurate among current approaches. The framework collects evidence from crawled webpages and selectively retrieves it during verification, overcoming the insufficiency of evidence found in previous pipelines. Extensive experiments on a manually annotated benchmark showcase the effectiveness and efficiency of FastFact in evaluating the factuality of LLM-generated text. The code and benchmark data for FastFact are available on GitHub for further exploration and usage. <br /> <div>
arXiv:2510.12839v1 Announce Type: cross 
Abstract: Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to accuracy issues and costly human assessment. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to complex pipeline components unsuitable for long LLM outputs, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence collection of one-line snippets.
  To address these limitations, we propose \name, a fast and strong evaluation framework that achieves the highest alignment with human evaluation and efficiency among existing baselines. \name first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the cost of web searching and inference calling while ensuring reliability. For searching and verification, it collects document-level evidence from crawled webpages and selectively retrieves it during verification, addressing the evidence insufficiency problem in previous pipelines.
  Extensive experiments based on an aggregated and manually annotated benchmark demonstrate the reliability of \name in both efficiently and effectively evaluating the factuality of long-form LLM generations. Code and benchmark data is available at https://github.com/Yingjia-Wan/FastFact.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the alignment problem in transportation policy making: an LLM approach</title>
<link>https://arxiv.org/abs/2510.13139</link>
<guid>https://arxiv.org/abs/2510.13139</guid>
<content:encoded><![CDATA[
<div> model-driven decision tools, transportation planning, large language models, multi-agent simulation, collective preferences <br />
Summary: <br />
The article explores using large language models (LLMs) to address the misalignment between traveler preferences and transportation policies. By creating a multi-agent simulation with LLMs representing residents in a city, the study analyzes how LLMs can inform transit policy decisions. Results show that LLM agents can approximate collective preferences and adapt to local contexts, but also exhibit behavioral biases and slight deviations from optimization-based benchmarks. This highlights both the potential and limitations of LLMs in solving the alignment problem in transportation planning. <div>
arXiv:2510.13139v1 Announce Type: cross 
Abstract: A key challenge in transportation planning is that the collective preferences of heterogeneous travelers often diverge from the policies produced by model-driven decision tools. This misalignment frequently results in implementation delays or failures. Here, we investigate whether large language models (LLMs), noted for their capabilities in reasoning and simulating human decision-making, can help inform and address this alignment problem. We develop a multi-agent simulation in which LLMs, acting as agents representing residents from different communities in a city, participate in a referendum on a set of transit policy proposals. Using chain-of-thought reasoning, LLM agents provide ranked-choice or approval-based preferences, which are aggregated using instant-runoff voting (IRV) to model democratic consensus. We implement this simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago and Houston. Our findings suggest that LLM agents are capable of approximating plausible collective preferences and responding to local context, while also displaying model-specific behavioral biases and modest divergences from optimization-based benchmarks. These capabilities underscore both the promise and limitations of LLMs as tools for solving the alignment problem in transportation decision-making.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GO-Diff: Data-free and amortized global structure optimization</title>
<link>https://arxiv.org/abs/2510.13448</link>
<guid>https://arxiv.org/abs/2510.13448</guid>
<content:encoded><![CDATA[
<div> Keywords: GO-Diff, global structure optimization, diffusion-based method, energy function, Boltzmann-weighted score-matching loss

Summary: 
GO-Diff is a novel diffusion-based method that optimizes global atomic structures without requiring prior data or explicit relaxation. It utilizes a Boltzmann-weighted score-matching loss to guide the generation of low-energy configurations. The method operates in a self-sampling and model refinement loop, continuously improving its ability to target favorable structures. Compared to traditional optimization pipelines, GO-Diff achieves competitive results with fewer energy evaluations. Additionally, it supports amortized optimization by reusing pretrained models across similar systems, enabling faster convergence on new tasks without retraining from scratch. <div>
arXiv:2510.13448v1 Announce Type: cross 
Abstract: We introduce GO-Diff, a diffusion-based method for global structure optimization that learns to directly sample low-energy atomic configurations without requiring prior data or explicit relaxation. GO-Diff is trained from scratch using a Boltzmann-weighted score-matching loss, leveraging only the known energy function to guide generation toward thermodynamically favorable regions. The method operates in a two-stage loop of self-sampling and model refinement, progressively improving its ability to target low-energy structures. Compared to traditional optimization pipelines, GO-Diff achieves competitive results with significantly fewer energy evaluations. Moreover, by reusing pretrained models across related systems, GO-Diff supports amortized optimization - enabling faster convergence on new tasks without retraining from scratch.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile Coverage Analysis using Crowdsourced Data</title>
<link>https://arxiv.org/abs/2510.13459</link>
<guid>https://arxiv.org/abs/2510.13459</guid>
<content:encoded><![CDATA[
<div> coverage analysis, mobile network, weak spot analysis, QoE data, crowdsourced

Summary:
The paper introduces a novel framework for mobile network coverage assessment and weak spot identification using crowdsourced QoE data. It focuses on analyzing coverage at the individual cell level, then aggregating to the site level using geolocation data. A One-Class Support Vector Machine (OC-SVM) algorithm is employed to calculate mobile network coverage, effectively mapping coverage areas for cells and sites. This methodology is extended to analyze crowdsourced service loss reports, pinpointing localized weak spots. The research demonstrates the framework's accuracy in mapping mobile coverage and highlighting signal deficiencies, especially in urban areas. The framework offers a targeted approach to enhancing user Quality of Experience (QoE) by identifying and addressing specific areas of network improvement. 

<br /><br />Summary: <div>
arXiv:2510.13459v1 Announce Type: cross 
Abstract: Effective assessment of mobile network coverage and the precise identification of service weak spots are paramount for network operators striving to enhance user Quality of Experience (QoE). This paper presents a novel framework for mobile coverage and weak spot analysis utilising crowdsourced QoE data. The core of our methodology involves coverage analysis at the individual cell (antenna) level, subsequently aggregated to the site level, using empirical geolocation data. A key contribution of this research is the application of One-Class Support Vector Machine (OC-SVM) algorithm for calculating mobile network coverage. This approach models the decision hyperplane as the effective coverage contour, facilitating robust calculation of coverage areas for individual cells and entire sites. The same methodology is extended to analyse crowdsourced service loss reports, thereby identifying and quantifying geographically localised weak spots. Our findings demonstrate the efficacy of this novel framework in accurately mapping mobile coverage and, crucially, in highlighting granular areas of signal deficiency, particularly within complex urban environments.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multifractality and its sources in the digital currency market</title>
<link>https://arxiv.org/abs/2510.13785</link>
<guid>https://arxiv.org/abs/2510.13785</guid>
<content:encoded><![CDATA[
<div> Keywords: Multifractality, Time series analysis, Digital currency markets, Long-range temporal correlations, Heavy-tailed distributions

Summary: 
Multifractality in time series analysis involves multiple scaling exponents, indicating complex dynamical behaviors beyond simple models. In digital currency markets, multifractal properties arise from long-range temporal correlations and heavy-tailed return distributions. Multifractal analysis enhances understanding of market inefficiencies and aids in volatility forecasting. The study applied the method of disentangling sources of multifractality to Bitcoin, Ethereum, decentralized exchanges, and non-fungible tokens. Results show that heavy tails play a significant role in generating a broad multifractal spectrum. Temporal correlations are the primary source of multifractality, independent of tail thickness. This observation in the digital currency market supports the validity of the disentangling methodology for understanding multifractality in time series.<br /><br />Summary: <div>
arXiv:2510.13785v1 Announce Type: cross 
Abstract: Multifractality in time series analysis characterizes the presence of multiple scaling exponents, indicating heterogeneous temporal structures and complex dynamical behaviors beyond simple monofractal models. In the context of digital currency markets, multifractal properties arise due to the interplay of long-range temporal correlations and heavy-tailed distributions of returns, reflecting intricate market microstructure and trader interactions. Incorporating multifractal analysis into the modeling of cryptocurrency price dynamics enhances the understanding of market inefficiencies, may improve volatility forecasting and facilitate the detection of critical transitions or regime shifts. Based on the multifractal cross-correlation analysis (MFCCA) whose spacial case is the multifractal detrended fluctuation analysis (MFDFA), as the most commonly used practical tools for quantifying multifractality, in the present contribution a recently proposed method of disentangling sources of multifractality in time series was applied to the most representative instruments from the digital market. They include Bitcoin (BTC), Ethereum (ETH), decentralized exchanges (DEX) and non-fungible tokens (NFT). The results indicate the significant role of heavy tails in generating a broad multifractal spectrum. However, they also clearly demonstrate that the primary source of multifractality are temporal correlations in the series, and without them, multifractality fades out. It appears characteristic that these temporal correlations, to a large extent, do not depend on the thickness of the tails of the fluctuation distribution. These observations, made here in the context of the digital currency market, provide a further strong argument for the validity of the proposed methodology of disentangling sources of multifractality in time series.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical reduced order modelling for the parametric Helmholtz equation</title>
<link>https://arxiv.org/abs/2407.04438</link>
<guid>https://arxiv.org/abs/2407.04438</guid>
<content:encoded><![CDATA[
<div> statistical finite element method, predictive modeling, reduced order model, sensor data, uncertainty

Summary: 
The paper discusses the challenge of predictive modeling using simulation and sensor data, focusing on the statistical finite element method (statFEM). It addresses the mismatch between simulation and sensor data due to uncertainty sources and proposes a reduced order statFEM framework using Krylov-based moment matching. This framework includes a data model that considers the bias induced by the reduced order approximation, estimated by an error indicator. The method aims to improve accuracy and convergence speed compared to the standard statFEM procedure applied to a reduced order model. Results from numerical examples show better accuracy and faster convergence across different frequency ranges. The proposed approach enhances the efficiency and effectiveness of predictive modeling in computational science by integrating a reduced order model with statistical methods to handle uncertainty in simulations. 

<br /><br />Summary: <div>
arXiv:2407.04438v2 Announce Type: replace 
Abstract: Predictive modeling involving simulation and sensor data at the same time, is a growing challenge in computational science. Even with large-scale finite element models, a mismatch to the sensor data often remains, which can be attributed to different sources of uncertainty. For such a scenario, the statistical finite element method (statFEM) can be used to condition a simulated field on given sensor data. This yields a posterior solution which resembles the data much better and additionally provides consistent estimates of uncertainty, including model misspecification. For frequency or parameter dependent problems, occurring, e.g. in acoustics or electromagnetism, solving the full order model at the frequency grid and conditioning it on data quickly results in a prohibitive computational cost. In this case, the introduction of a surrogate in form of a reduced order model yields much smaller systems of equations. In this paper, we propose a reduced order statFEM framework relying on Krylov-based moment matching. We introduce a data model which explicitly includes the bias induced by the reduced approximation, which is estimated by an inexpensive error indicator. The results of the new statistical reduced order method are compared to the standard statFEM procedure applied to a ROM prior, i.e. without explicitly accounting for the reduced order bias. The proposed method yields better accuracy and faster convergence throughout a given frequency range for different numerical examples.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deciphering the Crypto-shopper: Knowledge and Preferences of Consumers Using Cryptocurrencies for Purchases</title>
<link>https://arxiv.org/abs/2310.02911</link>
<guid>https://arxiv.org/abs/2310.02911</guid>
<content:encoded><![CDATA[
<div> Keywords: cryptocurrency, shopping habits, knowledge levels, purchase frequency, demographic segmentation <br />
<br />
Summary: 
The study examines the knowledge, expertise, and purchasing behavior of individuals using cryptocurrencies. Of the 516 participants surveyed, there was a range of knowledge levels, with around 30% displaying high purchase frequency despite limited knowledge. While domain knowledge was found to influence purchasing frequency to some extent, it only explained a small portion of the variance. Through K-means cluster analysis, respondents were divided into three distinct groups based on knowledge levels and shopping tendencies. The findings challenge the notion that extensive knowledge directly correlates with increased cryptocurrency usage, indicating other factors come into play. Businesses need to understand this diverse crypto-shopper demographic to tailor strategies and enhance user experiences. This research provides insights into current crypto-shopping behaviors and suggests future studies to explore broader impacts and potential shifts in the crypto-consumer landscape. <br /><br /> <div>
arXiv:2310.02911v5 Announce Type: replace-cross 
Abstract: The fast-growing cryptocurrency sector presents both challenges and opportunities for businesses and consumers alike. This study investigates the knowledge, expertise, and buying habits of people who shop using cryptocurrencies. Our survey of 516 participants shows that knowledge levels vary from beginners to experts. Interestingly, a segment of respondents, nearly 30%, showed high purchase frequency despite their limited knowledge. Regression analyses indicated that while domain knowledge plays a role, it only accounts for 11.6% of the factors affecting purchasing frequency. A K-means cluster analysis further segmented the respondents into three distinct groups, each having unique knowledge levels and purchasing tendencies. These results challenge the conventional idea linking extensive knowledge to increased cryptocurrency usage, suggesting other factors at play. Understanding this varying crypto-shopper demographic is pivotal for businesses, emphasizing the need for tailored strategies and user-friendly experiences. This study offers insights into current crypto-shopping behaviors and discusses future research exploring the broader impacts and potential shifts in the crypto-consumer landscape.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAID-0e: A Resilient Striping Array Architecture for Balanced Performance and Availability</title>
<link>https://arxiv.org/abs/2510.12139</link>
<guid>https://arxiv.org/abs/2510.12139</guid>
<content:encoded><![CDATA[
<div> RAID-0e, disk array architecture, fault tolerance layer, data protection, RAID 0, operational resilience <br />
Summary: <br />
This paper introduces RAID-0e, a novel disk array architecture that enhances data resilience by incorporating a separate parity domain to protect the primary data domain in traditional RAID 0. RAID-0e mitigates the risk of array-wide data loss from common media failures while maintaining the read performance advantages of RAID 0. The architecture's operational workflows, performance characteristics, failure mode analysis, and security considerations are comprehensively discussed. RAID-0e is designed for environments that prioritize I/O performance, storage cost, and data resilience over full drive failure concerns. It offers a pragmatic solution for balancing performance and data protection in scenarios where non-catastrophic media failures are a primary concern. <div>
arXiv:2510.12139v1 Announce Type: new 
Abstract: This paper introduces a novel disk array architecture, designated RAID-0e (Resilient Striping Array), designed to superimpose a low-overhead fault tolerance layer upon traditional RAID 0 (striping). By employing a logically and physically separate parity domain to protect a primary data domain, RAID-0e mitigates the risk of array-wide data loss from common, non-catastrophic media failures, such as isolated bad blocks, transient read errors, or sector-level corruption. The architecture is engineered to preserve the intrinsic read performance advantages of RAID 0 while significantly enhancing data availability and operational resilience. This document provides a comprehensive exposition of the architectural principles, operational workflows, performance characteristics, failure mode analysis, and security considerations of RAID-0e. It is presented as an experimental yet pragmatic solution for environments seeking a new equilibrium between I/O performance, storage cost, and data resilience, particularly where full drive failure is a secondary concern to media degradation.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Simulation of a Financial Market with Large Language Models</title>
<link>https://arxiv.org/abs/2510.12189</link>
<guid>https://arxiv.org/abs/2510.12189</guid>
<content:encoded><![CDATA[
<div> Keywords: stock markets, chart patterns, path dependence, human loss aversion, large language models

Summary:
In the study, the authors explore the presence of path dependence in stock markets, where investor decisions are influenced by historical price movements in addition to current market conditions. The phenomenon suggests the role of human loss aversion, anchored to individual reference points, in shaping investor behavior. To capture these subtle behavioral tendencies, the authors propose the Fundamental-Chartist-LLM-Agent framework, which uses large language models to emulate human-like trading decisions. Through simulations, the FCLAgents demonstrate the ability to reproduce path-dependent patterns that traditional agents miss. Additionally, an analysis of FCLAgents' behavior reveals that reference points guiding loss aversion vary with market trajectories, indicating the potential of LLM-based agents to model nuanced investor behavior. This research sheds light on the complex interactions between human psychology and market dynamics in real-world stock markets. 

<br /><br />Summary: <div>
arXiv:2510.12189v1 Announce Type: new 
Abstract: In real-world stock markets, certain chart patterns -- such as price declines near historical highs -- cannot be fully explained by fundamentals alone. These phenomena suggest the presence of path dependence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices leading up to the present. Path dependence has drawn attention in behavioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow standard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Furthermore, an analysis of FCLAgents' behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Sensing and Reliable State Estimation with Shallow Recurrent Decoders on a TRIGA Mark II Reactor</title>
<link>https://arxiv.org/abs/2510.12368</link>
<guid>https://arxiv.org/abs/2510.12368</guid>
<content:encoded><![CDATA[
<div> deep learning, state estimation, engineering systems, nuclear reactors, Shallow Recurrent Decoder networks

Summary: 
Shallow Recurrent Decoder networks are a data-driven methodology for accurate state estimation in engineering systems like nuclear reactors. This technique maps sparse measurements to the full state space, handling noisy data without hyperparameter tuning. The study applies this approach to a fluid dynamics model of the TRIGA Mark II research reactor, using synthetic and experimental temperature data. The objectives are to reconstruct the full system state and assess correction capabilities. Results show the architecture can accurately reconstruct characteristic fields in real-time, making it suitable for monitoring and control in a reactor digital twin. <div>
arXiv:2510.12368v1 Announce Type: new 
Abstract: Shallow Recurrent Decoder networks are a novel data-driven methodology able to provide accurate state estimation in engineering systems, such as nuclear reactors. This deep learning architecture is a robust technique designed to map the temporal trajectories of a few sparse measures to the full state space, including unobservable fields, which is agnostic to sensor positions and able to handle noisy data through an ensemble strategy, leveraging the short training times and without the need for hyperparameter tuning. Following its application to a novel reactor concept, this work investigates the performance of Shallow Recurrent Decoders when applied to a real system. The underlying model is represented by a fluid dynamics model of the TRIGA Mark II research reactor; the architecture will use both synthetic temperature data coming from the numerical model and leveraging experimental temperature data recorded during a previous campaign. The objective of this work is, therefore, two-fold: 1) assessing if the architecture can reconstruct the full state of the system (temperature, velocity, pressure, turbulence quantities) given sparse data located in specific, low-dynamics channels and 2) assessing the correction capabilities of the architecture (that is, given a discrepancy between model and data, assessing if sparse measurements can provide some correction to the architecture output). As will be shown, the accurate reconstruction of every characteristic field, using both synthetic and experimental data, in real-time makes this approach suitable for interpretable monitoring and control purposes in the framework of a reactor digital twin.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proceedings of the International Workshop on Verification of Scientific Software</title>
<link>https://arxiv.org/abs/2510.12314</link>
<guid>https://arxiv.org/abs/2510.12314</guid>
<content:encoded><![CDATA[
<div> workshop, Verification of Scientific Software, challenges, correctness, reliability<br />
Summary: 
The VSS 2025 workshop, held at McMaster University, focused on addressing challenges in ensuring the correctness and reliability of large-scale scientific codes. The event featured five peer-reviewed papers, three invited contributions, and challenge problems, covering topics such as deductive verification, floating-point error analysis, and domain-aware testing. The workshop built on previous Correctness Workshop series and the 2023 NSF/DOE report on scientific software correctness. It showcased a variety of perspectives, problems, and solutions in progress, with the potential for challenge problems to facilitate collaboration among different verification tools. Overall, VSS serves as an important snapshot of the intersection between software verification and scientific computing, highlighting ongoing efforts to enhance the reliability and accuracy of scientific codes. <br /><br />  <div>
arXiv:2510.12314v1 Announce Type: cross 
Abstract: This volume contains the proceedings of the Verification of Scientific Software (VSS 2025) workshop, held on 4 May 2025 at McMaster University, Canada, as part of ETAPS 2025. VSS brings together researchers in software verification and scientific computing to address challenges in ensuring the correctness and reliability of large-scale scientific codes. The program featured five peer-reviewed papers, three invited contributions, and a set of challenge problems, covering themes such as deductive verification, floating-point error analysis, specification of coupled models, and domain-aware testing. VSS builds on the Correctness Workshop series at Supercomputing and the 2023 NSF/DOE report on scientific software correctness. It serves as yet another snapshot of this important area, showcasing a wide range of perspectives, problems and their solutions in progress, with the challenge problems having the potential to bring together separate verification tools into concerted action.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Health-promoting Potential of Parks in 35 Cities Worldwide</title>
<link>https://arxiv.org/abs/2407.15770</link>
<guid>https://arxiv.org/abs/2407.15770</guid>
<content:encoded><![CDATA[
<div> Keywords: urban parks, health-related activities, park quality, city centers, public health

Summary:
Urban parks play a crucial role in public health by supporting various health-related activities such as physical exercise, mindfulness, nature appreciation, environmental education, social interaction, and cultural experiences. A study analyzing 23,477 parks in 35 global cities revealed distinct patterns in park design and quality. Parks in North America tend to focus more on physical activities, while European parks offer more opportunities for nature-based experiences. City center parks generally outperform suburban parks in terms of supporting health-promoting activities. Disparities in park quality were noted between cities, with Tokyo and Paris exhibiting more equal access to health-promoting spaces compared to Copenhagen and Rio de Janeiro. These findings highlight the need for cities to create more equitable urban parks that cater to a diverse range of health-enhancing activities, ultimately improving public health outcomes. 

<br /><br />Summary: <div>
arXiv:2407.15770v2 Announce Type: replace-cross 
Abstract: Urban parks are important for public health, but the role of specific spaces, such as playgrounds or lakes, and elements, such as benches or sports equipment, in supporting well-being is not well understood. Based on expert input and a review of the literature, we defined six types of health-related activities: physical, mindfulness, nature appreciation, environmental, social, and cultural. We built a lexicon that links each activity to specific elements and spaces within parks present in OpenStreetMap. Using this data, we scored 23,477 parks across 35 cities worldwide based on their ability to support these activities. We found clear patterns: parks in North America focus more on physical activity, while those in Europe offer more chances to enjoy nature. Parks near city centers support health-promoting activities better than those farther out. Suburban parks in many cities lack the spaces and equipment needed for nature-based, social, and cultural activities. We also found large gaps in park quality between cities. Tokyo and Paris provide more equal access, while Copenhagen and Rio de Janeiro show sharp contrasts. These results can help cities create fairer parks that better support public health.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Glaucoma Report Generation via Dual-Attention Semantic Parallel-LSTM and Multimodal Clinical Data Integration</title>
<link>https://arxiv.org/abs/2510.10037</link>
<guid>https://arxiv.org/abs/2510.10037</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, glaucoma diagnostic, multimodal architectures, Dual-Attention Semantic Parallel-LSTM Network, clinical reports

Summary: 
The article introduces the Dual-Attention Semantic Parallel-LSTM Network (DA-SPL) to improve automated glaucoma diagnostic report generation. The challenges of content redundancy and inadequate highlighting of key features in traditional models are addressed through DA-SPL's advanced framework. DA-SPL utilizes a dual-attention mechanism in the encoder, parallelized LSTM decoder architecture, and label enhancement module for accurate report generation. Evaluation on glaucoma datasets shows DA-SPL outperforming existing models in extracting pathological indicators and generating precise clinical reports aligned with expert annotations. The model's ability to process fundus imaging and supplementary visual inputs enhances its diagnostic accuracy and semantic consistency in producing comprehensive reports.<br /><br />Summary: <div>
arXiv:2510.10037v1 Announce Type: new 
Abstract: Generative AI for automated glaucoma diagnostic report generation faces two predominant challenges: content redundancy in narrative outputs and inadequate highlighting of pathologically significant features including optic disc cupping, retinal nerve fiber layer defects, and visual field abnormalities. These limitations primarily stem from current multimodal architectures' insufficient capacity to extract discriminative structural-textural patterns from fundus imaging data while maintaining precise semantic alignment with domain-specific terminology in comprehensive clinical reports. To overcome these constraints, we present the Dual-Attention Semantic Parallel-LSTM Network (DA-SPL), an advanced multimodal generation framework that synergistically processes both fundus imaging and supplementary visual inputs. DA-SPL employs an Encoder-Decoder structure augmented with the novel joint dual-attention mechanism in the encoder for cross-modal feature refinement, the parallelized LSTM decoder architecture for enhanced temporal-semantic consistency, and the specialized label enhancement module for accurate disease-relevant term generation. Rigorous evaluation on standard glaucoma datasets demonstrates DA-SPL's consistent superiority over state-of-the-art models across quantitative metrics. DA-SPL exhibits exceptional capability in extracting subtle pathological indicators from multimodal inputs while generating diagnostically precise reports that exhibit strong concordance with clinical expert annotations.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GrifFinNet: A Graph-Relation Integrated Transformer for Financial Predictions</title>
<link>https://arxiv.org/abs/2510.10387</link>
<guid>https://arxiv.org/abs/2510.10387</guid>
<content:encoded><![CDATA[
<div> Transformer, financial predictions, graph modeling, spatio-temporal dynamics, market behavior

Summary: 
GrifFinNet is a novel model proposed for predicting stock returns in the financial market by integrating multi-relational graph modeling with Transformer-based temporal encoding. The model constructs inter-stock relation graphs based on industry sectors and institutional ownership, enhancing its ability to capture spatial dependencies and temporal patterns. GrifFinNet incorporates an adaptive gating mechanism to dynamically integrate relational data, providing a comprehensive representation of market dynamics. Experimental results on Chinese A-share indices demonstrate that GrifFinNet outperforms baseline models consistently, offering valuable and interpretable insights into financial market behavior. The code and data for GrifFinNet are available for further exploration and analysis. <div>
arXiv:2510.10387v1 Announce Type: new 
Abstract: Predicting stock returns remains a central challenge in quantitative finance, transitioning from traditional statistical methods to contemporary deep learning techniques. However, many current models struggle with effectively capturing spatio-temporal dynamics and integrating multiple relational data sources. This study proposes GrifFinNet, a Graph-Relation Integrated Transformer for Financial Predictions, which combines multi-relational graph modeling with Transformer-based temporal encoding. GrifFinNet constructs inter-stock relation graphs based on industry sectors and institutional ownership, and incorporates an adaptive gating mechanism to dynamically integrate relational data in response to changing market conditions. This approach enables the model to jointly capture spatial dependencies and temporal patterns, offering a comprehensive representation of market dynamics. Extensive experiments on two Chinese A-share indices show that GrifFinNet consistently outperforms several baseline models and provides valuable, interpretable insights into financial market behavior. The code and data are available at: https://www.healthinformaticslab.org/supp/.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameterized crack modelling based on a localized non-intrusive reduced basis method</title>
<link>https://arxiv.org/abs/2510.10624</link>
<guid>https://arxiv.org/abs/2510.10624</guid>
<content:encoded><![CDATA[
<div> model order reduction, parametric modelling, cracks, spline discretizations, non-intrusive reduced basis methods

Summary: 
This article introduces a model order reduction strategy for fast parametric modelling of problems involving cracks on spline discretizations. The approach focuses on using non-intrusive reduced basis methods and a localization strategy tailored to parametric problems with moving discontinuities. By efficiently separating the offline and online simulation processes, the proposed framework allows for fast computations and accurate results. Tests conducted on linear elastic problems using splines and the extended isogeometric method demonstrate the effectiveness of the reduced order models in terms of both accuracy and real-time efficiency. This method shows promise for applications in damage detection and other areas where fast and accurate simulations are required. <div>
arXiv:2510.10624v1 Announce Type: new 
Abstract: This contribution presents a model order reduction strategy for fast parametric modelling of problems with cracks formulated on spline discretizations. In the context of damage detection, parametric reduced order models (ROMs) are well suited for fast computations by establishing an efficient offline/online split of the simulation process. The problems of interest focus on geometric parameters that describe the crack configuration and may pose challenges to constructing efficient ROMs. This work proposes a framework based on non-intrusive reduced basis methods and a localization strategy tailored to parametric problems with moving discontinuities. The combined benefits of non-intrusive ROMs and localization enable accurate and efficient reduction with low online cost. We demonstrate the applicability of the ROM approach with benchmark tests on linear elastic problems discretized with splines and the extended isogeometric method (XIGA) for crack modelling. The results we obtain show the accuracy and real-time efficiency of the constructed reduced order models.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence of coronary plaque morphology on local mechanical states and associated in-stent restenosis</title>
<link>https://arxiv.org/abs/2510.10763</link>
<guid>https://arxiv.org/abs/2510.10763</guid>
<content:encoded><![CDATA[
<div> Keywords: in-stent restenosis, morphological characteristics, local mechanical factors, stress distributions, computational simulations<br />
<br />
Summary: 
This study focuses on the relationship between specific morphological characteristics of coronary artery lesions and the occurrence of in-stent restenosis after percutaneous coronary intervention. By conducting computational simulations based on patient-specific coronary artery models, the researchers analyze the impact of plaque composition on local mechanical factors, particularly stress distributions in the artery wall during and after stent implantation. The findings suggest that morphological features like circumferential or asymmetric block calcifications can lead to higher stresses in the surrounding tissue, increasing the risk of in-stent restenosis. Therefore, understanding and evaluating local tensional stresses are crucial for assessing individual in-stent restenosis risk. This research sheds light on the importance of considering both morphological characteristics and local mechanical factors in predicting and managing in-stent restenosis. <br /><br /> <div>
arXiv:2510.10763v1 Announce Type: new 
Abstract: In-stent restenosis after percutaneous coronary intervention is a multifactorial process. Specific morphological lesion characteristics were observed to contribute to the occurrence of in-stent restenosis. Local mechanical factors, such as stresses and strains, are known to influence tissue adaptation after stent implantation. However, the influence of morphological features on those local mechanical states and, hence, on the occurrence of in-stent restenosis remains understudied. This work investigates the correlation between local mechanical quantities and in-stent restenosis by evaluating the stress distributions in the artery wall during and after stent implantation for informative lesion morphologies. We perform computational simulations of the stenting procedure with physics-based patient-specific coronary artery models. Different morphologies are assessed using the spatial plaque composition information from high-resolution coronary computed tomography angiography data. We quantify the correlation between in-stent restenosis and local tensional stresses. We found that specific morphological characteristics like circumferential or asymmetric block calcifications result in higher stresses in the surrounding tissue. This study concludes that local stresses are critical for assessing the individual in-stent restenosis risk.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Earth-Observing Satellite Sampling Effectiveness Using Kullback-Leibler Divergence</title>
<link>https://arxiv.org/abs/2510.10859</link>
<guid>https://arxiv.org/abs/2510.10859</guid>
<content:encoded><![CDATA[
<div> Earth-Observing Satellites; Sampling Effectiveness; Kullback-Leibler Divergence; Observational Representativeness; Monsoon
Summary:
This work presents a methodology for assessing the representativeness of geophysical variables sampled by Earth-observing satellites. The study evaluates the effectiveness of 20 satellite configurations for observing convective storm activity in the Southwestern U.S. during the North American Monsoon season. Results indicate that a two-satellite sun-synchronous system with an 8:00 PM LTAN achieved the most representative observation of storm clusters. Single-satellite configurations, especially those with late-night LTANs, had significantly higher KL divergence. The study concludes that dual-satellite configurations in sun-synchronous orbits with evening LTANs outperform single-satellite and inclined configurations in capturing representative convective storm activity. <div>
arXiv:2510.10859v1 Announce Type: new 
Abstract: This work presents an objective, repeatable, automatic, and fast methodology for assessing the representativeness of geophysical variables sampled by Earth-observing satellites. The primary goal is to identify and mitigate potential sampling biases attributed to orbit selection during pre-Phase A mission studies. This methodology supports current incubation activities for a future Planetary Boundary Layer observing system by incorporating a sampling effectiveness measure into a broader architectural study. The study evaluates the effectiveness of 20 satellite configurations for observing convective storm activity in the Southwestern U.S. during the North American Monsoon (NAM) season. The primary design variables are the number of satellites, orbit type (sun-synchronous or inclined), and Local Time of Ascending Node (LTAN). Using Kullback-Leibler (KL) divergence to assess observational representativeness and Kernel Density Estimation (KDE) to estimate probability density functions, the study quantifies the discrepancy between observed and ground truth storm features. Results indicate that a two-satellite sun-synchronous system with an 8:00 PM LTAN, achieved the lowest KL divergence, signifying the most representative observation of storm clusters. In contrast, single-satellite configurations, particularly those with late-night LTANs (e.g., 12:00 AM), demonstrated significantly higher KL divergence. The study concludes that dual-satellite configurations in sun-synchronous orbits with evening LTANs outperform single-satellite and inclined configurations in capturing representative convective storm activity. Keywords: Earth-Observing Satellites; Sampling Effectiveness; Kullback-Leibler Divergence; Observational Representativeness; Monsoon
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Evaluation of Neural Network Architectures for Generalizable Human Spatial Preference Prediction in Unseen Built Environments</title>
<link>https://arxiv.org/abs/2510.10954</link>
<guid>https://arxiv.org/abs/2510.10954</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Convolutional Neural Networks, deep learning, preference modeling, built environments <br />
<br />
Summary: 
This study focuses on predicting human spatial preferences within built environments to develop Cyber-Physical-Social Infrastructure Systems. The research compares Graph Neural Networks, Convolutional Neural Networks, and standard feedforward Neural Networks using synthetic data from a pocket park environment. The goal is to assess the generalizability of preference models to unseen layouts. The models are evaluated based on their ability to predict preferences influenced by physical, environmental, and social features. A generalizability score is calculated using the area under the precision-recall curve for seen and unseen layouts. This score is suitable for imbalanced data and provides insights into which neural network architecture is most effective for preference-aware human behavior modeling in unfamiliar built environments. <div>
arXiv:2510.10954v1 Announce Type: new 
Abstract: The capacity to predict human spatial preferences within built environments is instrumental for developing Cyber-Physical-Social Infrastructure Systems (CPSIS). A significant challenge in this domain is the generalizability of preference models, particularly their efficacy in predicting preferences within environmental configurations not encountered during training. While deep learning models have shown promise in learning complex spatial and contextual dependencies, it remains unclear which neural network architectures are most effective at generalizing to unseen layouts. To address this, we conduct a comparative study of Graph Neural Networks, Convolutional Neural Networks, and standard feedforward Neural Networks using synthetic data generated from a simplified and synthetic pocket park environment. Beginning with this illustrative case study, allows for controlled analysis of each model's ability to transfer learned preference patterns to unseen spatial scenarios. The models are evaluated based on their capacity to predict preferences influenced by heterogeneous physical, environmental, and social features. Generalizability score is calculated using the area under the precision-recall curve for the seen and unseen layouts. This generalizability score is appropriate for imbalanced data, providing insights into the suitability of each neural network architecture for preference-aware human behavior modeling in unseen built environments.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Physics-Enhanced Bayesian Inverse Analysis: Information Gain from Additional Fields</title>
<link>https://arxiv.org/abs/2510.11095</link>
<guid>https://arxiv.org/abs/2510.11095</guid>
<content:encoded><![CDATA[
<div> Bayesian inverse analysis, computational models, multi-physics data, uncertainty reduction,
information gain <br />
<br />
Summary: Many inverse problems in real-world applications suffer from limited data, leading to uncertainty in parameter estimation using Bayesian analysis. This study proposes enhancing the inverse analysis by incorporating data from additional physical fields into computational models. By extending the models to include these fields, even weakly or one-way coupled, the information gain from the prior to the posterior can be significantly increased. The research demonstrates the effectiveness of this multi-physics-enhanced approach using both simple and complex models. The results show that even a small amount of data from an additional physical field can greatly reduce uncertainty. This method presents a cost- and time-saving alternative to setting up additional experimental setups, making it a valuable tool across various scientific and industrial applications. <br /><br /> <div>
arXiv:2510.11095v1 Announce Type: new 
Abstract: Many real-world inverse problems suffer from limited data, often because they rely on measurements of a single physical field. Such data frequently fail to sufficiently reduce parameter uncertainty in Bayesian inverse analysis. Incorporating easily available data from additional physical fields can substantially decrease this uncertainty. We focus on Bayesian inverse analyses based on computational models, e.g., those using the finite element method. To incorporate data from additional physical fields, the computational model must be extended to include these fields. While this model extension may have little to no effect on forward model predictions, it can greatly enhance inverse analysis by leveraging the multi-physics data. Our work proposes this multi-physics-enhanced inverse approach and demonstrates its potential using two models: a simple model with one-way coupled fields and a complex computational model with fully coupled fields. We quantify the uncertainty reduction by comparing the effect of single-physics and multi-physics data on the information gain from the prior to the posterior. Our results show that even a few or noisy data points from an additional physical field can considerably increase the information gain, even if this field is weakly or one-way coupled. Although multi-physics data are often readily available, it is remarkable that their potential has been largely neglected in model calibration so far. Instead, costly and time-consuming additional experimental setups are often pursued. In contrast, incorporating multi-physics data requires minimal effort when multi-physics models are readily available or easy to implement, as is the case with uncoupled and one-way coupled models. This work proposes and promotes the future use of multi-physics-enhanced Bayesian inverse analysis as a cost- and time-saving game-changer across various fields of science and industry.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A mathematical model for pricing perishable goods for quick-commerce applications</title>
<link>https://arxiv.org/abs/2510.11360</link>
<guid>https://arxiv.org/abs/2510.11360</guid>
<content:encoded><![CDATA[
<div> q-commerce, informal employment, perishable goods, mathematical model, simulation

Summary:
The paper discusses the rapid growth of quick commerce (q-commerce) in India, providing informal employment to a large number of workers. The industry primarily deals with perishable goods, leading to high order volumes and repetitive purchases. A key challenge for retailers is finding the optimal pricing strategy for these goods to maximize revenue while avoiding unsold inventory. A mathematical model is proposed to address this pricing dilemma, aiming to improve the unit economics of q-commerce firms and potentially benefit gig workers. The simulation results will be presented in a future study. This research is significant not only for the industry's financial sustainability but also for its social impact on gig workers. <div>
arXiv:2510.11360v1 Announce Type: new 
Abstract: Quick commerce (q-commerce) is one of the fastest growing sectors in India. It provides informal employment to approximately 4,50,000 workers, and it is estimated to become a USD 200 Billion industry by 2026. A significant portion of this industry deals with perishable goods. (e.g. milk, dosa batter etc.) These are food items which are consumed relatively fresh by the consumers and therefore their order volume is high and repetitive even when the average basket size is relatively small. The fundamental challenge for the retailer is that, increasing selling price would hamper sales and would lead to unsold inventory. On the other hand setting a price less, would lead to forgoing of potential revenue. This paper attempts to propose a mathematical model which formalizes this dilemma. The problem statement is not only important for improving the unit economics of the perennially loss making quick commerce firms, but also would lead to a trickle-down effect in improving the conditions of the gig workers as observed in [4]. The sections below describe the mathematical formulation. The results from the simulation would be published in a follow-up study.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LRQ-Solver: A Transformer-Based Neural Operator for Fast and Accurate Solving of Large-scale 3D PDEs</title>
<link>https://arxiv.org/abs/2510.11636</link>
<guid>https://arxiv.org/abs/2510.11636</guid>
<content:encoded><![CDATA[
<div> low-rank query attention, PDE solver, physics-integrated framework, scalability, efficiency
Summary:
The article introduces LRQ-Solver, a framework designed for fast, accurate, and scalable simulations of complex three-dimensional geometries. It combines Parameter Conditioned Lagrangian Modeling (PCLM) to link physical states with design parameters, enhancing generalization, and a Low-Rank Query Attention (LR-QA) module to reduce computational complexity. LRQ-Solver reduces error rates on benchmark datasets, achieves significant training speedups, and handles up to 2 million points on a single GPU efficiently. By providing a powerful solution for multi-configuration physics simulations, LRQ-Solver demonstrates state-of-the-art performance in accuracy, scalability, and efficiency. The code for experiments can also be accessed on GitHub at https://github.com/LilaKen/LRQ-Solver.<br /><br />Summary: <div>
arXiv:2510.11636v1 Announce Type: new 
Abstract: Solving large-scale Partial Differential Equations (PDEs) on complex three-dimensional geometries represents a central challenge in scientific and engineering computing, often impeded by expensive pre-processing stages and substantial computational overhead. We introduce Low-Rank Query-based PDE Solver (LRQ-Solver), a physics-integrated framework engineered for rapid, accurate, and highly scalable simulations of industrial-grade models. This framework is built upon two primary technical innovations. First, our Parameter Conditioned Lagrangian Modeling (PCLM) approach explicitly couples local physical states with global design parameters, enabling robust predictions across varied simulation configurations. By embedding physical consistency directly into the learning architecture, PCLM ensures that predictions remain physically meaningful even under unseen design conditions, significantly enhancing generalization and reliability. Second, the Low-Rank Query Attention (LR-QA) module leverages the second-order statistics of physical fields to construct a global coherence kernel, reducing the computational complexity of attention from O(N2) to O(NC2 + C3). By replacing point-wise clustering with covariance decomposition, LRQ-Solver achieves exceptional scalability efficiently processing up to 2 million points on a single GPU. Validated on standard benchmarks, LRQ-Solver achieves a 38.9% error reduction on the DrivAer++ dataset and 28.76% on the 3D Beam dataset, alongside a training speedup of up to 50 times. Our results establish that LRQ-Solver offers a powerful paradigm for multi-configuration physics simulations, delivering a SOTA combination of accuracy, scalability, and efficiency. Code to reproduce the experiments is available at https://github.com/LilaKen/LRQ-Solver.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Relationship between Space-Time Accessibility and Leisure Activity Participation</title>
<link>https://arxiv.org/abs/2510.10307</link>
<guid>https://arxiv.org/abs/2510.10307</guid>
<content:encoded><![CDATA[
<div> Keywords: accessibility, leisure activities, space-time, capability approach, urban mobility

Summary: 
The study introduces a space-time accessibility metric based on the capability approach to assess how accessibility influences participation in leisure activities in urban areas. Using GPS data from residents in the Paris region, the research examines how space-time accessibility impacts travel time and diversity of leisure activity locations. Spatial patterns indicate that individuals, particularly active transport users, select destinations based on their opportunity sets defined by space-time accessibility. Structural equation modeling shows that space-time accessibility directly enhances leisure diversity while reducing travel time, which in turn decreases diversity. The findings emphasize the importance of person-centered, capability-informed accessibility metrics for understanding urban mobility inequalities and guiding transport planning initiatives aimed at expanding opportunities for social participation across diverse population groups. 

Summary: <div>
arXiv:2510.10307v1 Announce Type: cross 
Abstract: Understanding how accessibility shapes participation in leisure activities is central to promoting inclusive and vibrant urban life. Conventional accessibility measures often focus on potential access from fixed home locations, overlooking the constraints and opportunities embedded in daily routines. In this study, we introduce a space-time accessibility (SPA) metric rooted in the capability approach, capturing feasible leisure opportunities between home and work given a certain time budget, individual transport modes, and urban infrastructure. Using high-resolution GPS data from 2,415 residents in the Paris region, we assess how SPA influences total travel time and leisure participation, measured as the diversity of leisure activity locations. Spatial patterns show that most individuals-especially active transport users-choose destinations aligned with their SPA-defined opportunity sets, underscoring the metric's validity in capturing capability sets. Structural equation modeling reveals that SPA directly fosters leisure diversity but also reduces travel time, which in turn is associated with lower diversity. These findings highlight the value of person-centered, capability-informed accessibility metrics for understanding inequalities in urban mobility and informing transport planning strategies that expand real freedoms to participate in social life across diverse population groups.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance</title>
<link>https://arxiv.org/abs/2510.10402</link>
<guid>https://arxiv.org/abs/2510.10402</guid>
<content:encoded><![CDATA[
<div> diffusion, graph generation, Monte Carlo Tree Search, controllable, scalability

Summary:
TreeDiff is a novel framework for controllable graph generation using Monte Carlo Tree Search (MCTS). It addresses limitations of existing methods by introducing a macro-step expansion strategy, dual-space denoising, and a dual-space verifier. These innovations allow TreeDiff to efficiently explore long-horizon paths, maintain structural fidelity, and predict long-term rewards for partial graph denoising. Experimental results on molecular generation benchmarks demonstrate that TreeDiff outperforms existing methods, particularly in scaling up with additional computation. The framework shows superior performance in both unconditional and conditional settings, showcasing its potential for diverse applications in graph learning domains. Its scalability and controllability make it a promising approach for graph generation tasks, providing a valuable tool for a range of applications requiring high-quality graph synthesis. 

<br /><br />Summary: <div>
arXiv:2510.10402v1 Announce Type: cross 
Abstract: Graph generation is a fundamental problem in graph learning with broad applications across Web-scale systems, knowledge graphs, and scientific domains such as drug and material discovery. Recent approaches leverage diffusion models for step-by-step generation, yet unconditional diffusion offers little control over desired properties, often leading to unstable quality and difficulty in incorporating new objectives. Inference-time guidance methods mitigate these issues by adjusting the sampling process without retraining, but they remain inherently local, heuristic, and limited in controllability. To overcome these limitations, we propose TreeDiff, a Monte Carlo Tree Search (MCTS) guided dual-space diffusion framework for controllable graph generation. TreeDiff is a plug-and-play inference-time method that expands the search space while keeping computation tractable. Specifically, TreeDiff introduces three key designs to make it practical and scalable: (1) a macro-step expansion strategy that groups multiple denoising updates into a single transition, reducing tree depth and enabling long-horizon exploration; (2) a dual-space denoising mechanism that couples efficient latent-space denoising with lightweight discrete correction in graph space, ensuring both scalability and structural fidelity; and (3) a dual-space verifier that predicts long-term rewards from partially denoised graphs, enabling early value estimation and removing the need for full rollouts. Extensive experiments on 2D and 3D molecular generation benchmarks, under both unconditional and conditional settings, demonstrate that TreeDiff achieves state-of-the-art performance. Notably, TreeDiff exhibits favorable inference-time scaling: it continues to improve with additional computation, while existing inference-time methods plateau early under limited resources.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying and Quantifying Financial Bubbles with the Hyped Log-Periodic Power Law Model</title>
<link>https://arxiv.org/abs/2510.10878</link>
<guid>https://arxiv.org/abs/2510.10878</guid>
<content:encoded><![CDATA[
<div> Keywords: Hyped Log-Periodic Power Law Model, financial bubbles, sentiment scores, dual-stream transformer model, backtesting

Summary:
The Hyped Log-Periodic Power Law Model (HLPPL) is proposed as a novel approach to quantifying and detecting financial bubbles, incorporating bubble labels, sentiment scores, and a hype index. A dual-stream transformer model is trained using market data and machine learning methods, producing a Bubble Score time series. The model demonstrates an average annualized return of 34.13% when backtested on U.S. equities from 2018 to 2024, with strong generalization across industry sectors. Its conservative bias reduces false positives, making it useful for market signaling. The framework effectively identifies extreme overpricing and underpricing phases in a unified structure, offering real-time bubble identification and measurement using HLPPL signals. <br /><br />Summary: The HLPPL model combines theoretical and empirical advancements to quantify and detect financial bubbles, showing promising results in backtesting on U.S. equities. Its dual-stream transformer approach, incorporating sentiment analysis and hype index, contributes to accurate positive and negative bubble identification with a conservative bias to minimize false positives, enhancing market decision-making capabilities. <div>
arXiv:2510.10878v1 Announce Type: cross 
Abstract: We propose a novel model, the Hyped Log-Periodic Power Law Model (HLPPL), to the problem of quantifying and detecting financial bubbles, an ever-fascinating one for academics and practitioners alike. Bubble labels are generated using a Log-Periodic Power Law (LPPL) model, sentiment scores, and a hype index we introduced in previous research on NLP forecasting of stock return volatility. Using these tools, a dual-stream transformer model is trained with market data and machine learning methods, resulting in a time series of confidence scores as a Bubble Score. A distinctive feature of our framework is that it captures phases of extreme overpricing and underpricing within a unified structure.
  We achieve an average yield of 34.13 percentage annualized return when backtesting U.S. equities during the period 2018 to 2024, while the approach exhibits a remarkable generalization ability across industry sectors. Its conservative bias in predicting bubble periods minimizes false positives, a feature which is especially beneficial for market signaling and decision-making. Overall, this approach utilizes both theoretical and empirical advances for real-time positive and negative bubble identification and measurement with HLPPL signals.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Structural Engineering Workflows with Large Language Model Agents</title>
<link>https://arxiv.org/abs/2510.11004</link>
<guid>https://arxiv.org/abs/2510.11004</guid>
<content:encoded><![CDATA[
<div> Keywords: MASSE, Multi-Agent System, Structural Engineering, Large Language Model, Automation

Summary:
MASSE is introduced as the first Multi-Agent System for Structural Engineering, integrating large language model (LLM)-based agents with engineering workflows. Structural engineering, a traditionally stagnant domain, stands to benefit from recent advancements in LLMs for tasks like interpreting design codes and load calculations. A proof-of-concept demonstrates full automation of real-world engineering workflows with MASSE, reducing expert workload significantly. The system offers immediate deployment in professional environments and enhances reliability and accuracy in engineering scenarios. MASSE showcases the potential for LLM-based automation to revolutionize structural engineering practices. <div>
arXiv:2510.11004v1 Announce Type: cross 
Abstract: We introduce $\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hot-Starting Quantum Portfolio Optimization</title>
<link>https://arxiv.org/abs/2510.11153</link>
<guid>https://arxiv.org/abs/2510.11153</guid>
<content:encoded><![CDATA[
<div> Combinatorial optimization, convex objective function, adiabatic quantum optimization, quantum annealer, discrete solutions<br />
Summary:<br />
This study addresses the challenge of combinatorial optimization with a smooth and convex objective function. It focuses on applications like discrete mean-variance portfolio optimization, where assets are traded in integer quantities. Existing quantum optimization methods struggle to utilize optimal solutions efficiently. The study introduces a novel approach that constructs a compact Hilbert space to restrict the search space to discrete solutions near the continuous optimum, reducing the required number of qubits. Experiments on software solvers and a D-Wave Advantage quantum annealer showcase the superiority of this method over current techniques. The approach not only outperforms state-of-the-art methods, but also integrates insights from the relaxed continuous solution into the QUBO formulation, paving the way for improved quantum optimization strategies. <br />Summary: <div>
arXiv:2510.11153v1 Announce Type: cross 
Abstract: Combinatorial optimization with a smooth and convex objective function arises naturally in applications such as discrete mean-variance portfolio optimization, where assets must be traded in integer quantities. Although optimal solutions to the associated smooth problem can be computed efficiently, existing adiabatic quantum optimization methods cannot leverage this information. Moreover, while various warm-starting strategies have been proposed for gate-based quantum optimization, none of them explicitly integrate insights from the relaxed continuous solution into the QUBO formulation. In this work, a novel approach is introduced that restricts the search space to discrete solutions in the vicinity of the continuous optimum by constructing a compact Hilbert space, thereby reducing the number of required qubits. Experiments on software solvers and a D-Wave Advantage quantum annealer demonstrate that our method outperforms state-of-the-art techniques.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Molecular Property Prediction: A Survey</title>
<link>https://arxiv.org/abs/2510.08900</link>
<guid>https://arxiv.org/abs/2510.08900</guid>
<content:encoded><![CDATA[
<div> few-shot learning, molecular property prediction, AI, drug discovery, materials design

Summary:
This article introduces the concept of few-shot molecular property prediction (FSMPP), which addresses the challenge of limited labeled data in real-world molecules for AI-assisted prediction. The core challenges identified are cross-property generalization under distribution shifts and cross-molecule generalization under structural heterogeneity. The article presents a unified taxonomy to categorize existing methods for FSMPP based on data, model, and learning paradigm levels. It compares representative methods, discusses benchmark datasets, and evaluation protocols. The article concludes by highlighting key trends and suggesting future directions for research in FSMPP. <div>
arXiv:2510.08900v1 Announce Type: new 
Abstract: AI-assisted molecular property prediction has become a promising technique in early-stage drug discovery and materials design in recent years. However, due to high-cost and complex wet-lab experiments, real-world molecules usually experience the issue of scarce annotations, leading to limited labeled data for effective supervised AI model learning. In light of this, few-shot molecular property prediction (FSMPP) has emerged as an expressive paradigm that enables learning from only a few labeled examples. Despite rapidly growing attention, existing FSMPP studies remain fragmented, without a coherent framework to capture methodological advances and domain-specific challenges. In this work, we present the first comprehensive and systematic survey of few-shot molecular property prediction. We begin by analyzing the few-shot phenomenon in molecular datasets and highlighting two core challenges: (1) cross-property generalization under distribution shifts, where each task corresponding to each property, may follow a different data distribution or even be inherently weakly related to others from a biochemical perspective, requiring the model to transfer knowledge across heterogeneous prediction tasks, and (2) cross-molecule generalization under structural heterogeneity, where molecules involved in different or same properties may exhibit significant structural diversity, making model difficult to achieve generalization. Then, we introduce a unified taxonomy that organizes existing methods into data, model, and learning paradigm levels, reflecting their strategies for extracting knowledge from scarce supervision in few-shot molecular property prediction. Next, we compare representative methods, summarize benchmark datasets and evaluation protocols. In the end, we identify key trends and future directions for advancing the continued research on FSMPP.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Uncertainty Sets: Dependence of Uncertain Parameters via a Simple Polyhedral Set</title>
<link>https://arxiv.org/abs/2510.08843</link>
<guid>https://arxiv.org/abs/2510.08843</guid>
<content:encoded><![CDATA[
<div> Algorithm, Robust optimization, Uncertainty set, Pairwise differences, Numerical experiments <br /> 
<br />Summary: 
The article introduces a novel polyhedral uncertainty set called the smooth uncertainty set for robust optimization. This set captures dependencies between uncertain parameters by constraining their pairwise differences. The bounds on these differences can be determined based on expert knowledge or correlations. Specialized solution methods are explored, including compact reformulations and a column generation algorithm. Numerical experiments show that the smooth uncertainty set model performs similarly to the ellipsoidal uncertainty model but with shorter running times. Additionally, a column-generation algorithm proves to be more efficient than traditional approaches in terms of solution time and memory consumption. <div>
arXiv:2510.08843v1 Announce Type: cross 
Abstract: We propose a novel polyhedral uncertainty set for robust optimization, termed the smooth uncertainty set, which captures dependencies of uncertain parameters by constraining their pairwise differences. The bounds on these differences may be dictated by the underlying physics of the problem and may be expressed by domain experts. When correlations are available, the bounds can be set
  to ensure that the associated probabilistic constraints are satisfied for any given probability. We explore specialized solution methods for the resulting optimization problems, including compact reformulations that exploit special structures when
  they appear, a column generation algorithm, and a reformulation of the adversarial problem as a minimum-cost flow problem. Our numerical experiments, based on problems from literature, illustrate (i) that the performance of the smooth uncertainty set model solution is similar to that of the ellipsoidal uncertainty model solution, albeit, it is computed within significantly shorter running times, and (ii) our column-generation algorithm can outperform the classical cutting plane algorithm and dualized reformulation, respectively in terms of solution time and memory consumption.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-fidelity Batch Active Learning for Gaussian Process Classifiers</title>
<link>https://arxiv.org/abs/2510.08865</link>
<guid>https://arxiv.org/abs/2510.08865</guid>
<content:encoded><![CDATA[
<div> Gaussian Process, Multi-fidelity, Batch Active Learning, Mutual Information, Binary Simulation Output  
Summary:  
- The study focuses on optimizing simulation budgets using a multi-fidelity approach with a Gaussian Process (GP) model in binary simulation output scenarios.
- Bernoulli Parameter Mutual Information (BPMI) is introduced as a batch active learning algorithm for multi-fidelity GP classifiers, improving efficiency in parameter space exploration.
- BPMI addresses the challenge of calculating mutual information in probability space by utilizing a first-order Taylor expansion of the link function.
- Evaluation on synthetic and real-world simulation cases, such as a laser-ignited rocket combustor, shows that BPMI outperforms baseline methods by achieving higher predictive accuracy within a fixed computational budget.
- The results demonstrate the effectiveness of BPMI in accelerating the exploration of parameter space in expensive computational simulations. 

Summary: <div>
arXiv:2510.08865v1 Announce Type: cross 
Abstract: Many science and engineering problems rely on expensive computational simulations, where a multi-fidelity approach can accelerate the exploration of a parameter space. We study efficient allocation of a simulation budget using a Gaussian Process (GP) model in the binary simulation output case. This paper introduces Bernoulli Parameter Mutual Information (BPMI), a batch active learning algorithm for multi-fidelity GP classifiers. BPMI circumvents the intractability of calculating mutual information in the probability space by employing a first-order Taylor expansion of the link function. We evaluate BPMI against several baselines on two synthetic test cases and a complex, real-world application involving the simulation of a laser-ignited rocket combustor. In all experiments, BPMI demonstrates superior performance, achieving higher predictive accuracy for a fixed computational budget.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs</title>
<link>https://arxiv.org/abs/2510.08886</link>
<guid>https://arxiv.org/abs/2510.08886</guid>
<content:encoded><![CDATA[
<div> benchmark, financial auditing, LLMs, structured reasoning, taxonomy<br />
Summary:<br />
The article introduces FinAuditing, a benchmark for evaluating large language models (LLMs) on financial auditing tasks. It focuses on structured auditing reasoning using real US-GAAP-compliant XBRL filings, defining subtasks for semantic, relational, and numerical consistency. The evaluation framework integrates metrics across these subtasks. Experiments on 13 LLMs show inconsistent performance, particularly in reasoning over hierarchical structures, with accuracy drops up to 60-90%. The study reveals limitations in current LLMs for financial reasoning and highlights the need for structure-aware systems. FinAuditing aims to develop reliable financial intelligence systems aligned with regulations. The benchmark dataset is accessible through Hugging Face. 

<br />Summary: <div>
arXiv:2510.08886v1 Announce Type: cross 
Abstract: The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creation of the Chinese Adaptive Policy Communication Corpus</title>
<link>https://arxiv.org/abs/2510.08986</link>
<guid>https://arxiv.org/abs/2510.08986</guid>
<content:encoded><![CDATA[
<div> Keywords: CAPC-CG, Chinese policy directives, annotation, adaptive policy communication, NLP research<br />
Summary:<br />
- The Chinese Adaptive Policy Communication (Central Government) Corpus (CAPC-CG) is introduced as the first open dataset of Chinese policy directives annotated with a five-color taxonomy of clear and ambiguous language categories.
- The corpus spans from 1949 to 2023, including national laws, administrative regulations, and ministerial rules issued by China's top authorities, totaling 3.3 million units.
- Comprehensive metadata, a two-round labeling framework, and a gold-standard annotation set developed by expert and trained coders are provided alongside the corpus.
- Inter-annotator agreement achieves high reliability for supervised modeling, indicated by Fleiss's kappa of K = 0.86 on directive labels.
- Baseline classification results using large language models (LLMs) are presented, along with an annotation codebook and patterns from the dataset. This release aims to support downstream tasks and multilingual NLP research in policy communication.<br /> 
Summary: <div>
arXiv:2510.08986v1 Announce Type: cross 
Abstract: We introduce CAPC-CG, the Chinese Adaptive Policy Communication (Central Government) Corpus, the first open dataset of Chinese policy directives annotated with a five-color taxonomy of clear and ambiguous language categories, building on Ang's theory of adaptive policy communication. Spanning 1949-2023, this corpus includes national laws, administrative regulations, and ministerial rules issued by China's top authorities. Each document is segmented into paragraphs, producing a total of 3.3 million units. Alongside the corpus, we release comprehensive metadata, a two-round labeling framework, and a gold-standard annotation set developed by expert and trained coders. Inter-annotator agreement achieves a Fleiss's kappa of K = 0.86 on directive labels, indicating high reliability for supervised modeling. We provide baseline classification results with several large language models (LLMs), together with our annotation codebook, and describe patterns from the dataset. This release aims to support downstream tasks and multilingual NLP research in policy communication.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised full-field Bayesian inference of orthotropic hyperelasticity from a single biaxial test: a myocardial case study</title>
<link>https://arxiv.org/abs/2510.09498</link>
<guid>https://arxiv.org/abs/2510.09498</guid>
<content:encoded><![CDATA[
<div> discovery, constitutive models, parameter identification, orthotropic, uncertainty quantification <br />
<br />
Summary: In the study, the authors address the challenge of capturing complex tissue behavior in traditional testing methods by using heterogeneous deformation profiles. They employ the EUCLID method, along with Bayesian inference and three-dimensional continuum elements, to identify material model parameters for highly nonlinear, orthotropic constitutive models. The approach demonstrates success in quantitatively inferring these parameters from a single heterogeneous biaxial stretch test, even in the presence of noise. The results show good agreement with ground-truth simulations and credibility intervals, highlighting the potential for characterizing such material models with uncertainty quantification from a single test. This method offers a promising avenue for efficient parameter estimation in tissue testing, reducing the need for multiple samples and extensive manipulations. <div>
arXiv:2510.09498v1 Announce Type: cross 
Abstract: Fully capturing this behavior in traditional homogenized tissue testing requires the excitation of multiple deformation modes, i.e. combined triaxial shear tests and biaxial stretch tests. Inherently, such multimodal experimental protocols necessitate multiple tissue samples and extensive sample manipulations. Intrinsic inter-sample variability and manipulation-induced tissue damage might have an adverse effect on the inversely identified tissue behavior. In this work, we aim to overcome this gap by focusing our attention to the use of heterogeneous deformation profiles in a parameter estimation problem. More specifically, we adapt EUCLID, an unsupervised method for the automated discovery of constitutive models, towards the purpose of parameter identification for highly nonlinear, orthotropic constitutive models using a Bayesian inference approach and three-dimensional continuum elements. We showcase its strength to quantitatively infer, with varying noise levels, the material model parameters of synthetic myocardial tissue slabs from a single heterogeneous biaxial stretch test. This method shows good agreement with the ground-truth simulations and with corresponding credibility intervals. Our work highlights the potential for characterizing highly nonlinear and orthotropic material models from a single biaxial stretch test with uncertainty quantification.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Rigorous Modeling of Antenna--Medium Interactions Above Planar Stratified Media via the Generalized Scattering Matrix</title>
<link>https://arxiv.org/abs/2504.12613</link>
<guid>https://arxiv.org/abs/2504.12613</guid>
<content:encoded><![CDATA[
<div> Keywords: reflection coefficients, antennas, planar layered media, spherical vector wave functions, numerical implementation <br />
Summary: 
A rigorous method for evaluating reflection coefficients of antennas above planar layered media is presented. The approach uses the antenna's generalized scattering matrix (GSM) expressed in spherical vector wave functions (SVWFs) and spherical-to-planar vector wave transformations to model interaction with the layered structure. This formulation reduces algebraic complexity, enabling fast numerical implementation with high accuracy. Each evaluation for a specific configuration can be completed within milliseconds, offering significant speed improvement over full-wave solvers like FEKO. The method is suitable for real-time electromagnetic characterization and inverse modeling in planar layered environments. <br /><br />Summary: <div>
arXiv:2504.12613v2 Announce Type: replace 
Abstract: A rigorous and computationally efficient method is presented for evaluating the reflection coefficients of antennas operating above planar layered media. The approach reformulates the problem within the framework of the antenna's generalized scattering matrix (GSM), expressed in terms of spherical vector wave functions (SVWFs). The mutual interaction between the antenna and the layered structure is modeled through spherical-to-planar vector wave transformations that incorporate the exact Fresnel reflection response of the medium, without introducing any simplifying approximations. This formulation dramatically reduces algebraic complexity and enables fast, stable numerical implementation. Excluding the one-time preprocessing required to obtain the antenna's free-space GSM, each evaluation for a given layered configuration can be completed within milliseconds -- achieving several orders of magnitude speed improvement over full-wave solvers such as FEKO, while maintaining virtually identical accuracy. The proposed framework thus provides a powerful foundation for real-time electromagnetic characterization and inverse modeling involving planar layered environments.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IKNet: Interpretable Stock Price Prediction via Keyword-Guided Integration of News and Technical Indicators</title>
<link>https://arxiv.org/abs/2510.07661</link>
<guid>https://arxiv.org/abs/2510.07661</guid>
<content:encoded><![CDATA[
<div> interpretability, stock forecasting, news articles, contextual analysis, FinBERT <br />
Summary: 
The article introduces a new forecasting model, IKNet, which focuses on the interpretability of stock price movements based on news articles. IKNet utilizes keyword analysis to identify salient keywords through FinBERT-based contextual analysis. It then integrates these keywords with technical indicators to predict next-day closing prices. The model outperforms existing models, reducing RMSE by 32.9% and improving cumulative returns by 18.5% on S&amp;P 500 data from 2015 to 2024. IKNet not only provides accurate predictions but also offers quantifiable and interpretable attributions for the impact of each keyword on the forecasts. This approach enhances transparency by offering context-aware explanations for volatility events driven by public sentiment. <div>
arXiv:2510.07661v1 Announce Type: new 
Abstract: The increasing influence of unstructured external information, such as news articles, on stock prices has attracted growing attention in financial markets. Despite recent advances, most existing newsbased forecasting models represent all articles using sentiment scores or average embeddings that capture the general tone but fail to provide quantitative, context-aware explanations of the impacts of public sentiment on predictions. To address this limitation, we propose an interpretable keyword-guided network (IKNet), which is an explainable forecasting framework that models the semantic association between individual news keywords and stock price movements. The IKNet identifies salient keywords via FinBERTbased contextual analysis, processes each embedding through a separate nonlinear projection layer, and integrates their representations with the time-series data of technical indicators to forecast next-day closing prices. By applying Shapley Additive Explanations the model generates quantifiable and interpretable attributions for the contribution of each keyword to predictions. Empirical evaluations of S&amp;P 500 data from 2015 to 2024 demonstrate that IKNet outperforms baselines, including recurrent neural networks and transformer models, reducing RMSE by up to 32.9% and improving cumulative returns by 18.5%. Moreover, IKNet enhances transparency by offering contextualized explanations of volatility events driven by public sentiment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Forecasting of Network Dynamics through Weight Flow Matching</title>
<link>https://arxiv.org/abs/2510.07957</link>
<guid>https://arxiv.org/abs/2510.07957</guid>
<content:encoded><![CDATA[
<div> forecasting, network systems, deep learning, coefficient shift, zero-shot accuracy

Summary:
- Forecasting state evolution of network systems is crucial for effective policy interventions and resource management.
- Deep learning models face challenges in adapting to shifting propagation dynamics without retraining.
- Zero-Shot Forecasting of Network Dynamics through Weight Flow Matching (FNFM) is presented as a solution.
- FNFM is a generative framework that generates dynamic model weights for unseen coefficients, enabling zero-shot forecasting.
- The framework utilizes a Variational Encoder and Conditional Flow Matching module to achieve accurate predictions without gradient-based optimization.
- Empirical results show that FNFM outperforms baseline methods, especially under significant coefficient shifts. 

<br /><br />Summary: <div>
arXiv:2510.07957v1 Announce Type: new 
Abstract: Forecasting state evolution of network systems, such as the spread of information on social networks, is significant for effective policy interventions and resource management. However, the underlying propagation dynamics constantly shift with new topics or events, which are modeled as changing coefficients of the underlying dynamics. Deep learning models struggle to adapt to these out-of-distribution shifts without extensive new data and retraining. To address this, we present Zero-Shot Forecasting of Network Dynamics through Weight Flow Matching (FNFM), a generative, coefficient-conditioned framework that generates dynamic model weights for an unseen target coefficient, enabling zero-shot forecasting. Our framework utilizes a Variational Encoder to summarize the forecaster weights trained in observed environments into compact latent tokens. A Conditional Flow Matching (CFM) module then learns a continuous transport from a simple Gaussian distribution to the empirical distribution of these weights, conditioned on the dynamical coefficients. This process is instantaneous at test time and requires no gradient-based optimization. Across varied dynamical coefficients, empirical results indicate that FNFM yields more reliable zero-shot accuracy than baseline methods, particularly under pronounced coefficient shift.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse Supply Chain Network Design of a Polyurethane Waste Upcycling System</title>
<link>https://arxiv.org/abs/2510.08097</link>
<guid>https://arxiv.org/abs/2510.08097</guid>
<content:encoded><![CDATA[
<div> Keywords: supply chain, plastic waste, upcycling, mathematical programming, economic potential <br />
Summary:<br />
This paper presents a mathematical programming framework for optimizing supply chain infrastructures for upcycling plastic waste, using a multi-product, multi-echelon, multi-period mixed-integer linear programming model. The objective is to minimize costs from waste collection to high-value polymer production, considering various constraints. The framework aids in strategic planning by determining optimal facility numbers, sizes, and material transportation. A case study on rigid polyurethane foam waste upcycling in Germany shows economic feasibility is currently lacking compared to fossil-based feedstock. However, with appropriate incentives, there is potential for competitive value chains once technology and economic frameworks stabilize. <br /><br />Summary: <div>
arXiv:2510.08097v1 Announce Type: new 
Abstract: This paper presents a general mathematical programming framework for the design and optimization of supply chain infrastructures for the upcycling of plastic waste. For this purpose, a multi-product, multi-echelon, multi-period mixed-integer linear programming (MILP) model has been formulated. The objective is to minimize the cost of the entire circular supply chain starting from the collection of post-consumer plastic waste to the production of virgin-equivalent high value polymers, satisfying a large number of constraints from collection quota to the quality of the feedstock. The framework aims to support the strategic planning of future circular supply chains by determining the optimal number, locations and sizes of various types of facilities as well as the amounts of materials to be transported between the nodes of the supply chain network over a specified period. The functionality of the framework has been tested with a case study for the upcycling of rigid polyurethane foam waste coming from construction sites in Germany. The economic potential and infrastructure requirements are evaluated, and it has been found that from a solely economic perspective, the current status of the value chain is not competitive with fossil-based feedstock or incineration. However, with the right economic incentives, there is a considerable potential to establish such value chains, once the upcycling technology is ready and the economic framework conditions have stabilized.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Poisson Energy Formulation for Floorplanning: Variational Analysis and Mathematical Foundations</title>
<link>https://arxiv.org/abs/2510.08126</link>
<guid>https://arxiv.org/abs/2510.08126</guid>
<content:encoded><![CDATA[
<div> floorplanning, placement, Poisson energy, spectral framework, optimization

Summary:
This paper introduces a variational and spectral framework for Poisson energy-based floorplanning and placement in physical design, focusing on arranging modules in VLSI circuits without overlap. The Poisson energy is derived as the squared H^{-1} Sobolev norm of the density residual, acting as a low-pass filter to enforce uniformity. It serves as a smooth surrogate for nonoverlap constraints, with a lower bound relating it to geometric overlap area. Projected gradient descent converges globally to stationary points and exhibits local linear convergence near regular minima. The continuous-time dynamics are interpreted as a Wasserstein-2 gradient flow, highlighting nonlocality and global balancing behavior. These findings provide a mathematically principled foundation for PDE-regularized optimization in large-scale floorplanning and related geometric layout problems.


<br /><br />Summary: <div>
arXiv:2510.08126v1 Announce Type: new 
Abstract: Arranging many modules within a bounded domain without overlap, central to the Electronic Design Automation (EDA) of very large-scale integrated (VLSI) circuits, represents a broad class of discrete geometric optimization problems with physical constraints. This paper develops a variational and spectral framework for Poisson energy-based floorplanning and placement in physical design. We show that the Poisson energy, defined via a Neumann Poisson equation, is exactly the squared H^{-1} Sobolev norm of the density residual, providing a functional-analytic interpretation of the classical electrostatic analogy. Through spectral analysis, we demonstrate that the energy acts as an intrinsic low-pass filter, suppressing high-frequency fluctuations while enforcing large-scale uniformity. Under a mild low-frequency dominance assumption, we establish a quantitative linear lower bound relating the Poisson energy to the geometric overlap area, thereby justifying its use as a smooth surrogate for the hard nonoverlap constraint. We further show that projected gradient descent converges globally to stationary points and exhibits local linear convergence near regular minima. Finally, we interpret the continuous-time dynamics as a Wasserstein-2 gradient flow, revealing the intrinsic nonlocality and global balancing behavior of the model. These results provide a mathematically principled foundation for PDE-regularized optimization in large-scale floorplanning and related geometric layout problems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design of chemical recycling processes for PUR foam under uncertainty</title>
<link>https://arxiv.org/abs/2510.08301</link>
<guid>https://arxiv.org/abs/2510.08301</guid>
<content:encoded><![CDATA[
<div> Keywords: Optimization, Chemical process design, Uncertainty, Process simulation software, Evolutionary strategy

Summary:
- Optimization problems in chemical process design involve both discrete and continuous decisions, especially when considering uncertainties.
- The combination of commercial process simulation software with an evolutionary strategy can help address the complexity of design problems.
- Two-stage optimization problems, where some decisions are fixed at the design stage and others can be adapted during plant operation, pose challenges in exploration.
- The proposed algorithm outperformed a manually designed robust process for designing a downstream process for isolating valuable products from pyrolysis oil.
- Analysis of different scenarios provided valuable insights into potential changes in the overall layout of the recycling process. 

<br /><br />Summary: <div>
arXiv:2510.08301v1 Announce Type: new 
Abstract: Optimization problems in chemical process design involve a significant number of discrete and continuous decisions. When taking into account uncertainties, the search space is very difficult to explore, even for experienced engineers. Moreover, it should be taken into account that while some decisions are fixed at the design stage, other parameters can be adapted to the realization of the uncertainty during the operation of the plant. This leads to a two-stage optimization problem which is difficult to solve. To address this challenge, we propose to combine commercial process simulation software with an evolutionary strategy. This approach is applied to designing a downstream process to isolate valuable products from pyrolysis oil produced by the catalytic pyrolysis of rigid polyurethane foam. The suggested algorithm consistently performed better than a manually designed robust process. Additionally, the analysis of different scenarios provided insight into promising changes in the overall layout of the recycling process.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing the Value-at-Risk of Loan Portfolio via Deep Neural Networks</title>
<link>https://arxiv.org/abs/2510.07444</link>
<guid>https://arxiv.org/abs/2510.07444</guid>
<content:encoded><![CDATA[
<div> Neural network, peer-to-peer lending, risk management, Value-at-Risk, Conditional Value-at-Risk
Summary:
- Risk management in peer-to-peer lending is crucial, with diversification being an effective strategy to reduce exposure.
- Two deep neural network models, DeNN and DSNN, are proposed to minimize the Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) of loan portfolios.
- The models predict default probabilities and time to default, leading to significant reductions in portfolio VaRs at different confidence levels compared to benchmarks.
- The low degree of freedom model, DeNN, demonstrates superior performance over DSNN in most scenarios.
- Implementing these neural network models can enhance risk mitigation and decision-making in peer-to-peer lending environments. 
<br /><br />Summary: <div>
arXiv:2510.07444v1 Announce Type: cross 
Abstract: Risk management is a prominent issue in peer-to-peer lending. An investor may naturally reduce his risk exposure by diversifying instead of putting all his money on one loan. In that case, an investor may want to minimize the Value-at-Risk (VaR) or Conditional Value-at-Risk (CVaR) of his loan portfolio. We propose a low degree of freedom deep neural network model, DeNN, as well as a high degree of freedom model, DSNN, to tackle the problem. In particular, our models predict not only the default probability of a loan but also the time when it will default. The experiments demonstrate that both models can significantly reduce the portfolio VaRs at different confidence levels, compared to benchmarks. More interestingly, the low degree of freedom model, DeNN, outperforms DSNN in most scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Meet Virtual Cell: A Survey</title>
<link>https://arxiv.org/abs/2510.07706</link>
<guid>https://arxiv.org/abs/2510.07706</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, virtual cell modeling, cellular representation, perturbation prediction, gene regulation inference

Summary:<br /><br />Large language models (LLMs) are revolutionizing cellular biology by enabling the creation of virtual cells through computational systems. This review categorizes existing methods into two paradigms: LLMs as Oracles for direct cellular modeling and LLMs as Agents for orchestrating complex scientific tasks. The core tasks identified include cellular representation, perturbation prediction, and gene regulation inference. The review covers models, datasets, evaluation benchmarks, and challenges such as scalability, generalizability, and interpretability. This comprehensive overview highlights the transformative potential of LLMs in cellular biology and showcases the advancements in virtual cell modeling enabled by these models. <div>
arXiv:2510.07706v1 Announce Type: cross 
Abstract: Large language models (LLMs) are transforming cellular biology by enabling the development of "virtual cells"--computational systems that represent, predict, and reason about cellular states and behaviors. This work provides a comprehensive review of LLMs for virtual cell modeling. We propose a unified taxonomy that organizes existing methods into two paradigms: LLMs as Oracles, for direct cellular modeling, and LLMs as Agents, for orchestrating complex scientific tasks. We identify three core tasks--cellular representation, perturbation prediction, and gene regulation inference--and review their associated models, datasets, evaluation benchmarks, as well as the critical challenges in scalability, generalizability, and interpretability.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfitted hybrid high-order methods stabilized by polynomial extension for elliptic interface problems</title>
<link>https://arxiv.org/abs/2503.11397</link>
<guid>https://arxiv.org/abs/2503.11397</guid>
<content:encoded><![CDATA[
<div> Hybrid High-Order Method, Unfitted Meshes, Polynomial Unknowns, Interface, Gradient Reconstruction Operator <br />
<br />
Summary: <br />
This work introduces a novel hybrid high-order (HHO) method for unfitted meshes, utilizing polynomial unknowns attached to mesh faces and cells. In this method, the interface can traverse mesh cells in a general manner, with doubled polynomial unknowns in cut cells and faces. To address ill-conditioning issues from small cut cells, the method employs polynomial extensions in the gradient reconstruction operator. Stability and consistency are proven, yielding optimal error estimates. Numerical experiments validate the theory's applicability. <div>
arXiv:2503.11397v2 Announce Type: replace-cross 
Abstract: In this work, we study the design and analysis of a novel hybrid high-order (HHO) method on unfitted meshes. HHO methods rely on a pair of unknowns, combining polynomials attached to the mesh faces and the mesh cells. In the unfitted framework, the interface can cut through the mesh cells in a very general fashion, and the polynomial unknowns are doubled in the cut cells and the cut faces. In order to avoid the ill-conditioning issues caused by the presence of small cut cells, the novel approach introduced herein is to use polynomial extensions in the definition of the gradient reconstruction operator. Stability and consistency results are established, leading to optimally decaying error estimates. The theory is illustrated by numerical experiments.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-Enhanced Reinforcement Learning for Dynamic Portfolio Optimization</title>
<link>https://arxiv.org/abs/2510.06466</link>
<guid>https://arxiv.org/abs/2510.06466</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, dynamic portfolio optimization, Dirichlet policy, cross-sectional attention mechanisms, portfolio management<br />
<br />
Summary: 
This article presents a deep reinforcement learning framework for dynamic portfolio optimization. The framework combines a Dirichlet policy with cross-sectional attention mechanisms to ensure feasible portfolio weights, natural handling of tradability constraints, and stable exploration of the allocation space. By integrating per-asset temporal encoders with a global attention layer, the model can capture sector relationships, factor spillovers, and other cross-asset dependencies. The reward function considers transaction costs and portfolio variance penalties, linking the learning objective to traditional mean variance trade-offs. Results demonstrate that attention-based Dirichlet policies outperform equal-weight and standard reinforcement learning benchmarks in terms of terminal wealth and Sharpe ratio, while maintaining realistic turnover and drawdown levels. This study highlights the benefits of combining principled action design with attention-based representations for improving the stability and interpretability of reinforcement learning in portfolio management. <br /><br /> <div>
arXiv:2510.06466v1 Announce Type: new 
Abstract: We develop a deep reinforcement learning framework for dynamic portfolio optimization that combines a Dirichlet policy with cross-sectional attention mechanisms. The Dirichlet formulation ensures that portfolio weights are always feasible, handles tradability constraints naturally, and provides a stable way to explore the allocation space. The model integrates per-asset temporal encoders with a global attention layer, allowing it to capture sector relationships, factor spillovers, and other cross asset dependencies. The reward function includes transaction costs and portfolio variance penalties, linking the learning objective to traditional mean variance trade offs. The results show that attention based Dirichlet policies outperform equal-weight and standard reinforcement learning benchmarks in terms of terminal wealth and Sharpe ratio, while maintaining realistic turnover and drawdown levels. Overall, the study shows that combining principled action design with attention-based representations improves both the stability and interpretability of reinforcement learning for portfolio management.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Higher-Order Time Domain Boundary Element Formulation based on Isogeometric Analysis and the Convolution Quadrature Method</title>
<link>https://arxiv.org/abs/2510.06804</link>
<guid>https://arxiv.org/abs/2510.06804</guid>
<content:encoded><![CDATA[
<div> isogeometric boundary element method, scattering problems, wave equation, elastodynamics, convolution quadratures

Summary:
The article presents an isogeometric boundary element method for solving scattering problems in isotropic homogeneous media, focusing on wave problems governed by the scalar wave equation and the Lam\'e-Navier equations for elastodynamics. The method approximates time-dependent convolution integrals using multi-stage Runge-Kutta convolution quadratures based on steady-state solutions in the Laplace domain. Spatial discretization is done using isogeometric analysis with a patchwise smooth spline basis. The implementation scheme uses local, uniform Bernstein polynomials as basis functions and localizes them on the elements defined by the non-empty knot spans in the knot vectors. The solutions of mixed problems are approximated using a symmetric Galerkin variational formulation and a collocation method. Convergence rates of the approximations are investigated in a mixed space and time error norm. <div>
arXiv:2510.06804v1 Announce Type: new 
Abstract: An isogeometric boundary element method (BEM) is presented to solve scattering problems in an isotropic homogeneous medium. We consider wave problems governed by the scalar wave equation as in acoustics and the Lam\'e-Navier equations for elastodynamics considering the theory of linear elasticity. The underlying boundary integral equations imply time-dependent convolution integrals and allow us to determine the sought quantities in the bounded interior or the unbounded exterior after solving for the unknown Cauchy data. In the present work, the time-dependent convolution integrals are approximated by multi-stage Runge-Kutta (RK) based convolution quadratures that involve steady-state solutions in the Laplace domain. The proposed method discretizes the spatial variables in the framework of isogeometric analysis (IGA), entailing a patchwise smooth spline basis. Overall, it enables high convergence rates in space and time. The implementation scheme follows an element structure defined by the non-empty knot spans in the knot vectors and local, uniform Bernstein polynomials as basis functions. The algorithms to localize the basis functions on the elements are outlined and explained. The solutions of the mixed problems are approximated by the BEM based on a symmetric Galerkin variational formulation and a collocation method. We investigate convergence rates of the approximative solutions in a mixed space and time error norm.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Measuring How News Topics Drive Stock Movement</title>
<link>https://arxiv.org/abs/2510.06864</link>
<guid>https://arxiv.org/abs/2510.06864</guid>
<content:encoded><![CDATA[
<div> framework, news topics, stock price movements, sentiment analysis, financial markets

Summary: 
This paper introduces a novel framework that analyzes the impact of different news topics on stock price movements in financial markets. By encoding news headlines into dense semantic embeddings and clustering them into distinct topics, the framework uncovers the specific themes that influence daily stock returns. Through an ordinary least squares regression, the framework quantifies the effects of these topics on next-day stock returns. Applied to Apple Inc., the framework identifies certain topics that significantly affect stock returns, while others show no measurable impact. These findings underscore the significance of conducting topic-level analysis to understand the relationship between news content and market responses. The proposed framework offers a scalable approach for researchers and practitioners to evaluate the informational value of news topics, suggesting a promising avenue for enhancing predictive models of stock price movements. 

<br /><br />Summary: <div>
arXiv:2510.06864v1 Announce Type: new 
Abstract: In modern financial markets, news plays a critical role in shaping investor sentiment and influencing stock price movements. However, most existing studies aggregate daily news sentiment into a single score, potentially overlooking important variations in topic content and relevance. This simplification may mask nuanced relationships between specific news themes and market responses. To address this gap, this paper proposes a novel framework to examine how different news topics influence stock price movements. The framework encodes individual news headlines into dense semantic embeddings using a pretrained sentence transformer, then applies K-means clustering to identify distinct news topics. Topic exposures are incorporated as explanatory variables in an ordinary least squares regression to quantify their impact on daily stock returns. Applied to Apple Inc., the framework reveals that certain topics are significantly associated with positive or negative next-day returns, while others have no measurable effect. These findings highlight the importance of topic-level analysis in understanding the relationship between news content and financial markets. The proposed framework provides a scalable approach for both researchers and practitioners to assess the informational value of different news topics and suggests a promising direction for improving predictive models of stock price movement.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOMATOES: Topology and Material Optimization for Latent Heat Thermal Energy Storage Devices</title>
<link>https://arxiv.org/abs/2510.07057</link>
<guid>https://arxiv.org/abs/2510.07057</guid>
<content:encoded><![CDATA[
<div> optimize, latent heat thermal energy storage, topology optimization, material design, phase change material

Summary:
The article introduces a new approach for optimizing latent heat thermal energy storage (LHTES) systems by concurrently optimizing material choice and topology. Traditional topology optimization methods focus on optimizing geometry for pre-selected materials, limiting the exploration of novel materials. To address this limitation, the authors develop an automated design framework that integrates material databases using a data-driven variational autoencoder (VAE) to project discrete materials onto continuous latent spaces. This approach enables the co-design of material and geometry for LHTES systems, leading to improved performance. The framework is demonstrated on a problem of maximizing discharged energy within a specified time while considering cost constraints. Several illustrative examples validate the effectiveness of the proposed method. The novel approach offers a promising solution for developing efficient and cost-effective LHTES systems. 

<br /><br />Summary: <div>
arXiv:2510.07057v1 Announce Type: new 
Abstract: Latent heat thermal energy storage (LHTES) systems are compelling candidates for energy storage, primarily owing to their high storage density. Improving their performance is crucial for developing the next-generation efficient and cost effective devices. Topology optimization (TO) has emerged as a powerful computational tool to design LHTES systems by optimally distributing a high-conductivity material (HCM) and a phase change material (PCM). However, conventional TO typically limits to optimizing the geometry for a fixed, pre-selected materials. This approach does not leverage the large and expanding databases of novel materials. Consequently, the co-design of material and geometry for LHTES remains a challenge and unexplored.
  To address this limitation, we present an automated design framework for the concurrent optimization of material choice and topology. A key challenge is the discrete nature of material selection, which is incompatible with the gradient-based methods used for TO. We overcome this by using a data-driven variational autoencoder (VAE) to project discrete material databases for both the HCM and PCM onto continuous and differentiable latent spaces. These continuous material representations are integrated into an end-to-end differentiable, transient nonlinear finite-element solver that accounts for phase change. We demonstrate this framework on a problem aimed at maximizing the discharged energy within a specified time, subject to cost constraints. The effectiveness of the proposed method is validated through several illustrative examples.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models</title>
<link>https://arxiv.org/abs/2510.06439</link>
<guid>https://arxiv.org/abs/2510.06439</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, hyperparameter tuning, uncertainty, stochastic models, computational engineering <br />
Summary: 
The paper introduces a new Bayesian optimization framework designed for hyperparameter tuning in uncertain systems. The approach focuses on optimizing scale- or precision-type parameters in stochastic models. By utilizing a statistical surrogate for the random variable, the method allows for analytical evaluation of the expectation operator. A closed-form expression for the optimizer of the random acquisition function is derived, reducing computational costs significantly. Compared to traditional Monte Carlo-based optimization schemes, the proposed method requires 40 times fewer data points, leading to a substantial reduction in computational expenses. The effectiveness of the approach is demonstrated through numerical examples in computational engineering, showcasing its potential for efficiently optimizing hyperparameters in uncertain systems. <br /> <div>
arXiv:2510.06439v1 Announce Type: cross 
Abstract: Hyperparameter tuning is a challenging problem especially when the system itself involves uncertainty. Due to noisy function evaluations, optimization under uncertainty can be computationally expensive. In this paper, we present a novel Bayesian optimization framework tailored for hyperparameter tuning under uncertainty, with a focus on optimizing a scale- or precision-type parameter in stochastic models. The proposed method employs a statistical surrogate for the underlying random variable, enabling analytical evaluation of the expectation operator. Moreover, we derive a closed-form expression for the optimizer of the random acquisition function, which significantly reduces computational cost per iteration. Compared with a conventional one-dimensional Monte Carlo-based optimization scheme, the proposed approach requires 40 times fewer data points, resulting in up to a 40-fold reduction in computational cost. We demonstrate the effectiveness of the proposed method through two numerical examples in computational engineering.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEAorta: A Fully Automated Framework for Finite Element Analysis of the Aorta From 3D CT Images</title>
<link>https://arxiv.org/abs/2510.06621</link>
<guid>https://arxiv.org/abs/2510.06621</guid>
<content:encoded><![CDATA[
<div> Keywords: Aortic aneurysm, biomechanics, Finite Element Analysis, PyTorch, deep neural network

Summary:<br />
Aortic aneurysm disease is a common cause of death in the U.S. population, with thoracic aortic aneurysm being a significant contributor. Rupture occurs when the stress on the aortic wall exceeds its strength, which can be determined through biomechanical analyses like Finite Element Analysis (FEA). However, current methods for patient-specific risk assessment face challenges due to manual segmentation and computational burden. To address these issues, a PyTorch FEA library and FEA-DNN integration framework were developed, significantly reducing computation time. This study focuses on overcoming the labor-intensive 3D reconstruction barrier by creating a deep neural network that can generate patient-specific finite element meshes directly from 3D CT images. The integration of deep learning and FEA techniques offers a promising solution for efficient and accurate risk assessment of thoracic aortic aneurysms. <br /><br /> <div>
arXiv:2510.06621v1 Announce Type: cross 
Abstract: Aortic aneurysm disease ranks consistently in the top 20 causes of death in the U.S. population. Thoracic aortic aneurysm is manifested as an abnormal bulging of thoracic aortic wall and it is a leading cause of death in adults. From the perspective of biomechanics, rupture occurs when the stress acting on the aortic wall exceeds the wall strength. Wall stress distribution can be obtained by computational biomechanical analyses, especially structural Finite Element Analysis. For risk assessment, probabilistic rupture risk of TAA can be calculated by comparing stress with material strength using a material failure model. Although these engineering tools are currently available for TAA rupture risk assessment on patient specific level, clinical adoption has been limited due to two major barriers: labor intensive 3D reconstruction current patient specific anatomical modeling still relies on manual segmentation, making it time consuming and difficult to scale to a large patient population, and computational burden traditional FEA simulations are resource intensive and incompatible with time sensitive clinical workflows. The second barrier was successfully overcome by our team through the development of the PyTorch FEA library and the FEA DNN integration framework. By incorporating the FEA functionalities within PyTorch FEA and applying the principle of static determinacy, we reduced the FEA based stress computation time to approximately three minutes per case. Moreover, by integrating DNN and FEA through the PyTorch FEA library, our approach further decreases the computation time to only a few seconds per case. This work focuses on overcoming the first barrier through the development of an end to end deep neural network capable of generating patient specific finite element meshes of the aorta directly from 3D CT images.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios</title>
<link>https://arxiv.org/abs/2510.07099</link>
<guid>https://arxiv.org/abs/2510.07099</guid>
<content:encoded><![CDATA[
<div> framework, portfolio optimisation, financial markets, Diffusion-Augmented Reinforcement Learning, DDPMs <br />
Summary:<br />
The article introduces a novel framework called Diffusion-Augmented Reinforcement Learning (DARL) for portfolio management in financial markets. It combines Denoising Diffusion Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) to better capture market dynamics and investor preferences. DARL uses DDPMs to generate synthetic market crash scenarios, enhancing training data robustness. Empirical testing shows DARL outperforms traditional methods, delivering higher risk-adjusted returns and resilience during crises such as the 2025 Tariff Crisis. This innovative approach offers a practical solution for enhancing stress resilience in DRL-driven financial applications. <br />Summary: <div>
arXiv:2510.07099v1 Announce Type: cross 
Abstract: In the ever-changing and intricate landscape of financial markets, portfolio optimisation remains a formidable challenge for investors and asset managers. Conventional methods often struggle to capture the complex dynamics of market behaviour and align with diverse investor preferences. To address this, we propose an innovative framework, termed Diffusion-Augmented Reinforcement Learning (DARL), which synergistically integrates Denoising Diffusion Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for portfolio management. By leveraging DDPMs to generate synthetic market crash scenarios conditioned on varying stress intensities, our approach significantly enhances the robustness of training data. Empirical evaluations demonstrate that DARL outperforms traditional baselines, delivering superior risk-adjusted returns and resilience against unforeseen crises, such as the 2025 Tariff Crisis. This work offers a robust and practical methodology to bolster stress resilience in DRL-driven financial applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Default Resilience and Worst-Case Effects in Financial Networks</title>
<link>https://arxiv.org/abs/2403.10631</link>
<guid>https://arxiv.org/abs/2403.10631</guid>
<content:encoded><![CDATA[
<div> price fluctuations, network of banks, default contagion, default resilience margin, systemic loss

Summary:
This paper explores the resilience of a network of banks to joint price fluctuations of shared external assets and the impact on default contagion. It introduces the concept of a default resilience margin, $\epsilon^*$, which represents the maximum amplitude of asset price fluctuations that the network can withstand without defaults occurring. The threshold value $\epsilon^*$ is determined by considering different measures of price fluctuations. When the perturbation exceeds $\epsilon^*$, defaults may occur, leading to the calculation of the worst-case systemic loss. This involves determining the total unpaid debt under the most severe price variation scenario. The analysis involves solving linear programming problems to compute the threshold level $\epsilon^*$ and evaluate the worst-case loss and corresponding asset price scenario. <div>
arXiv:2403.10631v2 Announce Type: replace-cross 
Abstract: In this paper we analyze the resilience of a network of banks to joint price fluctuations of the external assets in which they have shared exposures, and evaluate the worst-case effects of the possible default contagion. Indeed, when the prices of certain external assets either decrease or increase, all banks exposed to them experience varying degrees of simultaneous shocks to their balance sheets. These coordinated and structured shocks have the potential to exacerbate the likelihood of defaults. In this context, we introduce first a concept of {default resilience margin}, $\epsilon^*$, i.e., the maximum amplitude of asset prices fluctuations that the network can tolerate without generating defaults. Such threshold value is computed by considering two different measures of price fluctuations, one based on the maximum individual variation of each asset, and the other based on the sum of all the asset's absolute variations. For any price perturbation having amplitude no larger than $\epsilon^*$, the network absorbs the shocks remaining default free. When the perturbation amplitude goes beyond $\epsilon^*$, however, defaults may occur. In this case we find the worst-case systemic loss, that is, the total unpaid debt under the most severe price variation of given magnitude. Computation of both the threshold level $\epsilon^*$ and of the worst-case loss and of a corresponding worst-case asset price scenario, amounts to solving suitable linear programming problems.}
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Ensemble Topology (GET): A New Explicit and Inherently Smooth Framework for Manufacture-Ready Topology Optimization</title>
<link>https://arxiv.org/abs/2510.05572</link>
<guid>https://arxiv.org/abs/2510.05572</guid>
<content:encoded><![CDATA[
<div> Gaussian Ensemble Topology, explicit framework, topology optimization, anisotropic Gaussian functions, smooth designs<br />
<br />
Summary: 
The article introduces the Gaussian Ensemble Topology (GET) method for topology optimization, utilizing an explicit framework with anisotropic Gaussian functions to represent design geometries. The method combines Gaussian descriptions with a Heaviside projection to generate smooth designs without the need for post-processing steps. Validated on compliance-minimization and compliant mechanism benchmarks, GET produces optimized designs with comparable objective values to classical approaches but with refined boundaries. The framework offers advantages such as mesh independence, strong geometric expressiveness, and control over smoothness, discreteness, and complexity through parameter tuning. GET provides a robust and manufacture-ready solution for explicit topology optimization, allowing for the tackling of advanced and complex design problems. <br /><br /> <div>
arXiv:2510.05572v1 Announce Type: new 
Abstract: We introduce the Gaussian Ensemble Topology (GET) method, a new explicit and manufacture-ready framework for topology optimization in which design geometries are represented as superpositions of anisotropic Gaussian functions. By combining explicit Gaussian descriptions with a level-set-like Heaviside projection, GET inherently generates smooth, curvature-continuous designs without requiring post-processing steps such as mesh or corner smoothing and feature extraction. The method is validated on standard compliance-minimization and compliant mechanism benchmarks in two and three dimensions. The optimized designs achieve objective values comparable to those obtained with classical Moving Morphable Component (MMC) approaches, but with geometrically consistent, refined boundaries. Numerical examples demonstrate additional advantages of the GET framework, including mesh independence inherent to explicit parameterizations, strong geometric expressiveness, and effective control over smoothness, discreteness, and structural complexity through parameter tuning. As a robust and manufacture-ready approach to explicit topology optimization, GET opens avenues for tackling advanced and complex design problems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy</title>
<link>https://arxiv.org/abs/2510.05747</link>
<guid>https://arxiv.org/abs/2510.05747</guid>
<content:encoded><![CDATA[
<div> transformer, T-cell receptors, physicochemical descriptors, generative modeling, cellular therapy

Summary:
PhysicoGPTCR is a generative protein transformer model designed for rapid and accurate generation of T-cell receptor (TCR) sequences. It is dual-conditioned on peptide and HLA context and trained to incorporate physicochemical descriptors in the sequence generation process. The model outperforms baseline models like ANN, GPTCR, LSTM, and VAE in terms of edit-distance, similarity, and longest-common-subsequence scores, generating a more diverse set of TCR sequences. In in-silico experiments, PhysicoGPTCR demonstrates a higher proportion of binding-competent clones compared to baselines, showcasing the benefits of context conditioning and physicochemical awareness. This physics-grounded generative modeling approach significantly accelerates the design of functional TCR candidates, reducing the discovery timeline from months to minutes while maintaining wet-lab verifiability.<br /><br />Summary: <div>
arXiv:2510.05747v1 Announce Type: new 
Abstract: Physicochemically informed biological sequence generation has the potential to accelerate computer-aided cellular therapy, yet current models fail to \emph{jointly} ensure novelty, diversity, and biophysical plausibility when designing variable regions of T-cell receptors (TCRs). We present \textbf{PhysicoGPTCR}, a large generative protein Transformer that is \emph{dual-conditioned} on peptide and HLA context and trained to autoregressively synthesise TCR sequences while embedding residue-level physicochemical descriptors. The model is optimised on curated TCR--peptide--HLA triples with a maximum-likelihood objective and compared against ANN, GPTCR, LSTM, and VAE baselines. Across multiple neoantigen benchmarks, PhysicoGPTCR substantially improves edit-distance, similarity, and longest-common-subsequence scores, while populating a broader region of sequence space. Blind in-silico docking and structural modelling further reveal a higher proportion of binding-competent clones than the strongest baseline, validating the benefit of explicit context conditioning and physicochemical awareness. Experimental results demonstrate that dual-conditioned, physics-grounded generative modelling enables end-to-end design of functional TCR candidates, reducing the discovery timeline from months to minutes without sacrificing wet-lab verifiability.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Smell Detection via Pearson Correlation and ML Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2510.05835</link>
<guid>https://arxiv.org/abs/2510.05835</guid>
<content:encoded><![CDATA[
<div> ML algorithms, code smells, software quality analysis, SMOTE, feature selection
Summary:
This study introduces a machine learning (ML) based model to detect code smells in large-scale software systems. The model utilizes eight diverse ML algorithms such as XGBoost and AdaBoost, along with techniques like SMOTE for class imbalance and Pearson correlation for feature selection. The methodology includes preprocessing data, balancing the dataset with SMOTE, reducing redundancy with Pearson correlation, training ML algorithms, and tuning hyperparameters. AdaBoost, Random Forest, and XGBoost exhibit the best performance, achieving accuracies of 100%, 99%, and 99%, respectively. This approach offers a scalable solution for accurately identifying code smells, enhancing software quality assurance. Overall, the study showcases the effectiveness of a comprehensive ML approach in detecting code smells and improving software quality. 
<br /><br />Summary: <div>
arXiv:2510.05835v1 Announce Type: new 
Abstract: This study addresses the challenge of detecting code smells in large-scale software systems using machine learning (ML). Traditional detection methods often suffer from low accuracy and poor generalization across different datasets. To overcome these issues, we propose a machine learning-based model that automatically and accurately identifies code smells, offering a scalable solution for software quality analysis. The novelty of our approach lies in the use of eight diverse ML algorithms, including XGBoost, AdaBoost, and other classifiers, alongside key techniques such as the Synthetic Minority Over-sampling Technique (SMOTE) for class imbalance and Pearson correlation for efficient feature selection. These methods collectively improve model accuracy and generalization. Our methodology involves several steps: first, we preprocess the data and apply SMOTE to balance the dataset; next, Pearson correlation is used for feature selection to reduce redundancy; followed by training eight ML algorithms and tuning hyperparameters through Grid Search, Random Search, and Bayesian Optimization. Finally, we evaluate the models using accuracy, F-measure, and confusion matrices. The results show that AdaBoost, Random Forest, and XGBoost perform best, achieving accuracies of 100%, 99%, and 99%, respectively. This study provides a robust framework for detecting code smells, enhancing software quality assurance, and demonstrating the effectiveness of a comprehensive, optimized ML approach.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive comparison of neural operators for 3D industry-scale engineering designs</title>
<link>https://arxiv.org/abs/2510.05995</link>
<guid>https://arxiv.org/abs/2510.05995</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural operators, engineering design, comparison, datasets, performance

Summary:
Neural operators are widely used in engineering design for predicting complex dynamics. To facilitate model selection, six 3D industry-scale engineering design datasets have been standardized, covering various problem settings. These datasets are preprocessed and ready for use with different neural operator architectures. Four types of neural operator variants are compared, including Branch-Trunk-based, Graph-based, Grid-based, and Point-based models, each with practical enhancements for different engineering scenarios. The comparison evaluates predictive performance, computational efficiency, memory usage, and deployment complexity of each model. The study provides valuable insights for future neural operator development, guiding researchers in optimizing their models for engineering design applications. 

<br /><br />Summary: <div>
arXiv:2510.05995v1 Announce Type: new 
Abstract: Neural operators have emerged as powerful tools for learning nonlinear mappings between function spaces, enabling real-time prediction of complex dynamics in diverse scientific and engineering applications. With their growing adoption in engineering design evaluation, a wide range of neural operator architectures have been proposed for various problem settings. However, model selection remains challenging due to the absence of fair and comprehensive comparisons. To address this, we propose and standardize six representative 3D industry-scale engineering design datasets spanning thermal analysis, linear elasticity, elasto-plasticity, time-dependent plastic problems, and computational fluid dynamics. All datasets include fully preprocessed inputs and outputs for model training, making them directly usable across diverse neural operator architectures. Using these datasets, we conduct a systematic comparison of four types of neural operator variants, including Branch-Trunk-based Neural Operators inspired by DeepONet, Graph-based Neural Operators inspired by Graph Neural Networks, Grid-based Neural Operators inspired by Fourier Neural Operators, and Point-based Neural Operators inspired by PointNet. We further introduce practical enhancements to adapt these models to different engineering settings, improving the fairness of the comparison. Our benchmarking study evaluates each model strengths and limitations in terms of predictive performance, computational efficiency, memory usage, and deployment complexity. The findings provide actionable insights to guide future neural operator development.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models</title>
<link>https://arxiv.org/abs/2510.05121</link>
<guid>https://arxiv.org/abs/2510.05121</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, structured knowledge extraction, Economics, trade agreement texts, triples <br />
Summary: 
This study explores the effectiveness of Large Language Models (LLMs) in extracting structured knowledge, specifically Subject-Predicate-Object triples, focusing on the domain of Economics. The research investigates the use of LLMs for creating economic trade knowledge graphs from natural language legal trade agreement texts. By applying zero-shot, one-shot, and few-shot prompting techniques with positive and negative examples, the study evaluates the performance of the model in extracting trade-related information triples. The Llama 3.1 model is utilized to process unstructured regional trade agreement texts and extract triples, leading to insights on the challenges and future directions of using language models in economic applications. The findings emphasize the potential of LLMs in extracting structured knowledge in various scenarios, highlighting their significance in economic contexts. <br /><br />Summary: <div>
arXiv:2510.05121v1 Announce Type: cross 
Abstract: This study investigates the effectiveness of Large Language Models (LLMs) for the extraction of structured knowledge in the form of Subject-Predicate-Object triples. We apply the setup for the domain of Economics application. The findings can be applied to a wide range of scenarios, including the creation of economic trade knowledge graphs from natural language legal trade agreement texts. As a use case, we apply the model to regional trade agreement texts to extract trade-related information triples. In particular, we explore the zero-shot, one-shot and few-shot prompting techniques, incorporating positive and negative examples, and evaluate their performance based on quantitative and qualitative metrics. Specifically, we used Llama 3.1 model to process the unstructured regional trade agreement texts and extract triples. We discuss key insights, challenges, and potential future directions, emphasizing the significance of language models in economic applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auditing Algorithmic Bias in Transformer-Based Trading</title>
<link>https://arxiv.org/abs/2510.05140</link>
<guid>https://arxiv.org/abs/2510.05140</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer models, financial applications, risk, biases, prediction confidence

Summary:
The study focuses on auditing the reliance of transformer models in financial applications on volatile data and quantifies the impact of frequency of price movements on prediction confidence. Using a transformer model for prediction, the researchers introduce a metric based on Partial Information Decomposition (PID) to assess the influence of each asset on the model's decision-making process. The analysis uncovers two key findings: firstly, the model ignores data volatility completely; and secondly, it displays bias towards data exhibiting lower-frequency price movements. These observations shed light on potential gaps in the model's decision-making process in financial scenarios, highlighting a need for further exploration and refinement to mitigate risks and biases in transformer model applications. 

<br /><br />Summary: <div>
arXiv:2510.05140v1 Announce Type: cross 
Abstract: Transformer models have become increasingly popular in financial applications, yet their potential risk making and biases remain under-explored. The purpose of this work is to audit the reliance of the model on volatile data for decision-making, and quantify how the frequency of price movements affects the model's prediction confidence. We employ a transformer model for prediction, and introduce a metric based on Partial Information Decomposition (PID) to measure the influence of each asset on the model's decision making. Our analysis reveals two key observations: first, the model disregards data volatility entirely, and second, it is biased toward data with lower-frequency price movements.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2510.05158</link>
<guid>https://arxiv.org/abs/2510.05158</guid>
<content:encoded><![CDATA[
<div> Physics-informed neural networks (PINNs), a powerful method for solving partial differential equations (PDEs, can be labor-intensive and error-prone to construct. Lang-PINN, a multi-agent system driven by large language models, processes natural language task descriptions to build trainable PINNs. It consists of a PDE Agent, PINN Agent, Code Agent, and Feedback Agent to transform informal tasks into executable PINN code. Lang-PINN outperforms baselines by reducing mean squared error, improving execution success, and decreasing time overhead. Keywords: Physics-informed neural networks, PINN, Partial differential equations, Large language models, Multi-agent system<br /><br />Summary: Lang-PINN, powered by large language models, translates natural language task descriptions into trainable physics-informed neural networks. It outperforms existing methods by significantly reducing errors, improving success rates in execution, and decreasing time overhead. By coordinating multiple agents to parse, select architectures, generate code, and provide feedback for refinement, Lang-PINN transforms informal problem statements into precise and efficient executable code for solving partial differential equations. <div>
arXiv:2510.05158v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) provide a powerful approach for solving partial differential equations (PDEs), but constructing a usable PINN remains labor-intensive and error-prone. Scientists must interpret problems as PDE formulations, design architectures and loss functions, and implement stable training pipelines. Existing large language model (LLM) based approaches address isolated steps such as code generation or architecture suggestion, but typically assume a formal PDE is already specified and therefore lack an end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system that builds trainable PINNs directly from natural language task descriptions. Lang-PINN coordinates four complementary agents: a PDE Agent that parses task descriptions into symbolic PDEs, a PINN Agent that selects architectures, a Code Agent that generates modular implementations, and a Feedback Agent that executes and diagnoses errors for iterative refinement. This design transforms informal task statements into executable and verifiable PINN code. Experiments show that Lang-PINN achieves substantially lower errors and greater robustness than competitive baselines: mean squared error (MSE) is reduced by up to 3--5 orders of magnitude, end-to-end execution success improves by more than 50\%, and reduces time overhead by up to 74\%.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentZero++: Modeling Fear-Based Behavior</title>
<link>https://arxiv.org/abs/2510.05185</link>
<guid>https://arxiv.org/abs/2510.05185</guid>
<content:encoded><![CDATA[
<div> AgentZero++, agent-based model, collective violence, cognitive mechanisms, emotional mechanisms, social mechanisms <br />
<br />
Summary: <br />
AgentZero++ is a new agent-based model that incorporates cognitive, emotional, and social mechanisms to simulate decentralized collective violence in spatially distributed systems. The model includes eight behavioral enhancements that allow agents to adapt based on internal states, previous experiences, and social feedback. These additions result in emergent dynamics such as protest asymmetries, escalation cycles, and localized retaliation. By implementing features such as memory-based risk estimation, affect-cognition coupling, and multi-agent coordination, the model demonstrates how small variations in memory, reactivity, and affective alignment can amplify or dampen unrest through feedback loops. Overall, AgentZero++ provides a flexible platform for analyzing affective contagion and psychologically grounded collective action, with implications for understanding the role of emotional thresholds, identity-driven behavior, and adaptive networks in shaping conflict patterns. <div>
arXiv:2510.05185v1 Announce Type: cross 
Abstract: We present AgentZero++, an agent-based model that integrates cognitive, emotional, and social mechanisms to simulate decentralized collective violence in spatially distributed systems. Building on Epstein's Agent\_Zero framework, we extend the original model with eight behavioral enhancements: age-based impulse control; memory-based risk estimation; affect-cognition coupling; endogenous destructive radius; fight-or-flight dynamics; affective homophily; retaliatory damage; and multi-agent coordination. These additions allow agents to adapt based on internal states, previous experiences, and social feedback, producing emergent dynamics such as protest asymmetries, escalation cycles, and localized retaliation. Implemented in Python using the Mesa ABM framework, AgentZero++ enables modular experimentation and visualization of how micro-level cognitive heterogeneity shapes macro-level conflict patterns. Our results highlight how small variations in memory, reactivity, and affective alignment can amplify or dampen unrest through feedback loops. By explicitly modeling emotional thresholds, identity-driven behavior, and adaptive networks, this work contributes a flexible and extensible platform for analyzing affective contagion and psychologically grounded collective action.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium</title>
<link>https://arxiv.org/abs/2510.05711</link>
<guid>https://arxiv.org/abs/2510.05711</guid>
<content:encoded><![CDATA[
<div> Keywords: Time-bound stablecoins, Liquidity-of-Time Premium, no-arbitrage pricing model, dynamic risk-control mechanism, empirical proxies.

Summary: 
Time-bound stablecoins tokenize traditional securities during market off-hours to provide continuous cross-market liquidity. The Liquidity-of-Time Premium (TLP) is introduced as the additional return or cost of providing liquidity when the primary market is closed. A no-arbitrage pricing model is developed to estimate fair values over different expiries, with a dynamic risk-control mechanism adjusting loan-to-value (LTV) ratios in real time to manage TLP. By blending financial engineering and empirical finance, the analysis measures TLP under time-zone frictions and proposes an LTV policy akin to a central bank adjusting rates. Empirical proxies for TLP include ADR premiums, index futures divergence, and pre-market gaps. Results indicate TLP increases with closure length and volatility but can be managed through adaptive LTV. The study provides insights for protocol design and suggests time-bound stablecoins as a tool to mitigate temporal market inefficiencies. 

Summary: <div>
arXiv:2510.05711v1 Announce Type: cross 
Abstract: Time-bound stablecoins are DeFi assets that temporarily tokenize traditional securities during market off-hours, enabling continuous cross-market liquidity. We introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of providing liquidity when the primary market is closed. We build a no-arbitrage pricing model that yields a band for fair values over different expiries, and a dynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real time to keep TLP within a target range. Our analysis blends financial engineering (no-arbitrage conditions, option-style pricing) with empirical finance (event studies on cross-listed stocks and futures) to measure TLP under time-zone frictions. We define TLP formally, derive closed-form expressions for its term structure under idealized assumptions, and simulate scenarios that vary volatility and collateralization. We then propose an LTV policy that raises or lowers collateral to expand or curtail time-bound stablecoin supply, analogous to a central bank adjusting rates to defend a peg. We outline empirical proxies for TLP, including ADR premiums, overseas index futures versus cash index divergence, and pre-market versus official close gaps. Results show that TLP grows with closure length and volatility, yet can be contained by adaptive LTV. We provide backtests and figures (term-structure curves, capital-efficiency versus tail-risk trade-offs, time-liquidity heatmaps) and discuss protocol design (vault structure, closing-price oracles, on-chain auction liquidations). The findings position time-bound stablecoins as a tool to reduce temporal market inefficiencies and inform future research and deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2510.06107</link>
<guid>https://arxiv.org/abs/2510.06107</guid>
<content:encoded><![CDATA[
<div> hallucination, language models, distributional semantics tracing, commitment layer, dual-process theory<br />
Summary:<br />
Large Language Models (LLMs) often generate factually incorrect statements known as hallucinations. This study investigates the underlying causes of this issue within the Transformer architecture. The authors introduce Distributional Semantics Tracing (DST), a framework that maps a model's reasoning and identifies a "commitment layer" where hallucinations become inevitable. They discover a conflict between fast, heuristic pathways and slow, deliberate pathways in the model, leading to predictable failure modes. By quantifying the coherence of the contextual pathway, they find a strong negative correlation with hallucination rates, linking these failures to internal semantic weakness. This mechanistic account provides insights into when and why hallucinations occur, shedding light on the architectural origins of this problem. <br /> <div>
arXiv:2510.06107v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary contributions.First, to enable the reliable tracing of internal semantic failures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific \textbf{commitment layer} where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic \textbf{associative pathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway} (akin to System 2), leading to predictable failure modes such as \textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation ($\rho = -0.863$) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Report of the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science</title>
<link>https://arxiv.org/abs/2510.03413</link>
<guid>https://arxiv.org/abs/2510.03413</guid>
<content:encoded><![CDATA[
<div> workshop, scientific computing, AI, cross-disciplinary collaboration, workforce development

Summary: 
The report discusses insights from the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing, focusing on harnessing community, software, and AI for cross-disciplinary team science. The participants emphasized the need for agile, robust ecosystems in scientific software development through socio-technical co-design. Recommendations include building trustworthy AI-enabled scientific software systems, integrating AI systems into workflows while preserving human creativity and rigor, and developing innovative training pipelines. Pilot projects were identified as catalysts for progress, with priorities on hybrid AI/HPC infrastructure, collaboration, responsible AI guidelines, and public-private partnerships. The vision presented aims to intertwine AI, software, hardware, and human expertise to drive discovery, enhance access, bolster the workforce, and hasten scientific advancements. <div>
arXiv:2510.03413v1 Announce Type: new 
Abstract: This report summarizes insights from the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science, which convened more than 40 experts from national laboratories, academia, industry, and community organizations to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems. To address urgent challenges at the intersection of high-performance computing (HPC), AI, and scientific software, participants envisioned agile, robust ecosystems built through socio-technical co-design--the intentional integration of social and technical components as interdependent parts of a unified strategy. This approach combines advances in AI, HPC, and software with new models for cross-disciplinary collaboration, training, and workforce development. Key recommendations include building modular, trustworthy AI-enabled scientific software systems; enabling scientific teams to integrate AI systems into their workflows while preserving human creativity, trust, and scientific rigor; and creating innovative training pipelines that keep pace with rapid technological change. Pilot projects were identified as near-term catalysts, with initial priorities focused on hybrid AI/HPC infrastructure, cross-disciplinary collaboration and pedagogy, responsible AI guidelines, and prototyping of public-private partnerships. This report presents a vision of next-generation ecosystems for scientific computing where AI, software, hardware, and human expertise are interwoven to drive discovery, expand access, strengthen the workforce, and accelerate scientific progress.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight and Data-Efficient MultivariateTime Series Forecasting using Residual-Stacked Gaussian (RS-GLinear) Architecture</title>
<link>https://arxiv.org/abs/2510.03788</link>
<guid>https://arxiv.org/abs/2510.03788</guid>
<content:encoded><![CDATA[
<div> Transformer, time-series forecasting, Gaussian Linear, RSGL model, prediction accuracy

Summary:
The study evaluates the Gaussian-based Linear architecture for time-series forecasting and proposes an enhanced version called the Residual Stacked Gaussian Linear (RSGL) model. The RSGL model is assessed for its performance in long-term forecasting tasks and its applicability in financial time series and epidemiological data. Experimental results show that the RSGL model outperforms both the baseline Gaussian Linear and Transformer-based models in terms of prediction accuracy and robustness. The study addresses the mixed results reported in previous research and demonstrates the effectiveness of the RSGL model in capturing both short- and long-term dependencies in forecasting tasks. Additionally, the study highlights the broader applicability of the RSGL model across different domains, showcasing its versatility in handling various types of time-series data. <div>
arXiv:2510.03788v1 Announce Type: new 
Abstract: Following the success of Transformer architectures in language modeling, particularly their ability to capture long-range dependencies, researchers have explored how these architectures can be adapted for time-series forecasting. Transformer-based models have been proposed to handle both short- and long-term dependencies when predicting future values from historical data. However, studies such as those by Zeng et al. (2022) and Rizvi et al. (2025) have reported mixed results in long-term forecasting tasks. In this work, we evaluate the Gaussian-based Linear architecture introduced by Rizvi et al. (2025) and present an enhanced version called the Residual Stacked Gaussian Linear (RSGL) model. We also investigate the broader applicability of the RSGL model in additional domains, including financial time series and epidemiological data. Experimental results show that the RSGL model achieves improved prediction accuracy and robustness compared to both the baseline Gaussian Linear and Transformer-based models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nystr\"om-Accelerated Primal LS-SVMs: Breaking the $O(an^3)$ Complexity Bottleneck for Scalable ODEs Learning</title>
<link>https://arxiv.org/abs/2510.04094</link>
<guid>https://arxiv.org/abs/2510.04094</guid>
<content:encoded><![CDATA[
<div> Kernel-based methods, LS-SVMs, ordinary differential equations, computational complexity, Nyström-accelerated framework<br />
<br />
Summary: 
This paper introduces a novel Nyström-accelerated LS-SVMs framework for solving linear and nonlinear ordinary differential equations efficiently. By reformulating ODEs as primal-space constraints and deriving an explicit Nyström-based mapping to a higher-dimensional feature space, the proposed method significantly reduces computational complexity. Numerical experiments on sixteen benchmark ODEs demonstrate faster computation (up to 6000 times) compared to classical LS-SVMs and PINNs, with comparable accuracy and scalability to a high number of time steps. The framework achieves accuracy improvements over PINNs by 72% in RMSE while maintaining a precision level similar to LS-SVMs. This work presents a breakthrough in kernel-based ODEs learning, providing a more efficient and scalable approach without compromising solution accuracy.<br /><br />Summary: <div>
arXiv:2510.04094v1 Announce Type: new 
Abstract: A major problem of kernel-based methods (e.g., least squares support vector machines, LS-SVMs) for solving linear/nonlinear ordinary differential equations (ODEs) is the prohibitive $O(an^3)$ ($a=1$ for linear ODEs and 27 for nonlinear ODEs) part of their computational complexity with increasing temporal discretization points $n$. We propose a novel Nystr\"om-accelerated LS-SVMs framework that breaks this bottleneck by reformulating ODEs as primal-space constraints. Specifically, we derive for the first time an explicit Nystr\"om-based mapping and its derivatives from one-dimensional temporal discretization points to a higher $m$-dimensional feature space ($1< m\le n$), enabling the learning process to solve linear/nonlinear equation systems with $m$-dependent complexity. Numerical experiments on sixteen benchmark ODEs demonstrate: 1) $10-6000$ times faster computation than classical LS-SVMs and physics-informed neural networks (PINNs), 2) comparable accuracy to LS-SVMs ($<0.13\%$ relative MAE, RMSE, and $\left \| y-\hat{y} \right \| _{\infty } $difference) while maximum surpassing PINNs by 72\% in RMSE, and 3) scalability to $n=10^4$ time steps with $m=50$ features. This work establishes a new paradigm for efficient kernel-based ODEs learning without significantly sacrificing the accuracy of the solution.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Complement to Neural Networks for Anisotropic Inelasticity at Finite Strains</title>
<link>https://arxiv.org/abs/2510.04187</link>
<guid>https://arxiv.org/abs/2510.04187</guid>
<content:encoded><![CDATA[
<div> neural networks, anisotropy, inelasticity, finite strains, dual potential<br />
Summary:<br />
The article introduces a novel approach that combines neural networks with material principles to capture anisotropy and inelasticity at finite strains. A dual potential is utilized to govern dissipation and incorporate anisotropy, while also satisfying the dissipation inequality without requiring convexity. The neural network architecture employs invariant-based input representations and introduces Input Monotonic Neural Networks to expand potential class. Recurrent Liquid Neural Networks are used to bypass time integration issues in the finite strain regime and stabilize training of inelastic materials. The method is evaluated at material point and structural scales, delivering accurate and stable performance beyond the training regime. The implementation is available as open-source and accessible to the public. <div>
arXiv:2510.04187v1 Announce Type: new 
Abstract: We propose a complement to constitutive modeling that augments neural networks with material principles to capture anisotropy and inelasticity at finite strains. The key element is a dual potential that governs dissipation, consistently incorporates anisotropy, and-unlike conventional convex formulations-satisfies the dissipation inequality without requiring convexity.
  Our neural network architecture employs invariant-based input representations in terms of mixed elastic, inelastic and structural tensors. It adapts Input Convex Neural Networks, and introduces Input Monotonic Neural Networks to broaden the admissible potential class. To bypass exponential-map time integration in the finite strain regime and stabilize the training of inelastic materials, we employ recurrent Liquid Neural Networks.
  The approach is evaluated at both material point and structural scales. We benchmark against recurrent models without physical constraints and validate predictions of deformation and reaction forces for unseen boundary value problems. In all cases, the method delivers accurate and stable performance beyond the training regime. The neural network and finite element implementations are available as open-source and are accessible to the public via https://doi.org/10.5281/zenodo.17199965.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-source FDTD solvers: The applicability of Elecode, gprMax and MEEP for simulations of lightning EM fields</title>
<link>https://arxiv.org/abs/2510.04262</link>
<guid>https://arxiv.org/abs/2510.04262</guid>
<content:encoded><![CDATA[
<div> FDTD solvers, gprMax, Elecode, MEEP, lightning electromagnetic field propagation, simulation accuracy <br />
<br />
Summary: 
This study evaluates the open-source FDTD solvers gprMax, Elecode, and MEEP for lightning electromagnetic field propagation simulations. The solvers are tested in various scenarios and compared to reference field results over conducting and lossy ground. Results show that all solvers perform satisfactorily, but careful consideration of spatial discretization and simulation cell boundaries is necessary to avoid numerical dispersion and reflections. Improper parameter choices can lead to inaccurate results, as demonstrated in some cases. The study highlights the features, performance, limitations, advantages, and drawbacks of the solvers. Scripts for initializing and running simulations are provided in a publicly accessible repository to aid the community in using the solvers' programmatic interfaces. <div>
arXiv:2510.04262v1 Announce Type: new 
Abstract: In this study, the open-source finite-difference time-domain (FDTD) solvers gprMax, Elecode and MEEP are investigated for their suitability to compute lightning electromagnetic field propagation. Several simulations are performed to reproduce the results of typical field propagation scenarios that can be found in the literature. The results of the presented solvers are validated through comparison with reference field results corresponding to propagation over perfectly conducting and lossy ground. In most of the tested scenarios, all solvers reproduce the reference fields with satisfactory accuracy. However, close attention must be paid to the proper choice of the spatial discretization to avoid artificial numerical dispersion, and the application of the simulation cell boundaries, which can cause significant impairment of the results due to undesired reflections. Some cases of inaccurate FDTD results due to improper choices of parameters are demonstrated. Further, the features, the performance and limitations, and the advantages and drawbacks of the presented solvers are highlighted. For familiarization with the solvers' programmatical interfaces to initialize and run the simulations, the developed scripts are made available to the community in an openly accessible repository.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fast Option Pricing PDE Solvers Powered by PIELM</title>
<link>https://arxiv.org/abs/2510.04322</link>
<guid>https://arxiv.org/abs/2510.04322</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Partial Differential Equations, Finance, Physics-Informed, Extreme Learning Machines<br />
Summary:<br />
Physics-Informed Extreme Learning Machines (PIELMs) are proposed as a fast alternative to Physics-Informed Neural Networks (PINNs) for solving forward and inverse problems in financial PDEs. PIELMs utilize least-squares solve instead of iterative optimization, leading to efficient and deterministic training. PIELMs are benchmarked on Black-Scholes and Heston-Hull-White models, demonstrating comparable accuracy to PINNs but with up to 30 times faster computation. They excel in inverse model calibration, accurately recovering volatility and interest rate parameters from noisy data. The potential of PIELMs for real-time financial modeling is highlighted, offering a promising solution for efficient and accurate option pricing and risk evaluation in quantitative finance applications.<br /> <div>
arXiv:2510.04322v1 Announce Type: new 
Abstract: Partial differential equation (PDE) solvers underpin modern quantitative finance, governing option pricing and risk evaluation. Physics-Informed Neural Networks (PINNs) have emerged as a promising approach for solving the forward and inverse problems of partial differential equations (PDEs) using deep learning. However they remain computationally expensive due to their iterative gradient descent based optimization and scale poorly with increasing model size. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs) as fast alternative to PINNs for solving both forward and inverse problems in financial PDEs. PIELMs replace iterative optimization with a single least-squares solve, enabling deterministic and efficient training. We benchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward pricing and demonstrate its capability in inverse model calibration to recover volatility and interest rate parameters from noisy data. From experiments we observe that PIELM achieve accuracy comparable to PINNs while being up to $30\times$ faster, highlighting their potential for real-time financial modeling.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep vs. Shallow: Benchmarking Physics-Informed Neural Architectures on the Biharmonic Equation</title>
<link>https://arxiv.org/abs/2510.04490</link>
<guid>https://arxiv.org/abs/2510.04490</guid>
<content:encoded><![CDATA[
<div> Mesh-based solvers, finite difference, finite volume, finite element, physics-informed neural networks (PINNs), RBF-PIELM<br />
Summary:<br />
This paper presents a benchmark of RBF-PIELM, an extreme learning machine with radial-basis activations, for solving higher-order partial differential equations (PDEs). Compared to traditional mesh-based approaches and physics-informed neural networks (PINNs), RBF-PIELM offers faster training (350 times faster than PINNs) and requires fewer parameters (10 times fewer for comparable accuracy). However, RBF-PIELM still falls short of mature mesh-based solvers' accuracy and struggles with highly oscillatory solutions. The study focuses on the fourth-order biharmonic equation and tests RBF-PIELM on lid-driven cavity flow and an oscillatory solution. This research highlights the potential of RBF-PIELM as a rapid and efficient PDE solver but also identifies the need for further development to address challenges in practical applications. <br /> <div>
arXiv:2510.04490v1 Announce Type: new 
Abstract: Partial differential equation (PDE) solvers are fundamental to engineering simulation. Classical mesh-based approaches (finite difference/volume/element) are fast and accurate on high-quality meshes but struggle with higher-order operators and complex, hard-to-mesh geometries. Recently developed physics-informed neural networks (PINNs) and their variants are mesh-free and flexible, yet compute-intensive and often less accurate. This paper systematically benchmarks RBF-PIELM, a rapid PINN variant-an extreme learning machine with radial-basis activations-for higher-order PDEs. RBF-PIELM replaces PINNs' time-consuming gradient descent with a single-shot least-squares solve. We test RBF-PIELM on the fourth-order biharmonic equation using two benchmarks: lid-driven cavity flow (streamfunction formulation) and a manufactured oscillatory solution. Our results show up to $(350\times)$ faster training than PINNs and over $(10\times)$ fewer parameters for comparable solution accuracy. Despite surpassing PINNs, RBF-PIELM still lags mature mesh-based solvers and its accuracy degrades on highly oscillatory solutions, highlighting remaining challenges for practical deployment.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems</title>
<link>https://arxiv.org/abs/2510.01396</link>
<guid>https://arxiv.org/abs/2510.01396</guid>
<content:encoded><![CDATA[
<div> surrogate framework, neural network, Cartesian coordinates, automatic differentiation, free energy reconstruction

Summary: 
The article presents a neural network surrogate framework that learns collective variables (CVs) directly from Cartesian coordinates, eliminating the need for analytical Jacobians in free energy reconstruction methods like Gaussian Process Regression (GPR). This innovative approach allows for the use of complex or machine-learned CVs and provides accurate Jacobians through automatic differentiation. The framework was tested on an MgCl2 ion-pairing system, demonstrating high accuracy with both simple distance CVs and complex coordination-number CVs. Additionally, the errors in Jacobians were found to follow a near-Gaussian distribution, making them suitable for GPR pipelines. By enabling gradient-based free energy methods to incorporate complex CVs, this framework expands the capabilities of biochemistry and materials simulations.<br /><br />Summary: <div>
arXiv:2510.01396v1 Announce Type: cross 
Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR) require Jacobians of the collective variables (CVs), a bottleneck that restricts the use of complex or machine-learned CVs. We introduce a neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to provide Jacobians, bypassing analytical forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for both a simple distance CV and a complex coordination-number CV. Moreover, Jacobian errors also followed a near-Gaussian distribution, making them suitable for GPR pipelines. This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening the scope of biochemistry and materials simulations.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark</title>
<link>https://arxiv.org/abs/2510.03261</link>
<guid>https://arxiv.org/abs/2510.03261</guid>
<content:encoded><![CDATA[
<div> Neural networks, thermal errors, machine tools, data-driven compensation, high-fidelity prediction <br />
Summary:  
Neural networks are utilized to predict temperature and heat flux fields in machine tools, offering a more versatile approach to thermal error correction. This method allows for the computation and correction of various error types using interchangeable components. By training the neural network on finite element method data under different initial conditions, accurate predictions can be made with minimal hardware requirements. Various time-series neural network architectures are benchmarked to determine their effectiveness in predicting temperature and heat flux fields. The results demonstrate the ability to predict these fields accurately and affordably, setting the stage for flexible and adaptable thermal error correction methods in machine tool environments. <br /> <div>
arXiv:2510.03261v1 Announce Type: cross 
Abstract: Thermal errors in machine tools significantly impact machining precision and productivity. Traditional thermal error correction/compensation methods rely on measured temperature-deformation fields or on transfer functions. Most existing data-driven compensation strategies employ neural networks (NNs) to directly predict thermal errors or specific compensation values. While effective, these approaches are tightly bound to particular error types, spatial locations, or machine configurations, limiting their generality and adaptability. In this work, we introduce a novel paradigm in which NNs are trained to predict high-fidelity temperature and heat flux fields within the machine tool. The proposed framework enables subsequent computation and correction of a wide range of error types using modular, swappable downstream components. The NN is trained using data obtained with the finite element method under varying initial conditions and incorporates a correlation-based selection strategy that identifies the most informative measurement points, minimising hardware requirements during inference. We further benchmark state-of-the-art time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit, Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal Convolutional Network, by training both specialised models, tailored for specific initial conditions, and general models, capable of extrapolating to unseen scenarios. The results show accurate and low-cost prediction of temperature and heat flux fields, laying the basis for enabling flexible and generalisable thermal error correction in machine tool environments.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions</title>
<link>https://arxiv.org/abs/2510.03370</link>
<guid>https://arxiv.org/abs/2510.03370</guid>
<content:encoded><![CDATA[
<div> fine-tuning, protein language models, multimodal, mutation-effect prediction, ESM2 <br />
Summary: <br />
The authors introduce a fine-tuning framework named InstructPLM-mu for improving mutation-effect prediction in protein language models. They investigate whether fine-tuning a pretrained sequence-only model with structural inputs can match the performance of end-to-end trained models. Surprisingly, experiments demonstrate that fine-tuning ESM2 with structural inputs can achieve performance comparable to ESM3. The study systematically compares different feature-fusion designs and fine-tuning recipes, highlighting the significant impact of fusion methods and tuning strategies on final accuracy. These findings suggest that the fine-tuning process is critical and nontrivial. The work aims to provide practical insights into integrating structure into pretrained protein language models and encourages further exploration of advanced fusion mechanisms and fine-tuning protocols. <br /> <div>
arXiv:2510.03370v1 Announce Type: cross 
Abstract: Multimodal protein language models deliver strong performance on mutation-effect prediction, but training such models from scratch demands substantial computational resources. In this paper, we propose a fine-tuning framework called InstructPLM-mu and try to answer a question: \textit{Can multimodal fine-tuning of a pretrained, sequence-only protein language model match the performance of models trained end-to-end? } Surprisingly, our experiments show that fine-tuning ESM2 with structural inputs can reach performance comparable to ESM3. To understand how this is achieved, we systematically compare three different feature-fusion designs and fine-tuning recipes. Our results reveal that both the fusion method and the tuning strategy strongly affect final accuracy, indicating that the fine-tuning process is not trivial. We hope this work offers practical guidance for injecting structure into pretrained protein language models and motivates further research on better fusion mechanisms and fine-tuning protocols.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design</title>
<link>https://arxiv.org/abs/2510.03650</link>
<guid>https://arxiv.org/abs/2510.03650</guid>
<content:encoded><![CDATA[
<div> point sets, digital sequences, quasi-Monte Carlo, program synthesis, evolutionary algorithm
Summary:
Quasi-Monte Carlo (QMC) methods for high-dimensional integration rely on low-discrepancy point sets and digital sequences. This study presents an approach using an evolutionary algorithm guided by program synthesis to tackle two QMC design problems. The first problem involves constructing finite 2D/3D point sets with low star discrepancy, while the second problem focuses on selecting Sobol' direction numbers to minimize randomized QMC error. The approach combines constructive code proposals with numerical refinement to achieve optimal results. The study successfully rediscovers known optimal designs and sets new benchmarks for 2D and 3D point sets. Additionally, evolving Sobol' parameters leads to reduced error in rQMC for 32-dimensional tasks, surpassing the performance of traditional parameters. The results highlight the effectiveness of using evolutionary program synthesis to automate the discovery of high-quality QMC constructions, improving upon existing designs and optimizing for specific integration tasks. Data and code resources are available for further exploration and implementation. 
<br /><br />Summary: <div>
arXiv:2510.03650v1 Announce Type: cross 
Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo (QMC) methods for high-dimensional integration. We cast two long-standing QMC design problems as program synthesis and solve them with an LLM-guided evolutionary loop that mutates and selects code under task-specific fitness: (i) constructing finite 2D/3D point sets with low star discrepancy, and (ii) choosing Sobol' direction numbers that minimize randomized QMC error on downstream integrands. Our two-phase procedure combines constructive code proposals with iterative numerical refinement. On finite sets, we rediscover known optima in small 2D cases and set new best-known 2D benchmarks for N >= 40, while matching most known 3D optima up to the proven frontier (N <= 8) and reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol' parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC) mean-squared error for several 32-dimensional option-pricing tasks relative to widely used Joe--Kuo parameters, while preserving extensibility to any sample size and compatibility with standard randomizations. Taken together, the results demonstrate that LLM-driven evolutionary program synthesis can automate the discovery of high-quality QMC constructions, recovering classical designs where they are optimal and improving them where finite-N structure matters. Data and code are available at https://github.com/hockeyguy123/openevolve-star-discrepancy.git.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling</title>
<link>https://arxiv.org/abs/2510.04204</link>
<guid>https://arxiv.org/abs/2510.04204</guid>
<content:encoded><![CDATA[
<div> keywords: Large Reasoning Models, domain adaptation, optimization modeling, CALM, STORM

Summary: 
Large Reasoning Models (LRMs) have advanced capabilities in multi-step reasoning, but traditional domain adaptation methods are ineffective for leveraging these abilities. Direct fine-tuning on non-reflective datasets yields limited improvements in LRMs' reasoning patterns. To address this issue, the framework CALM (Corrective Adaptation with Lightweight Modification) is proposed, which refines LRMs within their native reasoning modes for optimization modeling tasks. CALM involves an expert providing corrective hints to guide the model in producing improved reasoning trajectories with minimal token modification. This approach, combined with reinforcement learning, results in the development of STORM (Smart Thinking Optimization Reasoning Model), a high-performing LRM with 4B parameters. STORM achieves a state-of-the-art average accuracy of 68.9% across multiple optimization modeling benchmarks, demonstrating the effectiveness of hint-based data synthesis in preserving and enhancing LRMs' reasoning abilities for expert-level performance. 

<br /><br />Summary: <div>
arXiv:2510.04204v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional \textit{non-reflective} datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose \textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCR-EML: Explainable Model Layers for TCR-pMHC Prediction</title>
<link>https://arxiv.org/abs/2510.04377</link>
<guid>https://arxiv.org/abs/2510.04377</guid>
<content:encoded><![CDATA[
<div> Machine Learning, TCR-pMHC binding, Explainability, Vaccine design, Cancer immunotherapy <br />
Summary: <br />
This article introduces a new approach, TCR-EML, for modeling T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes using explainable model layers. These layers incorporate known binding mechanisms to provide high-quality explanations for predicted TCR-pMHC binding. The approach, based on prototype layers for amino acid residue contacts, improves upon existing black-box transformer models by offering insights into the biochemical mechanisms underlying TCR-pMHC interactions. Experimental results on large datasets show competitive predictive accuracy and generalization, with improved explainability compared to current methods. The proposed explainable model layers have potential applications in vaccine design, cancer immunotherapy, and autoimmune disease research, enhancing our understanding of TCR-pMHC binding and enabling the development of more effective therapies. <div>
arXiv:2510.04377v1 Announce Type: cross 
Abstract: T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is a central component of adaptive immunity, with implications for vaccine design, cancer immunotherapy, and autoimmune disease. While recent advances in machine learning have improved prediction of TCR-pMHC binding, the most effective approaches are black-box transformer models that cannot provide a rationale for predictions. Post-hoc explanation methods can provide insight with respect to the input but do not explicitly model biochemical mechanisms (e.g. known binding regions), as in TCR-pMHC binding. ``Explain-by-design'' models (i.e., with architectural components that can be examined directly after training) have been explored in other domains, but have not been used for TCR-pMHC binding. We propose explainable model layers (TCR-EML) that can be incorporated into protein-language model backbones for TCR-pMHC modeling. Our approach uses prototype layers for amino acid residue contacts drawn from known TCR-pMHC binding mechanisms, enabling high-quality explanations for predicted TCR-pMHC binding. Experiments of our proposed method on large-scale datasets demonstrate competitive predictive accuracy and generalization, and evaluation on the TCR-XAI benchmark demonstrates improved explainability compared with existing approaches.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overlapping Schwarz Scheme for Linear-Quadratic Programs in Continuous Time</title>
<link>https://arxiv.org/abs/2510.04478</link>
<guid>https://arxiv.org/abs/2510.04478</guid>
<content:encoded><![CDATA[
<div> framework, linear-quadratic optimal control, time-inhomogeneous ODEs, Hamiltonian systems, numerical integration <br />
<br />
Summary: 
The article presents an optimize-then-discretize framework for solving linear-quadratic optimal control problems governed by time-inhomogeneous ordinary differential equations. The method utilizes a modified overlapping Schwarz decomposition based on the Pontryagin Minimum Principle to partition the temporal domain into overlapping intervals and solve Hamiltonian systems independently. The convergence is ensured by updating the boundary conditions of the individual Hamiltonian dynamics. The analysis proves that the exponential decay of sensitivity seen in discrete-time OCPs extends to the continuous-time setting. Unlike the approach of discretize-then-optimize, this method can incorporate various numerical integration methods for solving the resulting Hamiltonian two-point boundary-value subproblems, including adaptive-time integrators. A numerical experiment on a linear-quadratic OCP demonstrates the practicality and versatility of this approach in diverse scientific applications. <br /> <div>
arXiv:2510.04478v1 Announce Type: cross 
Abstract: We present an optimize-then-discretize framework for solving linear-quadratic optimal control problems (OCP) governed by time-inhomogeneous ordinary differential equations (ODEs). Our method employs a modified overlapping Schwarz decomposition based on the Pontryagin Minimum Principle, partitioning the temporal domain into overlapping intervals and independently solving Hamiltonian systems in continuous time. We demonstrate that the convergence is ensured by appropriately updating the boundary conditions of the individual Hamiltonian dynamics. The cornerstone of our analysis is to prove that the exponential decay of sensitivity (EDS) exhibited in discrete-time OCPs carries over to the continuous-time setting. Unlike the discretize-then-optimize approach, our method can flexibly incorporate different numerical integration methods for solving the resulting Hamiltonian two-point boundary-value subproblems, including adaptive-time integrators. A numerical experiment on a linear-quadratic OCP illustrates the practicality of our approach in broad scientific applications.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering</title>
<link>https://arxiv.org/abs/2510.04514</link>
<guid>https://arxiv.org/abs/2510.04514</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal LLMs, visual question answering, ChartAgent, visual reasoning, chart understanding <br />
Summary:<br />
- The study introduces ChartAgent, a framework for visual reasoning on chart images, which outperforms previous methods on benchmarks by up to 16.07% on average and 17.31% for complex queries.<br />
- ChartAgent breaks down queries into visual subtasks and manipulates chart images using specialized actions, mimicking human cognitive strategies for chart comprehension.<br />
- The framework achieves high accuracy across various types of charts and complexity levels, proving its effectiveness in visually grounded reasoning.<br />
- ChartAgent serves as a plug-and-play tool that enhances the performance of different multimodal language models (LLMs) in chart-based visual question answering tasks.<br />
- This work showcases the potential of visually grounded reasoning in improving chart interpretation and understanding through tool-augmented multimodal agents.<br /> <div>
arXiv:2510.04514v1 Announce Type: cross 
Abstract: Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Time Series Foundation Models for Short-Term Household Electricity Load Forecasting</title>
<link>https://arxiv.org/abs/2410.09487</link>
<guid>https://arxiv.org/abs/2410.09487</guid>
<content:encoded><![CDATA[
<div> Time series foundation models, household electricity STLF, Chronos, TimesFM, Time-MoE <br />
Summary: <br />
Accurate short-term load forecasting for household electricity is crucial for sustainable energy systems. While various approaches have been used for this task, newer time series foundation models like Chronos and Time-MoE offer a different approach, trained on extensive time series data for zero-shot learning. This study compares the forecasting performance of these foundation models with Transformer-based approaches. Results show that foundation models perform similarly to Transformer models, with certain foundation models outperforming Transformers as input size increases. Foundation models also require less effort as they do not need domain-specific training and only limited contextual data for inference. <div>
arXiv:2410.09487v2 Announce Type: replace 
Abstract: Accurate household electricity short-term load forecasting (STLF) is key to future and sustainable energy systems. While various studies have analyzed statistical, machine learning, or deep learning approaches for household electricity STLF, recently proposed time series foundation models such as Chronos, TimesFM or Time-MoE promise a new approach for household electricity STLF. These models are trained on a vast amount of time series data and are able to forecast time series without explicit task-specific training (zero-shot learning). In this study, we benchmark the forecasting capabilities of time series foundation models compared to Trained-from-Scratch (TFS) Transformer-based approaches. Our results suggest that foundation models perform comparably to TFS Transformer models, while certain time series foundation models outperform all TFS models when the input size increases. At the same time, they require less effort, as they need no domain-specific training and only limited contextual data for inference.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGRDN-Data learned sparsification of graph reaction-diffusion networks</title>
<link>https://arxiv.org/abs/2303.11943</link>
<guid>https://arxiv.org/abs/2303.11943</guid>
<content:encoded><![CDATA[
<div> sparsification, graph, reaction-diffusion systems, Reduced Order Model, eigenmodes <br />
<br />
Summary: <br />
Graph sparsification is a challenging task in computer science and mathematics, aimed at reducing the number of edges while preserving graph properties. A new method called SGRDN extends sparsification to complex reaction-diffusion systems, preserving their dynamics. By framing sparsification as a data assimilation problem within a Reduced Order Model space, SGRDN enforces constraints to conserve the eigenmodes of the Laplacian matrix. Efficient eigenvalue and eigenvector approximations for perturbed Laplacian matrices are derived and integrated into the optimization process. An experiment on Neural Ordinary Differential Equations demonstrates SGRDN's ability to achieve parameter sparsity. This method broadens the applicability of sparsification techniques to complex systems while maintaining spectral preservation constraints. <div>
arXiv:2303.11943v4 Announce Type: replace-cross 
Abstract: Graph sparsification is an area of interest in computer science and applied mathematics. Sparsification of a graph, in general, aims to reduce the number of edges in the network while preserving specific properties of the graph, like cuts and subgraph counts. Computing the sparsest cuts of a graph is known to be NP-hard, and sparsification routines exist for generating linear-sized sparsifiers in almost quadratic running time $O(n^{2 + \epsilon})$. Consequently, obtaining a sparsifier can be a computationally demanding task, and the complexity varies based on the level of sparsity required. We propose SGRDN to extend sparsification to complex reaction-diffusion systems. This approach seeks to sparsify the graph such that the inherent reaction-diffusion dynamics are strictly preserved on the resulting structure. By selectively considering a subset of trajectories, we frame the network sparsification issue as a data assimilation problem within a Reduced Order Model (ROM) space, imposing constraints to conserve the eigenmodes of the Laplacian matrix ($L = D - A$), the difference between the degree matrix ($D$) and the adjacency matrix ($A$) despite perturbations. We derive computationally efficient eigenvalue and eigenvector approximations for perturbed Laplacian matrices and integrate these as spectral preservation constraints in the optimization problem. To further validate the method's broad applicability, we conducted an additional experiment on Neural Ordinary Differential Equations (neural ODEs), where SGRDN successfully achieved parameter sparsity.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Graph Representation of Stiffened Panels with Non-Uniform Boundary Conditions and Loads</title>
<link>https://arxiv.org/abs/2510.02472</link>
<guid>https://arxiv.org/abs/2510.02472</guid>
<content:encoded><![CDATA[
<div> Graph representation, heterogeneous graph neural network, stiffened panels, structural analysis, optimization

Summary:
The study introduces a heterogeneous graph representation using heterogeneous graph neural networks (HGNNs) for analyzing and optimizing stiffened panels. The structure is divided into distinct units with varying node types to consider geometrical variability, boundary conditions, and loading scenarios. Edge heterogeneity is incorporated to capture local orientations and spatial relationships. The proposed heterogeneous graph representations are implemented into a heterogeneous graph transformer (HGT) to predict stress and displacement fields across stiffened panels based on loading and boundary conditions. Numerical tests on panels and box beams demonstrate the superior performance of the heterogeneous graph representation compared to a homogeneous counterpart. The approach shows strong predictive accuracy for both displacement and stress, effectively capturing structural behavior patterns and maximum values.<br /><br />Summary: <div>
arXiv:2510.02472v1 Announce Type: new 
Abstract: Surrogate models are essential in structural analysis and optimization. We propose a heterogeneous graph representation of stiffened panels that accounts for geometrical variability, non-uniform boundary conditions, and diverse loading scenarios, using heterogeneous graph neural networks (HGNNs). The structure is partitioned into multiple structural units, such as stiffeners and the plates between them, with each unit represented by three distinct node types: geometry, boundary, and loading nodes. Edge heterogeneity is introduced by incorporating local orientations and spatial relationships of the connecting nodes. Several heterogeneous graph representations, each with varying degrees of heterogeneity, are proposed and analyzed. These representations are implemented into a heterogeneous graph transformer (HGT) to predict von Mises stress and displacement fields across stiffened panels, based on loading and degrees of freedom at their boundaries. To assess the efficacy of our approach, we conducted numerical tests on panels subjected to patch loads and box beams composed of stiffened panels under various loading conditions. The heterogeneous graph representation was compared with a homogeneous counterpart, demonstrating superior performance. Additionally, an ablation analysis was performed to evaluate the impact of graph heterogeneity on HGT performance. The results show strong predictive accuracy for both displacement and von Mises stress, effectively capturing structural behavior patterns and maximum values.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisitHGNN: Heterogeneous Graph Neural Networks for Modeling Point-of-Interest Visit Patterns</title>
<link>https://arxiv.org/abs/2510.02702</link>
<guid>https://arxiv.org/abs/2510.02702</guid>
<content:encoded><![CDATA[
<div> Keywords: urban travel, origin-destination flow, graph neural network, mobility data, public health

Summary: 
- The study focuses on understanding urban residents' travel patterns for transportation planning, mobility management, and public health.
- VisitHGNN, a graph neural network, predicts visit probabilities at individual Points of interest (POIs) using historical flow patterns and spatial, temporal, and functional relations.
- POIs and census block groups (CBGs) are linked through spatial adjacency and distance-annotated cross-type edges.
- The model achieves strong predictive performance with mean KL divergence of 0.287, MAE of 0.008, and high accuracy metrics, outperforming baseline models.
- The model closely mirrors observed travel behavior, demonstrating its potential for decision support in urban planning, transportation policy, mobility system design, and public health. 

<br /><br />Summary: <div>
arXiv:2510.02702v1 Announce Type: new 
Abstract: Understanding how urban residents travel between neighborhoods and destinations is critical for transportation planning, mobility management, and public health. By mining historical origin-to-destination flow patterns with spatial, temporal, and functional relations among urban places, we estimate probabilities of visits from neighborhoods to specific destinations. These probabilities capture neighborhood-level contributions to citywide vehicular and foot traffic, supporting demand estimation, accessibility assessment, and multimodal planning. Particularly, we introduce VisitHGNN, a heterogeneous, relation-specific graph neural network designed to predict visit probabilities at individual Points of interest (POIs). POIs are characterized using numerical, JSON-derived, and textual attributes, augmented with fixed summaries of POI--POI spatial proximity, temporal co-activity, and brand affinity, while census block groups (CBGs) are described with 72 socio-demographic variables. CBGs are connected via spatial adjacency, and POIs and CBGs are linked through distance-annotated cross-type edges. Inference is constrained to a distance-based candidate set of plausible origin CBGs, and training minimizes a masked Kullback-Leibler (KL) divergence to yield probability distribution across the candidate set. Using weekly mobility data from Fulton County, Georgia, USA, VisitHGNN achieves strong predictive performance with mean KL divergence of 0.287, MAE of 0.008, Top-1 accuracy of 0.853, and R-square of 0.892, substantially outperforming pairwise MLP and distance-only baselines, and aligning closely with empirical visitation patterns (NDCG@50 = 0.966); Recall@5 = 0.611). The resulting distributions closely mirror observed travel behavior with high fidelity, highlighting the model's potential for decision support in urban planning, transportation policy, mobility system design, and public health.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Hit Moving Targets? Tracking Evolving Signals in Corporate Disclosures</title>
<link>https://arxiv.org/abs/2510.03195</link>
<guid>https://arxiv.org/abs/2510.03195</guid>
<content:encoded><![CDATA[
<div> entity recognition, target extraction, strategic shifting, key performance metrics, stock underperformance

Summary:
- The study focuses on moving targets in management, where shifting of key performance metrics predicts stock underperformance.
- Limitations in the original method, such as noise in extracted targets and loss of contextual information, were identified due to named entity recognition (NER) use.
- An LLM-based target extraction method with a new metric was proposed to capture semantic context better.
- The approach preserves semantic context and yields higher predictive power compared to the original method.
- Overall, the proposed approach enhances the granularity and accuracy of predicting financial performance based on text analysis.

<br /><br />Summary: <div>
arXiv:2510.03195v1 Announce Type: new 
Abstract: Moving targets -- managers' strategic shifting of key performance metrics when the original targets become difficult to achieve -- have been shown to predict subsequent stock underperformance. However, our work reveals that the method employed in that study exhibits two key limitations that hinder the accuracy -- noise in the extracted targets and loss of contextual information -- both of which stem primarily from the use of a named entity recognition (NER). To address these two limitations, we propose an LLM-based target extraction} method with a newly defined metric that better captures semantic context. This approach preserves semantic context beyond simple entity recognition and yields consistently higher predictive power than the original approach. Overall, our approach enhances the granularity and accuracy of financial text-based performance prediction.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully automated inverse co-optimization of templates and block copolymer blending recipes for DSA lithography</title>
<link>https://arxiv.org/abs/2510.02715</link>
<guid>https://arxiv.org/abs/2510.02715</guid>
<content:encoded><![CDATA[
<div> Gaussian descriptor, template shape, self-assembly, block copolymers, Bayesian optimization <br />
Summary: <br />
The article discusses the directed self-assembly (DSA) of block copolymers (BCPs) for fabricating contact holes at sub-7nm technology nodes. A Gaussian descriptor with two parameters is proposed to characterize template shapes for guiding self-assembly. AB/AB binary blends are suggested for improved adaptability to template shapes. Bayesian optimization is used to co-optimize the binary blend and template shape, leading to optimal templates for various multi-hole patterns. Templates are optimized with constraints on curvature variation for manufacturability. Key parameters of the blend have wide tunable windows for high precision requirements. This research provides insights for advancing DSA technology and its practical applications. <br /> <div>
arXiv:2510.02715v1 Announce Type: cross 
Abstract: The directed self-assembly (DSA) of block copolymers (BCPs) offers a highly promising approach for the fabrication of contact holes or vertical interconnect access at sub-7nm technology nodes. To fabricate circular holes with precisely controlled size and positions, the self-assembly of block copolymers requires guidance from a properly designed template. Effectively parameterizing the template shape to enable efficient optimization remains a critical yet challenging problem. Moreover, the optimized template must possess excellent manufacturability for practical applications. In this work, we propose a Gaussian descriptor for characterizing the template shape with only two parameters. We further propose to use AB/AB binary blends instead of pure diblock copolymer to improve the adaptability of the block copolymer system to the template shape. The Bayesian optimization (BO) is applied to co-optimize the binary blend and the template shape. Our results demonstrate that BO based on the Gaussian descriptor can efficiently yield the optimal templates for diverse multi-hole patterns, all leading to highly matched self-assembled morphologies. Moreover, by imposing constraints on the variation of curvature of the template during optimization, superior manufacturability is ensured for each optimized template. It is noteworthy that each key parameter of the blend exhibits a relatively wide tunable window under the requirement of rather high precision. Our work provides valuable insights for advancing DSA technology, and thus potentially propels its practical applications forward.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories</title>
<link>https://arxiv.org/abs/2510.03152</link>
<guid>https://arxiv.org/abs/2510.03152</guid>
<content:encoded><![CDATA[
<div> modeling, human mobility, urban planning, spatiotemporal trajectories, Patterns of Life (PoLs) 

Summary:
The article introduces Markovian Reeb Graphs, a novel framework for simulating spatiotemporal trajectories that accurately capture Patterns of Life (PoLs) learned from baseline data. By incorporating individual- and population-level mobility structures in a probabilistic topological model, the approach generates realistic future trajectories that maintain both consistency and variability in daily life. Evaluations on the Urban Anomalies dataset (Atlanta and Berlin subsets) demonstrate that the method achieves high fidelity using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics. The results show that Markovian Reeb Graphs are data- and compute-efficient while providing a scalable framework for trajectory simulation suitable for various urban environments. <div>
arXiv:2510.03152v1 Announce Type: cross 
Abstract: Accurately modeling human mobility is critical for urban planning, epidemiology, and traffic management. In this work, we introduce Markovian Reeb Graphs, a novel framework for simulating spatiotemporal trajectories that preserve Patterns of Life (PoLs) learned from baseline data. By combining individual- and population-level mobility structures within a probabilistic topological model, our approach generates realistic future trajectories that capture both consistency and variability in daily life. Evaluations on the Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics demonstrate that the proposed method achieves strong fidelity while remaining data- and compute-efficient. These results position Markovian Reeb Graphs as a scalable framework for trajectory simulation with broad applicability across diverse urban environments.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation</title>
<link>https://arxiv.org/abs/2510.01225</link>
<guid>https://arxiv.org/abs/2510.01225</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial digests, data extraction, automated generation, actionable insights 

Summary: 
Large Language Models (LLMs), such as Google's Gemini Pro, offer a new framework for automatically generating insightful financial digests. By extracting data from OpenAlex, strategically engineering prompts, and utilizing LLM-driven analysis, this approach provides comprehensive digests that highlight key findings and emerging trends. This method overcomes traditional analysis limitations by efficiently processing vast amounts of unstructured data and delivering actionable insights in a user-friendly format. The paper explains the workings of LLMs in simple terms and demonstrates how their power can help researchers and scholars save time and stay informed on current trends. The study details the step-by-step process, from data acquisition and JSON construction to interacting with Gemini and automating PDF report generation. A link to the project's GitHub repository is provided for broader accessibility and continued development. 

<br /><br />Summary: <div>
arXiv:2510.01225v1 Announce Type: new 
Abstract: The exponential growth of information presents a significant challenge for researchers and professionals seeking to remain at the forefront of their fields and this paper introduces an innovative framework for automatically generating insightful financial digests using the power of Large Language Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of data extraction from OpenAlex, strategic prompt engineering, and LLM-driven analysis, we demonstrate the automated example of creating a comprehensive digests that generalize key findings, identify emerging trends. This approach addresses the limitations of traditional analysis methods, enabling the efficient processing of vast amounts of unstructured data and the delivery of actionable insights in an easily digestible format. This paper describes how LLMs work in simple words and how we can use their power to help researchers and scholars save their time and stay informed about current trends. Our study includes step-by-step process, from data acquisition and JSON construction to interaction with Gemini and the automated generation of PDF reports, including a link to the project's GitHub repository for broader accessibility and further development.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioRAG: A Retrieval-Augmented Generation Framework for Multimodal Chagas Disease Detection</title>
<link>https://arxiv.org/abs/2510.01558</link>
<guid>https://arxiv.org/abs/2510.01558</guid>
<content:encoded><![CDATA[
<div> AI-enhanced electrocardiogram (ECG) screening is proposed for Chagas disease diagnosis in regions with limited testing capabilities. A retrieval-augmented generation framework, CardioRAG, integrates large language models with ECG-based clinical features for improved accuracy and interpretability. Key features include right bundle branch block, left anterior fascicular block, and heart rate variability metrics. Variational autoencoder-learned representations aid in semantic case retrieval to guide clinical reasoning. Evaluation results show high recall performance (89.80%) and a maximum F1 score of 0.68, effectively identifying positive cases for prioritized serological testing. CardioRAG offers an evidence-based, interpretable approach suitable for resource-constrained settings, showcasing the potential of incorporating clinical indicators in medical AI systems. 

Keywords: Chagas disease, AI-enhanced ECG screening, clinical features, interpretation, interpretable medical AI systems

<br /><br />Summary: 
- Chagas disease diagnosis in regions with limited testing capabilities
- Integration of ECG-based clinical features for accuracy and interpretability
- Key features: right bundle branch block, left anterior fascicular block, heart rate variability metrics
- Utilization of variational autoencoder-learned representations for semantic case retrieval
- High recall performance and effective identification of positive cases for prioritized serological testing 
- Resource-constrained settings benefit from an evidence-based, interpretable approach
- Potential of incorporating clinical indicators in medical AI systems <div>
arXiv:2510.01558v1 Announce Type: new 
Abstract: Chagas disease affects nearly 6 million people worldwide, with Chagas cardiomyopathy representing its most severe complication. In regions where serological testing capacity is limited, AI-enhanced electrocardiogram (ECG) screening provides a critical diagnostic alternative. However, existing machine learning approaches face challenges such as limited accuracy, reliance on large labeled datasets, and more importantly, weak integration with evidence-based clinical diagnostic indicators. We propose a retrieval-augmented generation framework, CardioRAG, integrating large language models with interpretable ECG-based clinical features, including right bundle branch block, left anterior fascicular block, and heart rate variability metrics. The framework uses variational autoencoder-learned representations for semantic case retrieval, providing contextual cases to guide clinical reasoning. Evaluation demonstrated high recall performance of 89.80%, with a maximum F1 score of 0.68 for effective identification of positive cases requiring prioritized serological testing. CardioRAG provides an interpretable, clinical evidence-based approach particularly valuable for resource-limited settings, demonstrating a pathway for embedding clinical indicators into trustworthy medical AI systems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShapeGen3DCP: A Deep Learning Framework for Layer Shape Prediction in 3D Concrete Printing</title>
<link>https://arxiv.org/abs/2510.02009</link>
<guid>https://arxiv.org/abs/2510.02009</guid>
<content:encoded><![CDATA[
<div> ShapeGen3DCP, deep learning, filament cross-sectional geometry, 3D Concrete Printing, neural network<br />
<br />
Summary:<br />
ShapeGen3DCP introduces a deep learning framework for accurately predicting filament cross-sectional geometry in 3D Concrete Printing. The framework utilizes a neural network architecture that incorporates material properties and process parameters to predict extruded layer shapes. By reformulating inputs into dimensionless parameters and using Fourier descriptors to represent predicted geometries, the framework achieves compact and smooth profiles. The training dataset is generated synthetically using a PFEM model, enabling validation against numerical and experimental cases. This approach enhances generalization and accuracy of predictions, offering practical applications such as pre-calibrating print settings and optimizing toolpaths. Future developments may involve integrating the framework with simulations and sensor feedback, enabling real-time optimization, defect detection, and adaptive control in 3DCP processes. <div>
arXiv:2510.02009v1 Announce Type: new 
Abstract: This work introduces ShapeGen3DCP, a deep learning framework for fast and accurate prediction of filament cross-sectional geometry in 3D Concrete Printing (3DCP). The method is based on a neural network architecture that takes as input both material properties in the fluid state (density, yield stress, plastic viscosity) and process parameters (nozzle diameter, nozzle height, printing and flow velocities) to directly predict extruded layer shapes. To enhance generalization, some inputs are reformulated into dimensionless parameters that capture underlying physical principles. Predicted geometries are compactly represented using Fourier descriptors, which enforce smooth, closed, and symmetric profiles while reducing the prediction task to a small set of coefficients. The training dataset was synthetically generated using a well-established Particle Finite Element (PFEM) model of 3DCP, overcoming the scarcity of experimental data. Validation against diverse numerical and experimental cases shows strong agreement, confirming the framework's accuracy and reliability. This opens the way to practical uses ranging from pre-calibration of print settings, minimizing or even eliminating trial-and-error adjustments, to toolpath optimization for more advanced designs. Looking ahead, coupling the framework with simulations and sensor feedback could enable closed-loop digital twins for 3DCP, driving real-time process optimization, defect detection, and adaptive control of printing parameters.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Copula-Based Variational Autoencoder for Uncertainty Quantification in Inverse Problems: Application to Damage Identification in an Offshore Wind Turbine</title>
<link>https://arxiv.org/abs/2510.02013</link>
<guid>https://arxiv.org/abs/2510.02013</guid>
<content:encoded><![CDATA[
<div> varitational autoencoder, floating offshore wind turbines, structural health monitoring, damage identification, copula<br />
<br />
Summary: <br />
Structural Health Monitoring of Floating Offshore Wind Turbines (FOWTs) is crucial for safety and efficiency. This study addresses the challenge of identifying damage in components like mooring systems from limited sensor data by proposing a Variational Autoencoder (VAE) architecture. The VAE models the inverse operator through the encoder and the forward operator through the decoder, with a probabilistic representation of uncertainties in the estimates. A novel Copula-based VAE architecture is introduced, offering a flexible method for representing complex, correlated posterior distributions. Comparative analysis shows that the Copula VAE outperforms Gaussian Mixture alternatives in high-dimensional spaces with fewer parameters, making it a promising tool for uncertainty-aware damage identification in FOWT mooring systems. While the study is limited to a two-dimensional space, scalability to higher dimensions is anticipated based on the favorable performance of the Copula VAE. <div>
arXiv:2510.02013v1 Announce Type: new 
Abstract: Structural Health Monitoring of Floating Offshore Wind Turbines (FOWTs) is critical for ensuring operational safety and efficiency. However, identifying damage in components like mooring systems from limited sensor data poses a challenging inverse problem, often characterized by multimodal solutions where various damage states could explain the observed response. To overcome it, we propose a Variational Autoencoder (VAE) architecture, where the encoder approximates the inverse operator, while the decoder approximates the forward. The posterior distribution of the latent space variables is probabilistically modeled, describing the uncertainties in the estimates. This work tackles the limitations of conventional Gaussian Mixtures used within VAEs, which can be either too restrictive or computationally prohibitive for high-dimensional spaces. We propose a novel Copula-based VAE architecture that decouples the marginal distribution of the variables from their dependence structure, offering a flexible method for representing complex, correlated posterior distributions. We provide a comprehensive comparison of three different approaches for approximating the posterior: a Gaussian Mixture with a diagonal covariance matrix, a Gaussian Mixture with a full covariance matrix, and a Gaussian Copula. Our analysis, conducted on a high-fidelity synthetic dataset, demonstrates that the Copula VAE offers a promising and tractable solution in high-dimensional spaces. Although the present work remains in the two-dimensional space, the results suggest efficient scalability to higher dimensions. It achieves superior performance with significantly fewer parameters than the Gaussian Mixture alternatives, whose parametrization grows prohibitively with the dimensionality. The results underscore the potential of Copula-based VAEs as a tool for uncertainty-aware damage identification in FOWT mooring systems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast training of accurate physics-informed neural networks without gradient descent</title>
<link>https://arxiv.org/abs/2405.20836</link>
<guid>https://arxiv.org/abs/2405.20836</guid>
<content:encoded><![CDATA[
<div> PINNs, time-dependent Partial Differential Equations, Frozen-PINN, space-time separation, training efficiency 

Summary: 
Frozen-PINN introduces a novel approach to solving time-dependent Partial Differential Equations (PDEs) by leveraging space-time separation and random features instead of gradient descent for training. This method addresses the limitations of traditional Physics-Informed Neural Networks (PINNs) by incorporating temporal causality and achieving superior training efficiency and accuracy on eight challenging PDE benchmarks. The Frozen-PINN outperforms state-of-the-art PINNs by several orders of magnitude, providing a quick, accurate, and causal PDE solver. By challenging the reliance on stochastic gradient descent and specialized hardware, this approach presents a paradigm shift in PINN training and serves as a benchmark for the community. <div>
arXiv:2405.20836v2 Announce Type: cross 
Abstract: Solving time-dependent Partial Differential Equations (PDEs) is one of the most critical problems in computational science. While Physics-Informed Neural Networks (PINNs) offer a promising framework for approximating PDE solutions, their accuracy and training speed are limited by two core barriers: gradient-descent-based iterative optimization over complex loss landscapes and non-causal treatment of time as an extra spatial dimension. We present Frozen-PINN, a novel PINN based on the principle of space-time separation that leverages random features instead of training with gradient descent, and incorporates temporal causality by construction. On eight PDE benchmarks, including challenges such as extreme advection speeds, shocks, and high dimensionality, Frozen-PINNs achieve superior training efficiency and accuracy over state-of-the-art PINNs, often by several orders of magnitude. Our work addresses longstanding training and accuracy bottlenecks of PINNs, delivering quickly trainable, highly accurate, and inherently causal PDE solvers, a combination that prior methods could not realize. Our approach challenges the reliance of PINNs on stochastic gradient-descent-based methods and specialized hardware, leading to a paradigm shift in PINN training and providing a challenging benchmark for the community.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Enhanced, Data-Driven Personalized and Equitable Clinician Scheduling: A Predict-then-Optimize Approach</title>
<link>https://arxiv.org/abs/2510.02047</link>
<guid>https://arxiv.org/abs/2510.02047</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinician scheduling, Large language models, Predict-then-optimize framework, Schedule optimization, Clinician well-being<br />
Summary: <br />
Clinician scheduling in academic anesthesiology departments is a challenging task due to limited resources and varying demands. A new framework is proposed that combines availability predictions from large language models with a schedule optimization model. By leveraging unstructured data such as free-text notes, the framework aims to improve schedule alignment, reduce burnout, and optimize resource utilization. The framework considers four key objectives: ensuring compliance with full-time equivalent regulations, balancing workload distribution, maximizing clinician availability for shifts, and promoting schedule consistency. By integrating the interpretive power of large language models with mathematical optimization, the framework offers a data-driven solution to enhance operational efficiency and support equity and clinician well-being. <br /> <div>
arXiv:2510.02047v1 Announce Type: cross 
Abstract: Clinician scheduling remains a persistent challenge due to limited clinical resources and fluctuating demands. This complexity is especially acute in large academic anesthesiology departments as physicians balance responsibilities across multiple clinical sites with conflicting priorities. Further, scheduling must account for individual clinical and lifestyle preferences to ensure job satisfaction and well-being. Traditional approaches, often based on statistical or rule-based optimization models, rely on structured data and explicit domain knowledge. However, these methods often overlook unstructured information, e.g., free-text notes from routinely administered clinician well-being surveys and scheduling platforms. These notes may reveal implicit and underutilized clinical resources. Neglecting such information can lead to misaligned schedules, increased burnout, overlooked staffing flexibility, and suboptimal utilization of available resources. To address this gap, we propose a predict-then-optimize framework that integrates classification-based clinician availability predictions with a mixed-integer programming schedule optimization model. Large language models (LLMs) are employed to extract actionable preferences and implicit constraints from unstructured schedule notes, enhancing the reliability of availability predictions. These predictions then inform the schedule optimization considering four objectives: first, ensuring clinical full-time equivalent compliance, second, reducing workload imbalances by enforcing equitable proportions of shift types, third, maximizing clinician availability for assigned shifts, and fourth, schedule consistency. By combining the interpretive power of LLMs with the rigor of mathematical optimization, our framework provides a robust, data-driven solution that enhances operational efficiency while supporting equity and clinician well-being.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow of Knowledge: Federated Fine-Tuning of LLMs in Healthcare under Non-IID Conditions</title>
<link>https://arxiv.org/abs/2510.00543</link>
<guid>https://arxiv.org/abs/2510.00543</guid>
<content:encoded><![CDATA[
<div> privacy-preserving, federated learning, healthcare, large language models, LoRA 
Summary: 
Federated fine-tuning approach based on Low-Rank Adaptation (LoRA) is introduced for large language models (LLMs) in healthcare to overcome data privacy restrictions and facilitate cross-institution collaboration. The method combines local LoRA adaptation and global parameter aggregation, enabling knowledge sharing without exposing raw data. A blockchain identity scheme is utilized for LLM identification in distributed networks. Experiments on non-IID medical text datasets demonstrate that federated LoRA enhances cross-client generalization and boosts the performance of the weakest client, ensuring stable convergence and fairer outcomes. This approach offers a practical and effective paradigm for adapting LLMs in healthcare, opening up new possibilities for multi-center medical AI collaboration. 
<br /><br />Summary: <div>
arXiv:2510.00543v1 Announce Type: new 
Abstract: Large language models (LLMs) show great promise in healthcare, but their applications are hindered by data privacy restrictions and the challenges of cross-institution collaboration. Sensitive medical data cannot be centralized, while non-independent and identically distributed (non-IID) characteristics across institutions further complicate convergence and fairness. To address these issues, we present a federated fine-tuning approach based on Low-Rank Adaptation (LoRA), enabling privacy-preserving knowledge flow across institutions. The method iteratively combines local LoRA adaptation with global parameter aggregation, allowing efficient knowledge sharing without exposing raw data. A blockchain identity scheme is used for identifying individual LLM in such a distributed network. We evaluate this approach on heterogeneous and highly non-IID medical text datasets, where experiments demonstrate that federated LoRA not only enhances cross-client generalization but also improves the performance of the weakest client, achieving stable convergence and fairer outcomes. These findings highlight federated LoRA fine-tuning as a practical and effective paradigm for adapting LLMs in healthcare, offering a new path for multi-center medical AI collaboration.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal Classification Recovery Across Domains Using Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2510.00589</link>
<guid>https://arxiv.org/abs/2510.00589</guid>
<content:encoded><![CDATA[
<div> Adversarial learning, statistical distance alignment, stochastic modeling, unsupervised domain adaptation, deep neural networks <br />
Summary:
This paper investigates the use of unsupervised domain adaptation techniques to improve the generalization of signal classification models trained on controlled datasets to real-world scenarios with varying channel environments. The study focuses on aligning representations between simulated and over-the-air signal domains using methods such as adversarial learning, statistical distance alignment, and stochastic modeling. By deliberately generating modulated signals with realistic channel impairments, the researchers evaluate classification performance under different scenarios, including cross-SNR and SNR-matched cross-domain situations. The results demonstrate that unsupervised domain adaptation methods, specifically stochastic classifier (STAR) and joint adaptive networks (JAN), offer significant performance improvements over baseline models. These findings suggest the potential of these techniques for enhancing the deployment of deep neural networks in wireless systems. <br /> <div>
arXiv:2510.00589v1 Announce Type: new 
Abstract: Signal classification models based on deep neural networks are typically trained on datasets collected under controlled conditions, either simulated or over-the-air (OTA), which are constrained to specific channel environments with limited variability, such as fixed signal-to-noise ratio (SNR) levels. As a result, these models often fail to generalize when deployed in real-world scenarios where the feature distribution significantly differs from the training domain. This paper explores unsupervised domain adaptation techniques to bridge the generalization gap between mismatched domains. Specifically, we investigate adaptation methods based on adversarial learning, statistical distance alignment, and stochastic modeling to align representations between simulated and OTA signal domains. To emulate OTA characteristics, we deliberately generate modulated signals subjected to realistic channel impairments without demodulation. We evaluate classification performance under three scenarios, i.e., cross-SNR, SNR-matched cross-domain, and stepwise adaptation involving both SNR and domain shifts. Experimental results show that unsupervised domain adaptation methods, particularly stochastic classifier (STAR) and joint adaptive networks (JAN), enable consistent and substantial performance gains over baseline models, which highlight their promise for real-world deployment in wireless systems.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Economic Impact of DeFi Crime Events on Decentralized Autonomous Organizations (DAOs)</title>
<link>https://arxiv.org/abs/2510.00669</link>
<guid>https://arxiv.org/abs/2510.00669</guid>
<content:encoded><![CDATA[
<div> Decentralized Finance, DeFi, autonomous organizations, governance assets, crime events<br />
<br />
Summary:
Crime events in the Decentralized Finance (DeFi) ecosystem have resulted in over $10 billion in direct losses, triggering broader market reactions. Decentralized Autonomous Organizations (DAOs) govern DeFi applications through tradable governance assets, similar to corporate shares. A study conducted on 22 crime events between 2020 and 2022 examined their economic impact on governance asset prices, trading volumes, and market capitalization using a dynamic difference-in-differences (DiD) framework. Results indicate that 55% of crime events lead to significant negative price impacts, with an average decline of 14%. Furthermore, 68% of crime events result in increased trading volume of governance assets. The indirect economic losses estimated from these impacts exceed $1.3 billion in DAO market capitalization, surpassing direct victim costs and accounting for 74% of total losses. This study provides valuable insights into how crime events influence market dynamics and affect DAOs, offering a reproducible methodological approach applicable to other cryptoassets. <br /><br /> <div>
arXiv:2510.00669v1 Announce Type: new 
Abstract: The Decentralized Finance (DeFi) ecosystem has experienced over \$10 billion in direct losses due to crime events. Beyond these immediate losses, such events often trigger broader market reactions, including price declines, trading activity changes, and reductions in market capitalization. Decentralized Autonomous Organizations (DAOs) govern DeFi applications through tradable governance assets that function like corporate shares for voting and decision-making. Leveraging DeFi's granular trading data, we conduct an event study on 22 crime events between 2020 and 2022 to assess their economic impact on governance asset prices, trading volumes, and market capitalization. Using a dynamic difference-in-differences (DiD) framework with counterfactual governance assets, we aim for causal inference of intraday temporal effects. Our results show that 55% of crime events lead to significant negative price impacts, with an average decline of about 14%. Additionally, 68% of crime events lead to increased governance asset trading volume. Based on these impacts, we estimate indirect economic losses of over $1.3 billion in DAO market capitalization, far exceeding direct victim costs and accounting for 74% of total losses. Our study provides valuable insights into how crime events shape market dynamics and affect DAOs. Moreover, our methodological approach is reproducible and applicable beyond DAOs, offering a framework to assess the indirect economic impact on other cryptoassets.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMMET: orders-of-magnitude speed-up in finite element method via batch-vectorized neural constitutive updates</title>
<link>https://arxiv.org/abs/2510.00884</link>
<guid>https://arxiv.org/abs/2510.00884</guid>
<content:encoded><![CDATA[
<div> Keywords: Constitutive evaluations, Neural constitutive models, Finite element simulations, Computational mechanics, High-fidelity simulations

Summary:<br /><br />Constitutive evaluations in finite element simulations can be costly when using complex material models. Neural constitutive models (NCMs) provide a flexible framework for modeling such behavior in solid mechanics but have limited practical adoption due to high computational costs. The COMMET FE framework is introduced to address this issue, featuring a redesigned architecture that accelerates costly constitutive updates. It includes a novel assembly algorithm supporting batched and vectorized evaluations, optimized derivatives, and distributed-memory parallelism via MPI to reduce runtime significantly. The framework demonstrates speed-ups over traditional implementations, making it possible to efficiently perform large-scale simulations with high fidelity. These advancements primarily target NCMs but can be applied more broadly to improve performance where constitutive updates or assembly processes limit computational efficiency in computational mechanics. <div>
arXiv:2510.00884v1 Announce Type: new 
Abstract: Constitutive evaluations often dominate the computational cost of finite element (FE) simulations whenever material models are complex. Neural constitutive models (NCMs) offer a highly expressive and flexible framework for modeling complex material behavior in solid mechanics. However, their practical adoption in large-scale FE simulations remains limited due to significant computational costs, especially in repeatedly evaluating stress and stiffness. NCMs thus represent an extreme case: their large computational graphs make stress and stiffness evaluations prohibitively expensive, restricting their use to small-scale problems. In this work, we introduce COMMET, an open-source FE framework whose architecture has been redesigned from the ground up to accelerate high-cost constitutive updates. Our framework features a novel assembly algorithm that supports batched and vectorized constitutive evaluations, compute-graph-optimized derivatives that replace automatic differentiation, and distributed-memory parallelism via MPI. These advances dramatically reduce runtime, with speed-ups exceeding three orders of magnitude relative to traditional non-vectorized automatic differentiation-based implementations. While we demonstrate these gains primarily for NCMs, the same principles apply broadly wherever for-loop based assembly or constitutive updates limit performance, establishing a new standard for large-scale, high-fidelity simulations in computational mechanics.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Market States with Clustering and State Machines</title>
<link>https://arxiv.org/abs/2510.00953</link>
<guid>https://arxiv.org/abs/2510.00953</guid>
<content:encoded><![CDATA[
<div> probabilistic state machine, financial markets, market states, transition matrix, asset returns<br />
Summary:
This study presents a novel framework for modeling financial markets using an interpretable probabilistic state machine. By clustering historical returns based on momentum and risk features across different time horizons, distinct market states representing various regimes such as expansion, contraction, crisis, and recovery are identified. A transition matrix is then constructed to capture the dynamics between these states, creating a probabilistic state machine that models the market's temporal evolution. This state machine allows for the generation of a customized distribution of returns by combining Gaussian components weighted by state frequencies. Results show that this approach outperforms traditional methods in capturing key statistical properties of asset returns, including skewness and kurtosis. Robustness is confirmed through experiments across various assets and time periods. <div>
arXiv:2510.00953v1 Announce Type: new 
Abstract: This work introduces a new framework for modeling financial markets through an interpretable probabilistic state machine. By clustering historical returns based on momentum and risk features across multiple time horizons, we identify distinct market states that capture underlying regimes, such as expansion phase, contraction, crisis, or recovery. From a transition matrix representing the dynamics between these states, we construct a probabilistic state machine that models the temporal evolution of the market. This state machine enables the generation of a custom distribution of returns based on a mixture of Gaussian components weighted by state frequencies. We show that the proposed benchmark significantly outperforms the traditional approach in capturing key statistical properties of asset returns, including skewness and kurtosis, and our experiments across random assets and time periods confirm its robustness.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets</title>
<link>https://arxiv.org/abs/2510.00332</link>
<guid>https://arxiv.org/abs/2510.00332</guid>
<content:encoded><![CDATA[
<div> blind spot, adversarial environments, misinformation, financial decisions, model evaluation
<br />
Summary: 
The article introduces CAIA, a benchmark that highlights the inadequacy of state-of-the-art AI models in handling adversarial, high-stakes environments where misinformation is rampant and errors are irreversible. The benchmark evaluates 17 models on tasks related to distinguishing truth from manipulation, navigating fragmented information landscapes, and making financial decisions under adversarial pressure, using crypto markets as a testbed. The findings reveal a significant capability gap, with models achieving only 28% accuracy compared to a human baseline of 80%. Despite access to professional resources, model performance plateaus at 67.4% due to a preference for unreliable sources like web search over authoritative data. The benchmark also uncovers dangerous trial-and-error behavior in model decision-making, emphasizing the importance of adversarial robustness in AI deployment across various domains. <div>
arXiv:2510.00332v1 Announce Type: cross 
Abstract: We present CAIA, a benchmark exposing a critical blind spot in AI evaluation: the inability of state-of-the-art models to operate in adversarial, high-stakes environments where misinformation is weaponized and errors are irreversible. While existing benchmarks measure task completion in controlled settings, real-world deployment demands resilience against active deception. Using crypto markets as a testbed where $30 billion was lost to exploits in 2024, we evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish truth from manipulation, navigate fragmented information landscapes, and make irreversible financial decisions under adversarial pressure.
  Our results reveal a fundamental capability gap: without tools, even frontier models achieve only 28% accuracy on tasks junior analysts routinely handle. Tool augmentation improves performance but plateaus at 67.4% versus 80% human baseline, despite unlimited access to professional resources. Most critically, we uncover a systematic tool selection catastrophe: models preferentially choose unreliable web search over authoritative data, falling for SEO-optimized misinformation and social media manipulation. This behavior persists even when correct answers are directly accessible through specialized tools, suggesting foundational limitations rather than knowledge gaps. We also find that Pass@k metrics mask dangerous trial-and-error behavior for autonomous deployment.
  The implications extend beyond crypto to any domain with active adversaries, e.g. cybersecurity, content moderation, etc. We release CAIA with contamination controls and continuous updates, establishing adversarial robustness as a necessary condition for trustworthy AI autonomy. The benchmark reveals that current models, despite impressive reasoning scores, remain fundamentally unprepared for environments where intelligence must survive active opposition.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous Graphs for Urban Microclimate Prediction</title>
<link>https://arxiv.org/abs/2510.00457</link>
<guid>https://arxiv.org/abs/2510.00457</guid>
<content:encoded><![CDATA[
<div> framework, physics-informed, urban microclimates, spatio-temporal graphs, heterogeneous dynamic graphs
Summary:<br />
- The article introduces UrbanGraph, a framework for predicting urban microclimates that addresses shortcomings of existing approaches by integrating heterogeneous and dynamic spatio-temporal graphs.
- UrbanGraph encodes key physical processes like vegetation evapotranspiration, shading, and convective diffusion, while modeling complex spatial dependencies among urban entities and their temporal evolution.
- Evaluation on the UMC4/12 dataset shows that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% compared to baselines, with heterogeneous and dynamic graphs contributing to the gains.
- The dataset used provides a high-resolution benchmark for spatio-temporal microclimate modeling, allowing for more accurate predictions.
- UrbanGraph's approach can be extended to other urban heterogeneous dynamic computing tasks, highlighting its potential impact beyond microclimate modeling.
<br />Summary: <div>
arXiv:2510.00457v1 Announce Type: cross 
Abstract: With rapid urbanization, predicting urban microclimates has become critical, as it affects building energy demand and public health risks. However, existing generative and homogeneous graph approaches fall short in capturing physical consistency, spatial dependencies, and temporal variability. To address this, we introduce UrbanGraph, a physics-informed framework integrating heterogeneous and dynamic spatio-temporal graphs. It encodes key physical processes -- vegetation evapotranspiration, shading, and convective diffusion -- while modeling complex spatial dependencies among diverse urban entities and their temporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based simulation dataset covering diverse urban configurations and climates. Results show that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% over all baselines, with heterogeneous and dynamic graphs contributing 3.5% and 7.1% gains. Our dataset provides the first high-resolution benchmark for spatio-temporal microclimate modeling, and our method extends to broader urban heterogeneous dynamic computing tasks.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques</title>
<link>https://arxiv.org/abs/2510.00836</link>
<guid>https://arxiv.org/abs/2510.00836</guid>
<content:encoded><![CDATA[
<div> SMOTE, ensemble learning models, pump and dump, cryptocurrency markets, manipulation <br />
<br />
Summary: This study focuses on detecting pump and dump (P&amp;D) manipulation in cryptocurrency markets, addressing the challenge of severe class imbalance. By applying the Synthetic Minority Oversampling Technique (SMOTE) and evaluating advanced ensemble learning models, the researchers successfully identified manipulative trading behavior from normal market activity. The experimental results demonstrated that using SMOTE significantly improved the ability of all models to detect P&amp;D events, enhancing recall and achieving a better balance between precision and recall. Specifically, XGBoost and LightGBM stood out with high recall rates and strong F1-scores, along with fast computational performance suitable for near real-time surveillance. The integration of data balancing techniques with ensemble methods proved to be effective in early detection of manipulative activities, contributing to a more fair, transparent, and stable cryptocurrency market. 
<br /><br /> <div>
arXiv:2510.00836v1 Announce Type: cross 
Abstract: This study aims to detect pump and dump (P&amp;D) manipulation in cryptocurrency markets, where the scarcity of such events causes severe class imbalance and hinders accurate detection. To address this issue, the Synthetic Minority Oversampling Technique (SMOTE) was applied, and advanced ensemble learning models were evaluated to distinguish manipulative trading behavior from normal market activity. The experimental results show that applying SMOTE greatly enhanced the ability of all models to detect P&amp;D events by increasing recall and improving the overall balance between precision and recall. In particular, XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%, respectively) with strong F1-scores and demonstrated fast computational performance, making them suitable for near real time surveillance. These findings indicate that integrating data balancing techniques with ensemble methods significantly improves the early detection of manipulative activities, contributing to a fairer, more transparent, and more stable cryptocurrency market.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Machine Learning Approach in Augmenting RANS Models Using DNS Data and DeepInsight Method on FDA Nozzle</title>
<link>https://arxiv.org/abs/2510.01091</link>
<guid>https://arxiv.org/abs/2510.01091</guid>
<content:encoded><![CDATA[
<div> machine learning, turbulence modeling, OpenFOAM, Reynolds stress tensor, DNS 

Summary:
- The study introduces a data-driven framework for turbulence modeling in flow prediction in the FDA nozzle, using a hybrid implicit-explicit approach.
- New variables are introduced, and a solver is developed within the OpenFOAM framework, incorporating a machine learning module to estimate these variables.
- Invariant input features are derived based on Hilbert's basis theorem, and the machine learning model's outputs are obtained through eigenvalue-vector decomposition of the Reynolds stress tensor.
- Validation is performed using DNS data for turbulent flow in a square channel at various Reynolds numbers.
- A Deep-Insight network trained on benchmark DNS datasets as images demonstrates improved prediction of turbulence structures in the FDA blood nozzle, showcasing the potential of data-driven augmentation in turbulence modeling. 

Summary:<br />
Keywords: machine learning, turbulence modeling, OpenFOAM, Reynolds stress tensor, DNS <br /> <div>
arXiv:2510.01091v1 Announce Type: cross 
Abstract: We present a data-driven framework for turbulence modeling, applied to flow prediction in the FDA nozzle. In this study, the standard RANS equations have been modified using an implicit-explicit hybrid approach. New variables were introduced, and a solver was developed within the OpenFOAM framework, integrating a machine learning module to estimate these variables. The invariant input features were derived based on Hilbert's basis theorem, and the outputs of the machine learning model were obtained through eigenvalue-vector decomposition of the Reynolds stress tensor. Validation was performed using DNS data for turbulent flow in a square channel at various Reynolds numbers. A baseline MLP was first trained at $Re=2900$ and tested at $Re=3500$ to assess its ability to reproduce turbulence anisotropy and secondary flows. To further enhance generalization, three benchmark DNS datasets were transformed into images via the Deep-Insight method, enabling the use of convolutional neural networks. The trained Deep-Insight network demonstrated improved prediction of turbulence structures in the FDA blood nozzle, highlighting the promise of data-driven augmentation in turbulence modeling.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-patch isogeometric neural solver for partial differential equations on computer-aided design domains</title>
<link>https://arxiv.org/abs/2509.25450</link>
<guid>https://arxiv.org/abs/2509.25450</guid>
<content:encoded><![CDATA[
<div> neural networks, isogeometric analysis, partial differential equations, computational framework, finite element solvers
Summary:
This work presents a computational framework that combines physics-informed neural networks with multi-patch isogeometric analysis to solve partial differential equations on complex computer-aided design geometries. The method utilizes patch-local neural networks operating on the reference domain of isogeometric analysis and enforces Dirichlet boundary conditions with a custom output layer. Interface neural networks ensure solution conformity across non-uniform rational B-spline patch interfaces. Training follows a variational framework by minimizing the energy functional. The method's effectiveness is demonstrated on two challenging use-cases, a 2D magnetostatics model and a 3D nonlinear solid and contact mechanics model, showing excellent agreement with high-fidelity finite element solver solutions. This neural solver shows promise in addressing complex engineering problems with corresponding computer-aided design models. <br /><br />Summary: <div>
arXiv:2509.25450v1 Announce Type: new 
Abstract: This work develops a computational framework that combines physics-informed neural networks with multi-patch isogeometric analysis to solve partial differential equations on complex computer-aided design geometries. The method utilizes patch-local neural networks that operate on the reference domain of isogeometric analysis. A custom output layer enables the strong imposition of Dirichlet boundary conditions. Solution conformity across interfaces between non-uniform rational B-spline patches is enforced using dedicated interface neural networks. Training is performed using the variational framework by minimizing the energy functional derived after the weak form of the partial differential equation. The effectiveness of the suggested method is demonstrated on two highly non-trivial and practically relevant use-cases, namely, a 2D magnetostatics model of a quadrupole magnet and a 3D nonlinear solid and contact mechanics model of a mechanical holder. The results show excellent agreement to reference solutions obtained with high-fidelity finite element solvers, thus highlighting the potential of the suggested neural solver to tackle complex engineering problems given the corresponding computer-aided design models.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource Allocation under Stochastic Demands using Shrinking Horizon Optimization</title>
<link>https://arxiv.org/abs/2509.25412</link>
<guid>https://arxiv.org/abs/2509.25412</guid>
<content:encoded><![CDATA[
<div> method, resource allocation, revenue maximization, stochastic demands, shrinking horizon algorithm

Summary:
The article discusses the optimal allocation of limited resources over time to maximize revenue in the face of stochastic demands. It has applications in various control areas like supply chain management, healthcare operations, and power grid energy allocation. The proposed bisection method aims to solve the static optimization problem efficiently. The authors extend this approach to a shrinking horizon algorithm for sequential problems, updating future allocations based on observed demand values. A synthetic example with jointly log-normal demands illustrates the method's performance, demonstrating results close to those obtained by solving the prescient problem. This research provides valuable insights for industries seeking to optimize resource allocation in uncertain environments. <br /><br />Summary: <div>
arXiv:2509.25412v1 Announce Type: cross 
Abstract: We consider the problem of optimally allocating a limited number of resources across time to maximize revenue under stochastic demands. This formulation is relevant in various areas of control, such as supply chain, ticket revenue maximization, healthcare operations, and energy allocation in power grids. We propose a bisection method to solve the static optimization problem and extend our approach to a shrinking horizon algorithm for the sequential problem. The shrinking horizon algorithm computes future allocations after updating the distribution of future demands by conditioning on the observed values of demand. We illustrate the method on a simple synthetic example with jointly log-normal demands, showing that it achieves performance close to a bound obtained by solving the prescient problem.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quasi-Monte Carlo methods for uncertainty quantification of tumor growth modeled by a parametric semi-linear parabolic reaction-diffusion equation</title>
<link>https://arxiv.org/abs/2509.25753</link>
<guid>https://arxiv.org/abs/2509.25753</guid>
<content:encoded><![CDATA[
<div> tumor growth, quasi-Monte Carlo method, partial differential equations, uncertainty, mathematical models

Summary:<br />
This study focuses on applying a quasi-Monte Carlo method to analyze semi-linear parabolic reaction-diffusion equations used in modeling tumor growth. Tumor growth models are complex due to factors like patient variability, disease heterogeneity, and sparse data, leading to uncertainty in model parameters. Efficiently propagating these uncertainties is essential for computing quantities of interest (QoIs) to guide clinical decisions. The research demonstrates that quasi-Monte Carlo methods are effective in computing QoIs, with theoretical error bounds established for uniform random fields, showing a superior linear error rate compared to standard Monte Carlo. Numerical validations support this finding, with promising results for lognormal random fields prompting further investigation. The study provides a valuable contribution to the field of tumor growth modeling and uncertainty quantification. 

<br /><br /> <div>
arXiv:2509.25753v1 Announce Type: cross 
Abstract: We study the application of a quasi-Monte Carlo (QMC) method to a class of semi-linear parabolic reaction-diffusion partial differential equations used to model tumor growth. Mathematical models of tumor growth are largely phenomenological in nature, capturing infiltration of the tumor into surrounding healthy tissue, proliferation of the existing tumor, and patient response to therapies, such as chemotherapy and radiotherapy. Considerable inter-patient variability, inherent heterogeneity of the disease, sparse and noisy data collection, and model inadequacy all contribute to significant uncertainty in the model parameters. It is crucial that these uncertainties can be efficiently propagated through the model to compute quantities of interest (QoIs), which in turn may be used to inform clinical decisions. We show that QMC methods can be successful in computing expectations of meaningful QoIs. Well-posedness results are developed for the model and used to show a theoretical error bound for the case of uniform random fields. The theoretical linear error rate, which is superior to that of standard Monte Carlo, is verified numerically. Encouraging computational results are also provided for lognormal random fields, prompting further theoretical development.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding</title>
<link>https://arxiv.org/abs/2509.25803</link>
<guid>https://arxiv.org/abs/2509.25803</guid>
<content:encoded><![CDATA[
<div> Transformer models, financial transactions, analysis, LLMs, proprietary models<br />
Summary:<br />
The paper explores the use of Transformer-based models in understanding financial transactions. Three types of models were evaluated: Encoder-Only, Decoder-Only, and Encoder-Decoder, with a focus on pretrained LLMs, fine-tuned LLMs, and small proprietary models. While LLMs like LLaMA3-8b, Flan-T5, and SBERT excel in natural language processing tasks, they do not surpass small proprietary models in financial transaction understanding in terms of speed and cost efficiency. Customized proprietary models tailored to transaction data requirements prove to be more suitable for real-time applications in the financial sector. The implementation of a proprietary decoder-only model led to a 14% increase in transaction coverage and over $13 million in annual cost savings. <div>
arXiv:2509.25803v1 Announce Type: cross 
Abstract: Analyzing financial transactions is crucial for ensuring regulatory compliance, detecting fraud, and supporting decisions. The complexity of financial transaction data necessitates advanced techniques to extract meaningful insights and ensure accurate analysis. Since Transformer-based models have shown outstanding performance across multiple domains, this paper seeks to explore their potential in understanding financial transactions. This paper conducts extensive experiments to evaluate three types of Transformer models: Encoder-Only, Decoder-Only, and Encoder-Decoder models. For each type, we explore three options: pretrained LLMs, fine-tuned LLMs, and small proprietary models developed from scratch. Our analysis reveals that while LLMs, such as LLaMA3-8b, Flan-T5, and SBERT, demonstrate impressive capabilities in various natural language processing tasks, they do not significantly outperform small proprietary models in the specific context of financial transaction understanding. This phenomenon is particularly evident in terms of speed and cost efficiency. Proprietary models, tailored to the unique requirements of transaction data, exhibit faster processing times and lower operational costs, making them more suitable for real-time applications in the financial sector. Our findings highlight the importance of model selection based on domain-specific needs and underscore the potential advantages of customized proprietary models over general-purpose LLMs in specialized applications. Ultimately, we chose to implement a proprietary decoder-only model to handle the complex transactions that we previously couldn't manage. This model can help us to improve 14% transaction coverage, and save more than \$13 million annual cost.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning</title>
<link>https://arxiv.org/abs/2509.26116</link>
<guid>https://arxiv.org/abs/2509.26116</guid>
<content:encoded><![CDATA[
<div> Keywords: metagenomic binning, probabilistic embedding, DNA fragments, microbial communities, scalability<br />
<br />
Summary: 
This article introduces UncertainGen, a probabilistic embedding approach for metagenomic binning that represents DNA fragments as probability distributions in latent space. Unlike existing deterministic methods, UncertainGen captures the uncertainty inherent in DNA sequences due to inter-species DNA sharing. The approach offers theoretical guarantees on embedding distinguishability and enables more flexible separation of bins/clusters by introducing a data-adaptive metric in the latent space. Experiments with real metagenomic datasets demonstrate the superiority of UncertainGen over deterministic k-mer and LLM-based embeddings for the binning task. The probabilistic embedding framework not only enhances scalability but also provides a lightweight solution for large-scale metagenomic analysis. <div>
arXiv:2509.26116v1 Announce Type: cross 
Abstract: Metagenomic binning aims to cluster DNA fragments from mixed microbial samples into their respective genomes, a critical step for downstream analyses of microbial communities. Existing methods rely on deterministic representations, such as k-mer profiles or embeddings from large language models, which fail to capture the uncertainty inherent in DNA sequences arising from inter-species DNA sharing and from fragments with highly similar representations. We present the first probabilistic embedding approach, UncertainGen, for metagenomic binning, representing each DNA fragment as a probability distribution in latent space. Our approach naturally models sequence-level uncertainty, and we provide theoretical guarantees on embedding distinguishability. This probabilistic embedding framework expands the feasible latent space by introducing a data-adaptive metric, which in turn enables more flexible separation of bins/clusters. Experiments on real metagenomic datasets demonstrate the improvements over deterministic k-mer and LLM-based embeddings for the binning task by offering a scalable and lightweight solution for large-scale metagenomic analysis.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bubble, Bubble, AI's Rumble: Why Global Financial Regulatory Incident Reporting is Our Shield Against Systemic Stumbles</title>
<link>https://arxiv.org/abs/2509.26150</link>
<guid>https://arxiv.org/abs/2509.26150</guid>
<content:encoded><![CDATA[
<div> systemic risks, AI incident database, algorithmic trading, regulatory-grade global database, financial stability<br />
<br />Summary: 
The article discusses the need for a regulatory-grade global database to address the lack of transparency in AI-driven financial markets. It highlights the challenges posed by opaque AI systems and the potential systemic risks associated with algorithmic trading. The proposed database incorporates frameworks from healthcare and aviation industries to document AI incidents, allowing for cross-jurisdictional analysis of emerging risks. It employs a data omission technique to protect confidential information while enabling thorough analysis. Synthetic data validation reveals patterns such as market manipulation clusters and the influence of AI system typology on trading behavior, transcending geographical boundaries. The solution aims to empower regulators, financial institutions, and investors with enhanced oversight and transparency in AI-driven financial markets, promoting global financial stability. Immediate action is urged to strengthen risk management and resilience against AI-driven systemic risks. <div>
arXiv:2509.26150v1 Announce Type: cross 
Abstract: "Double, double toil and trouble; Fire burn and cauldron bubble." As Shakespeare's witches foretold chaos through cryptic prophecies, modern capital markets grapple with systemic risks concealed by opaque AI systems. According to IMF, the August 5, 2024, plunge in Japanese and U.S. equities can be linked to algorithmic trading yet ab-sent from existing AI incidents database exemplifies this transparency crisis. Current AI incident databases, reliant on crowdsourcing or news scraping, systematically over-look capital market anomalies, particularly in algorithmic and high-frequency trading. We address this critical gap by proposing a regulatory-grade global database that elegantly synthesises post-trade reporting frameworks with proven incident documentation models from healthcare and aviation. Our framework's temporal data omission technique masking timestamps while preserving percent-age-based metrics enables sophisticated cross-jurisdictional analysis of emerging risks while safeguarding confidential business information. Synthetic data validation (modelled after real life published incidents , sentiments, data) reveals compelling pat-terns: systemic risks transcending geographical boundaries, market manipulation clusters distinctly identifiable via K-means algorithms, and AI system typology exerting significantly greater influence on trading behaviour than geographical location, This tripartite solution empowers regulators with unprecedented cross-jurisdictional oversight, financial institutions with seamless compliance integration, and investors with critical visibility into previously obscured AI-driven vulnerabilities. We call for immediate action to strengthen risk management and foster resilience in AI-driven financial markets against the volatile "cauldron" of AI-driven systemic risks., promoting global financial stability through enhanced transparency and coordinated oversight.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing BEV Suitability and Charging Strategies Using Italian Driving Data</title>
<link>https://arxiv.org/abs/2509.26262</link>
<guid>https://arxiv.org/abs/2509.26262</guid>
<content:encoded><![CDATA[
<div> telemetry data, Battery Electric Vehicles, charging scenarios, mobility needs, BEV autonomy
Summary:<br /><br />Battery Electric Vehicles (BEVs) are becoming a popular choice for private transportation, replacing Internal Combustion Engine (ICE) vehicles. However, barriers such as range anxiety and charging station inconvenience persist. A study in Italy analyzed data from 10,441 ICE vehicle users to determine the feasibility of switching to BEVs without changing travel habits. By simulating trips and charging events, the study found that with overnight charging, at least 35% of users could switch to low-capacity BEVs without altering their mobility needs. The analysis highlights the importance of charging behaviors and BEV autonomy in transitioning to electric vehicles. <div>
arXiv:2509.26262v1 Announce Type: cross 
Abstract: Battery Electric Vehicles (BEVs) are rapidly evolving from a niche alternative to an established option for private transportation, often replacing Internal Combustion Engine (ICE) vehicles. Despite growing interest, significant barriers remain, including range anxiety, the inconvenience associated with public charging stations, and higher costs. This study analyses extensive telemetry data collected from 10,441 users using ICE vehicles in an Italian province to assess the potential for switching to BEVs without changing current travel behaviour. We evaluate to what extent the BEV models can fulfil their mobility needs under different charging scenarios. To do so, we replicate trips and parking events, simulating and monitoring the battery state of charge. The analysis reveals the compromises between charging behaviours and limited BEV autonomy. Assuming access to overnight charging, at least 35% of the users could already adopt even low-capacity BEVs.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Importance of localized dilatation and distensibility in identifying determinants of thoracic aortic aneurysm with neural operators</title>
<link>https://arxiv.org/abs/2509.26576</link>
<guid>https://arxiv.org/abs/2509.26576</guid>
<content:encoded><![CDATA[
<div> finite element framework, synthetic TAAs, neural networks, mechanobiological drivers, personalized treatment strategies
Summary:
The study investigates Thoracic aortic aneurysms (TAAs) development by simulating heterogeneous insults using a finite element framework. Neural networks are trained to predict the combined insult leading to TAA formation. Various network architectures are compared, with UNet showing the highest accuracy in predicting insults. Results emphasize the importance of including both dilatation and distensibility data for accurate predictions. Acquiring full-field measurements of these parameters is crucial for assessing the mechanobiological drivers of TAAs and developing personalized treatment strategies.<br /><br />Summary: <div>
arXiv:2509.26576v1 Announce Type: cross 
Abstract: Thoracic aortic aneurysms (TAAs) arise from diverse mechanical and mechanobiological disruptions to the aortic wall that increase the risk of dissection or rupture. Evidence links TAA development to dysfunctions in the aortic mechanotransduction axis, including loss of elastic fiber integrity and cell-matrix connections. Because distinct insults create different mechanical vulnerabilities, there is a critical need to identify interacting factors that drive progression. Here, we use a finite element framework to generate synthetic TAAs from hundreds of heterogeneous insults spanning varying degrees of elastic fiber damage and impaired mechanosensing. From these simulations, we construct spatial maps of localized dilatation and distensibility to train neural networks that predict the initiating combined insult. We compare several architectures (Deep Operator Networks, UNets, and Laplace Neural Operators) and multiple input data formats to define a standard for future subject-specific modeling. We also quantify predictive performance when networks are trained using only geometric data (dilatation) versus both geometric and mechanical data (dilatation plus distensibility). Across all networks, prediction errors are significantly higher when trained on dilatation alone, underscoring the added value of distensibility information. Among the tested models, UNet consistently provides the highest accuracy across all data formats. These findings highlight the importance of acquiring full-field measurements of both dilatation and distensibility in TAA assessment to reveal the mechanobiological drivers of disease and support the development of personalized treatment strategies.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of nested geometry treatments within GPU-based Monte Carlo neutron transport simulations of fission reactors</title>
<link>https://arxiv.org/abs/2406.13849</link>
<guid>https://arxiv.org/abs/2406.13849</guid>
<content:encoded><![CDATA[
<div> GPU-based neutron transport, Monte Carlo simulation, reactor physics, tracking algorithms, supercomputing 
Summary:<br />
- Monte Carlo neutron transport provides detailed estimates of radiological quantities in fission reactors by tracking individual neutrons through computational geometry.
- CPU-based MC codes utilize multiple tracker types with different algorithms, but virtual function calls have high GPU overhead.
- Shift MC code was modified to support GPU-based tracking using dynamic polymorphism, static polymorphism, and single tracker type with tree-based acceleration.
- Results on the Frontier supercomputer show efficient tracking rates using all three methods, suitable for typical reactor problems.
- The single tracker method demonstrates flexibility in handling hexagonal-grid microreactor problems without specific tracking routines, providing significant speedup over CPU execution. <br /><br />Summary: <div>
arXiv:2406.13849v2 Announce Type: replace-cross 
Abstract: Monte Carlo (MC) neutron transport provides detailed estimates of radiological quantities within fission reactors. This involves tracking individual neutrons through a computational geometry. CPU-based MC codes use multiple polymorphic tracker types with different tracking algorithms to exploit the repeated configurations of reactors, but virtual function calls have high overhead on the GPU. The Shift MC code was modified to support GPU-based tracking with three strategies: dynamic polymorphism with virtual functions, static polymorphism, and a single tracker type with tree-based acceleration. On the Frontier supercomputer these methods achieve 77.8%, 91.2%, and 83.4%, respectively, of the tracking rate obtained using a specialized tracker optimized for rectilinear-grid-based reactors. This indicates that all three methods are suitable for typical reactor problems in which tracking does not dominate runtime. The flexibility of the single tracker method is highlighted with a hexagonal-grid microreactor problem, performed without hexagonal-grid-specific tracking routines, providing a 2.19$\times$ speedup over CPU execution.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel, Trend and Periodic-Wise Representation Learning for Multivariate Long-term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.23583</link>
<guid>https://arxiv.org/abs/2509.23583</guid>
<content:encoded><![CDATA[
<div> Keywords: downsampling-based methods, time series forecasting, CTPNet, multi-head attention mechanism, Transformer

Summary:
CTPNet is a novel framework for time series forecasting that addresses limitations in downsampling-based methods by explicitly learning representations from three perspectives. It captures inter-channel dependencies using a temporal query-based multi-head attention mechanism, models intra-subsequence dependencies with a Transformer, and extracts inter-subsequence dependencies with residual connections to capture global patterns. By integrating these levels, CTPNet provides a more holistic representation of temporal dynamics, leading to improved forecasting accuracy. Experimental results demonstrate the superiority of CTPNet compared to existing methods. <div>
arXiv:2509.23583v1 Announce Type: new 
Abstract: Downsampling-based methods for time series forecasting have attracted increasing attention due to their superiority in capturing sequence trends. However, this approaches mainly capture dependencies within subsequences but neglect inter-subsequence and inter-channel interactions, which limits forecasting accuracy. To address these limitations, we propose CTPNet, a novel framework that explicitly learns representations from three perspectives: i) inter-channel dependencies, captured by a temporal query-based multi-head attention mechanism; ii) intra-subsequence dependencies, modeled via a Transformer to characterize trend variations; and iii) inter-subsequence dependencies, extracted by reusing the encoder with residual connections to capture global periodic patterns. By jointly integrating these levels, proposed method provides a more holistic representation of temporal dynamics. Extensive experiments demonstrate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-Code Generation for Modular Building Layouts in Building Information Modeling</title>
<link>https://arxiv.org/abs/2509.23713</link>
<guid>https://arxiv.org/abs/2509.23713</guid>
<content:encoded><![CDATA[
<div> Keywords: Text2MBL, text-to-code generation, Building Information Modeling (BIM), modular building layout (MBL), hierarchical structure

Summary: 
Text2MBL is a framework that generates executable BIM code from textual descriptions of MBL designs, moving beyond conventional 2D layout generation. It produces parametric BIM layouts through on-the-fly code instantiation, addressing the challenges of MBL's hierarchical structure. The framework uses an object-oriented code architecture and large language models to output structured action sequences in code format. A dataset of paired descriptions and ground truth layouts from modular housing projects was used to train and evaluate Text2MBL, ensuring executable validity, semantic fidelity, and geometric consistency. By integrating natural language understanding with BIM code generation, Text2MBL creates a scalable pipeline for modular construction workflows. The implementation of Text2MBL is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2509.23713v1 Announce Type: new 
Abstract: We present Text2MBL, a text-to-code generation framework that generates executable Building Information Modeling (BIM) code directly from textual descriptions of modular building layout (MBL) design. Unlike conventional layout generation approaches that operate in 2D space, Text2MBL produces fully parametric, semantically rich BIM layouts through on-the-fly code instantiation. To address MBLs' unique challenges due to their hierarchical three-tier structure: modules (physical building blocks), units (self-contained dwellings), and rooms (functional spaces), we developed an object-oriented code architecture and fine-tuned large language models to output structured action sequences in code format. To train and evaluate the framework, we curated a dataset of paired descriptions and ground truth layouts drawn from real-world modular housing projects. Performance was assessed using metrics for executable validity, semantic fidelity, and geometric consistency. By tightly unifying natural language understanding with BIM code generation, Text2MBL establishes a scalable pipeline from high-level conceptual design to automation-ready modular construction workflows. Our implementation is available at https://github.com/CI3LAB/Text2MBL.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid DNN Transformer AE Framework for Corporate Tax Risk Supervision and Risk Level Assessment</title>
<link>https://arxiv.org/abs/2509.23862</link>
<guid>https://arxiv.org/abs/2509.23862</guid>
<content:encoded><![CDATA[
<div> deep learning, tax risk supervision, Transformer, Autoencoder, risk level assessment

Summary:
The paper presents a hybrid deep learning framework, DNN-Transformer-Autoencoder, for corporate tax risk supervision. The framework combines a Deep Neural Network (DNN) for static enterprise attributes, a Transformer for long-term dependencies in financial time series, and an Autoencoder for detecting anomalous tax behaviors. By integrating these modules, the framework generates a comprehensive risk score and assigns discrete risk levels. Experimental results on a real-world tax dataset show the framework's effectiveness with an accuracy of 0.91 and a Macro F1-score of 0.88. The hybrid model not only enhances classification performance but also improves interpretability and applicability in tax regulation scenarios. This study contributes to methodological innovation and provides regulatory implications for intelligent tax risk management.<br /><br />Summary: <div>
arXiv:2509.23862v1 Announce Type: new 
Abstract: Tax risk supervision has become a critical component of modern financial governance, as irregular tax behaviors and hidden compliance risks pose significant challenges to regulatory authorities and enterprises alike. Traditional rule-based methods often struggle to capture complex and dynamic tax-related anomalies in large-scale enterprise data. To address this issue, this paper proposes a hybrid deep learning framework (DNN-Transformer-Autoencoder) for corporate tax risk supervision and risk level assessment. The framework integrates three complementary modules: a Deep Neural Network (DNN) for modeling static enterprise attributes, a Transformer-based architecture for capturing long-term dependencies in historical financial time series, and an Autoencoder (AE) for unsupervised detection of anomalous tax behaviors. The outputs of these modules are fused to generate a comprehensive risk score, which is further mapped into discrete risk levels (high, medium, low). Experimental evaluations on a real-world enterprise tax dataset demonstrate the effectiveness of the proposed framework, achieving an accuracy of 0.91 and a Macro F1-score of 0.88. These results indicate that the hybrid model not only improves classification performance but also enhances interpretability and applicability in practical tax regulation scenarios. This study provides both methodological innovation and regulatory implications for intelligent tax risk management.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying the Multimodal Hierarchy of Public Transit Systems Using Trip Chain Data</title>
<link>https://arxiv.org/abs/2509.24220</link>
<guid>https://arxiv.org/abs/2509.24220</guid>
<content:encoded><![CDATA[
<div> hierarchy, urban mobility, multimodal trips, smart card data, Seoul<br />
Summary:<br />
The article introduces the concept of a macroscopic multimodal hierarchy to understand interactions between different modes of urban transportation. Trips follow an ascending-descending order starting and ending with lower hierarchical modes like walking for high accessibility and utilizing higher modes for efficiency. A methodology to identify the multimodal hierarchy using smart card trip data is proposed and demonstrated with data from Seoul, South Korea. This approach helps in understanding how traditional and emerging modes of transportation interact in complex public transit systems, influencing users' multimodal itineraries. <div>
arXiv:2509.24220v1 Announce Type: new 
Abstract: As urban mobility integrates traditional and emerging modes, public transit systems are becoming increasingly complex. Some modes complement each other, while others compete, influencing users' multimodal itineraries. To provide a clear, high-level understanding of these interactions, we introduce the concept of a macroscopic multimodal hierarchy. In this framework, trips follow an "ascending-descending" order, starting and ending with lower hierarchical modes (e.g., walking) that offer high accessibility, while utilizing higher modes (e.g., subways) for greater efficiency. We propose a methodology to identify the multimodal hierarchy of a city using multimodal smart card trip chain data and demonstrate its application with actual data collected from Seoul and the surrounding metropolitan area in South Korea.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparison of Surrogate Constitutive Models for Viscoplastic Creep Simulation of HT-9 Steel</title>
<link>https://arxiv.org/abs/2509.22667</link>
<guid>https://arxiv.org/abs/2509.22667</guid>
<content:encoded><![CDATA[
<div> Keywords: microstructure, constitutive models, surrogate modeling, viscoplastic response, data-driven approach

Summary:
- Mechanistic microstructure-informed constitutive models for polycrystals are essential in computational materials science but can be computationally expensive.
- Data-driven surrogate models offer a solution by learning constitutive relations directly from data.
- Two local surrogate models, including a piecewise response surface method and a mixture of experts model, were developed for the viscoplastic response of steel to balance accuracy and computational efficiency.
- The surrogates adapt to varying material behavior based on parameters or conditions and were tested on HT-9 steel for creep simulations.
- Test metrics show that the mixture of experts model outperforms the piecewise response surface method in accuracy for predicting viscoplastic material behavior.<br /><br />Summary:Mechanistic microstructure models are crucial in computational materials science but can be computationally intensive. Data-driven surrogate models offer a promising solution. Two models were developed for steel's viscoplastic response, adapting to varying behavior and tested on HT-9 for creep simulations. The mixture of experts model was found to outperform the piecewise response surface method in accuracy. <div>
arXiv:2509.22667v1 Announce Type: cross 
Abstract: Mechanistic microstructure-informed constitutive models for the mechanical response of polycrystals are a cornerstone of computational materials science. However, as these models become increasingly more complex - often involving coupled differential equations describing the effect of specific deformation modes - their associated computational costs can become prohibitive, particularly in optimization or uncertainty quantification tasks that require numerous model evaluations. To address this challenge, surrogate constitutive models that balance accuracy and computational efficiency are highly desirable. Data-driven surrogate models, that learn the constitutive relation directly from data, have emerged as a promising solution. In this work, we develop two local surrogate models for the viscoplastic response of a steel: a piecewise response surface method and a mixture of experts model. These surrogates are designed to adapt to complex material behavior, which may vary with material parameters or operating conditions. The surrogate constitutive models are applied to creep simulations of HT-9 steel, an alloy of considerable interest to the nuclear energy sector due to its high tolerance to radiation damage, using training data generated from viscoplastic self-consistent (VPSC) simulations. We define a set of test metrics to numerically assess the accuracy of our surrogate models for predicting viscoplastic material behavior, and show that the mixture of experts model outperforms the piecewise response surface method in terms of accuracy.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents</title>
<link>https://arxiv.org/abs/2509.24063</link>
<guid>https://arxiv.org/abs/2509.24063</guid>
<content:encoded><![CDATA[
<div> agent-based simulation, distributed execution, serialization mechanism, delta encoding, extreme-scale simulations

Summary:
TeraAgent is introduced as a distributed agent-based simulation engine to address the scalability limitations of existing platforms like BioDynaMo. The key challenge of exchanging agent information across servers is tackled through a tailored serialization mechanism and the use of delta encoding to reduce data transfer. These solutions enable TeraAgent to support extreme-scale simulations with half a trillion agents, a significant improvement in scalability. The platform also facilitates faster time-to-result by utilizing additional compute nodes, enhances interoperability with third-party tools, and provides users with more hardware flexibility. TeraAgent marks a significant advancement in the field of agent-based simulation, offering a promising solution for studying complex systems on a massive scale. 

<br /><br />Summary: <div>
arXiv:2509.24063v1 Announce Type: cross 
Abstract: Agent-based simulation is an indispensable paradigm for studying complex systems. These systems can comprise billions of agents, requiring the computing resources of multiple servers to simulate. Unfortunately, the state-of-the-art platform, BioDynaMo, does not scale out across servers due to its shared-memory-based implementation.
  To overcome this key limitation, we introduce TeraAgent, a distributed agent-based simulation engine. A critical challenge in distributed execution is the exchange of agent information across servers, which we identify as a major performance bottleneck. We propose two solutions: 1) a tailored serialization mechanism that allows agents to be accessed and mutated directly from the receive buffer, and 2) leveraging the iterative nature of agent-based simulations to reduce data transfer with delta encoding.
  Built on our solutions, TeraAgent enables extreme-scale simulations with half a trillion agents (an 84x improvement), reduces time-to-result with additional compute nodes, improves interoperability with third-party tools, and provides users with more hardware flexibility.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting the Structure of Press Releases for Predicting Earnings Announcement Returns</title>
<link>https://arxiv.org/abs/2509.24254</link>
<guid>https://arxiv.org/abs/2509.24254</guid>
<content:encoded><![CDATA[
<div> Keywords: press releases, stock returns, earnings announcement, FinBERT, language analysis

Summary: 
Press releases are analyzed to predict stock returns on earnings announcement days. Traditional bag-of-words and BERT-based embeddings are compared, with FinBERT showing the highest predictive power. The study finds that press release content is as informative as earnings surprise in predicting stock returns. Combining models improves explanatory strength and interpretability of press release content. Stock prices reflect press release content at market open, and leaked press releases offer predictive advantage. Topic analysis reveals self-serving bias in managerial narratives. The framework supports real-time return prediction through online learning integration, providing interpretability and highlighting the role of language in price formation. <div>
arXiv:2509.24254v1 Announce Type: cross 
Abstract: We examine how textual features in earnings press releases predict stock returns on earnings announcement days. Using over 138,000 press releases from 2005 to 2023, we compare traditional bag-of-words and BERT-based embeddings. We find that press release content (soft information) is as informative as earnings surprise (hard information), with FinBERT yielding the highest predictive power. Combining models enhances explanatory strength and interpretability of the content of press releases. Stock prices fully reflect the content of press releases at market open. If press releases are leaked, it offers predictive advantage. Topic analysis reveals self-serving bias in managerial narratives. Our framework supports real-time return prediction through the integration of online learning, provides interpretability and reveals the nuanced role of language in price formation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from RNA-Seq Data</title>
<link>https://arxiv.org/abs/2509.24840</link>
<guid>https://arxiv.org/abs/2509.24840</guid>
<content:encoded><![CDATA[
<div> Keywords: single-cell RNA sequencing, Cell2Text, generative framework, gene expression, natural language descriptions

Summary:
Cell2Text is a multimodal generative framework that translates single-cell RNA sequencing profiles into structured natural language descriptions. By combining gene-level embeddings from single-cell foundation models with large language models, Cell2Text generates coherent summaries that capture cellular identity, tissue origin, disease associations, and pathway activity. This innovative approach outperforms baseline methods in classification accuracy, demonstrates strong ontological consistency, and achieves high semantic fidelity in text generation. The integration of expression data with natural language not only improves predictive performance but also provides inherently interpretable outputs. This advancement in technology offers a scalable solution for efficiently characterizing unseen cells with minimal label requirements. <div>
arXiv:2509.24840v1 Announce Type: cross 
Abstract: Single-cell RNA sequencing has transformed biology by enabling the measurement of gene expression at cellular resolution, providing information for cell types, states, and disease contexts. Recently, single-cell foundation models have emerged as powerful tools for learning transferable representations directly from expression profiles, improving performance on classification and clustering tasks. However, these models are limited to discrete prediction heads, which collapse cellular complexity into predefined labels that fail to capture the richer, contextual explanations biologists need. We introduce Cell2Text, a multimodal generative framework that translates scRNA-seq profiles into structured natural language descriptions. By integrating gene-level embeddings from single-cell foundation models with pretrained large language models, Cell2Text generates coherent summaries that capture cellular identity, tissue origin, disease associations, and pathway activity, generalizing to unseen cells. Empirically, Cell2Text outperforms baselines on classification accuracy, demonstrates strong ontological consistency using PageRank-based similarity metrics, and achieves high semantic fidelity in text generation. These results demonstrate that coupling expression data with natural language offers both stronger predictive performance and inherently interpretable outputs, pointing to a scalable path for label-efficient characterization of unseen cells.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction</title>
<link>https://arxiv.org/abs/2509.25075</link>
<guid>https://arxiv.org/abs/2509.25075</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, Cryo-electron microscopy, 3D reconstruction, Neural radiance fields, Training efficiency
<br />
Summary:<br />
The article introduces GEM, a cryo-EM reconstruction framework based on 3D Gaussian Splatting (3DGS), aimed at improving efficiency and accuracy in high-resolution structural biology. GEM operates directly in real-space by representing proteins with compact 3D Gaussians, significantly reducing memory and training costs. By implementing a novel gradient computation method for 3D Gaussians, GEM achieves up to 48% faster training and 12% lower memory usage compared to existing methods. Additionally, GEM improves local resolution by up to 38.8%, demonstrating its practicality and scalability in cryo-EM reconstruction. The framework unifies speed, efficiency, and high-resolution accuracy, making it a valuable tool in the field. The code for GEM is openly available on GitHub for researchers to utilize and further develop. 
<br /> <div>
arXiv:2509.25075v1 Announce Type: cross 
Abstract: Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive. Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead. Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency. Instead of modeling the entire density volume, GEM represents proteins with compact 3D Gaussians, each parameterized by only 11 values. To further improve the training efficiency, we designed a novel gradient computation to 3D Gaussians that contribute to each voxel. This design substantially reduced both memory footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to 48% faster training and 12% lower memory usage compared to state-of-the-art methods, while improving local resolution by as much as 38.8%. These results establish GEM as a practical and scalable paradigm for cryo-EM reconstruction, unifying speed, efficiency, and high-resolution accuracy. Our code is available at https://github.com/UNITES-Lab/GEM.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A bound-preserving multinumerics scheme for steady-state convection-diffusion equations</title>
<link>https://arxiv.org/abs/2509.25181</link>
<guid>https://arxiv.org/abs/2509.25181</guid>
<content:encoded><![CDATA[
<div> adaptive partitioning, finite volume, discontinuous Galerkin, convection-diffusion equation, bound-preserving<br />
Summary:<br />
This study proposes a novel approach to solving the convection-diffusion equation by combining cell-centered finite volume (FV) and discontinuous Galerkin (DG) methods. The domain is divided into FV and DG subdomains, connected through an interface term. An adaptive partitioning strategy is introduced, automatically selecting FV or DG based on solution accuracy. Whenever a cell average violates bounds, FV is applied in the vicinity of the element until all averages are bound-preserving within a specified tolerance. This process, akin to $p$-adaptivity, improves efficiency and accuracy in convection-dominated regimes. Standard benchmarks validate the effectiveness of this adaptive technique, showcasing its potential for accurate and computationally efficient convection-diffusion simulations. <div>
arXiv:2509.25181v1 Announce Type: cross 
Abstract: We solve the convection-diffusion equation using a coupling of cell-centered finite volume (FV) and discontinuous Galerkin (DG) methods. The domain is divided into disjoint regions assigned to FV or DG, and the two methods are coupled through an interface term. DG is stable and resolves sharp layers in convection-dominated regimes, but it can produce sizable spurious oscillations and is computationally expensive; FV (two-point flux) is low-order and monotone, but inexpensive. We propose a novel adaptive partitioning strategy that automatically selects FV and DG subdomains: whenever the solution's cell average violates the bounds, we switch to FV on a small neighborhood of that element. Viewed as a natural analog of $p$-adaptivity, this process is repeated until all cell averages are bound-preserving (up to some specified tolerance). Thereafter, standard conservative limiters may be applied to ensure the full solution is bound-preserving. Standard benchmarks confirm the effectiveness of the adaptive technique.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time</title>
<link>https://arxiv.org/abs/2412.18848</link>
<guid>https://arxiv.org/abs/2412.18848</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryptocurrency markets, pump-and-dump schemes, Telegram groups, real-time prediction pipeline, natural language processing

Summary: 
Cryptocurrency markets are susceptible to manipulation through pump-and-dump schemes orchestrated by large Telegram groups, leading to artificial inflation of coin prices. These groups exploit information asymmetry by selling inside information, posing financial risks to subscribers and all investors. A real-time prediction pipeline incorporating advanced natural language processing technology has been developed to forecast target coins and alert investors to potential pump-and-dump schemes. In a case study on Poloniex, the model successfully identified the target coin in 55.81% of observed pump-and-dump events. By analyzing Telegram messages, the pipeline has identified over 2,000 past pump events and can detect new schemes as they emerge. This innovative approach aims to mitigate the impact of pump-and-dump schemes on cryptocurrency markets and provide investors with timely information to make informed decisions. 

<br /><br />Summary: <div>
arXiv:2412.18848v2 Announce Type: replace 
Abstract: Cryptocurrency markets often face manipulation through prevalent pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some exceeding two million members, artificially inflate target cryptocurrency prices. These groups sell premium access to inside information, worsening information asymmetry and financial risks for subscribers and all investors. This paper presents a real-time prediction pipeline to forecast target coins and alert investors to possible P&amp;D schemes. In a Poloniex case study, the model accurately identified the target coin among the top five from 50 random coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural language processing (NLP) to classify Telegram messages, identifying 2,079 past pump events and detecting new ones in real-time.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of spatial coarsening on Parareal convergence for the linear advection equation</title>
<link>https://arxiv.org/abs/2111.10228</link>
<guid>https://arxiv.org/abs/2111.10228</guid>
<content:encoded><![CDATA[
<div> hyperbolic partial differential equations, Parareal method, parallel-in-time integration, spatial resolution, numerical time stepping

Summary: 
The Parareal method, commonly used for parallel-in-time integration, faces challenges when applied to hyperbolic partial differential equations, especially with reduced spatial resolution. For linear problems, it is difficult to predict convergence using the 2-norm of the Parareal iteration matrix. However, a theorem suggests that the pseudo-spectral radius may reliably indicate whether a Parareal configuration will exhibit transient growth or monotonic convergence. Numerical results support this, showing that the pseudo-spectral radius can also estimate the convergence rate in initial Parareal iterations. This research sheds light on distinguishing configurations where Parareal errors decrease steadily from those where errors initially increase significantly, providing insights for efficient application in hyperbolic problems. <div>
arXiv:2111.10228v3 Announce Type: replace-cross 
Abstract: The Parareal parallel-in-time integration method often performs poorly when applied to hyperbolic partial differential equations. This effect is even more pronounced when the coarse propagator uses a reduced spatial resolution. However, some combinations of spatial discretization and numerical time stepping nevertheless allow for Parareal to converge with monotonically decreasing errors. This raises the question how these configurations can be distinguished theoretically from those where the error initially increases, sometimes over many orders of magnitude. For linear problems, we prove a theorem that implies that the 2-norm of the Parareal iteration matrix is not a suitable tool to predict convergence for hyperbolic problems when spatial coarsening is used. We then show numerical results that suggest that the pseudo-spectral radius can reliably indicate if a given configuration of Parareal will show transient growth or monotonic convergence. For the studied examples, it also provides a good quantitative estimate of the convergence rate in the first few Parareal iterations.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Occasional to Steady: Habit Formation Insights From a Comprehensive Fitness Study</title>
<link>https://arxiv.org/abs/2501.01779</link>
<guid>https://arxiv.org/abs/2501.01779</guid>
<content:encoded><![CDATA[
<div> gym attendance, habit formation, survival metric, clusters, personalized guidance<br />
<br />
Summary: 
This study analyzes gym attendance data to investigate factors influencing the formation of exercise habits. It identifies critical periods for habit formation and segments gym-goers into clusters based on visit patterns. The research highlights the importance of personalized guidance and social dynamics in sustaining long-term engagement. Results show that specific interventions like group classes and personal trainer sessions have varied impacts on different subgroups. Causal inference analysis confirms the significance of tailored, multi-dimensional approaches in promoting exercise habits. By considering individual characteristics, social dynamics, and strategic interventions, the study emphasizes the need for a personalized approach to support consistent exercise routines. <div>
arXiv:2501.01779v2 Announce Type: replace-cross 
Abstract: Regular exercise is widely recognized as a cornerstone of health, yet sustaining consistent exercise habits remains challenging. Understanding the factors that influence the formation of these habits is crucial for developing effective interventions. This study utilizes data from Mars Athletic Club, T\"urkiye's largest sports chain, to investigate the dynamics of gym attendance and habit formation. The general problem addressed by this study is identifying the critical periods and factors that contribute to the successful establishment of consistent exercise routines among gym-goers. We show that specific periods of attendance are most crucial for habit formation. By developing a survival metric based on gym attendance patterns, we pinpoint these key phases and segment members into distinct clusters based on their visit patterns. Our analysis reveals significant differences in how various subgroups respond to interventions, such as group classes, personal trainer sessions, and visiting different clubs. Using causal inference analysis, we demonstrate that personalized guidance and social dynamics are key drivers of sustained long-term engagement. By systematically examining these variables and considering the specific characteristics of different clusters, our research highlights the importance of a tailored, multi-dimensional approach to promoting exercise habits, which integrates social dynamics, personalized guidance, and strategic interventions to sustain long-term engagement.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2502.01678</link>
<guid>https://arxiv.org/abs/2502.01678</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, Alzheimer's disease, deep learning, subject-level detection, pre-training <br />
Summary: <br />
The study focuses on using EEG for Alzheimer's disease detection, highlighting the challenges faced by existing methods. The authors introduce the largest EEG-AD dataset comprising 2,255 subjects and propose LEAD, a novel deep learning model for subject-level AD detection. The model includes a preprocessing pipeline, a subject-regularized spatio-temporal transformer with a unique loss function, and AD-guided contrastive pre-training. LEAD outperforms 10 baseline models in subject-level detection, achieving a Sensitivity of 90.91% on the ADFTD dataset using leave-one-subject-out cross-validation. The results demonstrate the efficacy of this approach for real-world EEG-based Alzheimer's disease detection. The source code for LEAD is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2502.01678v3 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) provides a non-invasive, highly accessible, and cost-effective approach for detecting Alzheimer's disease (AD). However, existing methods, whether based on handcrafted feature engineering or standard deep learning, face two major challenges: 1) the lack of large-scale EEG-AD datasets for robust representation learning, and 2) the absence of a dedicated deep learning pipeline for subject-level detection, which is more clinically meaningful than the commonly used sample-level detection. To address these gaps, we have curated the world's largest EEG-AD corpus to date, comprising 2,255 subjects. Leveraging this unique data corpus, we propose LEAD, the first large-scale foundation model for EEG analysis in dementia. Our approach provides an innovative framework for subject-level AD detection, including: 1) a comprehensive preprocessing pipeline such as artifact removal, resampling, and filtering, and a newly proposed multi-scale segmentation strategy, 2) a subject-regularized spatio-temporal transformer trained with a novel subject-level cross-entropy loss and an indices group-shuffling algorithm, and 3) AD-guided contrastive pre-training. We pre-train on 12 datasets (3 AD-related and 9 non-AD) and fine-tune/test on 4 AD datasets. Compared with 10 baselines, LEAD consistently obtains superior subject-level detection performance under the challenging subject-independent cross-validation protocol. On the benchmark ADFTD dataset, our model achieves an impressive subject-level Sensitivity of 90.91% under the leave-one-subject-out (LOSO) setting. These results strongly validate the effectiveness of our method for real-world EEG-based AD detection. Source code: https://github.com/DL4mHealth/LEAD
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sci2Pol: Evaluating and Fine-tuning LLMs on Scientific-to-Policy Brief Generation</title>
<link>https://arxiv.org/abs/2509.21493</link>
<guid>https://arxiv.org/abs/2509.21493</guid>
<content:encoded><![CDATA[
<div> Keywords: benchmark, training dataset, large language models, policy brief generation, fine-tuning

Summary: 
Sci2Pol-Bench and Sci2Pol-Corpus are introduced as benchmarks and training datasets for evaluating and fine-tuning large language models (LLMs) on policy brief generation from scientific papers. The Sci2Pol-Bench is structured based on a five-stage taxonomy mirroring the human writing process, featuring 18 tasks in various formats. Evaluation of LLMs using this benchmark reveals key limitations in current models. The new LLM-based evaluation metric introduced aligns with expert judgment, better capturing the quality of brief writing compared to existing metrics like BERTScore and ROUGE scores. The Sci2Pol-Corpus, curated for fine-tuning, pairs cited scientific papers with corresponding policy documents, filtered and polished using an LLM-as-a-judge and expert-written samples. Fine-tuning three models on the corpus leads to consistent performance improvements across the benchmark tasks. Notably, Gemma-27B outperforms larger models like GPT-4o and DeepSeek-V3 after fine-tuning, demonstrating the effectiveness of the corpus in bridging science and policy. 

<br /><br />Summary: <div>
arXiv:2509.21493v1 Announce Type: new 
Abstract: We propose Sci2Pol-Bench and Sci2Pol-Corpus, the first benchmark and training dataset for evaluating and fine-tuning large language models (LLMs) on policy brief generation from a scientific paper. We build Sci2Pol-Bench on a five-stage taxonomy to mirror the human writing process: (i) Autocompletion, (ii) Understanding, (iii) Summarization, (iv) Generation, and (v) Verification. It features 18 tasks in multiple-choice and open-ended formats. Specifically, for the Generation stage, we show that BERTScore and ROUGE scores fail to capture the quality of brief writing, and introduce a new LLM-based evaluation metric aligned with expert judgement. Using this benchmark, we evaluate 13 leading open-source and commercial LLMs to uncover key limitations. To improve LLM performance on brief writing, we curate the Sci2Pol-Corpus for fine-tuning. We start by linking each cited scientific paper to its corresponding policy document, drawn from 5.6 million policy records. This produces 140,000 candidate pairs. We then employ an LLM-as-a-judge to filter high-quality examples, followed by in-context polishing using three expert-written samples as references. This process yields a final set of 639 new pairs. Finally, we fine-tune three models on Sci2Pol-Corpus: LLaMA-3.1-8B, Gemma-12B, and Gemma-27B. Fine-tuning leads to consistent performance improvements across Sci2Pol-Bench. Notably, after fine-tuning, Gemma-27B surpasses the much larger GPT-4o and DeepSeek-V3 (671B). These demonstrate the effectiveness of our corpus in bridging the gap between science and policy.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantMind: A Context-Engineering Based Knowledge Framework for Quantitative Finance</title>
<link>https://arxiv.org/abs/2509.21507</link>
<guid>https://arxiv.org/abs/2509.21507</guid>
<content:encoded><![CDATA[
<div> Keywords: QuantMind, knowledge extraction, structured knowledge, quantitative finance, semantic search 

Summary: 
Quantitative research in finance increasingly relies on unstructured financial content like filings and research notes. Existing pipelines struggle with accuracy, evidence attribution, and integration into research workflows. To address these challenges, QuantMind is introduced as an intelligent knowledge extraction and retrieval framework tailored to quantitative finance. It consists of a two-stage architecture: knowledge extraction transforms diverse documents into structured knowledge through parsing text, tables, and formulas, while intelligent retrieval integrates semantic search with multi-hop reasoning for auditable outputs. A user study demonstrates that QuantMind enhances factual accuracy and user experience compared to unaided reading and generic AI assistance, showcasing the importance of domain-specific context engineering in finance.

Summary:  <div>
arXiv:2509.21507v1 Announce Type: new 
Abstract: Quantitative research increasingly relies on unstructured financial content such as filings, earnings calls, and research notes, yet existing LLM and RAG pipelines struggle with point-in-time correctness, evidence attribution, and integration into research workflows. To tackle this, We present QuantMind, an intelligent knowledge extraction and retrieval framework tailored to quantitative finance. QuantMind adopts a two-stage architecture: (i) a knowledge extraction stage that transforms heterogeneous documents into structured knowledge through multi-modal parsing of text, tables, and formulas, adaptive summarization for scalability, and domain-specific tagging for fine-grained indexing; and (ii) an intelligent retrieval stage that integrates semantic search with flexible strategies, multi-hop reasoning across sources, and knowledge-aware generation for auditable outputs. A controlled user study demonstrates that QuantMind improves both factual accuracy and user experience compared to unaided reading and generic AI assistance, underscoring the value of structured, domain-specific context engineering for finance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI for Sustainable Future Foods</title>
<link>https://arxiv.org/abs/2509.21556</link>
<guid>https://arxiv.org/abs/2509.21556</guid>
<content:encoded><![CDATA[
<div> AI for Food, transformative, molecular composition, functional performance, sustainability, innovation<br />
<br />
Summary: AI for Food is an emerging discipline that aims to revolutionize global food systems by leveraging artificial intelligence to enhance ingredient design, formulation development, sensory analysis, and more. While early successes show promise in predicting protein performance and mapping molecules to flavor, challenges such as lack of standardization, limited data, and cultural diversity still exist. To unlock the potential of AI in food innovation, three priorities are proposed: treating food as a programmable biomaterial, creating self-driving laboratories for automated discovery, and developing deep reasoning models that integrate sustainability and human health. By responsibly incorporating AI into the food innovation cycle, the transition to sustainable protein systems can be accelerated, leading to a predictive, design-driven science of food that benefits both human health and the planet.<br /><br /> <div>
arXiv:2509.21556v1 Announce Type: new 
Abstract: Global food systems must deliver nutritious and sustainable foods while sharply reducing environmental impact. Yet, food innovation remains slow, empirical, and fragmented. Artificial intelligence (AI) now offers a transformative path with the potential to link molecular composition to functional performance, bridge chemical structure to sensory outcomes, and accelerate cross-disciplinary innovation across the entire production pipeline. Here we outline AI for Food as an emerging discipline that integrates ingredient design, formulation development, fermentation and production, texture analysis, sensory properties, manufacturing, and recipe generation. Early successes demonstrate how AI can predict protein performance, map molecules to flavor, and tailor consumer experiences. But significant challenges remain: lack of standardization, scarce multimodal data, cultural and nutritional diversity, and low consumer confidence. We propose three priorities to unlock the field: treating food as a programmable biomaterial, building self-driving laboratories for automated discovery, and developing deep reasoning models that integrate sustainability and human health. By embedding AI responsibly into the food innovation cycle, we can accelerate the transition to sustainable protein systems and chart a predictive, design-driven science of food for our own health and the health of our planet.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Method of Moments and Generalized Scattering Matrix: Applications to Antennas in Radomes, Reflectors, and Implantable Media</title>
<link>https://arxiv.org/abs/2509.22000</link>
<guid>https://arxiv.org/abs/2509.22000</guid>
<content:encoded><![CDATA[
<div> Keywords: Antennas, Multiscale modeling, Method of moments, Generalized scattering matrix, Hybrid framework <br />
Summary: 
Antennas embedded in or interacting with large structures pose challenges due to their different scales. A hybrid method combining Method of Moments (MoM) and Generalized Scattering Matrix (GSM) has been developed to address these challenges. This framework allows for the separate treatment of fine-scale antenna details and large-scale environment characteristics while maintaining their full coupling. Antennas of any shape can be characterized and reused across various environments, or a single environment can accommodate multiple antenna designs. The framework is versatile and can be extended to include GSM-PO and GSM + T-matrix methods, creating a unified approach for multiscale antenna modeling. By representing the large structure in the formulation best suited to its scale and shape, the approach offers accuracy, efficiency, and adaptability. Numerical validations on different antenna systems show excellent agreement with full-wave solvers and significant computational cost reductions for design and optimization. <br /><br /> <div>
arXiv:2509.22000v1 Announce Type: new 
Abstract: Electromagnetic analysis of antennas embedded in or interacting with large surrounding structures poses inherent multiscale challenges: the antenna is electrically small yet geometrically detailed, while the environment is electrically large but comparatively smooth. To address this, we present a hybrid method of moments (MoM) and generalized scattering matrix (GSM) framework that achieves a clean separation between fine-scale and large-scale complexities while preserving their full mutual coupling. Antennas of arbitrary geometry can be characterized once and reused across different environments, or conversely, a given environment can be modeled once to accommodate multiple antenna designs. The framework is inherently versatile, encompassing GSM-PO and GSM + T-matrix extensions, and thus provides a unified paradigm for multiscale antenna modeling. With the large body always represented by the formulation best suited to its scale and shape, the approach combines accuracy, efficiency, and adaptability. Numerical validations on implantable antennas, radome-protected arrays, and reflector systems confirm excellent agreement with full-wave solvers while demonstrating dramatic reductions in computational cost for design and optimization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orochi: Versatile Biomedical Image Processor</title>
<link>https://arxiv.org/abs/2509.22583</link>
<guid>https://arxiv.org/abs/2509.22583</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, biomedical images, Orochi, computational efficiency, fine-tuning framework

Summary: 
Orochi is introduced as an application-oriented image processor focusing on biomedical image processing. The tool is pre-trained on a wide range of publicly available studies using a Random Multi-scale Sampling strategy. Additionally, the Task-related Joint-embedding Pre-Training (TJP) method is proposed for self-supervision, offering an alternative to traditional Masked Image Modelling (MIM) and enhancing performance in tasks like registration. The computational efficiency of Orochi is improved through the use of Mamba's linear complexity and Multi-head Hierarchy Mamba. The tool offers a three-tier fine-tuning framework (Full, Normal, Light) for flexibility and parameter efficiency. Orochi demonstrates comparable or superior performance to existing specialist models, providing biologists with a versatile and efficient tool to streamline their workflow by eliminating the need to choose among multiple models.

<br /><br />Summary: <div>
arXiv:2509.22583v1 Announce Type: new 
Abstract: Deep learning has emerged as a pivotal tool for accelerating research in the life sciences, with the low-level processing of biomedical images (e.g., registration, fusion, restoration, super-resolution) being one of its most critical applications. Platforms such as ImageJ (Fiji) and napari have enabled the development of customized plugins for various models. However, these plugins are typically based on models that are limited to specific tasks and datasets, making them less practical for biologists. To address this challenge, we introduce Orochi, the first application-oriented, efficient, and versatile image processor designed to overcome these limitations. Orochi is pre-trained on patches/volumes extracted from the raw data of over 100 publicly available studies using our Random Multi-scale Sampling strategy. We further propose Task-related Joint-embedding Pre-Training (TJP), which employs biomedical task-related degradation for self-supervision rather than relying on Masked Image Modelling (MIM), which performs poorly in downstream tasks such as registration. To ensure computational efficiency, we leverage Mamba's linear computational complexity and construct Multi-head Hierarchy Mamba. Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and Light) and demonstrate that Orochi achieves comparable or superior performance to current state-of-the-art specialist models, even with lightweight parameter-efficient options. We hope that our study contributes to the development of an all-in-one workflow, thereby relieving biologists from the overwhelming task of selecting among numerous models.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Risk Sharing on Networks</title>
<link>https://arxiv.org/abs/2509.21411</link>
<guid>https://arxiv.org/abs/2509.21411</guid>
<content:encoded><![CDATA[
<div> Linear risk sharing, network structures, stochastic matrices, risk redistribution, peer-to-peer insurance<br />
<br />Summary: The article presents a framework for linear risk sharing (LRS) in alternative insurance and banking systems that rely on network structures. It explores how random losses can be reallocated through nonnegative linear operators in various network topologies, ensuring constraints such as budget balance, fairness, and diversification. The analysis uses stochastic and doubly stochastic matrices to compare different risk allocations rigorously, emphasizing variance reduction and majorization through doubly stochastic mixing. The study extends to network-based risk sharing in different graph types, showing how network topology influences risk outcomes. Introducing randomness in the sharing matrix via Erd\H{o}s--R\'enyi and preferential-attachment networks links risk-sharing properties to degree distributions. The research also examines the trade-off between self-retention and diversification in peer-to-peer insurance and network-based risk pooling, offering design principles for fair and efficient risk redistribution. <div>
arXiv:2509.21411v1 Announce Type: cross 
Abstract: Over the past decade alternatives to traditional insurance and banking have grown in popularity. The desire to encourage local participation has lead products such as peer-to-peer insurance, reciprocal contracts, and decentralized finance platforms to increasingly rely on network structures to redistribute risk among participants. In this paper, we develop a comprehensive framework for linear risk sharing (LRS), where random losses are reallocated through nonnegative linear operators which can accommodate a wide range of networks. Building on the theory of stochastic and doubly stochastic matrices, we establish conditions under which constraints such as budget balance, fairness, and diversification are guaranteed. The convex order framework allows us to compare different allocations rigorously, highlighting variance reduction and majorization as natural consequences of doubly stochastic mixing. We then extend the analysis to network-based sharing, showing how their topology shapes risk outcomes in complete, star, ring, random, and scale-free graphs. A second layer of randomness, where the sharing matrix itself is random, is introduced via Erd\H{o}s--R\'enyi and preferential-attachment networks, connecting risk-sharing properties to degree distributions. Finally, we study convex combinations of identity and network-induced operators, capturing the trade-off between self-retention and diversification. Our results provide design principles for fair and efficient peer-to-peer insurance and network-based risk pooling, combining mathematical soundness with economic interpretability.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic</title>
<link>https://arxiv.org/abs/2509.21475</link>
<guid>https://arxiv.org/abs/2509.21475</guid>
<content:encoded><![CDATA[
<div> latency, decentralization, Ethereum, protocol design, geographical distribution
Summary:
The article explores how protocol design influences the geographical distribution of validators in decentralized systems like Ethereum. It compares two block-building paradigms, SSP and MSP, and finds that SSP concentrates around relay placement, while MSP centralizes faster due to location-dependent payoff dispersion. The article highlights the importance of source placement and consensus settings in shaping validator geography but notes that once validators are already clustered, source placement's impact on decentralization diminishes. North America consistently emerges as a focal hub in most scenarios, indicating a trend towards centralization in the region. The study suggests that protocol design can play a significant role in promoting geographical decentralization and offers insights into potential levers for achieving this goal. <div>
arXiv:2509.21475v1 Announce Type: cross 
Abstract: Decentralization has a geographic dimension that conventional metrics such as stake distribution overlook. Where validators run affects resilience to regional shocks (outages, disasters, government intervention) and fairness in reward access. Yet in permissionless systems, locations cannot be mandated, but they emerge from incentives. Today, Ethereum's validators cluster along the Atlantic (EU and U.S. East Coast), where latency is structurally favorable. This raises a key question: when some regions already enjoy latency advantages, how does protocol design shape validator incentives and the geography of (de)centralization? We develop a latency-calibrated agent-based model and compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP), akin to MEV-Boost, where proposers fetch full blocks from a relay that also propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate value from multiple sources and broadcast the block themselves. Simulations show that SSP concentrates around relay placement but more slowly, since proximity mainly affects propagation, and the marginal value of time is relatively uniform across regions. MSP centralizes faster: aggregating across sources makes marginal value location-dependent, amplifying payoff dispersion and migration toward latency minima. Source placement and consensus settings can dampen or intensify these effects, though once validators are already clustered, the impact of source placement on decentralization is marginal. In most cases, North America consistently emerges as the focal hub. These findings show that protocol design materially shapes validator geography and offer levers for promoting geographical decentralization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need</title>
<link>https://arxiv.org/abs/2509.21553</link>
<guid>https://arxiv.org/abs/2509.21553</guid>
<content:encoded><![CDATA[
<div> knowledge graph, AI agents, climate data science, data accessibility, scientific workflows

Summary:
This paper addresses the challenges faced in climate data science by integrating a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG acts as a unifying layer organizing datasets, tools, and workflows, while AI agents enhance natural language interaction, data access automation, and streamlined analysis using generative AI services. By leveraging cloud-ready API data portals, the system demonstrates that a knowledge graph can significantly reduce the technical barriers for engaging in climate data science, enabling non-specialists to identify and analyze relevant datasets. The open-source design allows for community contributions, supporting the evolution of the KG and associated tools as a shared commons. This approach aims to democratize access to climate data, establish a reproducible framework for scientific inquiry, and facilitate human-AI collaboration in research. 

<br /><br />Summary: <div>
arXiv:2509.21553v1 Announce Type: cross 
Abstract: Climate data science faces persistent barriers stemming from the fragmented nature of data sources, heterogeneous formats, and the steep technical expertise required to identify, acquire, and process datasets. These challenges limit participation, slow discovery, and reduce the reproducibility of scientific workflows. In this paper, we present a proof of concept for addressing these barriers through the integration of a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG provides a unifying layer that organizes datasets, tools, and workflows, while AI agents -- powered by generative AI services -- enable natural language interaction, automated data access, and streamlined analysis. Together, these components drastically lower the technical threshold for engaging in climate data science, enabling non-specialist users to identify and analyze relevant datasets. By leveraging existing cloud-ready API data portals, we demonstrate that "a knowledge graph is all you need" to unlock scalable and agentic workflows for scientific inquiry. The open-source design of our system further supports community contributions, ensuring that the KG and associated tools can evolve as a shared commons. Our results illustrate a pathway toward democratizing access to climate data and establishing a reproducible, extensible framework for human--AI collaboration in scientific research.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bacterial Gene Regulatory Neural Network as a Biocomputing Library of Mathematical Solvers</title>
<link>https://arxiv.org/abs/2509.21598</link>
<guid>https://arxiv.org/abs/2509.21598</guid>
<content:encoded><![CDATA[
<div> Keywords: biocomputing, gene expression dynamics, GRNN framework, mathematical solvers, reliability

Summary:
The article presents a novel approach to biocomputing by utilizing bacterial gene expression dynamics within a GRNN framework to create a library of mathematical solvers. A sub-GRNN search algorithm is introduced to customize functional subnetworks for specific mathematical tasks by analyzing gene expression patterns under different input conditions. Tasks such as identifying Fibonacci numbers, prime numbers, multiplication, and Collatz step counts are successfully achieved using this approach. The study assesses the identified sub-GRNNs for robustness and reliability through gene-wise and collective perturbation and Lyapunov-based stability analysis. The results highlight the potential of leveraging native transcriptional machinery for performing diverse mathematical calculations and classifications while ensuring computing stability and reliability. <div>
arXiv:2509.21598v1 Announce Type: cross 
Abstract: Current biocomputing approaches predominantly rely on engineered circuits with fixed logic, offering limited stability and reliability under diverse environmental conditions. Here, we use the GRNN framework introduced in our previous work to transform bacterial gene expression dynamics into a biocomputing library of mathematical solvers. We introduce a sub-GRNN search algorithm that identifies functional subnetworks tailored to specific mathematical calculation and classification tasks by evaluating gene expression patterns across chemically encoded input conditions. Tasks include identifying Fibonacci numbers, prime numbers, multiplication, and Collatz step counts. The identified problem-specific sub-GRNNs are then assessed using gene-wise and collective perturbation, as well as Lyapunov-based stability analysis, to evaluate robustness and reliability. Our results demonstrate that native transcriptional machinery can be harnessed to perform diverse mathematical calculation and classification tasks, while maintaining computing stability and reliability.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Assisted Sustainable Remanufacturing, Reusing and Recycling for Lithium-ion Batteries</title>
<link>https://arxiv.org/abs/2406.00276</link>
<guid>https://arxiv.org/abs/2406.00276</guid>
<content:encoded><![CDATA[
<div> Machine learning, lithium-ion batteries, sustainability, circular economy, carbon neutrality <br />
Summary: <br />
The dissertation proposes a machine learning framework to tackle data scarcity and heterogeneity in the lifecycle of lithium-ion batteries (LIBs). A physics-informed quality control model predicts long-term degradation, while a generative learning-based method evaluates retired batteries accurately. Federated learning ensures privacy-preserving and high-precision cathode material sorting for efficient recycling. A unified diagnostics and prognostics framework based on correlation alignment enhances adaptability across multiple tasks, including state of health estimation and remaining useful life prediction. These contributions integrate physics, data generation, privacy preservation, and adaptive learning to advance sustainable battery management, promoting circular economy and global carbon neutrality. <div>
arXiv:2406.00276v2 Announce Type: replace-cross 
Abstract: The sustainable utilization of lithium-ion batteries (LIBs) is crucial to the global energy transition and carbon neutrality, yet data scarcity and heterogeneity remain major barriers across remanufacturing, reusing, and recycling. This dissertation develops a machine learning assisted framework to address these challenges throughout the battery lifecycle. A physics informed quality control model predicts long-term degradation from limited early-cycle data, while a generative learning based residual value assessment method enables rapid and accurate evaluation of retired batteries under random conditions. A federated learning strategy achieves privacy preserving and high precision cathode material sorting, supporting efficient recycling. Furthermore, a unified diagnostics and prognostics framework based on correlation alignment enhances adaptability across tasks such as state of health estimation, state of charge estimation, and remaining useful life prediction under varied testing protocols. Collectively, these contributions advance sustainable battery management by integrating physics, data generation, privacy preserving collaboration, and adaptive learning, offering methodological innovations to promote circular economy and global carbon neutrality.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Domain-adaptive Post-training for Financial LLMs</title>
<link>https://arxiv.org/abs/2501.04961</link>
<guid>https://arxiv.org/abs/2501.04961</guid>
<content:encoded><![CDATA[
<div> Keywords: domain-adaptive, large language models, finance, post-training, evaluation

Summary:
The study introduces FINDAP, a systematic approach for domain-adaptive post-training of large language models (LLMs) in the finance domain. It includes FinCap, defining core domain capabilities, FinRec, an optimized training recipe incorporating data distillation, FinTrain, a set of curated training datasets, and FinEval, a comprehensive evaluation suite. The resulting Llama-Fin model achieves top performance in financial tasks, showcasing the effectiveness of each training stage in enhancing specific capabilities. The study provides insights into challenges and solutions for domain adaptation of LLMs, highlighting the importance of tailored training strategies and evaluation criteria in specialized domains like finance.<br /><br />Summary: <div>
arXiv:2501.04961v3 Announce Type: replace-cross 
Abstract: Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach consists of four key components: FinCap, which defines the core capabilities required for the target domain; FinRec, an effective training recipe that jointly optimizes continual pre-training and instruction-following, along with a novel preference data distillation method leveraging process signals from a generative reward model; FinTrain, a curated set of training datasets supporting FinRec; and FinEval, a comprehensive evaluation suite aligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Adaptive Diffusion Model for Flexible Protein-Protein Docking</title>
<link>https://arxiv.org/abs/2509.20542</link>
<guid>https://arxiv.org/abs/2509.20542</guid>
<content:encoded><![CDATA[
<div> Keywords: protein-protein interactions, structural prediction, hierarchical adaptive diffusion, conformational changes, DIPS-AF dataset <br />
Summary: 
The study introduces a hierarchical adaptive diffusion framework for predicting protein-protein interactions, particularly focusing on cases with significant conformational changes. The framework separates global rigid-body motions and local flexibility, incorporating noise schedules to mimic induced-fit effects. Adaptive scheduling based on predicted conformational changes allows for faster flexing in response to variations. By leveraging a dataset of 39,000 examples for pre-training, the model outperforms existing methods on a benchmark dataset in both rigid and flexible scenarios. Ablation studies highlight the significance of adaptive schedules, dynamics features, and pre-training. Despite improvements, gaps in sampling, scoring, and conformational resolution remain to be addressed. Case studies shed light on the practical implications of the proposed framework. This innovative approach holds promise for enhancing accuracy and efficiency in predicting complex protein-protein interactions. <br /> <div>
arXiv:2509.20542v1 Announce Type: new 
Abstract: Structural prediction of protein-protein interactions is important to understand the molecular basis of cellular interactions, but it still faces major challenges when significant conformational changes are present. We propose a generative framework of hierarchical adaptive diffusion to improve accuracy and efficiency in such cases. It is hierarchical in separating global inter-protein rigid-body motions and local intra-protein flexibility in diffusion processes, and the distinct local and global noise schedules are designed to mimic the induced-fit effect. It is adaptive in conditioning the local flexibility schedule on predicted levels of conformational change, allowing faster flexing for larger anticipated conformational changes. Furthermore, it couples the local and global diffusion processes through a common score and confidence network with sequence, evolution, structure, and dynamics features as inputs, and maintains rotational or translational invariance or equivariance in outputs. It builds on our newly curated DIPS-AF dataset of nearly 39,000 examples for pre-training. Experiments on the independent docking benchmark dataset DB5.5 show that our model outperforms an AlphaFold2-like iterative transformer (GeoDock) and a diffusion model (DiffDock-PP) in both rigid and flexible cases, with larger improvements in more flexible cases. Ablation studies prove the importance of adaptive schedules, dynamics features, and pre-training. Additional analyses and case studies reveal remaining gaps in sampling, scoring, and conformational resolution.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Difference-Guided Reasoning: A Temporal-Spatial Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2509.20713</link>
<guid>https://arxiv.org/abs/2509.20713</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, reasoning, differences, abnormal behavior detection, external information integration

Summary:
Large Language Models (LLMs) are powerful tools but often lack the ability to actively discover new questions, hindering human-like reasoning. To address this limitation, a difference-guided reasoning framework is proposed. This framework enables LLMs to identify and act upon changes over time and space by formalizing differences through feature extraction and prioritizing impactful changes. Additionally, mechanisms for abnormal behavior detection and the integration of external information are included to enhance the reliability of reasoning. Verification results demonstrate that prompting LLMs with differences enhances focus on critical issues, improving alignment with desired reasoning outcomes compared to direct prompting. The framework provides a structured approach for LLMs to reason more effectively and simulate human-like thinking. 

<br /><br />Summary: 
- Proposal of a difference-guided reasoning framework for Large Language Models (LLMs)
- LLMs can identify and act upon changes through feature extraction and prioritization
- Inclusion of mechanisms for abnormal behavior detection and external information integration
- Verification results show improved focus on critical issues and alignment with desired reasoning outcomes
- Framework enhances LLMs' ability to reason effectively and simulate human-like thinking. <div>
arXiv:2509.20713v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are important tools for reasoning and problem-solving, while they often operate passively, answering questions without actively discovering new ones. This limitation reduces their ability to simulate human-like thinking, where noticing differences is a key trigger for reasoning. Thus, in this paper we propose a difference-guided reasoning framework, which enables LLMs to identify and act upon changes across time and space. The model formalizes differences through feature extraction, prioritizes the most impactful and latest changes, and links them to appropriate actions. We further extend the framework with mechanisms for abnormal behavior detection and the integration of external information from users or sensors, ensuring more reliable and grounded reasoning. Verification results show that prompting LLMs with differences improves focus on critical issues, leading to higher alignment with desired reasoning outcomes compared to direct prompting.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extrapolating Phase-Field Simulations in Space and Time with Purely Convolutional Architectures</title>
<link>https://arxiv.org/abs/2509.20770</link>
<guid>https://arxiv.org/abs/2509.20770</guid>
<content:encoded><![CDATA[
<div> surrogate, convolutional self-attention, physics-aware padding, variable time-step skipping, alloy systems
Summary:
The article introduces a new approach for accelerating phase-field models of liquid metal dealloying (LMD) using a conditionally parameterized, fully convolutional U-Net surrogate. This surrogate model incorporates convolutional self-attention and physics-aware padding, allowing it to generalize beyond its training window in both space and time. By leveraging parameter conditioning, the surrogate can adapt to different alloy systems and skip variable time steps. Despite being trained on short simulations, the surrogate achieves high accuracy in reproducing LMD physics, with relative errors typically under 5%. It can accelerate computations by up to 16,000 times, significantly reducing simulation times from weeks to seconds. This method represents a promising advancement towards scalable and high-fidelity extrapolation of LMD phase-field models. <br /><br />Summary: <div>
arXiv:2509.20770v1 Announce Type: new 
Abstract: Phase-field models of liquid metal dealloying (LMD) can resolve rich microstructural dynamics but become intractable for large domains or long time horizons. We present a conditionally parameterized, fully convolutional U-Net surrogate that generalizes far beyond its training window in both space and time. The design integrates convolutional self-attention and physics-aware padding, while parameter conditioning enables variable time-step skipping and adaptation to diverse alloy systems. Although trained only on short, small-scale simulations, the surrogate exploits the translational invariance of convolutions to extend predictions to much longer horizons than traditional solvers. It accurately reproduces key LMD physics, with relative errors typically under 5% within the training regime and below 10% when extrapolating to larger domains and later times. The method accelerates computations by up to 16,000 times, cutting weeks of simulation down to seconds, and marks an early step toward scalable, high-fidelity extrapolation of LMD phase-field models.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh Interpolation Graph Network for Dynamic and Spatially Irregular Global Weather Forecasting</title>
<link>https://arxiv.org/abs/2509.20911</link>
<guid>https://arxiv.org/abs/2509.20911</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, weather forecasting, global, irregularly distributed, generalization<br />
<br />
Summary: 
The study focuses on global weather forecasting, accounting for the irregular distribution of weather stations and the dynamic nature of weather patterns. The proposed Mesh Interpolation Graph Network (MIGN) addresses the challenges by utilizing a regular mesh interpolation network to handle spatially irregular data and incorporating parametric spherical harmonics location embedding for enhanced spatial generalization. Experimental results demonstrate that MIGN outperforms existing data-driven models, showcasing its spatial generalization ability and capability to predict weather at unseen locations. <div>
arXiv:2509.20911v1 Announce Type: new 
Abstract: Graph neural networks have shown promising results in weather forecasting, which is critical for human activity such as agriculture planning and extreme weather preparation. However, most studies focus on finite and local areas for training, overlooking the influence of broader areas and limiting their ability to generalize effectively. Thus, in this work, we study global weather forecasting that is irregularly distributed and dynamically varying in practice, requiring the model to generalize to unobserved locations. To address such challenges, we propose a general Mesh Interpolation Graph Network (MIGN) that models the irregular weather station forecasting, consisting of two key designs: (1) learning spatially irregular data with regular mesh interpolation network to align the data; (2) leveraging parametric spherical harmonics location embedding to further enhance spatial generalization ability. Extensive experiments on an up-to-date observation dataset show that MIGN significantly outperforms existing data-driven models. Besides, we show that MIGN has spatial generalization ability, and is capable of generalizing to previous unseen stations.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extensions of a Line-Graph-Based Method for Token Routing in Decentralized Exchanges</title>
<link>https://arxiv.org/abs/2509.21152</link>
<guid>https://arxiv.org/abs/2509.21152</guid>
<content:encoded><![CDATA[
<div> Decentralized exchanges, DEXs, decentralized finance, DeFi, token trades<br />
Summary:<br />
Decentralized exchanges (DEXs) play a crucial role in the decentralized finance (DeFi) ecosystem by processing billions of dollars in token trades daily. However, a significant amount of these trades are inefficient, missing out on potential profits. This study introduces three key extensions to improve routing methods in DEXs. First, the implementation of a breadth-first search (BFS) link iteration rule reduces computational cost and execution time while maintaining profitability. Second, a route-splitting strategy divides large trades to minimize price slippage and increase trader profits, at the expense of higher computational overhead. Third, the method is expanded to a multi-DEX aggregator setting to reflect real-world trading environments. Empirical data from Uniswap V2 and Sushiswap V2 shows significant enhancements in both computational efficiency and profitability, paving the way for future routing improvements.<br /> <div>
arXiv:2509.21152v1 Announce Type: new 
Abstract: Decentralized exchanges (DEXs) form a cornerstone of the decentralized finance (DeFi) ecosystem, processing token trades worth billions of dollars daily. Yet, a significant fraction of these trades are suboptimal: alternative routing paths could yield more target tokens. Addressing this inefficiency is both practically urgent and theoretically compelling. Building on the linear line-graph-based routing method of Zhang et al. (2025), we propose three key extensions that better capture real-world trading complexity. First, we introduce a breadth-first search (BFS) link iteration rule that reduces computational cost and average execution time without sacrificing profitability. Second, we design a route-splitting strategy that divides large trades into smaller ones, alleviating price slippage and increasing average trader profits, albeit at the cost of higher computational overhead. Third, we generalize the method beyond a single DEX to a multi-DEX aggregator setting, reflecting actual trading environments. Using empirical data from Uniswap V2 and Sushiswap V2, we demonstrate that these extensions substantially improve both computational efficiency and profitability, establishing a foundation for future routing enhancements.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models</title>
<link>https://arxiv.org/abs/2509.20570</link>
<guid>https://arxiv.org/abs/2509.20570</guid>
<content:encoded><![CDATA[
<div> physics-informed generation, reward optimization, diffusion models, value function, PIRF <br />
Summary: 
The article introduces a new approach to physics-informed generation by treating adherence to physical constraints as a reward signal in a sparse reward optimization framework. The proposed method, Physics-Informed Reward Fine-tuning (PIRF), bypasses the use of diffusion posterior sampling-style value function approximations, which are prone to errors and training instability. PIRF computes trajectory-level rewards and backpropagates their gradients directly, enhancing efficiency and data fidelity. Two key strategies, layer-wise truncated backpropagation and weight-based regularization, improve sample efficiency and physical enforcement in scientific generative modeling. Across multiple benchmarks, PIRF consistently outperforms traditional methods in enforcing physical constraints while maintaining efficient sampling regimes. This highlights the potential of reward fine-tuning for advancing generative modeling in scientific domains. <br /> <div>
arXiv:2509.20570v1 Announce Type: cross 
Abstract: Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLIP Arena: Advancing Fairness and Transparency in Machine Learning Interatomic Potentials via an Open, Accessible Benchmark Platform</title>
<link>https://arxiv.org/abs/2509.20630</link>
<guid>https://arxiv.org/abs/2509.20630</guid>
<content:encoded><![CDATA[
<div> benchmark, machine learning interatomic potentials, physics awareness, chemical reactivity, predictive capabilities
Summary:
Machine learning interatomic potentials (MLIPs) have transformed molecular and materials modeling, but existing benchmarks face challenges like data leakage and limited transferability. The new MLIP Arena platform assesses force field performance based on physics awareness, chemical reactivity, and stability in extreme conditions. It evaluates predictive capabilities for thermodynamic properties and physical phenomena beyond error-based metrics tied to specific density functional theory references. By highlighting failure modes of current MLIPs in real-world scenarios, MLIP Arena guides the development of more accurate and efficient MLIPs while ensuring physical consistency. The Python package and online leaderboard for MLIP Arena can be accessed at https://github.com/atomind-ai/mlip-arena. 
<br /><br />Summary: <div>
arXiv:2509.20630v1 Announce Type: cross 
Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized molecular and materials modeling, but existing benchmarks suffer from data leakage, limited transferability, and an over-reliance on error-based metrics tied to specific density functional theory (DFT) references. We introduce MLIP Arena, a benchmark platform that evaluates force field performance based on physics awareness, chemical reactivity, stability under extreme conditions, and predictive capabilities for thermodynamic properties and physical phenomena. By moving beyond static DFT references and revealing the important failure modes of current foundation MLIPs in real-world settings, MLIP Arena provides a reproducible framework to guide the next-generation MLIP development toward improved predictive accuracy and runtime efficiency while maintaining physical consistency. The Python package and online leaderboard are available at https://github.com/atomind-ai/mlip-arena.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations</title>
<link>https://arxiv.org/abs/2509.20667</link>
<guid>https://arxiv.org/abs/2509.20667</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, chemistry computations, supercomputer, prediction, resource optimization

Summary:
Machine learning models were developed to predict the resources required for massively parallel chemistry computations, such as coupled-cluster methods, on supercomputers. The models aim to guide users in selecting optimal runtime parameter values to minimize execution time and resource usage. Two key questions addressed were the shortest-time question, determining parameter configurations for the shortest execution time, and the cheapest-run question, minimizing resource usage. Evaluation of ML models on CCSD application runtime parameter values showed a Gradient Boosting model achieving low Mean Absolute Percentage Errors on DOE Frontier and Aurora supercomputers. Active learning demonstrated effectiveness in achieving accurate predictions with limited experimental data points. This work provides valuable insights for application users to make informed decisions before running costly experiments on supercomputers.<br /><br />Summary: Machine learning models were developed to predict resources for chemistry computations on supercomputers, guiding users in optimizing runtime parameters for efficiency. Evaluation on CCSD application data showed Gradient Boosting model accuracy, with active learning efficiently obtaining predictions with limited data points. <div>
arXiv:2509.20667v1 Announce Type: cross 
Abstract: In this work, we develop machine learning (ML) based strategies to predict resources (costs) required for massively parallel chemistry computations, such as coupled-cluster methods, to guide application users before they commit to running expensive experiments on a supercomputer. By predicting application execution time, we determine the optimal runtime parameter values such as number of nodes and tile sizes. Two key questions of interest to users are addressed. The first is the shortest-time question, where the user is interested in knowing the parameter configurations (number of nodes and tile sizes) to achieve the shortest execution time for a given problem size and a target supercomputer. The second is the cheapest-run question in which the user is interested in minimizing resource usage, i.e., finding the number of nodes and tile size that minimizes the number of node-hours for a given problem size.
  We evaluate a rich family of ML models and strategies, developed based on the collections of runtime parameter values for the CCSD (Coupled Cluster with Singles and Doubles) application executed on the Department of Energy (DOE) Frontier and Aurora supercomputers. Our experiments show that when predicting the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora and Frontier, respectively. In the case where it is expensive to run experiments just to collect data points, we show that active learning can achieve a MAPE of about 0.2 with just around 450 experiments collected from Aurora and Frontier.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Semantic Representations of Social Interactions from Moving Shapes</title>
<link>https://arxiv.org/abs/2509.20673</link>
<guid>https://arxiv.org/abs/2509.20673</guid>
<content:encoded><![CDATA[
<div> Keywords: social interactions, visual features, semantic representations, human similarity judgments, verb-based embeddings

Summary:
Humans can recognize social interactions from simple moving shapes, using both visual features and semantic representations. Study 1 showed that human responses to labeled animations were varied. Study 2 analyzed the geometry of 27 social interactions through human similarity judgments and compared them with model predictions. Semantic models, particularly verb-based embeddings from descriptions, offered additional insights beyond visual features in explaining human judgments. The results indicate that social perception in animations incorporates both visual and abstract representations, with verb-based embeddings providing the most accurate reflection of human similarity judgments. These findings highlight the importance of considering semantic structures in understanding how humans perceive and interpret social interactions in visual displays.<br /><br />Summary: <div>
arXiv:2509.20673v1 Announce Type: cross 
Abstract: Humans are social creatures who readily recognize various social interactions from simple display of moving shapes. While previous research has often focused on visual features, we examine what semantic representations that humans employ to complement visual features. In Study 1, we directly asked human participants to label the animations based on their impression of moving shapes. We found that human responses were distributed. In Study 2, we measured the representational geometry of 27 social interactions through human similarity judgments and compared it with model predictions based on visual features, labels, and semantic embeddings from animation descriptions. We found that semantic models provided complementary information to visual features in explaining human judgments. Among the semantic models, verb-based embeddings extracted from descriptions account for human similarity judgments the best. These results suggest that social perception in simple displays reflects the semantic structure of social interactions, bridging visual and abstract representations.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting</title>
<link>https://arxiv.org/abs/2509.20852</link>
<guid>https://arxiv.org/abs/2509.20852</guid>
<content:encoded><![CDATA[
<div> Keywords: newborns, artificial intelligence, fetal heart rate monitoring, missing data, autoencoder <br />
Summary: <br />
Approximately 10% of newborns require assistance with breathing at birth, with fetal heart rate monitoring being crucial in assessing fetal well-being. Wearable FHR monitors have enabled continuous monitoring but face challenges due to signal dropouts from sensor displacement. Traditional methods like interpolation fail to preserve signal characteristics. This study proposes a masked transformer-based autoencoder to reconstruct missing FHR signals, capturing spatial and frequency components for signal inpainting and forecasting. The method shows robustness across varying durations of missing data and offers potential for AI-based risk algorithms. Integration into wearable monitors could enhance early and reliable risk detection. <div>
arXiv:2509.20852v1 Announce Type: cross 
Abstract: Approximately 10\% of newborns require assistance to initiate breathing at birth, and around 5\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem</title>
<link>https://arxiv.org/abs/2509.21039</link>
<guid>https://arxiv.org/abs/2509.21039</guid>
<content:encoded><![CDATA[
<div> MLIR, Mojo, GPU programming, scientific computing, Python interoperability <br />
Summary:
The article explores the performance and portability of Mojo, a novel language for scientific computing on GPUs. Mojo, based on LLVM's MLIR compiler infrastructure, aims to bridge gaps in performance and productivity by combining Python interoperability with CUDA-like syntax for GPU programming. Four scientific workloads were targeted for evaluation against vendor baselines on NVIDIA and AMD GPUs. Mojo's performance was competitive with CUDA and HIP for memory-bound kernels on both GPU platforms. However, discrepancies were observed on AMD GPUs for atomic operations and fast-math compute-bound kernels on both AMD and NVIDIA GPUs. Despite low-level programming requirements and learning curve, Mojo shows promise in bridging gaps in the Python ecosystem for scientific computing and AI convergence. <div>
arXiv:2509.21039v1 Announce Type: cross 
Abstract: We explore the performance and portability of the novel Mojo language for scientific computing workloads on GPUs. As the first language based on the LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure, Mojo aims to close performance and productivity gaps by combining Python's interoperability and CUDA-like syntax for compile-time portable GPU programming. We target four scientific workloads: a seven-point stencil (memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and Hartree-Fock (compute-bound with atomic operations); and compare their performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We show that Mojo's performance is competitive with CUDA and HIP for memory-bound kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve and programming requirements are still fairly low-level, Mojo can close significant gaps in the fragmented Python ecosystem in the convergence of scientific computing and AI.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust AI-ECG for Predicting Left Ventricular Systolic Dysfunction in Pediatric Congenital Heart Disease</title>
<link>https://arxiv.org/abs/2509.19564</link>
<guid>https://arxiv.org/abs/2509.19564</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, electrocardiogram, pediatric, left ventricular systolic dysfunction, low-resource<br />
Summary:<br />
- The study focuses on enhancing AI-ECG performance for detecting left ventricular systolic dysfunction in pediatric patients with congenital heart disease.
- Current AI-ECG methods rely on large datasets, limiting their practical application in hospitals with limited pediatric ECG data.
- The proposed training framework introduces an on-manifold adversarial perturbation strategy to generate synthetic noise samples that mimic real-world signal variations in pediatric ECGs.
- An uncertainty-aware adversarial training algorithm, independent of architecture, is developed to improve model robustness.
- Evaluation on a real-world pediatric dataset demonstrates the framework's effectiveness in enabling cost-effective and reliable detection of left ventricular systolic dysfunction, making it suitable for deployment in resource-limited clinical settings.<br /> 
Summary: <div>
arXiv:2509.19564v1 Announce Type: new 
Abstract: Artificial intelligence-enhanced electrocardiogram (AI-ECG) has shown promise as an inexpensive, ubiquitous, and non-invasive screening tool to detect left ventricular systolic dysfunction in pediatric congenital heart disease. However, current approaches rely heavily on large-scale labeled datasets, which poses a major obstacle to the democratization of AI in hospitals where only limited pediatric ECG data are available. In this work, we propose a robust training framework to improve AI-ECG performance under low-resource conditions. Specifically, we introduce an on-manifold adversarial perturbation strategy for pediatric ECGs to generate synthetic noise samples that better reflect real-world signal variations. Building on this, we develop an uncertainty-aware adversarial training algorithm that is architecture-agnostic and enhances model robustness. Evaluation on the real-world pediatric dataset demonstrates that our method enables low-cost and reliable detection of left ventricular systolic dysfunction, highlighting its potential for deployment in resource-limited clinical settings.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Language Models with Modality-Specific Experts for Financial Forecasting from Interleaved Sequences of Text and Time Series</title>
<link>https://arxiv.org/abs/2509.19628</link>
<guid>https://arxiv.org/abs/2509.19628</guid>
<content:encoded><![CDATA[
<div> Keywords: financial forecasting, neural architecture, multimodal understanding, cross-modal alignment, interpretability<br />
<br />
Summary: 
The article proposes a neural architecture that integrates text and time series data for financial forecasting. The model utilizes modality-specific experts to capture unique patterns in each modality while allowing for joint reasoning. It also introduces a cross-modal alignment framework with a token weighting mechanism to align representations across modalities. The approach achieves state-of-the-art performance compared to baselines and incorporates interpretability methods to understand the value of different modalities. Economic gains in investment simulations validate the effectiveness of the proposed method. <div>
arXiv:2509.19628v1 Announce Type: new 
Abstract: Text and time series data offer complementary views of financial markets: news articles provide narrative context about company events, while stock prices reflect how markets react to those events. However, despite their complementary nature, effectively integrating these interleaved modalities for improved forecasting remains challenging. In this work, we propose a unified neural architecture that models these interleaved sequences using modality-specific experts, allowing the model to learn unique time series patterns, while still enabling joint reasoning across modalities and preserving pretrained language understanding capabilities. To further improve multimodal understanding, we introduce a cross-modal alignment framework with a salient token weighting mechanism that learns to align representations across modalities with a focus on the most informative tokens. We demonstrate the effectiveness of our approach on a large-scale financial forecasting task, achieving state-of-the-art performance across a wide variety of strong unimodal and multimodal baselines. We develop an interpretability method that reveals insights into the value of time series-context and reinforces the design of our cross-modal alignment objective. Finally, we demonstrate that these improvements translate to meaningful economic gains in investment simulations.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing failure morphologies in fiber-reinforced composites via k-means clustering based multiscale framework</title>
<link>https://arxiv.org/abs/2509.20011</link>
<guid>https://arxiv.org/abs/2509.20011</guid>
<content:encoded><![CDATA[
<div> homogenization, composite materials, damage analysis, clustering methods, failure prediction 
Summary: 
A novel homogenization methodology is introduced for analyzing fiber-reinforced composite material failures. This method incorporates elastic and eigen influence tensors in a damage-informed transformation field analysis (D-TFA) framework, allowing for efficient and realistic simulations of damage under uniform stress and strain conditions. The use of a reduced-order modeling strategy improves computational efficiency, while clustering methods help partition the microscale domain for more accurate predictions. By simulating the response of a representative volume element (RVE) and comparing clustering schemes, the model's performance is evaluated. Damage morphologies are accurately captured and directional strengths predicted, with higher cluster counts proving essential for complex microstructures. The effectiveness of the proposed framework is demonstrated in open-hole specimen tests, accurately predicting failure paths for domains with different fiber layups. <div>
arXiv:2509.20011v1 Announce Type: new 
Abstract: A novel homogenization methodology is proposed for analyzing the failure of fiber-reinforced composite materials, utilizing elastic and eigen influence tensors within a damage informed transformation field analysis (D-TFA) framework. This approach includes a technique for calculating macroscopic damage under uniform stress and strain conditions, offering more realistic simulations. Computational efficiency is enhanced through a reduced-order modeling strategy, while elastic and eigen strain distribution driven k-means clustering methods are employed to partition the microscale domain. The model's performance is assessed by simulating the response of a representative volume element (RVE) treated as a homogenized continuum. Subsequently, a comparative assessment is carried out to check the efficacy of two clustering schemes. Damage morphologies are calculated using proposed framework and compared with predictions obtained using finite element method. Furthermore, open-hole specimen tests are simulated and failure paths are predicted for the domains with different fiber layups. Ultimately, we show that D-TFA can accurately capture damage patterns and directional strengths, providing improved predictions of the mechanical behavior of composite materials. It has been demonstrated that higher cluster counts are crucial for capturing a more accurate stress-strain response, especially for complex microstructures.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multi-Objective Constrained Bayesian Optimization of Bridge Girder</title>
<link>https://arxiv.org/abs/2509.20161</link>
<guid>https://arxiv.org/abs/2509.20161</guid>
<content:encoded><![CDATA[
<div> Keywords: greenhouse gas emissions, structural design optimization, Bayesian optimization, constrained optimization, finite element simulations

Summary: 
The article discusses the significant contribution of the buildings and construction sector to greenhouse gas emissions, with a focus on reducing emissions through optimized structural design. It introduces a novel approach using multi-objective constrained Bayesian optimization to achieve this goal efficiently. By incorporating proper orthogonal decomposition and Kriging partial least squares, the method aims to reduce the computational expenses associated with finite element simulations in structural design. The constrained expected improvement function is used for Bayesian optimization, demonstrated through a case study of a concrete bridge girder design. The results show potential cost savings of approximately 10% to 15% in financial costs and 20% in environmental costs while ensuring structural integrity. This approach showcases the effectiveness of optimization techniques in achieving both economic and environmental benefits in the construction industry. 

<br /><br />Summary: <div>
arXiv:2509.20161v1 Announce Type: new 
Abstract: The buildings and construction sector is a significant source of greenhouse gas emissions, with cement production alone contributing 7~\% of global emissions and the industry as a whole accounting for approximately 37~\%. Reducing emissions by optimizing structural design can achieve significant global benefits. This article introduces an efficient multi-objective constrained Bayesian optimization approach to address this challenge. Rather than attempting to determine the full set of non-dominated solutions with arbitrary trade-offs, the approach searches for a solution matching a specified trade-off. Structural design is typically conducted using computationally expensive finite element simulations, whereas Bayesian optimization offers an efficient approach for optimizing problems that involve such high-cost simulations. The proposed method integrates proper orthogonal decomposition for dimensionality reduction of simulation results with Kriging partial least squares to enhance efficiency. Constrained expected improvement is used as an acquisition function for Bayesian optimization. The approach is demonstrated through a case study of a two-lane, three-span post-tensioned concrete bridge girder, incorporating fifteen design variables and nine constraints. A comparison with conventional design methods demonstrates the potential of this optimization approach to achieve substantial cost reductions, with savings of approximately 10\% to 15\% in financial costs and about 20\% in environmental costs for the case study, while ensuring structural integrity.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Overview of Meshfree Collocation Methods</title>
<link>https://arxiv.org/abs/2509.20056</link>
<guid>https://arxiv.org/abs/2509.20056</guid>
<content:encoded><![CDATA[
<div> Keywords: meshfree collocation methods, differential operators, unstructured point clouds, numerical approximation, computational grid. 

Summary: 
This article provides a comprehensive overview of meshfree collocation methods used in numerically approximating differential operators on unstructured point clouds. The methods discussed in the literature do not rely on a computational grid, instead approximating functions and derivatives at irregularly distributed collocation points. The historical development of key concepts is traced, and a classification of methods based on their principle of derivation is proposed. While some methods are similar, subtle differences exist and are highlighted. A unifying formulation of meshfree collocation methods is presented to elucidate these differences, showing how each method can be derived from this formulation. Additionally, a generalized derivation for future meshfree collocation methods is suggested. <div>
arXiv:2509.20056v1 Announce Type: cross 
Abstract: We provide a comprehensive overview of meshfree collocation methods for numerically approximating differential operators on continuously labeled unstructured point clouds. Meshfree collocation methods do not require a computational grid or mesh. Instead, they approximate smooth functions and their derivatives at potentially irregularly distributed collocation points, often called particles, to a desired order of consistency. We review several meshfree collocation methods from the literature, trace the historical development of key concepts, and propose a classification of methods according to their principle of derivation. Although some of the methods reviewed are similar or identical, there are subtle yet important differences between many, which we highlight and discuss. We present a unifying formulation of meshfree collocation methods that renders these differences apparent and show how each method can be derived from this formulation. Finally, we propose a generalized derivation for meshfree collocation methods going forward.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Multi-Species Bird Classification on Low-Power Bioacoustic Loggers</title>
<link>https://arxiv.org/abs/2509.20103</link>
<guid>https://arxiv.org/abs/2509.20103</guid>
<content:encoded><![CDATA[
<div> Neural network, bird audio classification, microcontrollers, biodiversity monitoring, energy-efficient <br />
<br />
Summary: 
This paper introduces WrenNet, a neural network designed for real-time multi-species bird audio classification on low-power microcontrollers to facilitate scalable biodiversity monitoring. WrenNet features a semi-learnable spectral feature extractor that outperforms traditional alternatives. Testing on a 70-species dataset, WrenNet achieves high accuracy rates, particularly for acoustically distinct species. It displays energy efficiency, consuming only 77mJ per inference on an AudioMoth device with limited RAM. In comparison to Birdnet, WrenNet proves over 16 times more energy-efficient when running on a Raspberry Pi 3B+. This research presents a practical solution for continuous, multi-species acoustic monitoring on low-power edge devices, offering promising applications for biodiversity conservation efforts. <br /> <div>
arXiv:2509.20103v1 Announce Type: cross 
Abstract: This paper introduces WrenNet, an efficient neural network enabling real-time multi-species bird audio classification on low-power microcontrollers for scalable biodiversity monitoring. We propose a semi-learnable spectral feature extractor that adapts to avian vocalizations, outperforming standard mel-scale and fully-learnable alternatives. On an expert-curated 70-species dataset, WrenNet achieves up to 90.8\% accuracy on acoustically distinctive species and 70.1\% on the full task. When deployed on an AudioMoth device ($\leq$1MB RAM), it consumes only 77mJ per inference. Moreover, the proposed model is over 16x more energy-efficient compared to Birdnet when running on a Raspberry Pi 3B+. This work demonstrates the first practical framework for continuous, multi-species acoustic monitoring on low-power edge devices.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Fusion for Full-Range Response Reconstruction via Diffusion Models</title>
<link>https://arxiv.org/abs/2502.00795</link>
<guid>https://arxiv.org/abs/2502.00795</guid>
<content:encoded><![CDATA[
<div> Diffusion models, structural health monitoring, data fusion framework, sparse sensor measurements, probabilistic constraints<br />
Summary:<br />
Accurate monitoring of structures is essential for safety, but limited sensor deployment can be a challenge. This paper introduces a novel data fusion framework that utilizes diffusion models to reconstruct full-range structural responses from sparse sensor measurements. The framework incorporates Diffusion Posterior Sampling (DPS) to guide the reconstruction process using sensor measurements as probabilistic constraints. Three forward models are developed to adapt to different sensor placement scenarios and reconstruction targets. Validation on a steel plate shear wall shows promising results, with Weighted Mean Absolute Percentage Errors ranging from 1.62% to 3.49%. Sensitivity analyses demonstrate robust performance under various conditions. This framework presents a new approach for probabilistic modeling in structural health monitoring, offering a data fusion solution for comprehensive structural monitoring. <br /> <div>
arXiv:2502.00795v2 Announce Type: replace 
Abstract: Accurately capturing the full-range response of structures is crucial in structural health monitoring (SHM) for ensuring safety and operational integrity. However, limited sensor deployment due to cost, accessibility, or scale often hinders comprehensive monitoring. This paper presents a generative data fusion framework utilizing diffusion models, to reconstruct the full-range structural response from sparse and heterogeneous sensor measurements. We incorporate Diffusion Posterior Sampling (DPS) into the reconstruction framework, using sensor measurements as probabilistic constraints to guide the sampling process. Three forward models are designed: Direct Observation Mapping (DOM), Channel-based Observation Mapping (COM), and Neural Network Forward Model (NNFM), enabling flexible adaptation to different sensor placement conditions and reconstruction targets. The proposed framework is validated on a steel plate shear wall exhibiting nonlinear responses. By simultaneously sampling 100 realizations and averaging them as the ensemble prediction result, the three forward models achieve Weighted Mean Absolute Percentage Errors of 1.62% (DOM), 3.27% (COM), and 3.49% (NNFM). Sensitivity analyses further demonstrate robust performance under varying hyperparameters, sensor configurations, and noise levels. The proposed framework shows new possibilities for probabilistic modeling and decision-making in SHM by harnessing the capabilities of diffusion models, offering a novel data fusion approach for full-range monitoring of structures.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2D implementation of Kinetic-diffusion Monte Carlo in Eiron</title>
<link>https://arxiv.org/abs/2509.19140</link>
<guid>https://arxiv.org/abs/2509.19140</guid>
<content:encoded><![CDATA[
<div> Particle-based kinetic Monte Carlo simulations, neutral particles, computational bottlenecks, tokamak scrape-off layer simulations, high-collisional regimes<br />
Summary: 
The article introduces the Kinetic-diffusion Monte Carlo scheme for simulating neutral particles in high-collisional regimes, focusing on tokamak scrape-off layer simulations. By approximating high-collisional kinetic dynamics with diffusion in these regimes, computational costs are significantly reduced. The scheme's extension to the two-dimensional setting and implementation in the Eiron particle code are discussed. The results demonstrate a notable speedup in high-collisional cases compared to standard kinetic simulations. This advancement addresses the computational challenges associated with resolving individual collision events and showcases the potential of asymptotic-preserving schemes in improving the efficiency of particle-based simulations in tokamak environments. <div>
arXiv:2509.19140v1 Announce Type: new 
Abstract: Particle-based kinetic Monte Carlo simulations of neutral particles is one of the major computational bottlenecks in tokamak scrape-off layer simulations. This computational cost comes from the need to resolve individual collision events in high-collisional regimes. However, in such regimes, one can approximate the high-collisional kinetic dynamics with computationally cheaper diffusion. Asymptotic-preserving schemes make use of this limit to perform simulations in these regimes, without a blow-up in computational cost as incurred by standard kinetic approaches. One such scheme is Kinetic-diffusion Monte Carlo. In this paper, we present a first extension of this scheme to the two-dimensional setting and its implementation in the Eiron particle code. We then demonstrate that this implementation produces a significant speedup over kinetic simulations in high-collisional cases.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlloyInter: Visualising Alloy Mixture Interpolations in t-SNE Representations</title>
<link>https://arxiv.org/abs/2509.19202</link>
<guid>https://arxiv.org/abs/2509.19202</guid>
<content:encoded><![CDATA[
<div> keywords: AlloyInter, input mixtures, output parameters space, eXplainable Artificial Intelligence, interpolation

Summary: 
AlloyInter is a novel system proposed for joint exploration of input mixtures and output parameters space in the SciVis Contest 2025. The system utilizes an interpolation approach guided by eXplainable Artificial Intelligence (XAI) to allow users to discover input mixture ratios while specifying output parameter goals. The system leverages a learned model ensemble to iteratively adjust and improve towards a set goal. By incorporating robust XAI techniques and combining manifold learning with interpolation approaches, AlloyInter strengthens its capabilities for efficient exploration. The system aims to enhance user interaction and understanding by providing a platform for discovering optimal input mixtures based on desired output parameters. With a focus on improving usability and interpretability, AlloyInter offers a promising solution for navigating complex input-output relationships in scientific visualization tasks. <br /><br />Summary: <div>
arXiv:2509.19202v1 Announce Type: new 
Abstract: This entry description proposes AlloyInter, a novel system to enable joint exploration of input mixtures and output parameters space in the context of the SciVis Contest 2025. We propose an interpolation approach, guided by eXplainable Artificial Intelligence (XAI) based on a learned model ensemble that allows users to discover input mixture ratios by specifying output parameter goals that can be iteratively adjusted and improved towards a goal. We strengthen the capabilities of our system by building upon prior research within the robustness of XAI, as well as combining well-established techniques like manifold learning with interpolation approaches.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning</title>
<link>https://arxiv.org/abs/2509.18120</link>
<guid>https://arxiv.org/abs/2509.18120</guid>
<content:encoded><![CDATA[
<div> CoCross-silo federated learning, data privacy, economic competition, CoCoGen framework, generative AI, potential game theory  
Summary:  
CoCoGen is a framework designed to address the challenges of cross-silo federated learning in the presence of economic competition and statistical heterogeneity. It utilizes generative AI and potential game theory to optimize collaborative training among organizations while maximizing social welfare. The framework models competition and heterogeneity through learning performance and utility-based formulations, treating each training round as a weighted potential game. Experimental results on the Fashion-MNIST dataset demonstrate that CoCoGen outperforms baseline methods, showing how varying levels of heterogeneity and competition impact organizational behavior. By leveraging generative AI and game theory, CoCoGen enables organizations to engage in coopetitive-compatible data generation, fostering collaboration in a competitive landscape. <br /><br />Summary: <div>
arXiv:2509.18120v1 Announce Type: cross 
Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or banks) to collaboratively train artificial intelligence (AI) models while preserving data privacy by keeping data local. While prior work has primarily addressed statistical heterogeneity across organizations, a critical challenge arises from economic competition, where organizations may act as market rivals, making them hesitant to participate in joint training due to potential utility loss (i.e., reduced net benefit). Furthermore, the combined effects of statistical heterogeneity and inter-organizational competition on organizational behavior and system-wide social welfare remain underexplored. In this paper, we propose CoCoGen, a coopetitive-compatible data generation framework, leveraging generative AI (GenAI) and potential game theory to model, analyze, and optimize collaborative learning under heterogeneous and competitive settings. Specifically, CoCoGen characterizes competition and statistical heterogeneity through learning performance and utility-based formulations and models each training round as a weighted potential game. We then derive GenAI-based data generation strategies that maximize social welfare. Experimental results on the Fashion-MNIST dataset reveal how varying heterogeneity and competition levels affect organizational behavior and demonstrate that CoCoGen consistently outperforms baseline methods.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning</title>
<link>https://arxiv.org/abs/2509.18169</link>
<guid>https://arxiv.org/abs/2509.18169</guid>
<content:encoded><![CDATA[
<div> training, inference, neural networks, computation, reasoning

Summary:
- The article introduces the PiMoE (Physically-isolated Mixture of Experts) architecture, which integrates computation and reasoning capabilities into neural networks.
- PiMoE endogenously incorporates computational expertise within the network, allowing for iterative alternation within a single chain of thought.
- Evaluations against LLM finetuning and multi-agent system approaches show that PiMoE outperforms in accuracy and exhibits improvements in response latency, token usage, and GPU energy consumption.
- This architecture offers an efficient, interpretable, and scalable solution for next-generation intelligent systems in scientific or industrial applications. 

<br /><br />Summary: <div>
arXiv:2509.18169v1 Announce Type: cross 
Abstract: Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM</title>
<link>https://arxiv.org/abs/2509.18178</link>
<guid>https://arxiv.org/abs/2509.18178</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational Fluid Dynamics, Foam-Agent, automation, multi-agent framework, simulation

Summary:
Foam-Agent is a multi-agent framework designed to automate the entire OpenFOAM workflow, reducing the steep learning curve and manual setup challenges in Computational Fluid Dynamics (CFD) simulations. It offers end-to-end simulation automation, managing tasks from pre-processing to post-simulation visualization. The framework features a composable service architecture that allows for flexible integration with other systems, enhancing exploratory workflows. Foam-Agent ensures high-fidelity configuration generation through precise context retrieval and dependency-aware processes. Evaluation results show a high success rate in handling simulation tasks, outperforming existing frameworks. By democratizing complex scientific computing, Foam-Agent lowers the expertise barrier for CFD, making it more accessible to a wider range of users. The code for Foam-Agent is publicly available on GitHub for use and further development. 

<br /><br />Summary: <div>
arXiv:2509.18178v1 Announce Type: cross 
Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in engineering, yet its steep learning curve and complex manual setup create significant barriers. To address these challenges, we introduce Foam-Agent, a multi-agent framework that automates the entire end-to-end OpenFOAM workflow from a single natural language prompt. Our key innovations address critical gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation: Foam-Agent is the first system to manage the full simulation pipeline, including advanced pre-processing with a versatile Meshing Agent capable of handling external mesh files and generating new geometries via Gmsh, automatic generation of HPC submission scripts, and post-simulation visualization via ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent, the framework uses Model Context Protocol (MCP) to expose its core functions as discrete, callable tools. This allows for flexible integration and use by other agentic systems, such as Claude-code, for more exploratory workflows. 3. High-Fidelity Configuration Generation: We achieve superior accuracy through a Hierarchical Multi-Index RAG for precise context retrieval and a dependency-aware generation process that ensures configuration consistency. Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2% success rate with Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the expertise barrier for CFD, demonstrating how specialized multi-agent systems can democratize complex scientific computing. The code is public at https://github.com/csml-rpi/Foam-Agent.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market</title>
<link>https://arxiv.org/abs/2509.18820</link>
<guid>https://arxiv.org/abs/2509.18820</guid>
<content:encoded><![CDATA[
<div> cryptocurrency, correlation structures, fluctuation amplitudes, network structures, portfolio construction <br />
<br />
Summary: 
This study introduces a methodology for analyzing evolving correlation structures in complex systems based on cryptocurrency market dynamics. By using the $q$-dependent detrended cross-correlation coefficient $\rho(q,s)$, correlations at varying fluctuation amplitudes and time scales are captured. The approach utilizes $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Analysis of minute-by-minute exchange rate data for 140 cryptocurrencies on Binance from Jan 2021 to Oct 2024 reveals significant shifts in $q$MSTs, particularly during the Terra/Luna crash in April 2022. The network initially centered around Bitcoin (BTC) but later decentralized, with Ethereum (ETH) and other assets gaining prominence. Spectral analysis indicates decreasing dominance of BTC and increased diversification among assets. Medium-scale fluctuations exhibit stronger correlations than large-scale ones, influencing portfolio construction strategies. During crashes, major disruptions amplify correlation differences, leading to fully decentralized network structures. These findings demonstrate the effectiveness of $q$MSTs in uncovering fluctuation-dependent correlations and have potential applications in various complex systems beyond finance such as biology and social systems. <div>
arXiv:2509.18820v1 Announce Type: cross 
Abstract: Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A failure mode dependent continuum damage model for laminated composites with optimized model parameters : Application to curved beams</title>
<link>https://arxiv.org/abs/2509.19051</link>
<guid>https://arxiv.org/abs/2509.19051</guid>
<content:encoded><![CDATA[
<div> damage model, continuum damage modeling, laminated composite panels, optimization algorithm, finite element method

Summary: 
A new continuum damage model for laminated composite panels is proposed, utilizing polynomial-based damage hardening functions and characterized parameters based on experimental stress-strain curves. The model is optimized using a steepest descent algorithm to minimize differences between predicted and experimental data. Damage evolution equations are derived and applied to a curved beam model, showing non-linear behavior in load vs deflection curves and successfully capturing stiffness degradation and damage differences in tension and compression. The model is compared to existing models and found effective in representing material properties and non-linearity in composite behavior. The study showcases the model's ability to predict damage accurately and demonstrate thermodynamically consistent continuum damage modeling for laminated composite structures. <br /><br /> <div>
arXiv:2509.19051v1 Announce Type: cross 
Abstract: In this article, a failure mode dependent and thermodynamically consistent continuum damage model with polynomial-based damage hardening functions is proposed for continuum damage modeling of laminated composite panels. The damage model parameters are characterized based on all uniaxial/shear experimental stress-strain curves. Steepest descent optimization algorithm is used to minimize the difference between model predicted and experimental stress-strain curves to get the optimzed model parameters. The fully characterized damage evolution equations are used for damage prediction of a moderately thick laminated composite curved beam modeled using first-order shear deformation theory. Finite element method with load control is used to get the non-linear algebraic equations which are solved using Newton Raphson method. The developed model is compared with the existing failure mode dependent and failure mode independent damage models. The results depict the efficacy of the proposed model to capture non-linearity in the load vs deflection curve due to stiffness degradation and different damage in tension andcompression consistent with uniaxial/shear stress-strain response and strength properties of the material, respectively.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving Linear and Angular Momentum for Dynamical Systems</title>
<link>https://arxiv.org/abs/2501.07373</link>
<guid>https://arxiv.org/abs/2501.07373</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Multi-body Dynamical Systems, Physics-Informed, Interpretable, Real-time <br />
<br />
Summary: <br />
The paper introduces Dynami-CAL GraphNet, a Physics-Informed Graph Neural Network designed for accurate and interpretable modeling of multi-body dynamical systems. By integrating physics-based inductive biases with the learning capabilities of GNNs, the model enforces pairwise conservation of linear and angular momentum for interacting nodes using edge-local reference frames. This design ensures physically consistent predictions of node dynamics and offers interpretable edge-wise linear and angular impulses resulting from pairwise interactions. Evaluated on a 3D granular system, Dynami-CAL GraphNet demonstrates stable error accumulation over extended rollouts, effective extrapolations to unseen configurations, and robust handling of heterogeneous interactions and external forces. The model offers significant advantages in various fields like robotics, aerospace engineering, and materials science by providing scalable, physically consistent predictions that adhere to fundamental conservation laws, enabling the inference of forces and moments while efficiently handling diverse interactions and external forces. <div>
arXiv:2501.07373v2 Announce Type: replace-cross 
Abstract: Accurate, interpretable, and real-time modeling of multi-body dynamical systems is essential for predicting behaviors and inferring physical properties in natural and engineered environments. Traditional physics-based models face scalability challenges and are computationally demanding, while data-driven approaches like Graph Neural Networks (GNNs) often lack physical consistency, interpretability, and generalization. In this paper, we propose Dynami-CAL GraphNet, a Physics-Informed Graph Neural Network that integrates the learning capabilities of GNNs with physics-based inductive biases to address these limitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and angular momentum for interacting nodes using edge-local reference frames that are equivariant to rotational symmetries, invariant to translations, and equivariant to node permutations. This design ensures physically consistent predictions of node dynamics while offering interpretable, edge-wise linear and angular impulses resulting from pairwise interactions. Evaluated on a 3D granular system with inelastic collisions, Dynami-CAL GraphNet demonstrates stable error accumulation over extended rollouts, effective extrapolations to unseen configurations, and robust handling of heterogeneous interactions and external forces. Dynami-CAL GraphNet offers significant advantages in fields requiring accurate, interpretable, and real-time modeling of complex multi-body dynamical systems, such as robotics, aerospace engineering, and materials science. By providing physically consistent and scalable predictions that adhere to fundamental conservation laws, it enables the inference of forces and moments while efficiently handling heterogeneous interactions and external forces.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Engineering AGI: Benchmarking the Engineering Design Capabilities of LLMs</title>
<link>https://arxiv.org/abs/2509.16204</link>
<guid>https://arxiv.org/abs/2509.16204</guid>
<content:encoded><![CDATA[
<div> Engineering Design Benchmark, Large Language Models, Real-world tasks, Domain Knowledge, Simulation-based Evaluation<br />
<br />Summary:
Industry pioneers aim to develop general-purpose AI engineers for ambitious projects. Large language models face unique challenges in engineering design, requiring synthesis of domain knowledge, trade-off navigation, and process management. The ENGDESIGN benchmark evaluates LLMs across nine engineering domains, emphasizing design synthesis and objective-oriented solutions. Tasks simulate real-world engineering challenges with detailed descriptions of goals, constraints, and requirements. A simulation-based evaluation approach tests LLM-generated designs using domain-specific simulations. This benchmark fills a gap in existing benchmarks by focusing on design tasks rather than factual recall or question answering. Industry's vision of AI engineers shaping grand projects may become a reality with advancements in LLM capabilities through benchmarks like ENGDESIGN. <br /> <div>
arXiv:2509.16204v1 Announce Type: new 
Abstract: Today, industry pioneers dream of developing general-purpose AI engineers capable of designing and building humanity's most ambitious projects--from starships that will carry us to distant worlds to Dyson spheres that harness stellar energy. Yet engineering design represents a fundamentally different challenge for large language models (LLMs) compared to traditional textbook-style problem solving or factual question answering. Real-world engineering design demands the synthesis of domain knowledge, navigation of complex trade-offs, and management of the tedious processes that consume much of practicing engineers' time. Despite these shared challenges across engineering disciplines, no benchmark currently captures the unique demands of engineering design work. In this work, we introduce ENGDESIGN, an Engineering Design benchmark that evaluates LLMs' abilities to perform practical design tasks across nine engineering domains: Operating System Design, Computer Architecture Design, Control System Design, Mechanical Systems, Structural Design, Digital Hardware Design, Analog Integrated Circuit Design, Robotics, and Signal Processing. Unlike existing benchmarks that focus on factual recall or question answering, ENGDESIGN uniquely emphasizes LLMs' ability to synthesize domain knowledge, reason under constraints, and generate functional, objective-oriented designs. Each task in ENGDESIGN represents a real-world engineering design problem, accompanied by a detailed task description specifying design goals, constraints, and performance requirements. We pioneer a simulation-based evaluation paradigm where LLM-generated designs undergo rigorous testing through executable, domain-specific simulations-from circuit SPICE simulations to structural finite element analysis, from control system validation to robotic motion planning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning in Factor Investment</title>
<link>https://arxiv.org/abs/2509.16206</link>
<guid>https://arxiv.org/abs/2509.16206</guid>
<content:encoded><![CDATA[
<div> Factor Portfolio Construction, Deep Reinforcement Learning, Conditional Auto-encoded Factor-based Portfolio Optimisation, Performance Analysis, U.S. Equity Data

Summary: 
Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO) addresses the challenge of high-dimensional, unbalanced state space in factor portfolio construction by compressing stock-level returns into latent factors. Utilizing deep reinforcement learning (DRL) with PPO and DDPG, CAFPO outperforms traditional methods and other DRL approaches on 20 years of U.S. equity data. It achieves a 24.6% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis highlights the economically intuitive factor attributions. The study demonstrates that factor-aware representation learning enhances DRL for institutional, low-turnover portfolio management. <div>
arXiv:2509.16206v1 Announce Type: new 
Abstract: Deep reinforcement learning has shown promise in trade execution, yet its use in low-frequency factor portfolio construction remains under-explored. A key obstacle is the high-dimensional, unbalanced state space created by stocks that enter and exit the investable universe. We introduce Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO), which compresses stock-level returns into a small set of latent factors conditioned on 94 firm-specific characteristics. The factors feed a DRL agent implemented with both PPO and DDPG to generate continuous long-short weights. On 20 years of U.S. equity data (2000--2020), CAFPO outperforms equal-weight, value-weight, Markowitz, vanilla DRL, and Fama--French-driven DRL, delivering a 24.6\% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis further reveals economically intuitive factor attributions. Our results demonstrate that factor-aware representation learning can make DRL practical for institutional, low-turnover portfolio management.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Machine Learning to Increase the Throughput of Container Yards</title>
<link>https://arxiv.org/abs/2509.16207</link>
<guid>https://arxiv.org/abs/2509.16207</guid>
<content:encoded><![CDATA[
<div> Keywords: shipping container terminals, throughput rates, automation, Intelligent Planning System, Pareto Optimization 

Summary:
The study aims to enhance throughput rates for shipping container terminals, crucial for the U.S. economy. Despite the potential of automation, U.S. ports face limitations due to legal constraints on human labor. To address this, the paper presents an Intelligent Planning System (IPS) using Pareto Optimization and MILP. The IPS improves terminal throughput and processing times, offering recommendations for container positioning and truck pickup appointments. By optimizing container yard layout and flow, the IPS reduces real-time congestion and predicts future congestion. This innovative approach combines efficiency with cooperative agreement terms, providing a solution for U.S. ports seeking automation level efficiencies. The proposed IPS offers a valuable tool for enhancing operational efficiency and ensuring smoother container processing at shipping ports. 

<br /><br />Summary: <div>
arXiv:2509.16207v1 Announce Type: new 
Abstract: This study seeks to improve the throughput rates for shipping container terminals. In the United States, shipping ports link the domestic economy to global markets and are vital to sustain supply chain flow and economic stability. Maritime shipping accounts for nearly half of the U.S.'s annual international trade, two thirds of which are represented by container shipping. Previous studies highlighted the capability of automation in enhancing container processing; however, unlike in European and East Asian ports, full automation is limited in U.S. ports due to legal protections for human labor. Consequently, there is a need for alternative methods that deliver automation level efficiencies while maintaining the terms of cooperative agreements. This paper proposes an Intelligent Planning System (IPS) that applies the concept of Pareto Optimization to container yards through a mixed integer linear programming (MILP) based recursive appointment system. The results show an improvement from baseline for both daily terminal throughput volumes and processing times. The generated IPS can be employed to provide recommendations for container positioning and truck pickup appointments to optimize container yard layout and flow resulting in reduced realtime congestion and predictively mitigated future congestion.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesis of Service Life Prediction for Bridges in Texas</title>
<link>https://arxiv.org/abs/2509.16208</link>
<guid>https://arxiv.org/abs/2509.16208</guid>
<content:encoded><![CDATA[
<div> Bridge, design-build, service life, durability, prediction

Summary:
This research highlights the lack of standardized methods for achieving and verifying long-term service life requirements in design-build bridge contracts. It emphasizes the importance of accurately estimating remaining service life to prioritize repair and rehabilitation needs in aging bridges with limited financial resources. The study reviews current practices and recent advancements in bridge service life prediction, providing practical guidance for evaluating and extending the lifespan of both existing and new structures. By incorporating quantitative validation of durability practices, this research aims to support more efficient use of maintenance funds, enhance understanding of deterioration models and inspection methods, and inform strategies for ensuring long-term structural performance.<br /><br />Summary: <div>
arXiv:2509.16208v1 Announce Type: new 
Abstract: Design-build bridge contracts often include long-term service life requirements, but there are no clear technical guidelines or standardized methods to achieve or verify these goals. While durability practices are commonly applied, they lack quantitative validation. With many aging bridges and limited financial resources, accurately estimating remaining service life is essential for prioritizing repair and rehabilitation needs. This research reviews current practices and recent advancements in bridge service life prediction, providing practical guidance for evaluating and extending the lifespan of both existing and new structures. The findings support more efficient use of maintenance funds, better understanding of deterioration models and inspection methods, and informed strategies to ensure long-term structural performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Digital Twin Models</title>
<link>https://arxiv.org/abs/2509.16209</link>
<guid>https://arxiv.org/abs/2509.16209</guid>
<content:encoded><![CDATA[
<div> scale, digital twin, machine learning, dimensional analysis, calibration

Summary: 
The paper introduces a novel methodology and computational tool for scaling digital twin models using machine learning and dimensional analysis. This framework aims to address the complexity and barriers faced in developing accurate digital twin models for large-scale systems. By applying scaling techniques, the need for repetitive physical calibration of models in industries with varying product sizes can be eliminated. The methodology allows for transferring calibration data from smaller units to larger units within the same product line, without the need for additional data collection or experimentation. The paper also discusses challenges related to dimensional analysis for scaling and presents a framework for proper scaling of digital twin models. The results of the study demonstrate successful scaling between an industrial-size wheel loader vehicle and a miniaturized system in a laboratory setting. <div>
arXiv:2509.16209v1 Announce Type: new 
Abstract: In many industries, the scale and complexity of systems can present significant barriers to the development of accurate digital twin models. This paper introduces a novel methodology and a modular computational tool utilizing machine learning and dimensional analysis to establish a framework for scaling digital twin models. Scaling techniques have not yet been applied to digital twin technology, but they can eliminate the need for repetitive physical calibration of such models in industries where product lines include a variety of sizes of the same or similar products. In many cases, it may be easier or more cost-effective to perform physical calibration of the digital twin model on smaller units of a product line. Scaling techniques can then allow adapting the calibration data from the smaller units to other sizes of the product line without the need for additional data collection and experimentation for calibration. Conventional application of dimensional analysis for scaling in this context introduces several challenges due to distortion of scaling factors. This paper addresses these challenges and introduces a framework for proper scaling of digital twin models. The results are applied to scaling the models between an industrial-size wheel loader vehicle used in construction to a miniaturized system instrumented in a laboratory setting.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strain localization in reduced order asymptotic homogenization</title>
<link>https://arxiv.org/abs/2509.16210</link>
<guid>https://arxiv.org/abs/2509.16210</guid>
<content:encoded><![CDATA[
<div> damage, inelastic effects, multiscale technique, homogenization, composite materials

Summary:
- A new multiscale technique is proposed for capturing damage and inelastic effects in composite materials.
- The technique combines two-scale homogenization with eigen strain representation to model inelastic response. 
- Computational efficiency is improved through reduced order techniques.
- Macroscale stress is determined using influence tensors from representative volume element analysis. 
- Microscale damage is modeled using continuum damage mechanics with a method to prevent strain localization.
- Strategies are implemented to address spurious post-failure artificial stiffness at the macroscale.
- Verification studies show that the formulation accurately predicts macroscale response while capturing damage and inelastic strains. 

<br /><br />Summary: <div>
arXiv:2509.16210v1 Announce Type: new 
Abstract: A reduced order asymptotic homogenization based multiscale technique which can capture damage and inelastic effects in composite materials is proposed. This technique is based on two scale homogenization procedure where eigen strain representation accounts for the inelastic response and the computational efforts are alleviated by reduction of order technique. Macroscale stress is derived by calculating the influence tensors from the analysis of representative volume element (RVE). At microscale, the damage in the material is modeled using continuum damage mechanics (CDM) based framework. To solve the problem of strain localization a method of the alteration of stress-strain relation of micro con- stituents based on the dissipated fracture energy in a crack band is implemented. The issue of spurious post failure artificial stiffness at macroscale is discussed and effect of increasing the order to alleviate this problem is checked. Verification studies demonstrated the proposed formulation predicts the macroscale response and also captures the damage and plasticity induced inelastic strains.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E$^2$-TFA based multiscale analysis of failure in elasto-plastic composites</title>
<link>https://arxiv.org/abs/2509.16211</link>
<guid>https://arxiv.org/abs/2509.16211</guid>
<content:encoded><![CDATA[
<div> homogenization, elastoplastic composite materials, eigenstrain field, transformation field analysis, computational efficiency
Summary: 
The paper presents a novel homogenization methodology, $\mathtt{E}^2$-TFA, for analyzing the failure of elastoplastic composite materials. This technique incorporates the microscopic eigenstrain field to account for intra-phase damage and inelastic strains, overcoming the post-damage stiffness response limitation of traditional TFA-based methods. By utilizing elastic and eigen transformation functions, the model achieves computational efficiency and employs a reduced order modeling approach with a piecewise constant eigenstrain field. The performance of the model is evaluated through simulations on representative volume elements and various composites under complex load histories. The model accurately predicts the nonlinear shear stress-strain response of a glass fiber composite and compared well with experimental data on fracture initiation parameters, failure plane orientation, and strain histories. Overall, $\mathtt{E}^2$-TFA is shown to effectively capture damage and inelastic deformations, providing a more accurate estimation of the mechanical response in composite materials. <div>
arXiv:2509.16211v1 Announce Type: new 
Abstract: This paper describes a novel homogenization methodology for analyzing the failure of elastoplastic composite materials based on elastic and eigen influence tensors-driven transformation field analysis ($\mathtt{E}^2$-TFA). The proposed technique considers the microscopic eigenstrain field accounting for intra-phase damage and inelastic strains. This results in realistic computations by alleviating the post-damage stiffness response, which is a drawback of TFA-based methods. We attain computational efficiency by identifying the preprocessing data solely from the elastic and eigen transformation functions and adopting a reduced order modelling technique with a piecewise constant eigenstrain field throughout the subdomains. The performance of the model is assessed by simulating the response for (a) the representative volume element (RVE) as a homogenized continuum and (b) the various composites under complex load histories with intricate macroscale morphologies. Furthermore, the nonlinear shear stress-strain response of a glass fiber composite is calculated and compared to experimentally measured fracture initiation parameters, failure plane orientation, and strain histories. Finally, we show that $\mathtt{E}^2$-TFA can accurately and efficiently capture damage and inelastic deformations in order to estimate the mechanical response of composite materials in a better way.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An efficient framework for computing sensitivity of modal-related structural dynamic characteristics with multi-parameters</title>
<link>https://arxiv.org/abs/2509.16214</link>
<guid>https://arxiv.org/abs/2509.16214</guid>
<content:encoded><![CDATA[
<div> Keywords: structural dynamics, eigenmode sensitivity, sensitivity analysis, iterative method, computational efficiency 

Summary:
In this paper, a novel strategy is presented for computing the sensitivity of structural dynamic characteristics related to eigenmodes with multiple parameters. The method involves an algebraic approach to simplify the computation of eigenvector sensitivity, followed by the development of a framework for sensitivity analysis. By incorporating a preconditioning iterative method, the computational efficiency is enhanced, reducing the CPU computational time. The framework is user-friendly and minimizes the "Fill-in" operations of sparse matrices. Three numerical examples demonstrate the effectiveness of the algorithm in reducing computational time. This novel strategy provides a valuable tool for engineers in various fields to analyze and optimize structural dynamic characteristics related to eigenmodes efficiently. 

<br /><br />Summary: <div>
arXiv:2509.16214v1 Announce Type: new 
Abstract: The sensitivity of structural dynamic characteristics related to eigenmode (such as modal assurance criteria, modal flexibility, and modal mass etc.) has become a crucial and widely applied tool across various engineering fields. In this paper, a novel strategy is proposed for solving the sensitivity of structural dynamic characteristics related to eigenmode with respect to multiple variables. First, an algebraic method for computing the sensitivity of eigenvectors is developed to simplify the expression for sensitivity calculations. Subsequently, based on this new expression for eigenmode sensitivity, a framework for sensitivity analysis of structural dynamic characteristics related to eigenmodes with multiple parameters is established. With the incorporation of a preconditioning iterative method, the new computational framework effectively enhances the computational efficiency of sensitivity analysis for structural characteristics related to eigenmodes with multiple parameters. This framework is easy to operate and effectively reduces the "Fill-in" operations of sparse matrices. Three numerical examples are given to illustrate the effectiveness of the algorithm. The result shows that the novel strategy can significantly reduce central processing unit (CPU) computational time.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Detection of Internal Defects in Structured Media</title>
<link>https://arxiv.org/abs/2509.16216</link>
<guid>https://arxiv.org/abs/2509.16216</guid>
<content:encoded><![CDATA[
<div> Keywords: structural integrity, defects, imaging strategy, Laplace transform, optimization<br />
Summary:<br />
- Engineers face challenges in detecting internal fractures in structures using traditional methods like visual and audible aids.<br />
- This research proposes a strategy to image defects in structures using minimal, non-invasive measurements based on a one-dimensional wave equation model.<br />
- The study focuses on a homogeneous bar with a defect in Young's modulus, treating the problem as a spring-mass vibrational system.<br />
- The proposed imaging strategy utilizes MATLAB to collect synthetic data for various scenarios with defects, optimizing a residual function to identify defect location and stiffness.<br />
- By establishing an analytic map between defect characteristics and measurement data, engineers can effectively detect and assess the severity of fractures in structures. <br /> <div>
arXiv:2509.16216v1 Announce Type: new 
Abstract: A critical issue that affects engineers trying to assess the structural integrity of various infrastructures, such as metal rods or acoustic ducts, is the challenge of detecting internal fractures (defects). Traditionally, engineers depend on audible and visual aids to identify these fractures, as they do not physically dissect the object in question into multiple pieces to check for inconsistencies. This research introduces ideas towards the development of a robust strategy to image such defects using only a small set of minimal, non-invasive measurements.
  Assuming a one dimensional model (e.g. longitudinal waves in long and thin rods/acoustic ducts or transverse vibrations of strings), we make use of the continuous one-dimensional wave equation to model these physical phenomena and then employ specialized mathematical analysis tools (the Laplace transform and optimization) to introduce our defect imaging ideas. In particular, we will focus on the case of a long bar which is homogeneous throughout except in a small area where a defect in its Young's modulus is present. We will first demonstrate how the problem is equivalent to a spring-mass vibrational system, and then show how our imaging strategy makes use of the Laplace domain analytic map between the characteristics of the respective defect and the measurement data.
  More explicitly, we will utilize MATLAB (a platform for numerical computations) to collect synthetic data (computational alternative to real world measurements) for several scenarios with one defect of arbitrary location and stiffness. Subsequently, we will use this data along with our analytically developed map (between defect characteristics and measurements) to construct a residual function which, once optimized, will reveal the location and magnitude of the stiffness defect.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Transient Nonlinear Circuit Simulator Using Exponential Integration and Block-Jacobi Precondition</title>
<link>https://arxiv.org/abs/2509.16219</link>
<guid>https://arxiv.org/abs/2509.16219</guid>
<content:encoded><![CDATA[
<div> regularization, exponential integrators, matrix exponential vector products, structured block-Jacobi preconditioner, Additive Schwarz strategy <br />
Summary:
The article introduces a novel method for transient simulation of linear and nonlinear circuits in EDA tools. By applying a generalized row-echelon regularization approach, the method extends the use of exponential integrators to a wider range of differential algebraic equations, enabling larger time step sizes while maintaining accuracy and stability. To improve efficiency, a structured block-Jacobi preconditioner is designed for linear systems, and an Additive Schwarz strategy is employed for locally coupled circuits. Numerical experiments demonstrate significant speedup in computation time and reduced time steps compared to traditional methods, showcasing the potential for scalable and efficient nonlinear circuit simulation. <br /><br /> <div>
arXiv:2509.16219v1 Announce Type: new 
Abstract: Transient simulation of linear and nonlinear circuits remains an important task in modern EDA tools. At present, SPICE-like simulators face challenges in parallelization, nonlinear convergence and linear efficiency, especially when applied to large-scale circuits. To address the limitations of simulators in handling various nonlinear circuits, we adopt a generalized row-echelon regularization approach, which extends the applicability of exponential integrators to a broader class of differential algebraic equations. The proposed method employs matrix exponential vector products to integrate the regularized system, allowing for a larger time step size while preserving accuracy and stability. Furthermore, in order to accelerate GMRES-based solvers within Newton-Raphson iterations, a structured block-Jacobi preconditioner is designed for linear systems. For locally coupled circuits, Additive Schwarz overlapping strategy is adopted to enhance the solution performance. Numerical experiments of various nonlinear circuit models show that under same hardware environment, our method achieves a speedup of 1.95$\times$-- 3.27$\times$ in total computation time compared to Backward Euler with Inexact Newton iterations, and time steps have decreased by an average of 60.70\% (up to 74.59\%). Compared with EI-NK method, total computing time of our method has a speedup of 1.08$\times$-- 1.79$\times$. These results highlight the potential of proposed method for scalable and nonlinear circuit simulation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Open Dataset for Temperature Modelling in Machine Tools</title>
<link>https://arxiv.org/abs/2509.16222</link>
<guid>https://arxiv.org/abs/2509.16222</guid>
<content:encoded><![CDATA[
<div> dataset, transient thermal simulations, machine learning, deep learning, mechanical systems

Summary:
This article introduces a structured dataset of transient thermal simulations for a vertical axis of a machine tool test rig. The dataset includes temperature and heat flux values recorded at 29 probe locations over 1800 time steps. The simulations were derived from a fractional factorial design and aim to support machine learning and deep learning applications in thermal modelling. The dataset provides detailed information on material, mesh, and boundary conditions, as well as summary statistics, thermal evolution plots, and correlation matrix analyses. A reproducible Jupyter notebook is also included to support research and model development. The data is designed to help predict, correct, and compensate for thermally induced deviations in mechanical systems, making it valuable for researchers without finite element expertise. <div>
arXiv:2509.16222v1 Announce Type: new 
Abstract: This data set descriptor introduces a structured, high-resolution dataset of transient thermal simulations for a vertical axis of a machine tool test rig. The data set includes temperature and heat flux values recorded at 29 probe locations at 1800 time steps, sampled every second over a 30-minute range, across 17 simulation runs derived from a fractional factorial design. First, a computer-aided design model was de-featured, segmented, and optimized, followed by finite element (FE) modelling. Detailed information on material, mesh, and boundary conditions is included. To support research and model development, the dataset provides summary statistics, thermal evolution plots, correlation matrix analyses, and a reproducible Jupyter notebook. The data set is designed to support machine learning and deep learning applications in thermal modelling for prediction, correction, and compensation of thermally induced deviations in mechanical systems, and aims to support researchers without FE expertise by providing ready-to-use simulation data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Increasing Inter-Fiber Contact in the Altendorf-Jeulin Model</title>
<link>https://arxiv.org/abs/2509.16225</link>
<guid>https://arxiv.org/abs/2509.16225</guid>
<content:encoded><![CDATA[
<div> fiber materials, simulations, digital twins, inter-fiber contacts, parametric models

Summary:
- Fiber materials are crucial in fields like material design and biomedicine.
- Fiber simulations, known as digital twins, help in testing material behavior digitally.
- Inter-fiber contacts can impact the thermal and mechanical behavior of fiber systems.
- Existing parametric fiber models do not allow explicit modeling of the number of inter-fiber contacts.
- The proposed extension of the force-biased fiber packing model by Altendorf & Jeulin includes explicit modeling of inter-fiber contacts and an additional force to increase contacts.
- The packing is validated for parameter accuracy and shown to increase the number of contacts, potentially enhancing the accuracy of physical simulations. 

<br /><br />Summary: <div>
arXiv:2509.16225v1 Announce Type: new 
Abstract: In fields such as material design or biomedicine, fiber materials play an important role. Fiber simulations, also called digital twins, provide a basis for testing and optimizing the material's physical behavior digitally. Inter-fiber contacts can influence the thermal and mechanical behavior of a fiber system; to our knowledge, however, there exist no parametric fiber models allowing for explicit modeling of the number of inter-fiber contacts. Therefore, this paper proposes an extension of the iterative force-biased fiber packing by Altendorf \& Jeulin. In this extension, we model the inter-fiber contacts explicitly and add another force to the force-biased packing to increase the number of contacts. We successfully validate the packing with respect to its parameter accuracy. Moreover, we show that the extension indeed increases the number of contacts, even exceeding theoretical values. Hence, this packing scheme has the potential to achieve higher accuracy in physical simulations.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Rank Risky Investors: A Case Study of Predicting Retail Traders' Behaviour and Profitability</title>
<link>https://arxiv.org/abs/2509.16616</link>
<guid>https://arxiv.org/abs/2509.16616</guid>
<content:encoded><![CDATA[
<div> algorithm, financial markets, risk management, trader identification, profit-aware

Summary:
The article introduces a profit-aware risk ranker (PA-RiskRanker) for identifying risky traders in financial markets. Traditional methods often struggle to capture the complexity and dynamism of individual trader behaviors. PA-RiskRanker reframes the problem as a ranking task using Learning-to-Rank algorithms, incorporating profit and loss considerations through a Profit-Aware binary cross entropy loss function. It utilizes a transformer-based ranker with self-cross-trader attention to capture intra- and inter-trader relationships. The research highlights the limitations of existing deep learning-based algorithms in trading risk management and emphasizes the importance of profit and loss in financial scenarios. PA-RiskRanker outperforms state-of-the-art ranking models like Rankformer, showing an 8.4% increase in F1 score and a 10%-17% increase in average profit compared to benchmark models. <div>
arXiv:2509.16616v1 Announce Type: new 
Abstract: Identifying risky traders with high profits in financial markets is crucial for market makers, such as trading exchanges, to ensure effective risk management through real-time decisions on regulation compliance and hedging. However, capturing the complex and dynamic behaviours of individual traders poses significant challenges. Traditional classification and anomaly detection methods often establish a fixed risk boundary, failing to account for this complexity and dynamism. To tackle this issue, we propose a profit-aware risk ranker (PA-RiskRanker) that reframes the problem of identifying risky traders as a ranking task using Learning-to-Rank (LETOR) algorithms. Our approach features a Profit-Aware binary cross entropy (PA-BCE) loss function and a transformer-based ranker enhanced with a self-cross-trader attention pipeline. These components effectively integrate profit and loss (P&amp;L) considerations into the training process while capturing intra- and inter-trader relationships. Our research critically examines the limitations of existing deep learning-based LETOR algorithms in trading risk management, which often overlook the importance of P&amp;L in financial scenarios. By prioritising P&amp;L, our method improves risky trader identification, achieving an 8.4% increase in F1 score compared to state-of-the-art (SOTA) ranking models like Rankformer. Additionally, it demonstrates a 10%-17% increase in average profit compared to all benchmark models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rational Multi-Modal Transformers for TCR-pMHC Prediction</title>
<link>https://arxiv.org/abs/2509.17305</link>
<guid>https://arxiv.org/abs/2509.17305</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, TCR-pMHC interactions, explainability method, encoder-decoder, deep learning <br />
<br />Summary: 
This article introduces a novel approach using a transformer-based model to predict T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes. By employing a new explainability method, the model optimizes cross-attention strategies, incorporates additional training objectives, and introduces an early-stopping criterion based on explanation quality. This framework achieves improved predictive performance while enhancing explainability, robustness, and generalization. The approach establishes a principled, explanation-driven strategy for modeling TCR-pMHC binding and offers insights into sequence-level binding behavior through deep learning techniques. <div>
arXiv:2509.17305v1 Announce Type: new 
Abstract: T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is fundamental to adaptive immunity and central to the development of T cell-based immunotherapies. While transformer-based models have shown promise in predicting TCR-pMHC interactions, most lack a systematic and explainable approach to architecture design. We present an approach that uses a new post-hoc explainability method to inform the construction of a novel encoder-decoder transformer model. By identifying the most informative combinations of TCR and epitope sequence inputs, we optimize cross-attention strategies, incorporate auxiliary training objectives, and introduce a novel early-stopping criterion based on explanation quality. Our framework achieves state-of-the-art predictive performance while simultaneously improving explainability, robustness, and generalization. This work establishes a principled, explanation-driven strategy for modeling TCR-pMHC binding and offers mechanistic insights into sequence-level binding behavior through the lens of deep learning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$i$MIND: Insightful Multi-subject Invariant Neural Decoding</title>
<link>https://arxiv.org/abs/2509.17313</link>
<guid>https://arxiv.org/abs/2509.17313</guid>
<content:encoded><![CDATA[
<div> Keywords: visual signals, neural decoding, brain activity, semantic decoding, neural interpretation

Summary: 
The study introduces the iMIND model, focusing on decoding visual signals to understand brain cognition and perception. It utilizes a dual-decoding framework that combines biometric and semantic decoding to offer neural interpretability and deepen insights into visual processing mechanisms. The iMIND model consists of three key steps: creating a shared neural representation space across subjects, disentangling neural features into subject-specific and object-specific components, and performing dual decoding for biometric and semantic tasks. Experimental results show that iMIND achieves top decoding performance with minimal scalability constraints. It generates voxel-object activation fingerprints that uncover object-specific neural patterns and allow for the exploration of subject-specific attention variations to the same stimuli. This work lays the groundwork for more interpretable and generalizable subject-invariant neural decoding, enhancing knowledge of voxel semantic selectivity and neural vision processing dynamics. 

<br /><br />Summary: <div>
arXiv:2509.17313v1 Announce Type: new 
Abstract: Decoding visual signals holds the tantalizing potential to unravel the complexities of cognition and perception. While recent studies have focused on reconstructing visual stimuli from neural recordings to bridge brain activity with visual imagery, existing methods offer limited insights into the underlying mechanisms of visual processing in the brain. To mitigate this gap, we present an \textit{i}nsightful \textbf{M}ulti-subject \textbf{I}nvariant \textbf{N}eural \textbf{D}ecoding ($i$MIND) model, which employs a novel dual-decoding framework--both biometric and semantic decoding--to offer neural interpretability in a data-driven manner and deepen our understanding of brain-based visual functionalities. Our $i$MIND model operates through three core steps: establishing a shared neural representation space across subjects using a ViT-based masked autoencoder, disentangling neural features into complementary subject-specific and object-specific components, and performing dual decoding to support both biometric and semantic classification tasks. Experimental results demonstrate that $i$MIND achieves state-of-the-art decoding performance with minimal scalability limitations. Furthermore, $i$MIND empirically generates voxel-object activation fingerprints that reveal object-specific neural patterns and enable investigation of subject-specific variations in attention to identical stimuli. These findings provide a foundation for more interpretable and generalizable subject-invariant neural decoding, advancing our understanding of the voxel semantic selectivity as well as the neural vision processing dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction</title>
<link>https://arxiv.org/abs/2509.16369</link>
<guid>https://arxiv.org/abs/2509.16369</guid>
<content:encoded><![CDATA[
<div> Keywords: financial, retrieval, AI, Multi-HyDE, accuracy

Summary:
The article introduces a framework for financial Retrieval Augmented Generation (RAG) that utilizes agentic AI and the Multi-HyDE system to enhance knowledge retrieval in financial question-answering. Traditional systems are insufficient for handling the complexity of financial data sources, which require sophisticated approaches. The RAG framework focuses on token efficiency and multi-step financial reasoning to improve accuracy by 11.2% and reduce hallucinations by 15%. By integrating domain-specific retrieval mechanisms like Multi-HyDE and robust toolsets, including keyword and table-based retrieval, the framework significantly enhances the accuracy and reliability of answers in financial QA benchmarks. The research emphasizes the importance of structured agent workflows and multi-perspective retrieval for the trustworthy deployment of AI in high-stakes financial applications.

<br /><br />Summary: <div>
arXiv:2509.16369v1 Announce Type: cross 
Abstract: Accurate and reliable knowledge retrieval is vital for financial question-answering, where continually updated data sources and complex, high-stakes contexts demand precision. Traditional retrieval systems rely on a single database and retriever, but financial applications require more sophisticated approaches to handle intricate regulatory filings, market analyses, and extensive multi-year reports. We introduce a framework for financial Retrieval Augmented Generation (RAG) that leverages agentic AI and the Multi-HyDE system, an approach that generates multiple, nonequivalent queries to boost the effectiveness and coverage of retrieval from large, structured financial corpora. Our pipeline is optimized for token efficiency and multi-step financial reasoning, and we demonstrate that their combination improves accuracy by 11.2% and reduces hallucinations by 15%. Our method is evaluated on standard financial QA benchmarks, showing that integrating domain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets, including keyword and table-based retrieval, significantly enhances both the accuracy and reliability of answers. This research not only delivers a modular, adaptable retrieval framework for finance but also highlights the importance of structured agent workflows and multi-perspective retrieval for trustworthy deployment of AI in high-stakes financial applications.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction from PPG</title>
<link>https://arxiv.org/abs/2509.16491</link>
<guid>https://arxiv.org/abs/2509.16491</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, physiological data, fine-tuning, demographic fairness, heart rate prediction

Summary:
Fine-tuning foundation models pretrained on physiological data, such as photoplethysmography signals, can significantly improve heart rate prediction accuracy but may widen demographic fairness gaps, especially in different deployment domains. The study introduces FairTune, a bias-aware fine-tuning framework that includes mitigation strategies like class weighting, Group Distributionally Robust Optimization, and adversarial debiasing. These strategies effectively reduce fairness gaps without compromising prediction accuracy, with results varying based on the deployment domain. Representation analyses show that mitigation techniques reshape internal embeddings to reduce demographic clustering. The findings emphasize the necessity of explicit mitigation for equitable deployment of physiological foundation models.<br /><br />Summary: Fine-tuning can enhance HR prediction accuracy but may widen demographic fairness gaps, FairTune framework includes effective mitigation strategies, such as class weighting and GroupDRO, which reshape internal embeddings to reduce demographic clustering. <div>
arXiv:2509.16491v1 Announce Type: cross 
Abstract: Foundation models pretrained on physiological data such as photoplethysmography (PPG) signals are increasingly used to improve heart rate (HR) prediction across diverse settings. Fine-tuning these models for local deployment is often seen as a practical and scalable strategy. However, its impact on demographic fairness particularly under domain shifts remains underexplored. We fine-tune PPG-GPT a transformer-based foundation model pretrained on intensive care unit (ICU) data across three heterogeneous datasets (ICU, wearable, smartphone) and systematically evaluate the effects on HR prediction accuracy and gender fairness. While fine-tuning substantially reduces mean absolute error (up to 80%), it can simultaneously widen fairness gaps, especially in larger models and under significant distributional characteristics shifts. To address this, we introduce FairTune, a bias-aware fine-tuning framework in which we benchmark three mitigation strategies: class weighting based on inverse group frequency (IF), Group Distributionally Robust Optimization (GroupDRO), and adversarial debiasing (ADV). We find that IF and GroupDRO significantly reduce fairness gaps without compromising accuracy, with effectiveness varying by deployment domain. Representation analyses further reveal that mitigation techniques reshape internal embeddings to reduce demographic clustering. Our findings highlight that fairness does not emerge as a natural byproduct of fine-tuning and that explicit mitigation is essential for equitable deployment of physiological foundation models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cost-Effective ZK-Rollups: Modeling and Optimization of Proving Infrastructure</title>
<link>https://arxiv.org/abs/2509.16581</link>
<guid>https://arxiv.org/abs/2509.16581</guid>
<content:encoded><![CDATA[
<div> Zero-knowledge rollups, Halo2-based proving systems, cost model, Z3 SMT solver, simulator <br />
Summary:
Zero-knowledge rollups face challenges with costly hardware requirements, finality constraints, and rising transaction throughput. This study focuses on cost drivers such as transactions per second, gas usage, and finality time. A parametric cost model is proposed to optimize configurations and ensure provers can handle transaction loads efficiently. The model is formulated as a constraint system and solved using the Z3 SMT solver. A simulator is implemented to detect delays and estimate operational costs, showing a potential cost reduction of up to 70%. <div>
arXiv:2509.16581v1 Announce Type: cross 
Abstract: Zero-knowledge rollups rely on provers to generate multi-step state transition proofs under strict finality and availability constraints. These steps require expensive hardware (e.g., GPUs), and finality is reached only once all stages complete and results are posted on-chain. As rollups scale, staying economically viable becomes increasingly difficult due to rising throughput, fast finality demands, volatile gas prices, and dynamic resource needs. We base our study on Halo2-based proving systems and identify transactions per second (TPS), average gas usage, and finality time as key cost drivers. To address this, we propose a parametric cost model that captures rollup-specific constraints and ensures provers can keep up with incoming transaction load. We formulate this model as a constraint system and solve it using the Z3 SMT solver to find cost-optimal configurations. To validate our approach, we implement a simulator that detects lag and estimates operational costs. Our method shows a potential cost reduction of up to 70\%.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KANO: Kolmogorov-Arnold Neural Operator</title>
<link>https://arxiv.org/abs/2509.16825</link>
<guid>https://arxiv.org/abs/2509.16825</guid>
<content:encoded><![CDATA[
<div> Kolmogorov--Arnold Neural Operator, dual-domain neural operator, spectral bases, spatial bases, symbolic interpretability <br />
Summary:<br />
The Kolmogorov--Arnold Neural Operator (KANO) is introduced as a neural operator that combines spectral and spatial bases, providing intrinsic symbolic interpretability. KANO addresses the limitations of the Fourier Neural Operator (FNO) by remaining expressive over position-dependent dynamics, unlike FNO, which is only practical for spectrally sparse operators. Empirical verification shows that KANO generalizes robustly in position-dependent differential operators, while FNO does not. In a quantum Hamiltonian learning benchmark, KANO accurately reconstructs ground-truth Hamiltonians with closed-form symbolic representations and low state infidelity from projective measurement data. KANO significantly outperforms FNO in terms of state infidelity, highlighting its superior performance in capturing dynamics accurately. <br /> <div>
arXiv:2509.16825v1 Announce Type: cross 
Abstract: We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over generic position-dependent dynamics for any physical input, whereas FNO stays practical only for spectrally sparse operators and strictly imposes a fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\approx 6\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\approx 1.5\times10^{-2}$, by orders of magnitude.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability matters: The effect of liability rules on the healthcare sector</title>
<link>https://arxiv.org/abs/2509.17334</link>
<guid>https://arxiv.org/abs/2509.17334</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainability, Artificial Intelligence, Healthcare, Liability, Legal Framework

Summary: 
Explainability of artificial intelligence systems (AIS) is crucial in critical sectors like healthcare. This perspective analyzes the impact of explainability on liability determinations in healthcare settings. Contrasting the extreme cases of an "Oracle" AIS without explainability and an "AI Colleague" AIS with explainability, the discussion explores how automation and explainability affect liability assignment between medical practitioners, healthcare facilities, and AIS manufacturers. The article argues that explainability is essential in establishing a responsibility framework in healthcare, shaping behavior, and reducing the risk of defensive medicine practices. Ultimately, the level of explainability in AIS is vital for legal considerations and improving overall outcomes in healthcare. 

<br /><br />Summary: <div>
arXiv:2509.17334v1 Announce Type: cross 
Abstract: Explainability, the capability of an artificial intelligence system (AIS) to explain its outcomes in a manner that is comprehensible to human beings at an acceptable level, has been deemed essential for critical sectors, such as healthcare. Is it really the case? In this perspective, we consider two extreme cases, ``Oracle'' (without explainability) versus ``AI Colleague'' (with explainability) for a thorough analysis. We discuss how the level of automation and explainability of AIS can affect the determination of liability among the medical practitioner/facility and manufacturer of AIS. We argue that explainability plays a crucial role in setting a responsibility framework in healthcare, from a legal standpoint, to shape the behavior of all involved parties and mitigate the risk of potential defensive medicine practices.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures</title>
<link>https://arxiv.org/abs/2509.17734</link>
<guid>https://arxiv.org/abs/2509.17734</guid>
<content:encoded><![CDATA[
<div> Keywords: forecasting, deep learning, maximum daily temperature, climatology, AutoGluonTS <br />
Summary: <br />
In the field of meteorological forecasting, deep learning has shown significant advancements in predicting the daily average temperature over a ten-day horizon. However, accurately forecasting maximum daily temperatures over short horizons remains a challenge. This study focuses on predicting maximum daily temperatures over medium-term periods of 90 days, approaching the issue from a climatological perspective. Utilizing a large historical dataset from South America and incorporating information from various ocean basins, the AutoGluonTS platform was employed to address the forecasting problem. By framing the problem as a temporal classification task with classes of "above normal," "normal," or "below normal" temperatures, competitive forecasting performance was achieved. AutoGluonTS offers efficient forecasting capabilities with relatively low computational costs compared to other operational platforms, making it a promising tool for addressing complex climatological forecasting challenges. <br /> <div>
arXiv:2509.17734v1 Announce Type: cross 
Abstract: In recent years, great progress has been made in the field of forecasting meteorological variables. Recently, deep learning architectures have made a major breakthrough in forecasting the daily average temperature over a ten-day horizon. However, advances in forecasting events related to the maximum temperature over short horizons remain a challenge for the community. A problem that is even more complex consists in making predictions of the maximum daily temperatures in the short, medium, and long term. In this work, we focus on forecasting events related to the maximum daily temperature over medium-term periods (90 days). Therefore, instead of addressing the problem from a meteorological point of view, this article tackles it from a climatological point of view. Due to the complexity of this problem, a common approach is to frame the study as a temporal classification problem with the classes: maximum temperature "above normal", "normal" or "below normal". From a practical point of view, we created a large historical dataset (from 1981 to 2018) collecting information from weather stations located in South America. In addition, we also integrated exogenous information from the Pacific, Atlantic, and Indian Ocean basins. We applied the AutoGluonTS platform to solve the above-mentioned problem. This AutoML tool shows competitive forecasting performance with respect to large operational platforms dedicated to tackling this climatological problem; but with a "relatively" low computational cost in terms of time and resources.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRFT: Mining High-Frequency Risk Factor Collections End-to-End via Transformer</title>
<link>https://arxiv.org/abs/2408.01271</link>
<guid>https://arxiv.org/abs/2408.01271</guid>
<content:encoded><![CDATA[
<div> Keywords: quantitative trading, neural networks, risk factors, symbolic mathematics, transformer model 

Summary: 
Quantitative trading relies on transforming historical stock data into interpretable risk factors to identify market volatility and risk effectively. While neural networks have advanced in extracting latent risk factors, they lack explicit, formulaic designs. This paper introduces the Intraday Risk Factor Transformer (IRFT) methodology, which treats the mining of risk factors as a language modeling problem using symbolic mathematics. The IRFT model generates complete formulaic risk factors, including constants, without a predefined skeleton of operators. By training on high frequency trading datasets, IRFT determines the general form of stock volatility laws, refining predicted constants using the Broyden Fletcher Goldfarb Shanno (BFGS) algorithm. In comparative analysis, IRFT outperforms existing methods in HF risk factor mining tasks, achieving a 30% higher investment return on datasets like HS300 and SP500 while significantly reducing inference times. <br /><br />Summary: <div>
arXiv:2408.01271v5 Announce Type: replace 
Abstract: In quantitative trading, transforming historical stock data into interpretable, formulaic risk factors enhances the identification of market volatility and risk. Despite recent advancements in neural networks for extracting latent risk factors, these models remain limited to feature extraction and lack explicit, formulaic risk factor designs. By viewing symbolic mathematics as a language where valid mathematical expressions serve as meaningful "sentences" we propose framing the task of mining formulaic risk factors as a language modeling problem. In this paper, we introduce an end to end methodology, Intraday Risk Factor Transformer (IRFT), to directly generate complete formulaic risk factors, including constants. We use a hybrid symbolic numeric vocabulary where symbolic tokens represent operators and stock features, and numeric tokens represent constants. We train a Transformer model on high frequency trading (HFT) datasets to generate risk factors without relying on a predefined skeleton of operators. It determines the general form of the stock volatility law, including constants. We refine the predicted constants using the Broyden Fletcher Goldfarb Shanno (BFGS) algorithm to mitigate non linear issues. Compared to the ten approaches in SRBench, an active benchmark for symbolic regression (SR), IRFT achieves a 30% higher investment return on the HS300 and SP500 datasets, while achieving inference times that are orders of magnitude faster than existing methods in HF risk factor mining tasks.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StockGenChaR: A Study on the Evaluation of Large Vision-Language Models on Stock Chart Captioning</title>
<link>https://arxiv.org/abs/2412.04041</link>
<guid>https://arxiv.org/abs/2412.04041</guid>
<content:encoded><![CDATA[
<div> Keywords: Technical analysis, finance, AI tools, image captioning, stock charts

Summary:<br /><br />
The article focuses on using AI tools to assist non-expert investors in analyzing stock charts for better decision-making. It introduces a new dataset, StockGenChaR, to evaluate large vision-language models in image captioning with stock charts. The main goal is to generate informative descriptions of stock charts that can help investors in understanding market sentiment towards specific stocks. By utilizing AI technology, investors can gain insights into past market data and forecast potential price movements in the future. This can be particularly useful for those who may not have expertise in technical analysis in finance. Overall, the proposed task aims to provide valuable information through image captioning that can assist investors in making informed decisions in the stock market. <div>
arXiv:2412.04041v2 Announce Type: replace 
Abstract: Technical analysis in finance, which aims at forecasting price movements in the future by analyzing past market data, relies on the in- sights that can be gained from the interpretation of stock charts; therefore, non-expert investors could greatly benefit from AI tools that can assist with the captioning of such charts. In our work, we introduce a new dataset StockGenChaR to evaluate large vision-language models in image captioning with stock charts. The purpose of the proposed task is to generate informative descriptions of the depicted charts and help to read the sentiment of the market regarding specific stocks, thus providing useful information for investors
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPH-Net: A Co-Attention Hybrid Model for Accurate Stock Price Prediction</title>
<link>https://arxiv.org/abs/2509.15414</link>
<guid>https://arxiv.org/abs/2509.15414</guid>
<content:encoded><![CDATA[
<div> SPH-Net, Stock Price Prediction, Hybrid Neural Network, Deep Learning, Time Series Forecasting
<br />
Summary: 
SPH-Net is a new deep learning framework designed for stock price prediction that incorporates a co-attention mechanism. It utilizes a Vision Transformer for processing temporal patterns and an attention mechanism for feature extraction, capturing global and local dependencies in market data. The model is evaluated on eight stock datasets using six fundamental market indicators. Results show that SPH-Net outperforms existing models in stock prediction accuracy. Its success lies in effectively capturing complex temporal patterns and maintaining robustness against market noise. This improved accuracy can provide valuable decision-support for investors and financial analysts, improving investment strategies and risk assessment in volatile market conditions. 
<br /> <div>
arXiv:2509.15414v1 Announce Type: new 
Abstract: Prediction of stock price movements presents a formidable challenge in financial analytics due to the inherent volatility, non-stationarity, and nonlinear characteristics of market data. This paper introduces SPH-Net (Stock Price Prediction Hybrid Neural Network), an innovative deep learning framework designed to enhance the accuracy of time series forecasting in financial markets. The proposed architecture employs a novel co-attention mechanism that initially processes temporal patterns through a Vision Transformer, followed by refined feature extraction via an attention mechanism, thereby capturing both global and local dependencies in market data. To rigorously evaluate the model's performance, we conduct comprehensive experiments on eight diverse stock datasets: AMD, Ebay, Facebook, FirstService Corp, Tesla, Google, Mondi ADR, and Matador Resources. Each dataset is standardized using six fundamental market indicators: Open, High, Low, Close, Adjusted Close, and Volume, representing a complete set of features for comprehensive market analysis. Experimental results demonstrate that SPH-Net consistently outperforms existing stock prediction models across all evaluation metrics. The model's superior performance stems from its ability to effectively capture complex temporal patterns while maintaining robustness against market noise. By significantly improving prediction accuracy in financial time series analysis, SPH-Net provides valuable decision-support capabilities for investors and financial analysts, potentially enabling more informed investment strategies and risk assessment in volatile market conditions.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Memory Efficient Adjoint Method to Enable Billion Parameter Optimization on a Single GPU in Dynamic Problems</title>
<link>https://arxiv.org/abs/2509.15744</link>
<guid>https://arxiv.org/abs/2509.15744</guid>
<content:encoded><![CDATA[
<div> sensitivity computations, dynamic optimization, adjoint method, superposition principle, CUDA implementation <br />
Summary: <br />
Dynamic optimization faces limitations due to memory requirements for sensitivity computations relying on full forward and adjoint wave fields. A new approach based on the adjoint method and superposition principle is introduced to approximate sensitivity computations for self-adjoint problems, enabling iterative computation and reducing memory burden to the number of degrees of freedom. This allows for sensitivity computations on GPUs with limited memory capacity, such as the A100 from NVIDIA. The approach is demonstrated on full waveform inversion and transient acoustic topology optimization, utilizing a highly efficient finite difference forward solver implemented in CUDA. However, the technique is limited to self-adjoint problems and does not account for phenomena like damping. <br /> <div>
arXiv:2509.15744v1 Announce Type: new 
Abstract: Dynamic optimization is currently limited by sensitivity computations that require information from full forward and adjoint wave fields. Since the forward and adjoint solutions are computed in opposing time directions, the forward solution must be stored. This requires a substantial amount of memory for large-scale problems even when using check pointing or data compression techniques. As a result, the problem size is memory bound rather than bound by wall clock time, when working with modern GPU-based implementations that have limited memory capacity. To overcome this limitation, we introduce a new approach for approximate sensitivity computation based on the adjoint method (for self-adjoint problems) that relies on the principle of superposition. The approximation allows an iterative computation of the sensitivity, reducing the memory burden to that of the solution at a small number of time steps, i.e., to the number of degrees of freedom. This enables sensitivity computations for problems with billions of degrees of freedom on current GPUs, such as the A100 from NVIDIA (from 2020). We demonstrate the approach on full waveform inversion and transient acoustic topology optimization problems, relying on a highly efficient finite difference forward solver implemented in CUDA. Phenomena such as damping cannot be considered, as the approximation technique is limited to self-adjoint problems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A CARLA-based Simulation of Electrically Driven Forklifts</title>
<link>https://arxiv.org/abs/2509.15909</link>
<guid>https://arxiv.org/abs/2509.15909</guid>
<content:encoded><![CDATA[
<div> Keywords: electric forklift fleet, simulation, intralogistics, CARLA, traffic detection

Summary: 
This paper discusses the simulation of an electric forklift fleet within an intralogistics scenario using the open-source simulation tool CARLA. The study involves generating and visualizing a 3D outdoor warehouse scenario with moving forklifts, simulating intralogistics transport tasks, and playing back localization data from a real forklift fleet. The simulation also includes modeling the energy consumption of forklift trucks using a physical battery model. Two use cases are explored: detecting regions with high traffic densities and optimizing the placement of charging stations for the forklift trucks. Both scenarios are analyzed in an exemplary warehouse model to demonstrate the versatility of the CARLA simulation platform. <div>
arXiv:2509.15909v1 Announce Type: new 
Abstract: This paper presents the simulation of the operation of an electric forklift fleet within an intralogistics scenario. For this purpose, the open source simulation tool CARLA is used; according to our knowledge this is a novel approach in the context of logistics simulation. First, CARLA is used to generate and visualize a realistic 3D outdoor warehouse scenario, incorporating a number of randomly moving forklifts. In a next step, intralogistics transport tasks, such as pick-and-place, are simulated for the forklift fleet, including shortest-path finding. Furthermore, the capability to play back localization data, previously recorded from a ''real'' forklift fleet, is demonstrated.This play back is done in the original recreated environment, thereby enabling the visualization of the forklifts movements. Finally, the energy consumption of the forklift trucks is simulated by integrating a physical battery model that generates the state of charge (SOC) of each truck as a function of load and activity. To demonstrate the wide range of possible applications for the CARLA simulation platform, we describe two use cases. The first deals with the problem of detecting regions with critically high traffic densities, the second with optimal placement of charging stations for the forklift trucks. Both use cases are calculated for an exemplary warehouse model.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Experimental Design of a Moving Sensor for Linear Bayesian Inverse Problems</title>
<link>https://arxiv.org/abs/2509.15961</link>
<guid>https://arxiv.org/abs/2509.15961</guid>
<content:encoded><![CDATA[
<div> optimize, mobile sensor, Bayesian inverse problem, partial differential equation, uncertainty

Summary:
The article focuses on optimizing the path of a mobile sensor to reduce posterior uncertainty in a Bayesian inverse problem. Measurements are taken continuously along the path to estimate the state, modeled as a solution of a partial differential equation with uncertain parameters. The posterior covariance matrix of the model parameters is derived in closed-form for linear PDEs, enabling the formulation of optimal experimental design to minimize uncertainty. A discretization approach is used to maintain cost function consistency under temporal refinement, while constraints ensure obstacle avoidance and path interpretability. The constrained optimization problem is solved using an interior-point method. Computational results for a convection-diffusion equation with unknown initial conditions are presented. <div>
arXiv:2509.15961v1 Announce Type: new 
Abstract: We optimize the path of a mobile sensor to minimize the posterior uncertainty of a Bayesian inverse problem. Along its path, the sensor continuously takes measurements of the state, which is a physical quantity modeled as the solution of a partial differential equation (PDE) with uncertain parameters. Considering linear PDEs specifically, we derive the closed-form expression of the posterior covariance matrix of the model parameters as a function of the path, and formulate the optimal experimental design problem for minimizing the posterior's uncertainty. We discretize the problem such that the cost function remains consistent under temporal refinement. Additional constraints ensure that the path avoids obstacles and remains physically interpretable through a control parameterization. The constrained optimization problem is solved using an interior-point method. We present computational results for a convection-diffusion equation with unknown initial condition.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria</title>
<link>https://arxiv.org/abs/2509.16040</link>
<guid>https://arxiv.org/abs/2509.16040</guid>
<content:encoded><![CDATA[
<div> Sparse regression, constitutive model discovery, automated framework, hyperelasticity, model selection criteria  
Summary:  
- An automated framework for constitutive model discovery is presented, utilizing three sparse regression algorithms and three model selection criteria.  
- The framework pairs algorithms like LASSO, LARS, and OMP with selection criteria including $K$-fold cross-validation, AIC, and BIC.  
- Results show that all nine algorithm-criterion combinations perform well for discovering isotropic and anisotropic materials.  
- LARS efficiently solves the $\ell_1$-constrained problem, while OMP serves as a heuristic for $\ell_0$-regularized selection.  
- The study broadens the range of viable discovery algorithms beyond typical $\ell_1$-based approaches like LASSO.  

<br /><br />Summary: <div>
arXiv:2509.16040v1 Announce Type: cross 
Abstract: The automated discovery of constitutive models from data has recently emerged as a promising alternative to the traditional model calibration paradigm. In this work, we present a fully automated framework for constitutive model discovery that systematically pairs three sparse regression algorithms (Least Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression (LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). This pairing yields nine distinct algorithms for model discovery and enables a systematic exploration of the trade-off between sparsity, predictive performance, and computational cost. While LARS serves as an efficient path-based solver for the $\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for $\ell_0$-regularized selection. The framework is applied to both isotropic and anisotropic hyperelasticity, utilizing both synthetic and experimental datasets. Results reveal that all nine algorithm-criterion combinations perform consistently well for the discovery of isotropic and anisotropic materials, yielding highly accurate constitutive models. These findings broaden the range of viable discovery algorithms beyond $\ell_1$-based approaches such as LASSO.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Loss Balancing in Physics-Informed Neural Networks for Fluid Flow Applications</title>
<link>https://arxiv.org/abs/2509.14437</link>
<guid>https://arxiv.org/abs/2509.14437</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, Partial Differential Equations, Loss Balancing, Navier-Stokes<br />
Summary:<br />
Physics-Informed Neural Networks (PINNs) are used to solve partial differential equations (PDEs) by balancing multiple competing loss terms. Challenges arise in weighting physics residuals, boundary conditions, and initial conditions. Existing loss balancing schemes have been limited to fixed activation functions in neural network architectures. This study introduces trainable activation functions within neural networks and evaluates their effectiveness on complex fluid flow problems represented by Navier-Stokes equations. The proposed solution shows significant root mean square error (RMSE) improvements, ranging from 7.4% to 95.2%, across various scenarios. The results highlight the importance of considering the interplay between activation function selection and balancing algorithms when designing effective loss balancing strategies.<br /> 
Summary: <div>
arXiv:2509.14437v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising machine learning approach for solving partial differential equations (PDEs). However, PINNs face significant challenges in balancing multi-objective losses, as multiple competing loss terms such as physics residuals, boundary conditions, and initial conditions must be appropriately weighted. While various loss balancing schemes have been proposed, they have been implemented within neural network architectures with fixed activation functions, and their effectiveness has been assessed using simpler PDEs. We hypothesize that the effectiveness of loss balancing schemes depends not only on the balancing strategy itself, but also on the neural network's inherent function approximation capabilities, which are influenced by the choice of activation function. In this paper, we extend existing solutions by incorporating trainable activation functions within the neural network architecture and evaluate the proposed approach on complex fluid flow applications modeled by the Navier-Stokes equations. Our evaluation across diverse Navier-Stokes problems demonstrates that this proposed solution achieves root mean square error (RMSE) improvements ranging from 7.4\% to 95.2\% across different scenarios. These findings underscore the importance of carefully considering the interaction between activation function selection and balancing algorithms when designing loss balancing strategies.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lagrangian-Eulerian Multiscale Data Assimilation in Physical Domain based on Conditional Gaussian Nonlinear System</title>
<link>https://arxiv.org/abs/2509.14586</link>
<guid>https://arxiv.org/abs/2509.14586</guid>
<content:encoded><![CDATA[
<div> Keywords: Lagrangian-Eulerian Multiscale Data Assimilation, physical domain, sea ice floe trajectories, two-layer Quasi geostrophic model, Conditional Gaussian Nonlinear System

Summary: 
This research explores Lagrangian-Eulerian Multiscale Data Assimilation (LEMDA) by transitioning from Fourier space to the physical domain. Focusing on sea ice floe trajectories in the Arctic, a two-layer Quasi geostrophic model is used to recover ocean eddies. The model employs Conditional Gaussian Nonlinear System (CGNS) to handle non-linearity effectively. Performance evaluation using normalised root mean square error (RMSE) and pattern correlation (Corr) supports the efficacy of the physical domain approach. Future enhancements, like integrating neural networks (NN) to speed up localized particle recovery in Lagrangian DA, are discussed. Overall, the study demonstrates the benefits of utilizing the two-layer QG model in the physical domain for accurate data assimilation in non-periodic systems. 

<br /><br />Summary: <div>
arXiv:2509.14586v1 Announce Type: new 
Abstract: This research aims to further investigate the process of Lagrangian-Eulerian Multiscale Data Assimilation (LEMDA) by replacing the Fourier space with the physical domain. Such change in the perspective of domain introduces the advantages of being able to deal in non-periodic system and more intuitive representation of localised phenomena or time-dependent problems. The context of the domain for this paper was set as sea ice floe trajectories to recover the ocean eddies in the Arctic regions, which led the model to be derived from two-layer Quasi geostrophic (QG) model. The numerical solution to this model utilises the Conditional Gaussian Nonlinear System (CGNS) to accommodate the inherent non-linearity in analytical and continuous manner. The normalised root mean square error (RMSE) and pattern correlation (Corr) are used to evaluate the performance of the posterior mean of the model. The results corroborate the effectiveness of exploiting the two-layer QG model in physical domain. Nonetheless, the paper still discusses opportunities of improvement, such as deploying neural network (NN) to accelerate the recovery of local particle of Lagrangian DA for the fine scale.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of conductive nonmagnetic objects in presence of the Lenz effect</title>
<link>https://arxiv.org/abs/2509.14976</link>
<guid>https://arxiv.org/abs/2509.14976</guid>
<content:encoded><![CDATA[
<div> dynamics, MRI room, Lenz effect, numerical procedure, experimental data <br />
Summary:<br />
The study aims to model and predict the behavior of conductive nonmagnetic objects in an MRI room influenced by the Lenz effect. By formulating an ordinary differential equation and neglecting the skin effect, the Lenz effect's impact on object dynamics is separated into position and velocity dependencies, enabling the development of a straightforward numerical procedure applicable to objects of any shape. The model's accuracy was confirmed through experiments involving the rotation and translation of an aluminum plate within a 1.5 T MRI scanner. The findings highlight that precise predictions of motion in the presence of the Lenz effect can be achieved by accurately determining induced electric currents in the metal objects during motion steps without considering the skin effect. <div>
arXiv:2509.14976v1 Announce Type: new 
Abstract: Purpose: To model and predict the dynamics of conductive nonmagnetic objects within the MRI room under the influence of Lenz effect. Methods: The dynamics are described by an ordinary differential equation and the Lenz effect approximated by recognizing that the skin effect is negligible. This separated Lenz effect dependency on the object position and velocity, leading to a simple numerical procedure for objects of any shape. Results: The model and numerical procedure were validated with experimental data recording the rotation of an aluminum plate falling inside a 1.5 T MRI scanner. The model was also applied for studying the translation of an aluminum plate pushed with constant force towards the MRI bore through the fringe field. Conclusion: The collected results showed that it is possible to obtain accurate predictions of motion in the presence of Lenz effect by neglecting the skin effect while determining the electric currents induced in the metallic object during each infinitesimal motion step.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Warp Quantification Analysis: A Framework For Path-based Signal Alignment Metrics</title>
<link>https://arxiv.org/abs/2509.14994</link>
<guid>https://arxiv.org/abs/2509.14994</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic time warping, warp quantification analysis, fMRI, schizophrenia, alignment descriptors

Summary: 
Dynamic Time Warping (DTW) is commonly used to align time series with different timescales, but traditional applications only focus on a single distance metric. This study introduces Warp Quantification Analysis (WQA), a novel framework that extracts geometric and structural descriptors from DTW paths. Simulations demonstrate that each metric in WQA accurately tracks specific characteristics without interference from others. When applied to large-scale fMRI data, WQA reveals distinct network signatures and their varied correlations with schizophrenia negative symptom severity. By expanding DTW into a family of alignment descriptors, WQA offers a more comprehensive approach for analyzing temporal coupling in domains requiring nonlinear normalization. WQA enables rich characterization beyond traditional DTW distance measurements, enhancing interpretations and insights in various research fields. 

<br /><br />Summary: <div>
arXiv:2509.14994v1 Announce Type: new 
Abstract: Dynamic time warping (DTW) is widely used to align time series evolving on mismatched timescales, yet most applications reduce alignment to a scalar distance. We introduce warp quantification analysis (WQA), a framework that derives interpretable geometric and structural descriptors from DTW paths. Controlled simulations showed that each metric selectively tracked its intended driver with minimal crosstalk. Applied to large-scale fMRI, WQA revealed distinct network signatures and complementary associations with schizophrenia negative symptom severity, capturing clinically meaningful variability beyond DTW distance. WQA transforms DTW from a single-score method into a family of alignment descriptors, offering a principled and generalizable extension for richer characterization of temporal coupling across domains where nonlinear normalization is essential.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A cell centered Galerkin method for miscible displacement in heterogeneous porous media</title>
<link>https://arxiv.org/abs/2509.14864</link>
<guid>https://arxiv.org/abs/2509.14864</guid>
<content:encoded><![CDATA[
<div> CCG method, miscible displacement, heterogeneous porous media, finite volume, discontinuous Galerkin<br />
<br />
Summary: The paper introduces the cell centered Galerkin (CCG) method for solving miscible displacement problems in porous media. This approach combines finite volume and discontinuous Galerkin methods to achieve an efficient lowest-order approximation, with only one unknown per cell. By utilizing classical DG weak formulations, the CCG method shows comparable accuracy and improved efficiency compared to traditional higher-order interior penalty DG methods. The study also proves that the CCG method produces an inverse-positive matrix for a model Poisson problem in 1D. Computational experiments in 2D and 3D highlight the effectiveness of CCG for highly heterogeneous flow and transport problems, with comparisons to classical DG methods demonstrating its advantages. <br /><br /> <div>
arXiv:2509.14864v1 Announce Type: cross 
Abstract: In this paper we present a cell centered Galerkin (CCG) method applied to miscible displacement problems in heterogeneous porous media. The CCG approach combines concepts from finite volume and discontinuous Galerkin (DG) methods to arrive at an efficient lowest-order approximation (one unknown per cell). We demonstrate that the CCG method can be defined using classical DG weak formulations, only requires one unknown per cell, and is able to deliver comparable accuracy and improved efficiency over traditional higher-order interior penalty DG methods. In addition, we prove that the CCG method for a model Poisson problem gives rise to a inverse-positive matrix in 1D. A plethora of computational experiments in 2D and 3D showcase the effectiveness of the CCG method for highly heterogeneous flow and transport problems in porous media. Comparisons between CCG and classical DG methods are included.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed GCN-LSTM Framework for Long-Term Forecasting of 2D and 3D Microstructure Evolution</title>
<link>https://arxiv.org/abs/2509.15029</link>
<guid>https://arxiv.org/abs/2509.15029</guid>
<content:encoded><![CDATA[
<div> framework, graph convolutional networks, long short-term memory, microstructure evolution, composition-aware <br />
Summary:
This paper introduces a novel framework that combines graph convolutional networks (GCN) and long short-term memory (LSTM) to predict microstructure evolution in 2D and 3D systems effectively. The framework considers the composition of the datasets and operates in latent graph space to efficiently capture composition and morphological changes. By compressing phase-field simulation data with convolutional autoencoders, the model can accurately forecast microstructural evolution in different compositions and dimensions over long time spans. The framework successfully captures spatial and temporal patterns in evolving microstructures, allowing for efficient long-term forecasting post-training. The integration of GCN and LSTM enables robust performance across various evaluation metrics, making it a promising tool for studying microstructure evolution. <br /><br /> <div>
arXiv:2509.15029v1 Announce Type: cross 
Abstract: This paper presents a physics-informed framework that integrates graph convolutional networks (GCN) with long short-term memory (LSTM) architecture to forecast microstructure evolution over long time horizons in both 2D and 3D with remarkable performance across varied metrics. The proposed framework is composition-aware, trained jointly on datasets with different compositions, and operates in latent graph space, which enables the model to capture compositions and morphological dynamics while remaining computationally efficient. Compressing and encoding phase-field simulation data with convolutional autoencoders and operating in Latent graph space facilitates efficient modeling of microstructural evolution across composition, dimensions, and long-term horizons. The framework captures the spatial and temporal patterns of evolving microstructures while enabling long-range forecasting at reduced computational cost after training.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying companies and financial actors exposed to marine tipping points</title>
<link>https://arxiv.org/abs/2411.10307</link>
<guid>https://arxiv.org/abs/2411.10307</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, marine ecosystems, tipping points, fisheries, investors<br />
<br />
Summary: 
Climate change and other anthropogenic pressures are likely to induce tipping points in marine ecosystems, potentially leading to declines in primary productivity and fisheries. Despite increasing attention to nature-related financial risks and opportunities within the ocean economy, the extent to which these tipping points could affect investors has remained largely unexplored. Tracking fishing vessels in areas prone to marine regime shifts revealed key countries, companies, and shareholders exposed to tipping risk. Data gaps were acknowledged, but potential challenges and opportunities for these actors if marine ecosystems shift to less productive states were outlined. <div>
arXiv:2411.10307v2 Announce Type: replace 
Abstract: Climate change and other anthropogenic pressures are likely to induce tipping points in marine ecosystems, potentially leading to declines in primary productivity and fisheries. Despite increasing attention to nature-related financial risks and opportunities within the ocean economy, the extent to which these tipping points could affect investors has remained largely unexplored. Here we used satellite data to track fishing vessels operating in areas prone to marine regime shifts, as identified by their loss of resilience and vulnerability to marine heatwaves, and uncovered their corporate beneficial owners and shareholders. Despite some data gaps, we identified key countries, companies, and shareholders exposed to tipping risk. We also outline the potential challenges and opportunities that these actors may face if marine ecosystems shift to less productive states.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-based deep kernel learning for parameter estimation in high dimensional PDEs</title>
<link>https://arxiv.org/abs/2509.14054</link>
<guid>https://arxiv.org/abs/2509.14054</guid>
<content:encoded><![CDATA[
<div> train, deep kernel learning, Hamiltonian Monte Carlo, parameter inference, uncertainty quantification

Summary: 
This paper proposes a novel two-stage Bayesian framework for inferring parameters of high-dimensional partial differential equations (PDEs). It combines physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to accurately estimate unknown PDE parameters and quantify their uncertainties from sparse observations. In the first stage, a surrogate model is trained using DKL to optimize a neural network feature extractor and provide initial parameter estimates. The second stage uses fixed neural network weights and HMC to sample the joint posterior distribution of kernel hyperparameters and PDE parameters efficiently. Numerical experiments show that the framework accurately estimates parameters, offers reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity. This approach provides a robust and scalable tool for a wide range of scientific and engineering applications.<br /><br />Summary: <div>
arXiv:2509.14054v1 Announce Type: new 
Abstract: Inferring parameters of high-dimensional partial differential equations (PDEs) poses significant computational and inferential challenges, primarily due to the curse of dimensionality and the inherent limitations of traditional numerical methods. This paper introduces a novel two-stage Bayesian framework that synergistically integrates training, physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE parameters and quantify their uncertainties from sparse, exact observations. The first stage leverages physics-based DKL to train a surrogate model, which jointly yields an optimized neural network feature extractor and robust initial estimates for the PDE parameters. In the second stage, with the neural network weights fixed, HMC is employed within a full Bayesian framework to efficiently sample the joint posterior distribution of the kernel hyperparameters and the PDE parameters. Numerical experiments on canonical and high-dimensional inverse PDE problems demonstrate that our framework accurately estimates parameters, provides reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity, offering a robust and scalable tool for diverse scientific and engineering applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Cognitive Bias in Social Agents</title>
<link>https://arxiv.org/abs/2509.13588</link>
<guid>https://arxiv.org/abs/2509.13588</guid>
<content:encoded><![CDATA[
<div> cognitive bias, social simulation, agent behavior, CoBRA, HCI toolkit

Summary:
CoBRA is a new toolkit for specifying agent behavior in LLM-based social simulations. Traditional methods using natural language descriptions often result in inconsistent behaviors that do not accurately represent the intended nuances. CoBRA introduces a novel approach by explicitly programming agents' cognitive biases based on social science experiments. It includes a Cognitive Bias Index that quantifies agents' reactions in validated experiments and a Behavioral Regulation Engine to align behaviors with controlled cognitive bias. Evaluation as an HCI toolkit demonstrated CoBRA's ability to precisely program cognitive bias in social agents across different models. This approach enables accurate representation of cognitive biases in social simulations, enhancing the realism and effectiveness of agent behavior programming. <div>
arXiv:2509.13588v1 Announce Type: cross 
Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying agent behavior in LLM-based social simulation. We found that conventional approaches that specify agent behaviors through implicit natural language descriptions cannot yield consistent behaviors across models, and the produced agent behaviors do not capture the nuances of the descriptions. In contrast, CoBRA presents a new approach to program agents' cognitive biases explicitly, by grounding agents' expected behaviors using classic social science experiments. CoBRA has two components: (1) Cognitive Bias Index that measures the cognitive bias of a social agent, by quantifying the agent's reactions in a set of validated classical social science experiments; (2) Behavioral Regulation Engine that aligns the agent's behavior to demonstrate controlled cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and technical benchmarks. Our results suggest that CoBRA can precisely program the cognitive bias demonstrated in a social agent in a model-agnostic manner.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Meshing Framework for Digital Twins for Extrusion based Additive Manufacturing</title>
<link>https://arxiv.org/abs/2509.12436</link>
<guid>https://arxiv.org/abs/2509.12436</guid>
<content:encoded><![CDATA[
<div> Keywords: additive manufacturing, computational meshes, finite element analysis, toolpath infill, design optimization<br />
Summary:<br />
Additive manufacturing (AM) allows for the production of complex 3D geometries with unique internal microstructures that influence mechanical properties. A new framework is proposed for creating computational meshes suitable for finite element analysis (FEA) of fine-scale features generated by AM tool paths. This framework enables in-depth numerical simulations to assess the impact of internal microstructures on component properties without the need for physical testing. By analyzing toolpath infill, the framework facilitates design optimization for components such as soft elastomeric lattices. This approach reduces time and resource waste typically associated with trial-and-error design cycles and enables the exploration of unconventional design spaces. Overall, the framework enhances the process-structure-property-performance linkage in AM components, opening up possibilities for innovative design solutions. <br /><br />Summary: <div>
arXiv:2509.12436v1 Announce Type: new 
Abstract: Additive manufacturing (AM) allows for manufacturing of complex three-dimensional geometries not typically realizable with standard subtractive manufacturing practices. The internal microstructure of a 3D printed component can have a significant impact on its mechanical, vibrational, and shock properties and allows for a richer design space when this is controllable. Due to the complex interactions of the internal geometry of an extrusion-based AM component, it is common practice to assume a homogeneous behavior or to perform characterization testing on the specific toolpath configurations. To avoid unnecessary testing or material waste, it is necessary to develop an accurate and consistent numerical simulation framework with relevant boundary value problems that can handle the complicated geometry of internal material microstructure present in AM components. Herein, a framework is proposed to directly create computational meshes suitable for finite element analysis (FEA) of the fine-scale features generated from extrusion-based AM tool paths to maintain a strong process-structure-property-performance linkage. This mesh can be manually or automatically analyzed using standard FEA simulations such as quasi-static preloading, modal analysis, or thermal analysis. The framework allows an in-silico assessment of a target AM geometry where fine-scale features may greatly impact quantities of design interest such as in soft elastomeric lattices where toolpath infill can greatly influence the self contact of a structure in compression, which we will use as a motivating exemplar. This approach greatly reduces the waste of both time and resources consumed through traditional build and test design cycles for non-intuitive design spaces. It also further allows for the exploration of toolpath infill to optimize component properties beyond simple linear properties such as density and stiffness.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Language Models for Forecasting Market Impact from Sequences of Financial News</title>
<link>https://arxiv.org/abs/2509.12519</link>
<guid>https://arxiv.org/abs/2509.12519</guid>
<content:encoded><![CDATA[
<div> financial news, stock prices, information diffusion, historical context, large language models

Summary:
- Financial news is a key driver of stock prices and plays a crucial role in information dissemination in financial markets.
- Each news article may require broader historical context for accurate interpretation, posing challenges in identifying relevant information.
- Historical context significantly improves performance in understanding the market impact of financial news across methods and time horizons.
- An efficient method is proposed that uses large language models to process the main article and small models to encode historical context for improved performance.
- Qualitative and quantitative tests reveal insights into the value of contextualization in model behavior and predictions, leading to substantial improvements in simulated investment performance. <div>
arXiv:2509.12519v1 Announce Type: new 
Abstract: Financial news plays a critical role in the information diffusion process in financial markets and is a known driver of stock prices. However, the information in each news article is not necessarily self-contained, often requiring a broader understanding of the historical news coverage for accurate interpretation. Further, identifying and incorporating the most relevant contextual information presents significant challenges. In this work, we explore the value of historical context in the ability of large language models to understand the market impact of financial news. We find that historical context provides a consistent and significant improvement in performance across methods and time horizons. To this end, we propose an efficient and effective contextualization method that uses a large LM to process the main article, while a small LM encodes the historical context into concise summary embeddings that are then aligned with the large model's representation space. We explore the behavior of the model through multiple qualitative and quantitative interpretability tests and reveal insights into the value of contextualization. Finally, we demonstrate that the value of historical context in model predictions has real-world applications, translating to substantial improvements in simulated investment performance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Geometric Uncertainty on the Computation of Abdominal Aortic Aneurysm Wall Strain</title>
<link>https://arxiv.org/abs/2509.12550</link>
<guid>https://arxiv.org/abs/2509.12550</guid>
<content:encoded><![CDATA[
<div> geometry, abdominal aortic aneurysm, wall strain, wall stress, computed tomography angiography<br />
<br />
Summary:<br />
Abdominal aortic aneurysm (AAA) is a life-threatening condition characterized by the permanent enlargement of the aorta. Current management relies on aneurysm diameter and growth rate, which may not accurately predict rupture risk. This study examined the impact of geometric uncertainty on AAA wall strain calculated from 4D-CTA. Results showed that uncertainties in wall geometry reduce the accuracy of computed strain, with inward bias causing greater deviations than outward bias. Peak strain is more sensitive but less robust, while the 99th percentile strain remains stable. To ensure accurate strain estimation, geometric uncertainty should remain within one wall thickness. This research emphasizes the importance of considering geometric uncertainty in AAA risk assessments and highlights the need for precise image-derived geometry in computational analyses. <div>
arXiv:2509.12550v1 Announce Type: new 
Abstract: Abdominal aortic aneurysm (AAA) is a life-threatening condition characterized by permanent enlargement of the aorta, often detected incidentally during imaging for unrelated conditions. Current management relies primarily on aneurysm diameter and growth rate, which may not reliably predict patient-specific rupture risk. Computation of AAA wall stress and strain has the potential to improve individualized risk assessment, but these analyses depend on image-derived geometry, which is subject to segmentation uncertainty and lacks a definitive ground truth for the wall boundary. While the effect of geometric uncertainty on wall stress has been studied, its influence on wall strain remains unclear. In this study, we assessed the impact of geometric uncertainty on AAA wall strain computed using deformable image registration of time-resolved 3D computed tomography angiography (4D-CTA). Controlled perturbations were applied to the wall geometry along the surface normal, parameterized by the standard deviation for random variation and the mean for systematic inward or outward bias, both scaled relative to wall thickness. Results show that uncertainties in AAA wall geometry reduce the accuracy of computed strain, with inward bias (toward the blood lumen and intraluminal thrombus) consistently causing greater deviations than outward bias (toward regions external to the aortic wall). Peak strain is more sensitive but less robust, whereas the 99th percentile strain remains more stable under perturbations. We concluded that, for sufficiently accurate strain estimation, geometric uncertainty should remain within one wall thickness (typically 1.5 mm).
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSentLLM: Multi-LLM and Structured Semantic Signals for Enhanced Financial Sentiment Forecasting</title>
<link>https://arxiv.org/abs/2509.12638</link>
<guid>https://arxiv.org/abs/2509.12638</guid>
<content:encoded><![CDATA[
<div> Sentiment Analysis, Large Language Models, Financial Signals, Stock Markets, Forecasting <br />
Summary: <br />
The study introduces FinSentLLM, a framework that combines multiple large language models and financial signals for financial sentiment analysis. It outperforms existing models on accuracy and F1-score without the need for extensive retraining. The framework shows a 3-6% improvement over baseline models using the Financial PhraseBank dataset. Additionally, econometric analysis using the DCC-GARCH and Johansen cointegration test demonstrates a significant long-term relationship between financial sentiment and stock market movements. This suggests that sentiment signals can effectively predict equity market dynamics in the long run. <div>
arXiv:2509.12638v1 Announce Type: new 
Abstract: Financial sentiment analysis (FSA) has attracted significant attention, and recent studies increasingly explore large language models (LLMs) for this field. Yet most work evaluates only classification metrics, leaving unclear whether sentiment signals align with market behavior. We propose FinSentLLM, a lightweight multi-LLM framework that integrates an expert panel of sentiment forecasting LLMs, and structured semantic financial signals via a compact meta-classifier. This design captures expert complementarity, semantic reasoning signal, and agreement/divergence patterns without costly retraining, yielding consistent 3-6% gains over strong baselines in accuracy and F1-score on the Financial PhraseBank dataset. In addition, we also provide econometric evidence that financial sentiment and stock markets exhibit statistically significant long-run comovement, applying Dynamic Conditional Correlation GARCH (DCC-GARCH) and the Johansen cointegration test to daily sentiment scores computed from the FNSPID dataset and major stock indices. Together, these results demonstrate that FinSentLLM delivers superior forecasting accuracy for financial sentiment and further establish that sentiment signals are robustly linked to long-run equity market dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cost-Optimization Model for EV Charging Stations Utilizing Solar Energy and Variable Pricing</title>
<link>https://arxiv.org/abs/2509.12214</link>
<guid>https://arxiv.org/abs/2509.12214</guid>
<content:encoded><![CDATA[
<div> framework, electric vehicle charging stations, cost optimization, photovoltaic generation, electricity price uncertainty
Summary:
The paper introduces a cost optimization framework for electric vehicle (EV) charging stations that integrates on-site photovoltaic (PV) generation and considers electricity price uncertainty. The model, formulated as a linear program, ensures the satisfaction of vehicle energy demands, adherence to charging and grid capacity constraints, and minimization of procurement cost. Evaluations based on actual charging data demonstrate average savings of approximately 12% compared to a traditional first-come-first-served approach, with potential peak monthly reductions reaching 19.2%. A sensitivity analysis suggests that a slight increase in nominal cost can significantly mitigate worst-case exposure by 14%. Computational tests confirm the practicality and scalability of the proposed solution by successfully solving instances with up to 50 concurrent EVs in under 5 seconds on a standard laptop. This innovative method offers a grid-friendly and efficient approach for future EV charging operations. 
<br /><br />Summary: <div>
arXiv:2509.12214v1 Announce Type: cross 
Abstract: This paper presents a cost optimization framework for electric vehicle (EV) charging stations that leverages on-site photovoltaic (PV) generation and explicitly accounts for electricity price uncertainty through a Bertsimas--Sim robust formulation. The model is formulated as a linear program that satisfies vehicle energy demands, respects charging and grid capacity constraints, and minimizes procurement cost. Evaluations on real charging data from the Caltech ACN dataset show average savings of about 12\% compared to a first-come--first-served baseline, with peak monthly reductions up to 19.2\%. A lightweight sensitivity analysis indicates that a modest $\sim$5\% increase in nominal cost can reduce worst-case exposure by 14\%. Computational tests confirm real-time feasibility, with instances of up to 50 concurrent EVs solved in under 5 seconds on a standard laptop. The proposed method provides a practical, grid-friendly, and scalable solution for future EV charging operations.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End to End Edge to Cloud Data and Analytics Strategy</title>
<link>https://arxiv.org/abs/2509.12296</link>
<guid>https://arxiv.org/abs/2509.12296</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, cloud, edge, data strategy, reference architectures

Summary:
Internet of Things (IoT) devices are rapidly increasing, leading to the need for real-time data for critical decision-making. Enterprises are quickly embracing cloud technology, creating a demand for secure and efficient strategies to maximize cloud and edge capabilities. This paper presents an end-to-end secure edge-to-cloud data and analytics strategy. It includes reference architectures for the device layer, edge layer, and cloud layer, facilitating practical implementation. By addressing the exponential growth of IoT devices, leveraging cloud technology, and ensuring secure data transmission, this strategy aims to optimize the utilization of cloud and edge assets for real-time decision-making. The proposed approach not only enhances data security but also enables efficient data analysis, ultimately contributing to the successful implementation of IoT applications in various industries. 

Summary: <div>
arXiv:2509.12296v1 Announce Type: cross 
Abstract: There is an exponential growth of connected Internet of Things (IoT) devices. These have given rise to applications that rely on real time data to make critical decisions quickly. Enterprises today are adopting cloud at a rapid pace. There is a critical need to develop secure and efficient strategy and architectures to best leverage capabilities of cloud and edge assets. This paper provides an end to end secure edge to cloud data and analytics strategy. To enable real life implementation, the paper provides reference architectures for device layer, edge layer and cloud layer.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparison of pipelines for the translation of a low resource language based on transformers</title>
<link>https://arxiv.org/abs/2509.12514</link>
<guid>https://arxiv.org/abs/2509.12514</guid>
<content:encoded><![CDATA[
<div> transformer-based neural networks, machine translation, Bambara language, low-resource translation, language distillation

Summary:
This work compares three pipelines for training transformer-based neural networks to produce machine translators for the Bambara language. The first pipeline focuses on translating French into Bambara using a simple transformer model. The second pipeline fine-tunes instructor models for French-to-Bambara translation, achieving varying results based on dataset specificity. The third pipeline utilizes language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model for language-agnostic embeddings, followed by a BERT extension for translation. Results show the first pipeline achieves the best accuracy overall, particularly on low-resource datasets. The instructor-based models perform better on individual datasets compared to aggregated collections, indicating their efficacy in capturing dataset-specific patterns. <div>
arXiv:2509.12514v1 Announce Type: cross 
Abstract: This work compares three pipelines for training transformer-based neural networks to produce machine translators for Bambara, a Mand\`e language spoken in Africa by about 14,188,850 people. The first pipeline trains a simple transformer to translate sentences from French into Bambara. The second fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures for French-to-Bambara translation. Models from the first two pipelines were trained with different hyperparameter combinations to improve BLEU and chrF scores, evaluated on both test sentences and official Bambara benchmarks. The third pipeline uses language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model, which provides language-agnostic embeddings. A BERT extension is then applied to LaBSE to generate translations. All pipelines were tested on Dokotoro (medical) and Bayelemagaba (mixed domains). Results show that the first pipeline, although simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on Bayelemagaba), consistent with low-resource translation results. On the Yiri dataset, created for this work, it achieves 33.81% BLEU and 41% chrF. Instructor-based models perform better on single datasets than on aggregated collections, suggesting they capture dataset-specific patterns more effectively.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Pipeline for Patient-Specific Modeling of Thoracic Aortic Aneurysm: From Medical Image to Finite Element Analysis</title>
<link>https://arxiv.org/abs/2509.12596</link>
<guid>https://arxiv.org/abs/2509.12596</guid>
<content:encoded><![CDATA[
<div> Keywords: aorta, thoracic aortic aneurysm, finite element analysis, deep learning, patient-specific modeling

Summary: 
The article discusses the significance of thoracic aortic aneurysms (TAAs) as a leading cause of mortality, emphasizing the importance of accurate diagnosis for treatment. It highlights the use of three-dimensional computed tomography (3D CT) imaging for precise evaluation of aortic geometry and stresses on the aortic wall. Deep learning-based image segmentation is recognized as a reliable method for extracting anatomical structures from medical images. The conversion of segmentation masks into structured mesh representation enables accurate finite element analysis (FEA) simulations, with hexahedral meshes commonly used for efficiency and accuracy in aortic biomechanics. Patient-specific modeling allows for detailed assessment of individual anatomical and biomechanical characteristics, supporting personalized treatment strategies. Developing accurate FE models is crucial for establishing a biomechanically based framework to predict the risk of TAA. <div>
arXiv:2509.12596v1 Announce Type: cross 
Abstract: The aorta is the body's largest arterial vessel, serving as the primary pathway for oxygenated blood within the systemic circulation. Aortic aneurysms consistently rank among the top twenty causes of mortality in the United States. Thoracic aortic aneurysm (TAA) arises from abnormal dilation of the thoracic aorta and remains a clinically significant disease, ranking as one of the leading causes of death in adults. A thoracic aortic aneurysm ruptures when the integrity of all aortic wall layers is compromised due to elevated blood pressure. Currently, three-dimensional computed tomography (3D CT) is considered the gold standard for diagnosing TAA. The geometric characteristics of the aorta, which can be quantified from medical imaging, and stresses on the aortic wall, which can be obtained by finite element analysis (FEA), are critical in evaluating the risk of rupture and dissection. Deep learning based image segmentation has emerged as a reliable method for extracting anatomical regions of interest from medical images. Voxel based segmentation masks of anatomical structures are typically converted into structured mesh representation to enable accurate simulation. Hexahedral meshes are commonly used in finite element simulations of the aorta due to their computational efficiency and superior simulation accuracy. Due to anatomical variability, patient specific modeling enables detailed assessment of individual anatomical and biomechanics behaviors, supporting precise simulations, accurate diagnoses, and personalized treatment strategies. Finite element (FE) simulations provide valuable insights into the biomechanical behaviors of tissues and organs in clinical studies. Developing accurate FE models represents a crucial initial step in establishing a patient-specific, biomechanically based framework for predicting the risk of TAA.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry</title>
<link>https://arxiv.org/abs/2509.12679</link>
<guid>https://arxiv.org/abs/2509.12679</guid>
<content:encoded><![CDATA[
<div> transformer-based NQS, neural quantum states, scaling laws, language models, second-quantized quantum chemistry<br />
Summary: <br />
The study investigates scaling laws for neural quantum states (NQS) incorporating language model components. It aims to understand NQS scalability and optimal performance trade-offs. By analyzing transformer-based NQS in second-quantized quantum chemistry applications, the research identifies scaling laws predicting performance based on problem size. The study delves into the relationship between model size, training time, and performance metrics, such as absolute error and V-score. Unlike language models, the relationship between model size and training time in NQS is highly dependent on the loss metric and ansatz used, challenging the linear relationship observed in traditional language models. This research sheds light on the scalability of NQS ansatze and provides insights into performance-resource trade-offs in quantum chemistry applications. <br /> <div>
arXiv:2509.12679v1 Announce Type: cross 
Abstract: Scaling laws have been used to describe how large language model (LLM) performance scales with model size, training data size, or amount of computational resources. Motivated by the fact that neural quantum states (NQS) has increasingly adopted LLM-based components, we seek to understand NQS scaling laws, thereby shedding light on the scalability and optimal performance--resource trade-offs of NQS ansatze. In particular, we identify scaling laws that predict the performance, as measured by absolute error and V-score, for transformer-based NQS as a function of problem size in second-quantized quantum chemistry applications. By performing analogous compute-constrained optimization of the obtained parametric curves, we find that the relationship between model size and training time is highly dependent on loss metric and ansatz, and does not follow the approximately linear relationship found for language models.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs</title>
<link>https://arxiv.org/abs/2509.12730</link>
<guid>https://arxiv.org/abs/2509.12730</guid>
<content:encoded><![CDATA[
<div> Keywords: digital ecosystems, financial sector, graph machine learning, network analysis, financial crime

Summary:
The article discusses the challenges faced by traditional rule-based systems in detecting sophisticated criminal behaviors in digital ecosystems within the financial sector. It proposes an approach that integrates graph machine learning and network analysis to improve the detection of topological patterns in transactional graphs. The key challenge lies in the sparse and unlabeled information in traditional financial datasets, prompting the development of a four-step preprocessing framework to generate weak ground-truth labels for analysis. Graph Autoencoders are then implemented to distinguish among topological patterns, with three variants compared in the analysis. Preliminary results suggest that this method is effective in detecting complex financial crime schemes, presenting a promising alternative to rule-based detection systems.<br /><br />Summary: <div>
arXiv:2509.12730v1 Announce Type: cross 
Abstract: The rise of digital ecosystems has exposed the financial sector to evolving abuse and criminal tactics that share operational knowledge and techniques both within and across different environments (fiat-based, crypto-assets, etc.). Traditional rule-based systems lack the adaptability needed to detect sophisticated or coordinated criminal behaviors (patterns), highlighting the need for strategies that analyze actors' interactions to uncover suspicious activities and extract their modus operandi. For this reason, in this work, we propose an approach that integrates graph machine learning and network analysis to improve the detection of well-known topological patterns within transactional graphs. However, a key challenge lies in the limitations of traditional financial datasets, which often provide sparse, unlabeled information that is difficult to use for graph-based pattern analysis. Therefore, we firstly propose a four-step preprocessing framework that involves (i) extracting graph structures, (ii) considering data temporality to manage large node sets, (iii) detecting communities within, and (iv) applying automatic labeling strategies to generate weak ground-truth labels. Then, once the data is processed, Graph Autoencoders are implemented to distinguish among the well-known topological patterns. Specifically, three different GAE variants are implemented and compared in this analysis. Preliminary results show that this pattern-focused, topology-driven method is effective for detecting complex financial crime schemes, offering a promising alternative to conventional rule-based detection systems.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanics-Informed Machine Learning for Geospatial Modeling of Soil Liquefaction: Global and National Surrogate Models for Simulation and Near-Real-Time Response</title>
<link>https://arxiv.org/abs/2509.10962</link>
<guid>https://arxiv.org/abs/2509.10962</guid>
<content:encoded><![CDATA[
<div> machine learning, soil liquefaction, geospatial information, high performance computing, regional-scale modeling

Summary:<br />
Using machine learning and geospatial information, surrogate models are developed to predict soil liquefaction at regional scales. Global and New Zealand-specific models are trained to mimic geotechnical models, anchored to mechanics and driven by ML for more predictive information. The models are geostatistically updated by subsurface data and precomputed globally for all earthquake scenarios, making them easy to execute and encouraging user adoption. Test applications show the proposed models outperform others significantly, with geostatistical updating further improving performance. Region-specific models may not offer significant advantages over global datasets. These models are ideal for regional-scale liquefaction hazard simulation and near-real-time response, with accompanying variance products indicating the influence of local geotechnical data on predicted liquefaction response. <br />Summary: <div>
arXiv:2509.10962v1 Announce Type: new 
Abstract: Using machine learning (ML), high performance computing, and a large body of geospatial information, we develop surrogate models to predict soil liquefaction across regional scales. Two sets of models - one global and one specific to New Zealand - are trained by learning to mimic geotechnical models at the sites of in-situ tests. Our geospatial approach has conceptual advantages in that predictions: (i) are anchored to mechanics, which encourages more sensible response and scaling across the domains of soil, site, and loading characteristics; (ii) are driven by ML, which allows more predictive information to be used, with greater potential for it to be exploited; (iii) are geostatistically updated by subsurface data, which anchors the predictions to known conditions; and (iv) are precomputed everywhere on earth for all conceivable earthquakes, which allows the models to be executed very easily, thus encouraging user adoption and evaluation. Test applications suggest that: (i) the proposed models outperform others to a statistically significant degree; (ii) the geostatistical updating further improves performance; and (iii) the anticipated advantages of region-specific models may largely be negated by the benefits of learning from larger global datasets. These models are best suited for regional-scale liquefaction hazard simulation and near-real-time response and are accompanied by variance products that convey where, and to what degree, the ML-predicted liquefaction response is influenced by local geotechnical data.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial AI for Liquefaction Hazard and Impact Forecasting: A Demonstrative Study in the U.S. Pacific Northwest</title>
<link>https://arxiv.org/abs/2509.10965</link>
<guid>https://arxiv.org/abs/2509.10965</guid>
<content:encoded><![CDATA[
<div> machine learning, liquefaction hazard, geospatial model, regional-scale, earthquake

Summary:
- The study focuses on predicting liquefaction hazards in the Pacific Northwest region, specifically in Washington and Oregon, using a geospatial model driven by machine learning.
- The model is able to predict the probability of damaging ground deformation for 85 scenario earthquakes, providing high-resolution forecasts across regional scales.
- The model improves upon prior approaches by incorporating mechanics-based predictions, utilizing machine learning for more geospatial information, and anchoring predictions to known subsurface conditions.
- The resulting liquefaction hazard forecasts are made available in a GIS-ready, public repository for various regional-scale applications such as disaster simulations, evacuation route planning, and infrastructure vulnerability assessments.
- These predictions can be utilized for land-use planning, insurance loss modeling, hazard communication, and public investment prioritization to better prepare for and mitigate the consequences of potential large-magnitude earthquakes in the region.

<br /><br />Summary: <div>
arXiv:2509.10965v1 Announce Type: new 
Abstract: Recent large-magnitude earthquakes have demonstrated the damaging consequences of soil liquefaction and reinforced the need to understand and plan for liquefaction hazards at a regional scale. In the United States, the Pacific Northwest is uniquely vulnerable to such consequences given the potential for crustal, intraslab, and subduction zone earthquakes. In this study, the liquefaction hazard is predicted geospatially at high resolution and across regional scales for 85 scenario earthquakes in the states of Washington and Oregon. This is accomplished using an emergent geospatial model that is driven by machine learning, and which predicts the probability of damaging ground deformation by surrogating state-of-practice geotechnical models. The adopted model shows improved performance and has conceptual advantages over prior regional-scale modeling approaches in that predictions (i) are informed by mechanics, (ii) employ more geospatial information using machine learning, and (iii) are geostatistically anchored to known subsurface conditions. The utility of the resulting predictions for the 85 scenarios is then demonstrated via asset and network infrastructure vulnerability assessments. The liquefaction hazard forecasts are published in a GIS-ready, public repository and are suitable for disaster simulations, evacuation route planning, network vulnerability analysis, land-use planning, insurance loss modeling, hazard communication, public investment prioritization, and other regional-scale applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why "AI" Models for Predicting Soil Liquefaction have been Ignored, Plus Some that Shouldn't Be</title>
<link>https://arxiv.org/abs/2509.10966</link>
<guid>https://arxiv.org/abs/2509.10966</guid>
<content:encoded><![CDATA[
<div> AI, soil liquefaction, prediction models, model development, state-of-practice models

Summary:
- AI liquefaction models are increasingly used but often lack comparison to state-of-practice models, deviate from best practices, may not be used effectively, are presented in a complex manner, and are not provided for use.
- Understanding and addressing these issues can improve the direction and perception of AI in liquefaction research.
- While not all prior efforts are without merit, recognizing recurring shortcomings can guide future research.
- Highlighted papers show applications where AI can add value by enabling new modeling approaches and improving predictions of liquefaction phenomena. 

<br /><br />Summary: <div>
arXiv:2509.10966v1 Announce Type: new 
Abstract: Soil liquefaction remains an important and interesting problem that has attracted the development of enumerable prediction models. Increasingly, these models are utilizing algorithmic learning, or "artificial intelligence" (AI). The rapid growth of AI in the liquefaction literature is unsurprising, given its ease of implementation and potential advantages over traditional statistical methods. However, AI liquefaction models have been widely ignored by practitioners and researchers alike; the objective of this paper is to investigate "why?" Through a sample review of 75 publications, we identify several good reasons. Namely, these models frequently: (i) are not compared to state-of-practice models, making it unclear why they should be adopted; (ii) depart from best practices in model development; (iii) use AI in ways that may not be useful; (iv) are presented in ways that overstate their complexity and make them unapproachable; and (v) are discussed but not actually provided, meaning that no one can use the models even if they wanted to. These prevailing problems must be understood, identified, and remedied, but this does not mean that AI itself is problematic, or that all prior efforts have been without merit or utility. Instead, understanding these recurrent shortcomings can help improve the direction and perceptions of this growing body of work. Towards this end, we highlight papers that are generally free from these shortcomings, and which demonstrate applications where AI is more likely to provide value in the near term: permitting new modeling approaches and potentially improving predictions of liquefaction phenomena.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language model-empowered next-generation computer-aided engineering</title>
<link>https://arxiv.org/abs/2509.11447</link>
<guid>https://arxiv.org/abs/2509.11447</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, computer-aided engineering, model order reduction, autonomous collaborators, parametric analysis<br />
<br />Summary: <br />
The article introduces the concept of using large language models (LLMs) as autonomous collaborators in computer-aided engineering (CAE) to automate and optimize workflows. Specifically, it focuses on data-free model order reduction (MOR) for ultra-fast large-scale parametric analysis. By leveraging LLMs, intrusive MOR, a powerful but underused approach, can be made more accessible and practical through automation of derivations, code restructuring, and implementation. An LLM-empowered CAE agent for solving ultra-large-scale space-parameter-time (S-P-T) physical problems is presented, demonstrating the capability to translate natural language prompts into efficient solver implementations and generate novel MOR solvers for unseen cases. This showcases the potential of LLMs in establishing next-generation CAE systems. <div>
arXiv:2509.11447v1 Announce Type: new 
Abstract: Software development has entered a new era where large language models (LLMs) now serve as general-purpose reasoning engines, enabling natural language interaction and transformative applications across diverse domains. This paradigm is now extending into computer-aided engineering (CAE). Recent applications of LLMs in CAE have successfully automated routine tasks, including CAD model generation and FEM simulations. Nevertheless, these contributions, which primarily serve to reduce manual labor, are often insufficient for addressing the significant computational challenges posed by large-scale, high-dimensional systems. To this aim, we first introduce the concept of LLM-empowered CAE agent, where LLMs act as autonomous collaborators that plan, execute, and adapt CAE workflows. Then, we propose an LLM-empowered CAE agent for data-free model order reduction (MOR), a powerful yet underused approach for ultra-fast large-scale parametric analysis due to the intrusive nature and labor-intensive redevelopment of solvers. LLMs can alleviate this barrier by automating derivations, code restructuring, and implementation, making intrusive MOR both practical and broadly accessible. To demonstrate feasibility, we present an LLM-empowered CAE agent for solving ultra-large-scale space-parameter-time (S-P-T) physical problems using Tensor-decomposition-based A Priori Surrogates (TAPS). Our results show that natural language prompts describing parametric partial differential equations (PDEs) can be translated into efficient solver implementations, substantially reducing human effort while producing high-fidelity reduced-order models. Moreover, LLMs can synthesize novel MOR solvers for unseen cases such as nonlinear and high-dimensional parametric problems based on their internal knowledge base. This highlights the potential of LLMs to establish the foundation for next-generation CAE systems.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward lean industry 5.0: a human-centered model for integrating lean and industry 4.0 in an automotive supplier</title>
<link>https://arxiv.org/abs/2509.11658</link>
<guid>https://arxiv.org/abs/2509.11658</guid>
<content:encoded><![CDATA[
<div> lean, Industry 4.0, human-centered, automotive, case study 

Summary: 
This paper presents a human-centered conceptual model that integrates lean principles and Industry 4.0. It fills a gap in research by offering theoretical insights and practical findings through a case study at an advanced automotive supplier. The study emphasizes the importance of a human-centered approach, identifying key enablers and barriers in lean Industry 4.0 implementation. Through a five-phase multi-method approach, the study examines operational, social, and technological perspectives at both group and model site levels. It illustrates effective implementation strategies and showcases how advanced lean tools can be digitized. The case study identifies 26 positive aspects and 10 negative aspects, showcasing their causal relationships. Successful implementation is shown to benefit organizations and employees when supported by appropriate technological knowledge and people skills, paving the way for lean Industry 5.0. <br /><br /> <div>
arXiv:2509.11658v1 Announce Type: new 
Abstract: This paper proposes a human-centered conceptual model integrating lean and Industry 4.0 based on the literature review and validated it through a case study in the context of an advanced automotive first-tier supplier. Addressing a significant gap in existing research on lean Industry 4.0 implementations, the study provides both theoretical insights and practical findings. It emphasizes the importance of a human-centered approach, identifies key enablers and barriers. In the implementation process of the case study, it is considered at group level and model site level through operational, social and technological perspectives in a five-phase multi-method approach. It shows what effective human-centered lean Industry 4.0 implementation look like and how advanced lean tools can be digitized. It highlights 26 positive and 10 negative aspects of the case and their causal relation. With the appropriate internal and external technological knowhow and people skills, it shows how successful implementation can benefit the organization and employees based on the conceptual model that serves as a first step toward lean Industry 5.0.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Very-low-field MRI scanners: from the ideal to the real permanent magnet array</title>
<link>https://arxiv.org/abs/2509.11762</link>
<guid>https://arxiv.org/abs/2509.11762</guid>
<content:encoded><![CDATA[
<div> MRI, very-low-field, permanent magnets, spatial homogeneity, numerical model<br />
Summary:<br />
Very-low-field MRI technology, utilizing permanent magnets for B0 generation, is gaining popularity due to its portable and cost-effective nature. This article explores the importance of magnet performance in achieving spatial homogeneity of the magnetic field. It investigates factors affecting homogeneity and discrepancies between numerical predictions and actual measurements on fabricated magnets. The study also evaluates the impact of different numerical model approximations on results, highlighting the trade-offs between computational efficiency and result reliability. By providing insights into magnet characterization and model assumptions, this research contributes to enhancing the design and performance of low-cost MRI scanners. <div>
arXiv:2509.11762v1 Announce Type: new 
Abstract: Very-low-field MRIs are becoming increasingly popular due to their portability and adaptability to different environments. They are being successfully used for various clinical applications, leading to a paradigm shift in the way imaging care is typically performed. The development of low-cost MRI scanner prototypes began a few years ago, with some interesting and promising open-source projects emerging in both hardware and software design. Using permanent magnets (PMs) to generate the static magnetic field B0 can substantially reduce the manufacturing cost of low-field scanners while achieving satisfactory homogeneity. This article focuses on characterizing magnet performance in terms of B0 spatial homogeneity. Specifically, it investigates its sensitivity to various factors and explores the reasons for discrepancies between numerical expectations and actual measurements on fabricated magnets. The analysis also examines the consequences of using different numerical model approximations, revisiting concepts most frequently used in other design contexts. While these assumptions simplify the numerical model and may improve its performance in terms of computational time, this paper demonstrates that they also impact the reliability of the obtained results.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hetero-EUCLID: Interpretable model discovery for heterogeneous hyperelastic materials using stress-unsupervised learning</title>
<link>https://arxiv.org/abs/2509.11784</link>
<guid>https://arxiv.org/abs/2509.11784</guid>
<content:encoded><![CDATA[
<div> segmentation, parameter identification, hyperelastic behavior, heterogeneous material, Bayesian-EUCLID

Summary:
The computational framework Hetero-EUCLID is proposed for the segmentation and parameter identification of heterogeneous materials' hyperelastic behavior. Leveraging the Bayesian-EUCLID framework, the approach efficiently solves the heterogenized formulation through model selection with sparsity-promoting priors and Monte Carlo Markov Chain sampling. By utilizing experimentally observable data from non-equi-biaxial tension tests, the framework involves residual force-based segmentation and constitutive parameter identification. Validation shows its capability to segment domains and characterize constituent materials on thin square heterogeneous domains, even with noise in displacement data and non-native mesh discretizations. The framework's potential applications include Digital Image/Volume Correlation-based experimental scenarios like aerospace composites and medical conditions such as fibroatheroma, atherosclerosis, or cancer, offering rapid and interpretable model discovery from a single experiment. <br /><br />Summary: <div>
arXiv:2509.11784v1 Announce Type: new 
Abstract: We propose a computational framework, Hetero-EUCLID, for segmentation and parameter identification to characterize the full hyperelastic behavior of all constituents of a heterogeneous material. In this work, we leverage the Bayesian-EUCLID (Efficient Unsupervised Constitutive Law Identification and Discovery) framework to efficiently solve the heterogenized formulation through parsimonious model selection using sparsity-promoting priors and Monte Carlo Markov Chain sampling. We utilize experimentally observable 3D surface displacement and boundary-averaged force data generated from Finite Element simulations of non-equi-biaxial tension tests on heterogeneous specimens. The framework broadly consists of two steps -- residual force-based segmentation, and constitutive parameter identification. We validate and demonstrate the ability of the proposed framework to segment the domain, and characterize the constituent materials on various types of thin square heterogeneous domains. We validate of the framework's ability to segment and characterize materials with various levels of displacement noises and non-native mesh discretizations, i.e, using different meshes for the forward FE simulations and the inverse EUCLID problem. This demonstrates Hetero-EUCLID framework's applicability in Digital Image/Volume Correlation-based experimental scenarios. Furthermore, the proposed framework performs successful segmentation and material characterizations based on data from a single experiment, thereby making it viable for rapid, interpretable model discovery in domains such as aerospace and defense composites and for characterization of selective tissue stiffening in medical conditions such as fibroatheroma, atherosclerosis, or cancer.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical analysis of fluid estimation for source terms in neutral particles simulation</title>
<link>https://arxiv.org/abs/2509.11883</link>
<guid>https://arxiv.org/abs/2509.11883</guid>
<content:encoded><![CDATA[
<div> Keywords: plasma edge simulations, kinetic Monte Carlo, asymptotic-preserving method, convergence analysis, numerical analysis

Summary:
In this study, the authors investigate the efficiency of a kinetic-diffusion Monte Carlo (KDMC) simulation method coupled with a fluid estimation technique in plasma edge simulations. The aim is to address the computational cost associated with high particle collision rates in large-sized reactors like ITER and DEMO. Through numerical analysis, the researchers compare the accuracy of the proposed algorithm with an approximate fluid method and traditional kinetic Monte Carlo method as a reference. Results show that KDMC with the fluid estimation exhibits significantly lower errors than the fluid method in both high- and low-collisional regimes. Additionally, the KDMC method demonstrates a clear speed-up compared to the kinetic Monte Carlo approach. Overall, the study confirms the effectiveness of the KDMC algorithm in improving computational efficiency and accuracy in plasma edge simulations. <br /><br />Summary: <div>
arXiv:2509.11883v1 Announce Type: new 
Abstract: In plasma edge simulations, kinetic Monte Carlo (MC) is often used to simulate neutral particles and estimate source terms. For large-sized reactors, like ITER and DEMO, high particle collision rates lead to a substantial computational cost for such schemes. To address this challenge, an asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation method and a corresponding fluid estimation technique have been proposed in the literature. In this work, we perform numerical analysis on the convergence of KDMC with the fluid estimation. To do so, we compare the accuracy of the analyzed algorithm with the accuracy of an approximate fluid method using the kinetic MC method as a reference. In a one-dimensional test case, KDMC with the fluid estimation achieves at least one order of magnitude lower errors than the fluid method for both high- and low-collisional regimes. Moreover, KDMC with the fluid estimation outperforms the kinetic MC method with a clear speed-up. Overall, our analysis confirms the effectiveness of the discussed algorithm.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval</title>
<link>https://arxiv.org/abs/2509.12042</link>
<guid>https://arxiv.org/abs/2509.12042</guid>
<content:encoded><![CDATA[
<div> keywords: financial disclosures, retrieval framework, FinGEAR, FLAM, 10-K filings

Summary:
FinGEAR is a retrieval framework designed specifically for financial documents, addressing challenges such as complex regulatory language and document structure. It incorporates the finance lexicon for Item-level guidance (FLAM), dual hierarchical indices for within-Item search, and a two-stage cross-encoder reranker. This framework aligns retrieval with the structure and terminology of financial disclosures, resulting in improved precision, recall, F1 score, and relevancy compared to existing models. FinGEAR outperforms flat RAG models by up to 56.7%, graph-based RAGs by 12.5%, and prior tree-based systems by 217.6%. It also enhances downstream answer accuracy when used with a fixed reader. By combining section hierarchy and domain lexicon signals, FinGEAR enhances retrieval fidelity, making it a valuable tool for high-stakes financial analysis.

<br /><br />Summary: <div>
arXiv:2509.12042v1 Announce Type: new 
Abstract: Financial disclosures such as 10-K filings present challenging retrieval problems due to their length, regulatory section hierarchy, and domain-specific language, which standard retrieval-augmented generation (RAG) models underuse. We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a retrieval framework tailored to financial documents. FinGEAR combines a finance lexicon for Item-level guidance (FLAM), dual hierarchical indices for within-Item search (Summary Tree and Question Tree), and a two-stage cross-encoder reranker. This design aligns retrieval with disclosure structure and terminology, enabling fine-grained, query-aware context selection. Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR delivers consistent gains in precision, recall, F1, and relevancy, improving F1 by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over prior tree-based systems, while also increasing downstream answer accuracy with a fixed reader. By jointly modeling section hierarchy and domain lexicon signals, FinGEAR improves retrieval fidelity and provides a practical foundation for high-stakes financial analysis.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing MacPherson Suspension Architectures using Bayesian Optimization</title>
<link>https://arxiv.org/abs/2206.09022</link>
<guid>https://arxiv.org/abs/2206.09022</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, engineering design, compliance, target specifications, discipline model
Summary: 
The article introduces a Bayesian optimization system for enhancing the traditional manual engineering design process. By optimizing compliance with target specifications directly and without requiring gradient information, the system aims to automate and expedite the design process. The method focuses on computing a generalized inverse of a high-dimensional non-linear function, enabling efficient optimization in a scalable manner. Additionally, the proposed two-tier convergence criterion ensures either convergence to an optimal solution satisfying all specified design criteria or convergence to a minimum-norm solution. The system's effectiveness is demonstrated through a vehicle chassis design problem in an industry context, utilizing a commercial discipline model. The results highlight the system's general applicability, scalability, and efficiency, showcasing the straightforward implementation of novel convergence criteria within popular Bayesian optimization software packages. 
<br /><br />Summary: <div>
arXiv:2206.09022v1 Announce Type: cross 
Abstract: Engineering design is traditionally performed by hand: an expert makes design proposals based on past experience, and these proposals are then tested for compliance with certain target specifications. Testing for compliance is performed first by computer simulation using what is called a discipline model. Such a model can be implemented by a finite element analysis, multibody systems approach, etc. Designs passing this simulation are then considered for physical prototyping. The overall process may take months, and is a significant cost in practice. We have developed a Bayesian optimization system for partially automating this process by directly optimizing compliance with the target specification with respect to the design parameters. The proposed method is a general framework for computing a generalized inverse of a high-dimensional non-linear function that does not require e.g. gradient information, which is often unavailable from discipline models. We furthermore develop a two-tier convergence criterion based on (i) convergence to a solution optimally satisfying all specified design criteria, or (ii) convergence to a minimum-norm solution. We demonstrate the proposed approach on a vehicle chassis design problem motivated by an industry setting using a state-of-the-art commercial discipline model. We show that the proposed approach is general, scalable, and efficient, and that the novel convergence criteria can be implemented straightforwardly based on existing concepts and subroutines in popular Bayesian optimization software packages.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs</title>
<link>https://arxiv.org/abs/2509.10504</link>
<guid>https://arxiv.org/abs/2509.10504</guid>
<content:encoded><![CDATA[
<div> Reframe retrosynthesis planning, worst-path optimization, tree-structured Markov Decision Processes, Interactive Retrosynthesis Planning, self-imitation learning <br />
Summary: 
This paper proposes a new approach to retrosynthesis planning by treating it as a worst-path optimization problem within tree-structured Markov Decision Processes, ensuring a unique optimal solution and offering improvement guarantees. The Interactive Retrosynthesis Planning (InterRetro) method is introduced, which interacts with the tree MDP, learns a value function for worst-path outcomes, and improves its policy through self-imitation learning. Empirical results show that InterRetro achieves state-of-the-art performance by solving 100% of targets on the Retro*-190 benchmark, shortening synthetic routes by 4.9%, and exhibiting promising performance with only 10% of the training data. This approach represents a significant advancement in computational retrosynthesis planning. <br /><br />Summary: <div>
arXiv:2509.10504v1 Announce Type: cross 
Abstract: Retrosynthesis planning aims to decompose target molecules into available building blocks, forming a synthesis tree where each internal node represents an intermediate compound and each leaf ideally corresponds to a purchasable reactant. However, this tree becomes invalid if any leaf node is not a valid building block, making the planning process vulnerable to the "weakest link" in the synthetic route. Existing methods often optimise for average performance across branches, failing to account for this worst-case sensitivity. In this paper, we reframe retrosynthesis as a worst-path optimisation problem within tree-structured Markov Decision Processes (MDPs). We prove that this formulation admits a unique optimal solution and offers monotonic improvement guarantees. Building on this insight, we introduce Interactive Retrosynthesis Planning (InterRetro), a method that interacts with the tree MDP, learns a value function for worst-path outcomes, and improves its policy through self-imitation, preferentially reinforcing past decisions with high estimated advantage. Empirically, InterRetro achieves state-of-the-art results, solving 100% of targets on the Retro*-190 benchmark, shortening synthetic routes by 4.9%, and achieving promising performance using only 10% of the training data - representing a significant advance in computational retrosynthesis planning.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective</title>
<link>https://arxiv.org/abs/2509.10506</link>
<guid>https://arxiv.org/abs/2509.10506</guid>
<content:encoded><![CDATA[
<div> boosting, retail demand forecasting, interpretable learning, feature attention, supply chain management <br />
Summary: <br />
The article discusses the challenges faced in forecasting product demand in retail supply chains and introduces a new framework called AttnBoost. AttnBoost integrates feature-level attention into the boosting process to improve predictive accuracy and explainability in the presence of noisy and heterogeneous data. By dynamically adjusting feature importance through an attention mechanism, AttnBoost can focus on high-impact variables like promotions, pricing, and seasonal trends. The model outperforms traditional machine learning and deep tabular models on a large-scale retail sales dataset, providing valuable insights for supply chain managers. An ablation study confirms the effectiveness of the attention module in reducing overfitting and enhancing interpretability. The results highlight the potential of attention-guided boosting for scalable and interpretable AI in real-world forecasting applications. <div>
arXiv:2509.10506v1 Announce Type: cross 
Abstract: Forecasting product demand in retail supply chains presents a complex challenge due to noisy, heterogeneous features and rapidly shifting consumer behavior. While traditional gradient boosting decision trees (GBDT) offer strong predictive performance on structured data, they often lack adaptive mechanisms to identify and emphasize the most relevant features under changing conditions. In this work, we propose AttnBoost, an interpretable learning framework that integrates feature-level attention into the boosting process to enhance both predictive accuracy and explainability. Specifically, the model dynamically adjusts feature importance during each boosting round via a lightweight attention mechanism, allowing it to focus on high-impact variables such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a large-scale retail sales dataset and demonstrate that it outperforms standard machine learning and deep tabular models, while also providing actionable insights for supply chain managers. An ablation study confirms the utility of the attention module in mitigating overfitting and improving interpretability. Our results suggest that attention-guided boosting represents a promising direction for interpretable and scalable AI in real-world forecasting applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Temporal Fusion Transformers for Cryptocurrency Price Prediction</title>
<link>https://arxiv.org/abs/2509.10542</link>
<guid>https://arxiv.org/abs/2509.10542</guid>
<content:encoded><![CDATA[
<div> Transformers, short-term prediction, cryptocurrency market, adaptive modeling, price forecasting

Summary: 
This paper introduces an adaptive Temporal Fusion Transformer (TFT) approach for precise short-term price prediction in the cryptocurrency market. By leveraging dynamic subseries lengths and pattern-based categorization, the model addresses the market's non-stationary nature and extreme volatility. A novel segmentation method is proposed, where subseries end at relative maxima, capturing significant upward movements and filtering noise. The fixed-length pattern ending each subseries determines the category assigned to the subsequent variable-length subseries, allowing for specialized prediction models for each category. Experimental results on ETH-USDT 10-minute data demonstrate that the adaptive approach outperforms baseline models in prediction accuracy and simulated trading profitability. The combination of adaptive segmentation and pattern-conditioned forecasting enhances robust and responsive cryptocurrency price prediction. 

<br /><br />Summary: <div>
arXiv:2509.10542v1 Announce Type: cross 
Abstract: Precise short-term price prediction in the highly volatile cryptocurrency market is critical for informed trading strategies. Although Temporal Fusion Transformers (TFTs) have shown potential, their direct use often struggles in the face of the market's non-stationary nature and extreme volatility. This paper introduces an adaptive TFT modeling approach leveraging dynamic subseries lengths and pattern-based categorization to enhance short-term forecasting. We propose a novel segmentation method where subseries end at relative maxima, identified when the price increase from the preceding minimum surpasses a threshold, thus capturing significant upward movements, which act as key markers for the end of a growth phase, while potentially filtering the noise. Crucially, the fixed-length pattern ending each subseries determines the category assigned to the subsequent variable-length subseries, grouping typical market responses that follow similar preceding conditions. A distinct TFT model trained for each category is specialized in predicting the evolution of these subsequent subseries based on their initial steps after the preceding peak. Experimental results on ETH-USDT 10-minute data over a two-month test period demonstrate that our adaptive approach significantly outperforms baseline fixed-length TFT and LSTM models in prediction accuracy and simulated trading profitability. Our combination of adaptive segmentation and pattern-conditioned forecasting enables more robust and responsive cryptocurrency price prediction.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COVID-BLUeS -- A Prospective Study on the Value of AI in Lung Ultrasound Analysis</title>
<link>https://arxiv.org/abs/2509.10556</link>
<guid>https://arxiv.org/abs/2509.10556</guid>
<content:encoded><![CDATA[
<div> Keywords: lung ultrasound, Artificial intelligence, COVID-19, severity, multi-modal models<br />
Summary:<br />
In a study at Maastricht University Medical Centre, lung ultrasound (LUS) data from COVID-19 suspects were analyzed using Artificial intelligence (AI) models for detecting and assessing pulmonary infections. The severity of lung involvement in COVID-19 positive and negative patients was similar according to human annotators. AI models showed 65% accuracy in detecting COVID-19 without specific training, improving to 79% with targeted training. Multi-modal models combining images and CBC data outperformed image-only models. However, the performance of AI models was limited due to heterogeneous LUS datasets, frame-based processing ignoring video-level information, and a lack of focus on multi-modal models. The dataset used in the study has been made publicly available to aid future research efforts. <div>
arXiv:2509.10556v1 Announce Type: cross 
Abstract: As a lightweight and non-invasive imaging technique, lung ultrasound (LUS) has gained importance for assessing lung pathologies. The use of Artificial intelligence (AI) in medical decision support systems is promising due to the time- and expertise-intensive interpretation, however, due to the poor quality of existing data used for training AI models, their usability for real-world applications remains unclear. In a prospective study, we analyze data from 63 COVID-19 suspects (33 positive) collected at Maastricht University Medical Centre. Ultrasound recordings at six body locations were acquired following the BLUE protocol and manually labeled for severity of lung involvement. Several AI models were applied and trained for detection and severity of pulmonary infection. The severity of the lung infection, as assigned by human annotators based on the LUS videos, is not significantly different between COVID-19 positive and negative patients (p = 0.89). Nevertheless, the predictions of image-based AI models identify a COVID-19 infection with 65% accuracy when applied zero-shot (i.e., trained on other datasets), and up to 79% with targeted training, whereas the accuracy based on human annotations is at most 65%. Multi-modal models combining images and CBC improve significantly over image-only models. Although our analysis generally supports the value of AI in LUS assessment, the evaluated models fall short of the performance expected from previous work. We find this is due to 1) the heterogeneity of LUS datasets, limiting the generalization ability to new data, 2) the frame-based processing of AI models ignoring video-level information, and 3) lack of work on multi-modal models that can extract the most relevant information from video-, image- and variable-based inputs. To aid future research, we publish the dataset at: https://github.com/NinaWie/COVID-BLUES.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations</title>
<link>https://arxiv.org/abs/2509.10659</link>
<guid>https://arxiv.org/abs/2509.10659</guid>
<content:encoded><![CDATA[
<div> segmentation, hierarchical network, mesh-based, graph neural networks, PDE simulations

Summary:
M4GN is a hierarchical network for mesh-based graph neural networks, addressing challenges of high cost and over-smoothing on large meshes. It utilizes a hybrid segmentation strategy to create contiguous segments of nodes that respect mesh topology and geometry. The segments are encoded by a permutation-invariant aggregator to capture local dynamics efficiently. M4GN incorporates a micro-level GNN and a macro-level transformer to balance accuracy and efficiency. Results show an improvement in prediction accuracy of up to 56% with up to 22% faster inference compared to state-of-the-art baselines. <div>
arXiv:2509.10659v1 Announce Type: cross 
Abstract: Mesh-based graph neural networks (GNNs) have become effective surrogates for PDE simulations, yet their deep message passing incurs high cost and over-smoothing on large, long-range meshes; hierarchical GNNs shorten propagation paths but still face two key obstacles: (i) building coarse graphs that respect mesh topology, geometry, and physical discontinuities, and (ii) maintaining fine-scale accuracy without sacrificing the speed gained from coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric hierarchical network. M4GN begins with a hybrid segmentation strategy that pairs a fast graph partitioner with a superpixel-style refinement guided by modal-decomposition features, producing contiguous segments of dynamically consistent nodes. These segments are encoded by a permutation-invariant aggregator, avoiding the order sensitivity and quadratic cost of aggregation approaches used in prior works. The resulting information bridges a micro-level GNN, which captures local dynamics, and a macro-level transformer that reasons efficiently across segments, achieving a principled balance between accuracy and efficiency. Evaluated on multiple representative benchmark datasets, M4GN improves prediction accuracy by up to 56% while achieving up to 22% faster inference than state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data</title>
<link>https://arxiv.org/abs/2509.11053</link>
<guid>https://arxiv.org/abs/2509.11053</guid>
<content:encoded><![CDATA[
<div> conditional consistent latent representation, generative adversarial network, contrastive learning, 1D fourier convolution neural network, bearing fault diagnosis

Summary:<br />
- The article proposes a novel framework, DAC-FCF, for bearing fault diagnosis using limited data by addressing data scarcity and model limitations.
- It introduces a Conditional Consistent Latent Representation Generative Adversarial Network (CCLR-GAN) to generate diverse data and a contrastive learning mechanism for better modeling relationships between training samples.
- The framework utilizes a 1D Fourier Convolution Neural Network (1D-FCNN) to extract global features from complex vibration signals.
- Experimental results on the CWRU dataset and a self-collected test bench show that DAC-FCF outperforms baselines by up to 32% and 10% respectively.
- Ablation experiments confirm the effectiveness of the proposed components, demonstrating the promising potential of DAC-FCF for bearing fault diagnosis under limited data.<br /><br />Summary: <div>
arXiv:2509.11053v1 Announce Type: cross 
Abstract: In the area of bearing fault diagnosis, deep learning (DL) methods have been widely used recently. However, due to the high cost or privacy concerns, high-quality labeled data are scarce in real world scenarios. While few-shot learning has shown promise in addressing data scarcity, existing methods still face significant limitations in this domain. Traditional data augmentation techniques often suffer from mode collapse and generate low-quality samples that fail to capture the diversity of bearing fault patterns. Moreover, conventional convolutional neural networks (CNNs) with local receptive fields makes them inadequate for extracting global features from complex vibration signals. Additionally, existing methods fail to model the intricate relationships between limited training samples. To solve these problems, we propose an advanced data augmentation and contrastive fourier convolution framework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, a novel conditional consistent latent representation and reconstruction generative adversarial network (CCLR-GAN) is proposed to generate more diverse data. Secondly, a contrastive learning based joint optimization mechanism is utilized to better model the relations between the available training data. Finally, we propose a 1D fourier convolution neural network (1D-FCNN) to achieve a global-aware of the input data. Experiments demonstrate that DAC-FCF achieves significant improvements, outperforming baselines by up to 32\% on case western reserve university (CWRU) dataset and 10\% on a self-collected test bench. Extensive ablation experiments prove the effectiveness of the proposed components. Thus, the proposed DAC-FCF offers a promising solution for bearing fault diagnosis under limited data.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations</title>
<link>https://arxiv.org/abs/2509.11376</link>
<guid>https://arxiv.org/abs/2509.11376</guid>
<content:encoded><![CDATA[
<div> framework, multimodal data fusion, reservoir analysis, AI models, operational efficiency

Summary:
The study presents an integrated framework utilizing cutting-edge AI models and advanced prompt engineering techniques for reservoir management in the petroleum industry. The framework combines large language models with multimodal data fusion to analyze seismic interpretations, well logs, and production data for real-time decision support. Field validation across 15 reservoir environments shows high accuracy in reservoir characterization, production forecasting, and well placement optimization. The system achieves rapid response times and high safety reliability with significant cost reductions compared to traditional methods. Few-shot learning and prompt optimization improve field adaptation time and reasoning quality. Real-time data processing includes anomaly detection and reduces environmental incidents. The research highlights the practical integration of AI technologies with domain expertise to enhance operational efficiency, safety, and economic performance. 

<br /><br />Summary: <div>
arXiv:2509.11376v1 Announce Type: cross 
Abstract: The petroleum industry faces unprecedented challenges in reservoir management, requiring rapid integration of complex multimodal datasets for real-time decision support. This study presents a novel integrated framework combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data fusion for comprehensive reservoir analysis. The framework implements domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum engineering documents, chain-of-thought reasoning, and few-shot learning for rapid field adaptation. Multimodal integration processes seismic interpretations, well logs, and production data through specialized AI models with vision transformers. Field validation across 15 diverse reservoir environments demonstrates exceptional performance: 94.2% reservoir characterization accuracy, 87.6% production forecasting precision, and 91.4% well placement optimization success rate. The system achieves sub-second response times while maintaining 96.2% safety reliability with no high-risk incidents during evaluation. Economic analysis reveals 62-78% cost reductions (mean 72%) relative to traditional methods with 8-month payback period. Few-shot learning reduces field adaptation time by 72%, while automated prompt optimization achieves 89% improvement in reasoning quality. The framework processed real-time data streams with 96.2% anomaly detection accuracy and reduced environmental incidents by 45%. We provide detailed experimental protocols, baseline comparisons, ablation studies, and statistical significance testing to ensure reproducibility. This research demonstrates practical integration of cutting-edge AI technologies with petroleum domain expertise for enhanced operational efficiency, safety, and economic performance.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.11420</link>
<guid>https://arxiv.org/abs/2509.11420</guid>
<content:encoded><![CDATA[
<div> Trading-R1, reasoning LLMs, financial decision making, risk-sensitive, supervised fine-tuning<br />
<br />
Summary: 
Trading-R1 is a financially-aware model designed to improve reasoning and decision-making in finance, addressing the need for interpretable and trustworthy AI in the market. It incorporates strategic thinking and planning, aligned with trading principles, through a comprehensive thesis composition and volatility-adjusted decision-making process. The model is trained on a diverse dataset and shows improved risk-adjusted returns and lower drawdowns compared to other models. By generating evidence-based investment theses, Trading-R1 supports structured and interpretable trading decisions. The system's approach combines supervised fine-tuning and reinforcement learning, following a three-stage curriculum. This innovative model fills a gap in applying reasoning LLMs to risk-sensitive financial tasks, providing a valuable tool for traders and analysts. Trading-R1 Terminal is available for access on GitHub, offering a practical application for financial professionals. <br /><br /> <div>
arXiv:2509.11420v1 Announce Type: cross 
Abstract: Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. We present Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. Trading-R1 Terminal will be released at https://github.com/TauricResearch/Trading-R1.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Requirements for Early Quantum Advantage and Quantum Utility in the Capacitated Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2509.11469</link>
<guid>https://arxiv.org/abs/2509.11469</guid>
<content:encoded><![CDATA[
<div> framework, Capacitated Vehicle Routing Problem, early quantum advantage, NISQ hardware, encoding <br />
Summary: 
This study introduces a framework for assessing the potential for early quantum advantage in solving the Capacitated Vehicle Routing Problem (CVRP). The analysis suggests that achieving quantum advantage on noisy intermediate scale quantum (NISQ) hardware is unlikely, even with the most qubit-efficient encoding methods. Through closed-form resource calculations and device benchmarks, key figures of merit such as the quantum feasibility point, qubit-feasibility line, and gate-feasibility line are identified to evaluate the feasibility of solving CVRP on quantum devices. A comparison of direct QUBO mapping and a space-efficient HOBO encoding reveals significant differences in resource requirements. The framework highlights that CVRP instances might need innovative problem decomposition techniques to leverage quantum devices effectively. Additionally, benchmarking on early-advantage instances like Golden-5 shows that HOBO circuits outperform QUBO encodings in terms of qubit utilization. <div>
arXiv:2509.11469v1 Announce Type: cross 
Abstract: We introduce a transparent, encoding-agnostic framework for determining when the Capacitated Vehicle Routing Problem (CVRP) can achieve early quantum advantage. Our analysis shows this is unlikely on noisy intermediate scale quantum (NISQ) hardware even in best case scenarios that use the most qubit-efficient direct encodings. Closed-form resource counts, combined with recent device benchmarks, yield three decisive go/no-go figures of merit: the quantum feasibility point and the qubit- and gate-feasibility lines, which place any CVRP instance on a single decision diagram. Contrasting a direct QUBO mapping with a space-efficient higher-order (HOBO) encoding reveals a large gap. Applied to early-advantage benchmarks such as Golden-5, our diagram shows that HOBO circuits require only 7,685 qubits, whereas comparable QUBO encodings still exceed 200,000 qubits. In addition to identifying candidate instances for early quantum advantage in CVRP, the framework provides a unifying go/no-go metric that ingests any CVRP encoding together with any hardware profile and highlights when quantum devices could challenge classical heuristics. Quantum advantage in CVRP would likely require innovative problem decomposition techniques.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions</title>
<link>https://arxiv.org/abs/2509.11595</link>
<guid>https://arxiv.org/abs/2509.11595</guid>
<content:encoded><![CDATA[
<div> transaction datasets, AML research, AMLNet, synthetic transactions, detection ensemble<br />
<br />
Summary: A new framework called AMLNet has been introduced to address the lack of publicly shareable transaction datasets for anti-money laundering (AML) research. AMLNet consists of a regulation-aware transaction generator producing over 1 million synthetic transactions covering various money laundering phases and typologies. The generated transactions have a high regulatory alignment and technical fidelity score. The detection ensemble within AMLNet achieves high performance on internal test partitions and shows adaptability to external datasets. The framework enables multi-dimensional evaluation and the release of the dataset for reproducible and regulation-conscious AML experimentation. <div>
arXiv:2509.11595v1 Announce Type: cross 
Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly shareable, regulation-aligned transaction datasets. We present AMLNet, a knowledge-based multi-agent framework with two coordinated units: a regulation-aware transaction generator and an ensemble detection pipeline. The generator produces 1,090,173 synthetic transactions (approximately 0.16\% laundering-positive) spanning core laundering phases (placement, layering, integration) and advanced typologies (e.g., structuring, adaptive threshold behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage (Section 4.2), while a composite technical fidelity score of 0.75 summarizes temporal, structural, and behavioral realism components (Section 4.4). The detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the internal test partitions of AMLNet and adapts to the external SynthAML dataset, indicating architectural generalizability across different synthetic generation paradigms. We provide multi-dimensional evaluation (regulatory, temporal, network, behavioral) and release the dataset (Version 1.0, https://doi.org/10.5281/zenodo.16736515), to advance reproducible and regulation-conscious AML experimentation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing</title>
<link>https://arxiv.org/abs/2402.16445</link>
<guid>https://arxiv.org/abs/2402.16445</guid>
<content:encoded><![CDATA[
<div> Keywords: Protein Language Models, Protein Engineering, ProLLaMA, Evolutionary Protein Generation Framework, Multitask Model

Summary:
ProLLaMA is a multitask protein language model that addresses the limitations of current Protein Language Models in both Protein Language Understanding and Generation. It is enhanced by the Evolutionary Protein Generation Framework (EPGF) and trained on a comprehensive dataset with superfamily annotations. ProLLaMA excels in both unconditional and controllable protein generation tasks, displaying superior structural quality metrics. It also demonstrates strong understanding capabilities with a high exact match rate in superfamily prediction. The EPGF significantly improves the biological viability of generated sequences, as shown by enhanced biophysical scores and structural metrics. This model bridges the gap between PLU and PLG, boosting progress in protein engineering. The project code is available on GitHub for further exploration and development. <br /><br />Summary: <div>
arXiv:2402.16445v3 Announce Type: replace 
Abstract: Recent advances in Protein Language Models (PLMs) have transformed protein engineering, yet unlike their counterparts in Natural Language Processing (NLP), current PLMs exhibit a fundamental limitation: they excel in either Protein Language Understanding (PLU) or Protein Language Generation (PLG), but rarely both. This fragmentation hinders progress in protein engineering. To bridge this gap, we introduce ProLLaMA, a multitask protein language model enhanced by the Evolutionary Protein Generation Framework (EPGF). We construct a comprehensive instruction dataset containing approximately 13 million samples with over 11,000 superfamily annotations to facilitate better modeling of sequence-function landscapes. We leverage a two-stage training approach to develop ProLLaMA, a multitask LLM with protein domain expertise. Our EPGF addresses the mismatch between statistic language modeling and biological constraints through three innovations: a multi-dimensional interpretable scorer, hierarchical efficient decoding, and a probabilistic-biophysical joint selection mechanism. Extensive experiments demonstrate that ProLLaMA excels in both unconditional and controllable protein generation tasks, achieving superior structural quality metrics compared to existing PLMs. Additionally, ProLLaMA demonstrates strong understanding capabilities with a 67.1% exact match rate in superfamily prediction. EPGF significantly enhances the biological viability of generated sequences, as evidenced by improved biophysical scores (+4.3%) and structural metrics (+14.5%). The project is available at https://github.com/PKU-YuanGroup/ProLLaMA.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Machine Learning Models for Predicting the Next Targets of Activist Funds</title>
<link>https://arxiv.org/abs/2404.16169</link>
<guid>https://arxiv.org/abs/2404.16169</guid>
<content:encoded><![CDATA[
<div> activist investment, predictive model, machine learning, Shapley value, corporate governance<br />
<br />
Summary: This research introduces a predictive model to identify potential targets of activist investment funds, essential for companies to reduce intervention risks, activist funds to make optimal investments, and investors to capitalize on stock price gains. Evaluating 123 model configurations using data from the Russell 3000 index, the best model achieved an AUC-ROC of 0.782, showcasing its ability to predict activist fund targets effectively. The Shapley value method was employed to determine key factors influencing a company's likelihood of being targeted, shedding light on the dynamic mechanisms behind activist fund target selection. These findings provide valuable insights for proactive corporate governance and informed investment strategies, contributing to a deeper understanding of the factors driving activist investment decisions. <br /><br /> <div>
arXiv:2404.16169v3 Announce Type: replace 
Abstract: This research presents a predictive model to identify potential targets of activist investment funds--entities that acquire significant corporate stakes to influence strategic and operational decisions, ultimately enhancing shareholder value. Predicting such targets is crucial for companies aiming to mitigate intervention risks, activist funds seeking optimal investments, and investors looking to leverage potential stock price gains. Using data from the Russell 3000 index from 2016 to 2022, we evaluated 123 model configurations incorporating diverse imputation, oversampling, and machine learning techniques. Our best model achieved an AUC-ROC of 0.782, demonstrating its capability to effectively predict activist fund targets. To enhance interpretability, we employed the Shapley value method to identify key factors influencing a company's likelihood of being targeted, highlighting the dynamic mechanisms underlying activist fund target selection. These insights offer a powerful tool for proactive corporate governance and informed investment strategies, advancing understanding of the mechanisms driving activist investment decisions.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plastic Arbor: a modern simulation framework for synaptic plasticity -- from single synapses to networks of morphological neurons</title>
<link>https://arxiv.org/abs/2411.16445</link>
<guid>https://arxiv.org/abs/2411.16445</guid>
<content:encoded><![CDATA[
<div> Keywords: Arbor, neuronal networks, synaptic plasticity, computational modeling, dendritic structures 

Summary: 
Arbor is a software library for simulating large-scale networks of biological neurons with detailed morphological structures. It supports multi-core CPU and GPU systems, combining customizable neuronal and synaptic mechanisms for efficient simulation. The library has been extended to model a variety of spike-driven plasticity paradigms, from single-synapse dynamics to large recurrent networks. By comparing with other simulators, it is shown that Arbor allows simulating plastic networks of multi-compartment neurons at minimal runtime cost. The extension also demonstrates high efficiency in terms of runtime and memory usage. Using this framework, the impact of dendritic structures on network dynamics over hours is investigated, revealing a relationship between dendritic tree length and information storage efficiency. This extended Arbor framework provides a valuable tool for future studies on the influence of synaptic plasticity in large networks, particularly in conjunction with neuronal morphology.<br /><br />Summary: <div>
arXiv:2411.16445v3 Announce Type: replace 
Abstract: Arbor is a software library designed for efficient simulation of large-scale networks of biological neurons with detailed morphological structures. It combines customizable neuronal and synaptic mechanisms with high-performance computing, supporting multi-core CPU and GPU systems. In humans and other animals, synaptic plasticity processes play a vital role in cognitive functions, including learning and memory. Recent studies have shown that intracellular molecular processes in dendrites significantly influence single-neuron dynamics. However, for understanding how the complex interplay between dendrites and synaptic processes influences network dynamics, computational modeling is required.
  To enable the modeling of large-scale networks of morphologically detailed neurons with diverse plasticity processes, we have extended the Arbor library to support simulations of a large variety of spike-driven plasticity paradigms. To showcase the features of the extended framework, we present examples of computational models, beginning with single-synapse dynamics, progressing to multi-synapse rules, and finally scaling up to large recurrent networks. While cross-validating our implementations by comparison with other simulators, we show that Arbor allows simulating plastic networks of multi-compartment neurons at nearly no additional cost in runtime compared to point-neuron simulations. In addition, we demonstrate that Arbor is highly efficient in terms of runtime and memory use as compared to other simulators. Using the extended framework, as an example, we investigate the impact of dendritic structures on network dynamics across a timescale of several hours, finding a relation between the length of dendritic trees and the ability of the network to efficiently store information. By our extension of Arbor, we aim to provide a valuable tool that will support future studies on the impact of synaptic plasticity, especially, in conjunction with neuronal morphology, in large networks.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superstructure Optimization with Embedded Neural Networks for Sustainable Aviation Fuel Production</title>
<link>https://arxiv.org/abs/2509.09796</link>
<guid>https://arxiv.org/abs/2509.09796</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-objective optimization, sustainable aviation fuel, artificial neural networks, Fischer-Tropsch kerosene production, carbon emissions constraints

Summary: 
This study introduces a novel framework for sustainable aviation fuel (SAF) production that combines artificial neural networks (ANNs) with mathematical optimization techniques. By integrating ANNs into a mixed-integer quadratically constrained programming (MIQCP) formulation, the framework allows for simultaneous optimization of discrete process choices and continuous operating parameters. The application of this framework to Fischer-Tropsch kerosene production reveals that configurations minimizing costs under unconstrained CO2 emissions favor fossil-based autothermal reforming (ATR). However, imposing carbon emission constraints leads to the integration of biomass gasification and direct air capture coupled with carbon sequestration (DAC-CS) for reduced net emissions at higher production costs. Hybrid configurations with flexible process parameters, enabled by embedded ANNs, outperform fixed setups and achieve cost savings of up to 20%. Sensitivity analyses show the impact of process conditions on economic and environmental performance, emphasizing the importance of process adaptability in SAF production. 

<br /><br />Summary: <div>
arXiv:2509.09796v1 Announce Type: new 
Abstract: This study presents a multi-objective optimization framework for sustainable aviation fuel (SAF) production, integrating artificial neural networks (ANNs) within a mixed-integer quadratically constrained programming (MIQCP) formulation. By embedding data-driven surrogate models into the mathematical optimization structure, the proposed methodology addresses key limitations of conventional superstructure-based approaches, enabling simultaneous optimization of discrete process choices and continuous operating parameters. The framework captures variable input and output stream compositions, facilitating the joint optimization of target product composition and system design. Application to Fischer-Tropsch (FT) kerosene production demonstrates that cost-minimizing configurations under unconstrained CO2 emissions are dominated by the fossil-based autothermal reforming (ATR) route. Imposing carbon emission constraints necessitates the integration of biomass gasification and direct air capture coupled with carbon sequestration (DAC-CS), resulting in substantially reduced net emissions but higher production costs. At the zero-emission limit, hybrid configurations combining ATR and biomass gasification achieve the lowest costs (~2.38 \$/kg-kerosene), followed closely by biomass gasification-only (~2.43 \$/kg), both of which outperform the ATR-only pathway with DAC-CS (~2.65 \$/kg). In contrast, DAC-only systems relying exclusively on atmospheric CO2 and water electrolysis are prohibitively expensive (~10.8 \$/kg). The results highlight the critical role of process adaptability: configurations exploiting flexible process parameters, facilitated by embedded ANNs, consistently outperform fixed setups, achieving up to 20% cost savings. Sensitivity analyses elucidate the influence of process conditions, such as FT reactor pressure and gasification temperature, on economic and environmental performance.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fraud detection and risk assessment of online payment transactions on e-commerce platforms based on LLM and GCN frameworks</title>
<link>https://arxiv.org/abs/2509.09928</link>
<guid>https://arxiv.org/abs/2509.09928</guid>
<content:encoded><![CDATA[
<div> Keywords: e-commerce, fraud detection, Large Language Models, Graph Convolutional Networks, online payment

Summary: 
This study presents a novel fraud detection framework for e-commerce online payment transactions that combines Large Language Models (LLM) with Graph Convolutional Networks (GCN). A dataset of 2,840,000 transactions involving consumers and merchants was used to train the model, addressing the imbalanced nature of fraud instances in the data. The model represents consumers and merchants as nodes and transactions as edges in a heterogeneous graph, enabling the GCN to learn complex behavioral patterns. By integrating semantic features from GPT-4o and Tabformer with structural features, the model achieves an accuracy of 0.98 in fraud detection, balancing precision and sensitivity effectively. This framework offers a scalable, real-time solution for securing online payment environments and demonstrates the potential of graph-based deep learning in financial fraud prevention.

<br /><br />Summary: <div>
arXiv:2509.09928v1 Announce Type: new 
Abstract: With the rapid growth of e-commerce, online payment fraud has become increasingly complex, posing serious threats to financial security and consumer trust. Traditional detection methods often struggle to capture the intricate relational structures inherent in transactional data. This study presents a novel fraud detection framework that combines Large Language Models (LLM) with Graph Convolutional Networks (GCN) to effectively identify fraudulent activities in e-commerce online payment transactions. A dataset of 2,840,000 transactions was collected over 14 days from major platforms such as Amazon, involving approximately 2,000 U.S.-based consumers and 30 merchants. With fewer than 6000 fraudulent instances, the dataset represents a highly imbalanced scenario. Consumers and merchants were modeled as nodes and transactions as edges to form a heterogeneous graph, upon which a GCN was applied to learn complex behavioral patterns. Semantic features extracted via GPT-4o and Tabformer were integrated with structural features to enhance detection performance. Experimental results demonstrate that the proposed model achieves an accuracy of 0.98, effectively balancing precision and sensitivity in fraud detection. This framework offers a scalable and real-time solution for securing online payment environments and provides a promising direction for applying graph-based deep learning in financial fraud prevention.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading</title>
<link>https://arxiv.org/abs/2509.09995</link>
<guid>https://arxiv.org/abs/2509.09995</guid>
<content:encoded><![CDATA[
<div> Language Models, Financial Reasoning, High-Frequency Trading, Multi-Agent Framework, Algorithmic Trading 
<br />
Summary:
QuantAgent is a novel multi-agent Large Language Model (LLM) framework designed specifically for high-frequency algorithmic trading. It consists of four specialized agents - Indicator, Pattern, Trend, and Risk - each equipped with tools and reasoning capabilities tailored for short-term market dynamics. In evaluations across various financial instruments, QuantAgent outperformed neural and rule-based baselines in terms of predictive accuracy and cumulative return over 4-hour trading intervals. By combining structured financial priors with language-native reasoning, QuantAgent demonstrates the potential for creating real-time decision systems in high-frequency financial markets. <div>
arXiv:2509.09995v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated impressive capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks, leveraging fundamental and sentiment-based inputs for strategic decision-making. However, such systems are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT requires rapid, risk-aware decisions based on structured, short-horizon signals, including technical indicators, chart patterns, and trend-based features, distinct from the long-term semantic reasoning typical of traditional financial LLM applications. To this end, we introduce QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents, Indicator, Pattern, Trend, and Risk, each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. In zero-shot evaluations across ten financial instruments, including Bitcoin and Nasdaq futures, QuantAgent demonstrates superior performance in both predictive accuracy and cumulative return over 4-hour trading intervals, outperforming strong neural and rule-based baselines. Our findings suggest that combining structured financial priors with language-native reasoning unlocks new potential for traceable, real-time decision systems in high-frequency financial markets.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Freight Rail Electrification: A Framework for Charge Station Selection and Battery Charge/Swap Scheduling</title>
<link>https://arxiv.org/abs/2509.10157</link>
<guid>https://arxiv.org/abs/2509.10157</guid>
<content:encoded><![CDATA[
<div> Keywords: battery electric freight trains, charging infrastructure, charge scheduling, optimization, algorithms <br />
Summary: <br />
Battery electric freight trains play a vital role in achieving decarbonization goals through zero-emission transportation options. The study focuses on developing an efficient strategy for the adoption of battery electric freight trains, involving the optimal design of charging infrastructure and charge scheduling for each train. The model allows for flexibility by enabling batteries to be either charged or swapped at deployed stations, with each train able to carry multiple batteries. The problem is formulated as a mixed integer linear programming model. Three algorithms are proposed to solve the optimization problem, including a Rectangle Piecewise Linear Approximation technique, a Fixed Algorithm heuristic, and a Benders Decomposition algorithm. Computational experiments show that the Benders Decomposition algorithm outperforms the other two algorithms in terms of objective function value, with the Rectangle Piecewise Linear Approximation technique closely following. The Fixed Algorithm provides the least optimal solution. <div>
arXiv:2509.10157v1 Announce Type: new 
Abstract: Battery electric freight trains are crucial for decarbonization by providing zero-emission transportation alternatives. The proper adoption of battery electric freight trains depends on an efficient battery electrification strategy, involving both infrastructure setup and charge scheduling. The study presents a comprehensive model for the optimal design of charging infrastructure and charge scheduling for each train. To provide more refueling flexibility, we allow batteries to be either charged or swapped in a deployed station, and each train can carry multiple batteries. This problem is formulated as a mixed integer linear programming model. To obtain real-time solutions for a large scale network, we develop three algorithms to solve the optimization problem: (1) a Rectangle Piecewise Linear Approximation technique, (2) a Fixed Algorithm heuristic, and (3) Benders Decomposition algorithm. In computational experiments, we use the three proposed algorithms to solve instances with up to 25 stations. Statistical analysis verifies that Benders Decomposition outperforms the other two algorithms with respect to the objective function value, closely followed by the Rectangle Piecewise Linear Approximation technique, and the Fixed Algorithm provides the least optimal solution.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Flow Separation Control Strategies in 3D Wings via Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.10185</link>
<guid>https://arxiv.org/abs/2509.10185</guid>
<content:encoded><![CDATA[
<div> keywords: deep reinforcement learning, active flow control, SD7003 wing, aerodynamic efficiency, turbulence <br />
Summary:
In this study, deep reinforcement learning (DRL) is utilized to optimize active flow control (AFC) on a three-dimensional SD7003 wing at specific aerodynamic conditions. The uncontrolled baseline case exhibits significant flow separation and a turbulent wake. Through the use of a GPU-accelerated CFD solver and multi-agent training, DRL successfully identifies control strategies that significantly enhance lift (79%), reduce drag (65%), and improve aerodynamic efficiency (408%). Flow visualizations validate the reattachment of the separated shear layer, highlighting the potential of DRL in tackling complex and turbulent flows. This research showcases the effectiveness of DRL in improving aerodynamic performance by discovering optimized control strategies in challenging flow conditions. <br /><br />Summary: <div>
arXiv:2509.10185v1 Announce Type: new 
Abstract: In this work, deep reinforcement learning (DRL) is applied to active flow control (AFC) over a threedimensional SD7003 wing at a Reynolds number of Re = 60,000 and angle of attack of AoA = 14 degrees. In the uncontrolled baseline case, the flow exhibits massive separation and a fully turbulent wake. Using a GPU-accelerated CFD solver and multi-agent training, DRL discovers control strategies that enhance lift (79%), reduce drag (65%), and improve aerodynamic efficiency (408%). Flow visualizations confirm reattachment of the separated shear layer, demonstrating the potential of DRL for complex and turbulent flows.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Active Flow Control around a Three-Dimensional Flow-Separated Wing at Re = 1,000</title>
<link>https://arxiv.org/abs/2509.10195</link>
<guid>https://arxiv.org/abs/2509.10195</guid>
<content:encoded><![CDATA[
<div> Keywords: deep reinforcement learning, active flow control, NACA0012 wing, computational fluid dynamics, aerodynamic performance

Summary:<br /><br />
This study investigates the application of deep reinforcement learning (DRL) for active flow control (AFC) to mitigate flow separation on wings at high angles of attack. The DRL agent autonomously adjusts the flow over a three-dimensional NACA0012 wing section at Re = 1,000 and AoA = 20 degrees by analyzing real-time flow data and utilizing a reward function focused on enhancing aerodynamic performance. By integrating the GPU-accelerated computational fluid dynamics (CFD) solver SOD2D with the TF-Agents DRL library through a Redis in-memory database, the framework enables efficient training. This research showcases the potential of DRL in addressing intricate aerodynamic challenges and pushing the boundaries of conventional AFC techniques. The study underscores the effectiveness of DRL in optimizing control actions to improve aerodynamic performance on wing sections, thereby advancing the field of flow control in aerodynamics. <div>
arXiv:2509.10195v1 Announce Type: new 
Abstract: This study explores the use of deep reinforcement learning (DRL) for active flow control (AFC) to reduce flow separation on wings at high angles of attack. Concretely, here the DRL agent controls the flow over the three-dimensional NACA0012 wing section at the Reynolds number Re = 1,000 and angle of attack AoA = 20 degrees, autonomously identifying optimal control actions through real-time flow data and a reward function focused on improving aerodynamic performance. The framework integrates the GPU-accelerated computational fluid dynamics (CFD) solver SOD2D with the TF-Agents DRL library via a Redis in-memory database, enabling rapid training. This work builds on previous DRL flow-control studies, demonstrating DRL potential to address complex aerodynamic challenges and push the boundaries of traditional AFC methods.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TubeBEND: A Real-World Dataset for Geometry Prediction in Rotary Draw Bending</title>
<link>https://arxiv.org/abs/2509.10272</link>
<guid>https://arxiv.org/abs/2509.10272</guid>
<content:encoded><![CDATA[
<div> Dataset, TubeBEND, rotary tube bending processes, machine learning, signal analysis<br />
<br />
Summary: <br />
This paper introduces TubeBEND, a dataset consisting of 318 rotary tube bending processes curated by experts. The dataset aims to solve the industrial challenge of predicting the geometry of a first-stage bend to optimize machine clamping molds for the second-stage bend in two-stage rotary draw bending. It includes criteria such as final bent angle and cross-sectional deformation of the tube. The dataset enables the development and testing of machine learning models to predict tube geometry, aiding machine operators in optimizing springback and deformation. By recording process parameters like tool movements and forces, the dataset provides detailed information on their effects on tube geometry. The goal is to explore solutions that leverage experimental process variables in machine learning algorithms to replace traditional trial-and-error or simulation-based methods. The dataset is publicly available for further research and improvement of data-driven approaches in the domain. <div>
arXiv:2509.10272v1 Announce Type: new 
Abstract: This paper presents TubeBEND, a real-world dataset comprising 318 rotary tube bending processes, which were collected and sorted by experts from various fields to evaluate machine learning and signal analysis methods. The dataset addresses the industrial challenge of predicting the geometry of a first-stage bend, which can be beneficial for designing machine clamping molds for the second-stage bend in two-stage rotary draw bending. Some geometry criteria, such as the tube's final bent angle (or springback) and its cross-sectional deformation, are being recorded in this dataset. This dataset gives us the possibility to build and test machine learning models that can predict the geometry and help the machine operators with a better machine setup to optimize the tube's springback and deformation. Moreover, by recording some process parameters, such as tool movements and forces or torques applied to them, we deliver detailed information about their impacts on the final tube geometry. The focus of our work is to discover solutions that can replace traditional methods, such as trial-and-error or simulation-based predictions, by including experimental process variables in ML algorithms. Our dataset is publicly available at https://github.com/zeyneddinoz/tubebend and https://zenodo.org/records/16614082 as a benchmark to improve data-driven methods in this field.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs</title>
<link>https://arxiv.org/abs/2509.09727</link>
<guid>https://arxiv.org/abs/2509.09727</guid>
<content:encoded><![CDATA[
<div> framework, financial education, question answering, multi-agent, domain-specific 

Summary: 
The article introduces a multi-agent framework for financial question answering in education. Existing large language models often struggle with the nuanced reasoning required in finance. The proposed framework includes a Base Generator, Evidence Retriever, and Expert Reviewer agent to enhance domain-specific QA. By leveraging retrieval-augmented generation and prompting strategies, the framework improves answer accuracy by 6.6-8.3% over baselines. The Gemini-2.0-Flash model shows the highest performance. Additionally, the method enables GPT-4o-mini to achieve performance comparable to FinGPT-mt_Llama3-8B_LoRA. The results suggest a cost-effective approach to enhancing financial QA and provide insights for future research in multi-agent financial language model systems. 

<br /><br />Summary: <div>
arXiv:2509.09727v1 Announce Type: cross 
Abstract: Question answering (QA) plays a central role in financial education, yet existing large language model (LLM) approaches often fail to capture the nuanced and specialized reasoning required for financial problem-solving. The financial domain demands multistep quantitative reasoning, familiarity with domain-specific terminology, and comprehension of real-world scenarios. We present a multi-agent framework that leverages role-based prompting to enhance performance on domain-specific QA. Our framework comprises a Base Generator, an Evidence Retriever, and an Expert Reviewer agent that work in a single-pass iteration to produce a refined answer. We evaluated our framework on a set of 3,532 expert-designed finance education questions from Study.com, an online learning platform. We leverage retrieval-augmented generation (RAG) for contextual evidence from 6 finance textbooks and prompting strategies for a domain-expert reviewer. Our experiments indicate that critique-based refinement improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines, with the highest performance from Gemini-2.0-Flash. Furthermore, our method enables GPT-4o-mini to achieve performance comparable to the finance-tuned FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to enhancing financial QA and offer insights for further research in multi-agent financial LLM systems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing understanding and clinical applications of cerebral autoregulation: A novel integrated numerical framework</title>
<link>https://arxiv.org/abs/2509.10295</link>
<guid>https://arxiv.org/abs/2509.10295</guid>
<content:encoded><![CDATA[
<div> cerebral autoregulation, numerical algorithm, cerebral blood flow, hemodynamic parameters, personalized model 
Summary: 
This study introduces a novel numerical algorithm that incorporates key factors driving cerebral autoregulation (CA) to regulate cerebral blood flow (CBF). The algorithm utilizes partial and ordinary differential equations to capture spatial and temporal distributions of arterial pressure, oxygen, and carbon dioxide levels in the cerebral vasculature. Validation with two datasets confirms its reliability in simulating the regulatory effects of CA on CBF under various physiological conditions. By integrating with a personalized multi-dimensional model, this framework enhances our understanding of CA and offers potential for developing hemodynamic-based therapeutic strategies for cerebrovascular disorders. <div>
arXiv:2509.10295v1 Announce Type: cross 
Abstract: Cerebral autoregulation (CA) is a fundamental mechanism that modulates cerebrovascular resistance, primarily by regulating the diameter of small cerebral vessels to maintain stable cerebral blood flow (CBF) in response to fluctuations in systemic arterial pressure. However, the clinical understanding of CA remains limited due to the intricate structure of the cerebral vasculature and the challenges in accurately quantifying the hemodynamic and physiological parameters that govern this autoregulatory process. Method: In this study, we introduced a novel numerical algorithm that employs three partial differential equations and one ordinary differential equation to capture both the spatial and temporal distributions of key CA-driving factors, including the arterial pressure (P) and the partial pressures of oxygen (PO_2) and carbon dioxide (PCO_2) within the cerebral vasculature, together with a Windkessel model in turn to regulate the CBF based on the calculated P, PO_2, and PCO_2. This algorithm was sequentially integrated with our previously developed personalized 0D-1D multi-dimensional model to account for the patient-specific effects. Results: The integrated framework was rigorously validated using two independent datasets, demonstrating its high reliability and accuracy in capturing the regulatory effects of CA on CBF across a range of physiological conditions. Conclusion: This work significantly advances our understanding of CA and provides a promising foundation for developing hemodynamic-based therapeutic strategies aimed at improving clinical outcomes in patients with cerebrovascular disorders.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetries in stochastic homogenization and acclimatizations for the RVE method</title>
<link>https://arxiv.org/abs/2509.08977</link>
<guid>https://arxiv.org/abs/2509.08977</guid>
<content:encoded><![CDATA[
<div> symmetry, microstructure, effective tensor, thermal conductivity, RVE method
<br />
Summary:
The study explores the impact of symmetry in a random microstructure on effective tensor and fluctuations in thermal conductivity. It investigates methods to enforce symmetries in postprocessing using orthogonal projectors. In the context of the Representative Volume Element (RVE) method, invariance conditions for effective tensor and fluctuations under different microstructure symmetry groups are established. It is found that the symmetry of the RVE cell type can disrupt ensemble symmetry, affecting effective property approximation. Strategies are introduced to enforce expected symmetries, reducing errors and improving accuracy. Theoretical arguments support the use of projections for unbiased variance reduction and exact symmetry enforcement. Large-scale simulations confirm the effectiveness of symmetry-projection techniques, especially in fiber-reinforced composites of industrial size. <div>
arXiv:2509.08977v1 Announce Type: new 
Abstract: We investigate the implications of a given symmetry of a random microstructure on the obtained effective tensor and its fluctuation in the context of thermal conductivity, and study strategies for enforcing these symmetries in postprocessing via orthogonal projectors. Within the framework of the representative volume element (RVE) method, we establish the invariance conditions for the effective tensor and its fluctuation under different symmetry groups of the microstructure. Interestingly, the symmetry of the considered cell type in the RVE method may break the ensemble symmetry and compromise the approximation of the effective properties. To rectify this issue, we introduce dedicated techniques which permit to enforce the expected symmetries in postprocessing and study the implications on the bounds for the effective properties as well as the total, the random and the systematic errors. We provide theoretical arguments that suitable projections lead to unbiased variance-reduction strategies which furthermore enforce the expected symmetries exactly. Through large-scale FFT-based homogenization simulations, we study the symmetry structure of the estimated effective conductivities and their fluctuations. Moreover, we demonstrate the power of the symmetry-projection techniques for fiber-reinforced composite microstructures of industrial scale.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Isogeometric Topology Optimization Based on Topological Derivatives</title>
<link>https://arxiv.org/abs/2509.09236</link>
<guid>https://arxiv.org/abs/2509.09236</guid>
<content:encoded><![CDATA[
<div> Topology optimization, isogeometric approach, topological derivatives, level-set method, immersed isogeometric framework <br />
<br />
Summary: In this work, an isogeometric approach to topology optimization driven by topological derivatives is proposed. This approach allows for seamless geometry updates without the need for remeshing. The combination of a level-set method and an immersed isogeometric framework enables topological modifications without the requirement of defining initial holes. The influence of higher-degree basis functions in both the level-set representation and solution approximation is investigated, showing that using higher-degree basis functions for the solution improves accuracy, while linear basis functions are sufficient for the level-set function representation. Two numerical examples are presented to demonstrate the effectiveness of the proposed approach. <div>
arXiv:2509.09236v1 Announce Type: cross 
Abstract: Topology optimization is a valuable tool in engineering, facilitating the design of optimized structures. However, topological changes often require a remeshing step, which can become challenging. In this work, we propose an isogeometric approach to topology optimization driven by topological derivatives. The combination of a level-set method together with an immersed isogeometric framework allows seamless geometry updates without the necessity of remeshing. At the same time, topological derivatives provide topological modifications without the need to define initial holes [7]. We investigate the influence of higher-degree basis functions in both the level-set representation and the approximation of the solution. Two numerical examples demonstrate the proposed approach, showing that employing higher-degree basis functions for approximating the solution improves accuracy, while linear basis functions remain sufficient for the level-set function representation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified RIME algorithm with covariance learning and diversity enhancement for numerical optimization</title>
<link>https://arxiv.org/abs/2509.09529</link>
<guid>https://arxiv.org/abs/2509.09529</guid>
<content:encoded><![CDATA[
<div> Covariance learning, diversity enhancement, metaheuristic algorithm, optimization, RIME.<br />
Summary:<br />
The modified RIME algorithm with covariance learning and diversity enhancement (MRIME-CD) addresses the shortcomings of the RIME algorithm by introducing three key strategies. Firstly, a covariance learning strategy increases population diversity and balances exploitation and exploration abilities. Secondly, an average bootstrapping strategy guides population search in the early stage for better global search abilities. Lastly, a new stagnation indicator and stochastic covariance learning enhance the ability to escape local optima. Validation on test sets shows that MRIME-CD improves solution accuracy, convergence speed, and stability compared to basic RIME. The algorithm outperforms in terms of performance and demonstrates its effectiveness in various experiments. <br />Summary: <div>
arXiv:2509.09529v1 Announce Type: cross 
Abstract: Metaheuristics are widely applied for their ability to provide more efficient solutions. The RIME algorithm is a recently proposed physical-based metaheuristic algorithm with certain advantages. However, it suffers from rapid loss of population diversity during optimization and is prone to fall into local optima, leading to unbalanced exploitation and exploration. To address the shortcomings of RIME, this paper proposes a modified RIME with covariance learning and diversity enhancement (MRIME-CD). The algorithm applies three strategies to improve the optimization capability. First, a covariance learning strategy is introduced in the soft-rime search stage to increase the population diversity and balance the over-exploitation ability of RIME through the bootstrapping effect of dominant populations. Second, in order to moderate the tendency of RIME population to approach the optimal individual in the early search stage, an average bootstrapping strategy is introduced into the hard-rime puncture mechanism, which guides the population search through the weighted position of the dominant populations, thus enhancing the global search ability of RIME in the early stage. Finally, a new stagnation indicator is proposed, and a stochastic covariance learning strategy is used to update the stagnant individuals in the population when the algorithm gets stagnant, thus enhancing the ability to jump out of the local optimal solution. The proposed MRIME-CD algorithm is subjected to a series of validations on the CEC2017 test set, the CEC2022 test set, and the experimental results are analyzed using the Friedman test, the Wilcoxon rank sum test, and the Kruskal Wallis test. The results show that MRIME-CD can effectively improve the performance of basic RIME and has obvious superiorities in terms of solution accuracy, convergence speed and stability.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An improved educational competition optimizer with multi-covariance learning operators for global optimization problems</title>
<link>https://arxiv.org/abs/2509.09552</link>
<guid>https://arxiv.org/abs/2509.09552</guid>
<content:encoded><![CDATA[
<div> Keywords: educational competition optimizer, metaheuristic algorithm, multi-covariance learning operators, optimization problems, constrained optimization

Summary:
The study introduces an enhanced version of the educational competition optimizer (IECO-MCO) that utilizes multi-covariance learning operators to improve performance in tackling complex optimization problems. Three distinct covariance learning operators are introduced in IECO to balance exploitation and exploration effectively, preventing premature convergence. Experimental results using benchmark functions from CEC 2017 and CEC 2022 test suites show that IECO-MCO outperforms basic ECO and other algorithms in terms of convergence speed, stability, and avoiding local optima. Statistical analyses support the superiority of IECO-MCO. The algorithm demonstrates practical applicability in solving constrained optimization problems, showcasing its robustness and effectiveness in real-world scenarios.<br /><br />Summary: <div>
arXiv:2509.09552v1 Announce Type: cross 
Abstract: The educational competition optimizer is a recently introduced metaheuristic algorithm inspired by human behavior, originating from the dynamics of educational competition within society. Nonetheless, ECO faces constraints due to an imbalance between exploitation and exploration, rendering it susceptible to local optima and demonstrating restricted effectiveness in addressing complex optimization problems. To address these limitations, this study presents an enhanced educational competition optimizer (IECO-MCO) utilizing multi-covariance learning operators. In IECO, three distinct covariance learning operators are introduced to improve the performance of ECO. Each operator effectively balances exploitation and exploration while preventing premature convergence of the population. The effectiveness of IECO is assessed through benchmark functions derived from the CEC 2017 and CEC 2022 test suites, and its performance is compared with various basic and improved algorithms across different categories. The results demonstrate that IECO-MCO surpasses the basic ECO and other competing algorithms in convergence speed, stability, and the capability to avoid local optima. Furthermore, statistical analyses, including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test, are conducted to validate the superiority of IECO-MCO over the compared algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test suites. Additionally, the practical applicability of the proposed IECO-MCO algorithm is verified by solving constrained optimization problems. The experimental outcomes demonstrate the superior performance of IECO-MCO in tackling intricate optimization problems, underscoring its robustness and practical effectiveness in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Intraday Risk Factor Collections via Hierarchical Reinforcement Learning based on Transferred Options</title>
<link>https://arxiv.org/abs/2501.07274</link>
<guid>https://arxiv.org/abs/2501.07274</guid>
<content:encoded><![CDATA[
<div> Keywords: risk factors, stock return volatility, genetic programming, Hierarchical Proximal Policy Optimization, transfer learning

Summary:
The traditional risk factors used to measure and predict stock return volatility often lag behind market dynamics. Statistical models like PCA and factor analysis struggle to capture hidden nonlinear relationships. Genetic programming (GP) can identify nonlinear factors but lacks mechanisms for evaluating factor quality and results in complex formulas. In response to these challenges, the authors propose a Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor generation and evaluation. HPPO utilizes two PPO models: a high-level policy assigns weights to stock features, and a low-level policy identifies latent nonlinear relationships. The Pearson correlation between generated factors and return volatility serves as the reward signal. Transfer learning is employed to pre-train the high-level policy on historical data and fine-tune it with the latest data. Experimental results demonstrate that the HPPO-TO algorithm achieves a 25% excess return in HFT markets across China (CSI 300/800), India (Nifty 100), and the US (S&amp;P 500). <div>
arXiv:2501.07274v3 Announce Type: replace 
Abstract: Traditional risk factors like beta, size/value, and momentum often lag behind market dynamics in measuring and predicting stock return volatility. Statistical models like PCA and factor analysis fail to capture hidden nonlinear relationships. Genetic programming (GP) can identify nonlinear factors but often lacks mechanisms for evaluating factor quality, and the resulting formulas are complex. To address these challenges, we propose a Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor generation and evaluation. HPPO uses two PPO models: a high-level policy assigns weights to stock features, and a low-level policy identifies latent nonlinear relationships. The Pearson correlation between generated factors and return volatility serves as the reward signal. Transfer learning pre-trains the high-level policy on large-scale historical data, fine-tuning it with the latest data to adapt to new features and shifts. Experiments show the HPPO-TO algorithm achieves a 25\% excess return in HFT markets across China (CSI 300/800), India (Nifty 100), and the US (S\&amp;P 500). Code and data are available at https://github.com/wencyxu/HRL-HF_risk_factor_set.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Physics-Data Enrichments to Represent Uncertainty in Reduced Gas-Surface Chemistry Models for Hypersonic Flight</title>
<link>https://arxiv.org/abs/2509.08137</link>
<guid>https://arxiv.org/abs/2509.08137</guid>
<content:encoded><![CDATA[
<div> Reaction products, thermal protection system, ablation modeling, gas-surface chemistry, data-driven enrichments

Summary:
- During hypersonic flight, reactions with air deplete a re-entry vehicle's thermal protection system (TPS).
- Accurate ablation models are crucial for assessing TPS performance.
- New finite-rate gas-surface chemistry models are improving TPS ablation modeling.
- Model reductions may be necessary for computational tractability, but can lead to discrepancies in predicted carbon monoxide production.
- Hybrid physics-based and data-driven enrichments are developed to enhance predictive capability and quantify uncertainties in low-fidelity models.
- The enrichments significantly improve accuracy with the addition of only three reactions.<br /><br />Summary: <div>
arXiv:2509.08137v1 Announce Type: new 
Abstract: During hypersonic flight, air reacts with a planetary re-entry vehicle's thermal protection system (TPS), creating reaction products that deplete the TPS. Reliable assessment of TPS performance depends on accurate ablation models. New finite-rate gas-surface chemistry models are advancing state-of-the-art in TPS ablation modeling, but model reductions that omit chemical species and reactions may be necessary in some cases for computational tractability. This work develops hybrid physics-based and data-driven enrichments to improve the predictive capability and quantify uncertainties in such low-fidelity models while maintaining computational tractability. We focus on discrepancies in predicted carbon monoxide production that arise because the low-fidelity model tracks only a subset of reactions. To address this, we embed targeted enrichments into the low-fidelity model to capture the influence of omitted reactions. Numerical results show that the hybrid enrichments significantly improve predictive accuracy while requiring the addition of only three reactions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving contact problems using Fiber Monte Carlo</title>
<link>https://arxiv.org/abs/2509.08609</link>
<guid>https://arxiv.org/abs/2509.08609</guid>
<content:encoded><![CDATA[
<div> contact algorithm, computational modeling, contact forces, geometric descriptors, finite element method <br />
Summary: 
This work presents a novel contact algorithm that computes contact forces by calculating the gradient of an energy function with respect to geometric descriptors. The algorithm, inspired by the Fiber Monte Carlo method, accurately computes contact forces for bodies with complex geometries, independent of mesh conformity. It eliminates the need for master-slave identification and projection iterations, making it easy to incorporate into existing numerical solvers like the finite element method. Various numerical examples demonstrate the algorithm's efficiency in handling a wide range of contact scenarios, from small-deformation static contact to large-deformation dynamic contact with nonlinear material behavior. Examples include Hertzian contact for small-deformation verification, contact between different-shaped bodies, contact with hyperelastic materials, and dynamic collision cases to examine transient behavior. <div>
arXiv:2509.08609v1 Announce Type: new 
Abstract: Computational modeling of contact is fundamental to many engineering applications, yet accurately and efficiently solving complex contact problems remains challenging. In this work, we propose a new contact algorithm that computes contact forces by taking the gradient of an energy function of the contact volume (overlap) with respect to the geometry descriptors. While elegant in concept, evaluating this gradient is non-trivial due to the arbitrary geometry of the contact region. Inspired by the recently proposed Fiber Monte Carlo (FMC) method, we develop an algorithm that accurately computes contact forces based on the overlap volume between bodies with complex geometries. Our computational framework operates independently of mesh conformity, eliminating the need for master-slave identification and projection iterations, thus handling arbitrary discretizations. Moreover, by removing explicit complementarity constraints, the method retains a simple structure that can be easily incorporated into existing numerical solvers, such as the finite element method. In this paper, numerical examples cover a wide range of contact scenarios, from classical small-deformation static contact to complex large-deformation dynamic contact in both two- and three-dimensional settings with nonlinear material behavior. These cases include Hertzian contact for small-deformation verification; contact between wedge- and cone-shaped bodies to assess pressure and displacement predictions at non-smooth boundaries; contact involving Neo-Hookean hyperelastic materials for evaluating nonlinear responses under finite deformation; and dynamic collision cases to examine transient behavior.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying model prediction sensitivity to model-form uncertainty</title>
<link>https://arxiv.org/abs/2509.08708</link>
<guid>https://arxiv.org/abs/2509.08708</guid>
<content:encoded><![CDATA[
<div> Model-form uncertainty, Physics-based model, Uncertainty quantification, Sensitivity analysis, Model assumptions<br />
<br />
Summary: 
Model-form uncertainty (MFU) in physics-based model development is a significant source of uncertainty. A novel method is proposed to quantify the importance of uncertainties associated with model assumptions. By using parameterized modifications to assumptions (MFU representations) and grouped variance-based sensitivity analysis, the importance of assumptions can be measured. This approach can be applied even without calibration data. However, if calibration data is available, it can inform the MFU representation. The method is shown to be effective even when there is dependence between parameters, which often occurs during calibration. This method can help prioritize resources and efforts to reduce error in model predictions by understanding the importance of model assumptions relative to other sources of uncertainty. <div>
arXiv:2509.08708v1 Announce Type: new 
Abstract: Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration).
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.08207</link>
<guid>https://arxiv.org/abs/2509.08207</guid>
<content:encoded><![CDATA[
<div> Keywords: Aurora, Exascale supercomputer, Intel Xeon Data Center GPU Max Series, High Bandwidth Memory, DAOS <br />
Summary: <br />
Aurora, the Exascale supercomputer from Argonne National Laboratory, showcases advanced technologies like Intel Xeon Data Center GPU Max Series and High Bandwidth Memory. It also features the Distributed Asynchronous Object Storage (DAOS) and utilizes Intel's oneAPI programming environment. The node architecture, HPE Slingshot interconnect, and software ecosystem of Aurora are explored in detail in this paper. Standard benchmark performance and applications readiness are highlighted through the Early Science Program and the Exascale Computing Project. The integration of Intel's Data Center GPU Max Series on each compute node provides enhanced computational power, while the innovative DAOS storage solution offers high-speed data access. This comprehensive analysis demonstrates Aurora's potential to revolutionize scientific discovery through cutting-edge technologies and infrastructure. <br /> <div>
arXiv:2509.08207v1 Announce Type: cross 
Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer, designed to accelerate scientific discovery with cutting-edge architectural innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named Ponte Vecchio) on each compute node. Aurora also integrates the Distributed Asynchronous Object Storage (DAOS), a novel exascale storage solution, and leverages Intel's oneAPI programming environment. This paper presents an in-depth exploration of Aurora's node architecture, the HPE Slingshot interconnect, the supporting software ecosystem, and DAOS. We provide insights into standard benchmark performance and applications readiness efforts via Aurora's Early Science Program and the Exascale Computing Project.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic AI Workflow to Simplify Parameter Estimation of Complex Differential Equation Systems</title>
<link>https://arxiv.org/abs/2509.07283</link>
<guid>https://arxiv.org/abs/2509.07283</guid>
<content:encoded><![CDATA[
<div> Parameter identification, Ordinary Differential Equation models, AI workflow, calibration pipeline, optimization. 
<br /> 
Summary: 
An AI workflow is introduced to streamline the parameter identification process for mechanistic Ordinary Differential Equation models. The system translates a human-readable specification into a parallel and differentiable calibration pipeline. Users input an XML description and fill in a Python code skeleton, with the AI agent handling validation and remediation of common issues. Python callables are converted to JAX functions for efficient compilation and parallelization. The workflow includes global exploration of the parameter space followed by gradient-based refinement. This approach provides a reproducible workflow that simplifies advanced calibration while maintaining expert involvement. The open-source implementation allows for quick progression from problem statement to fitted models with minimal boilerplate. <div>
arXiv:2509.07283v1 Announce Type: new 
Abstract: Parameter identification for mechanistic Ordinary Differential Equation (ODE) models underpins prediction and control in several applications, yet remains a labor-intensive and brittle process: datasets are noisy and partial, models can be stiff or misspecified, and differentiable implementations demand framework expertise. An agentic AI workflow is presented that converts a lightweight, human-readable specification into a compiled, parallel, and differentiable calibration pipeline. Users supply an XML description of the problem and fill in a Python code skeleton; the agent automatically validates consistency between spec and code, and auto-remediates common pathologies. It transforms Python callables into pure JAX functions for efficient just-in-time compilation and parallelization. The system then orchestrates a two-stage search comprising global exploration of the parameter space followed by gradient-based refinement. The result is an AD-native, reproducible workflow that lowers the barrier to advanced calibration while preserving expert control. An open-source implementation with a documented API and examples is released, enabling rapid movement from problem statement to fitted, auditable models with minimal boilerplate.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Data-Driven Framework for Efficient Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.07303</link>
<guid>https://arxiv.org/abs/2509.07303</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific discovery, data-driven, formula discovery, dimensional constraints, symbolic regression

Summary:
Scientific discovery is crucial for progress across disciplines, but identifying physical laws from datasets can be challenging and resource-intensive. This study introduces a novel approach, FIND, which utilizes the Buckingham $\Pi$ theorem and Taylor's theorem to create a unified representation of formulas. FIND focuses on determining the structure of latent formulas first, then streamlines parameter identification by adhering to dimensional constraints and simplicity in formulas. Strategic optimization techniques are employed to minimize search iterations, with complex outcomes refined using symbolic regression. Validation across 11 datasets showcases FIND's effectiveness in discovering physical laws, dimensionless numbers, partial differential equations, and critical system parameters in fields like astronomy, physics, chemistry, and electronics. The results position FIND as a versatile and powerful tool for advancing data-driven scientific discovery in diverse domains. 

<br /><br />Summary: <div>
arXiv:2509.07303v1 Announce Type: new 
Abstract: Scientific discovery drives progress across disciplines, from fundamental physics to industrial applications. However, identifying physical laws automatically from gathered datasets requires identifying the structure and parameters of the formula underlying the data, which involves navigating a vast search space and consuming substantial computational resources. To address these issues, we build on the Buckingham $\Pi$ theorem and Taylor's theorem to create a unified representation of diverse formulas, which introduces latent variables to form a two-stage structure. To minimize the search space, we initially focus on determining the structure of the latent formula, including the relevant contributing inputs, the count of latent variables, and their interconnections. Following this, the process of parameter identification is expedited by enforcing dimensional constraints for physical relevance, favoring simplicity in the formulas, and employing strategic optimization techniques. Any overly complex outcomes are refined using symbolic regression for a compact form. These general strategic techniques drastically reduce search iterations from hundreds of millions to just tens, significantly enhancing the efficiency of data-driven formula discovery. We performed comprehensive validation to demonstrate FIND's effectiveness in discovering physical laws, dimensionless numbers, partial differential equations, and uniform critical system parameters across various fields, including astronomy, physics, chemistry, and electronics. The excellent performances across 11 distinct datasets position FIND as a powerful and versatile tool for advancing data-driven scientific discovery in multiple domains.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Driven Hierarchical Sampling for Unbalanced Continual Malware Detection with Time-Series Update-Based Retrieval</title>
<link>https://arxiv.org/abs/2509.07532</link>
<guid>https://arxiv.org/abs/2509.07532</guid>
<content:encoded><![CDATA[
<div> Hierarchical balanced sampler, uncertainty-guided continual learning, dynamic class balancing, vector retrieval mechanism, Android malware detection <br />
Summary: <br />
- The study addresses challenges in Android malware detection due to concept drift and class imbalance.
- Existing replay-based methods have bias issues as they prioritize the benign class, leading to overfitting.
- A novel uncertainty-guided continual learning framework is proposed to balance benign and malicious samples and select high-information instances.
- The framework incorporates a vector retrieval mechanism using historical malware embeddings to identify evolved variants.
- Experimental results show significant performance improvement over state-of-the-art methods, achieving high true positive rate and mean accuracy, making it effective for sustainable Android malware detection. <div>
arXiv:2509.07532v1 Announce Type: new 
Abstract: Android malware detection continues to face persistent challenges stemming from long-term concept drift and class imbalance, as evolving malicious behaviors and shifting usage patterns dynamically reshape feature distributions. Although continual learning (CL) mitigates drift, existing replay-based methods suffer from inherent bias. Specifically, their reliance on classifier uncertainty for sample selection disproportionately prioritizes the dominant benign class, causing overfitting and reduced generalization to evolving malware. To address these limitations, we propose a novel uncertainty-guided CL framework. First, we introduce a hierarchical balanced sampler that employs a dual-phase uncertainty strategy to dynamically balance benign and malicious samples while simultaneously selecting high-information, high-uncertainty instances within each class. This mechanism ensures class equilibrium across both replay and incremental data, thereby enhancing adaptability to emerging threats. Second, we augment the framework with a vector retrieval mechanism that exploits historical malware embeddings to identify evolved variants via similarity-based retrieval, thereby complementing classifier updates. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods under strict low-label conditions (50 labels per phase). It achieves a true positive rate (TPR) of 92.95\% and a mean accuracy (mACC) of 94.26\%, which validates its efficacy for sustainable Android malware detection.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
<link>https://arxiv.org/abs/2509.07627</link>
<guid>https://arxiv.org/abs/2509.07627</guid>
<content:encoded><![CDATA[
<div> Framework, TCR design, epitope-specific, gene-aware Transformer, multi-architecture, LSMTCR <br />
Summary: <br />
The article introduces LSMTCR, a multi-architecture framework for designing full-length, epitope-specific TCRs. It separates specificity from constraint learning to generate paired TCRs conditioned on epitopes. By utilizing a diffusion-enhanced BERT encoder and conditional GPT decoders, LSMTCR can generate chain-specific CDR3 sequences with high predicted binding and diversity. The gene-aware Transformer ensures immunogenetic fidelity by predicting V/J usage for complete TCR assembly. LSMTCR outperforms baselines in predicted binding, grammar fidelity, and diversity on various datasets. Transfer learning improves predicted binding, length realism, and diversity for TCR generation. Full-length TCR assembly from known or de novo CDR3s maintains k-mer spectra and achieves high pTM/ipTM scores in paired co-modelling with epitopes. LSMTCR enables the generation of diverse, gene-contextualized TCR designs solely from epitope input, facilitating high-throughput screening and iterative optimization. <br /> <div>
arXiv:2509.07627v1 Announce Type: new 
Abstract: Designing full-length, epitope-specific TCR {\alpha}\b{eta} remains challenging due to vast sequence space, data biases and incomplete modeling of immunogenetic constraints. We present LSMTCR, a scalable multi-architecture framework that separates specificity from constraint learning to enable de novo, epitope-conditioned generation of paired, full-length TCRs. A diffusion-enhanced BERT encoder learns time-conditioned epitope representations; conditional GPT decoders, pretrained on CDR3\b{eta} and transferred to CDR3{\alpha}, generate chain-specific CDR3s under cross-modal conditioning with temperature-controlled diversity; and a gene-aware Transformer assembles complete {\alpha}/\b{eta} sequences by predicting V/J usage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and our curated dataset, LSMTCR achieves higher predicted binding than baselines on most datasets, more faithfully recovers positional and length grammars, and delivers superior, temperature-tunable diversity. For {\alpha}-chain generation, transfer learning improves predicted binding, length realism and diversity over representative methods. Full-length assembly from known or de novo CDR3s preserves k-mer spectra, yields low edit distances to references, and, in paired {\alpha}/\b{eta} co-modelling with epitope, attains higher pTM/ipTM than single-chain settings. LSMTCR outputs diverse, gene-contextualized, full-length TCR designs from epitope input alone, enabling high-throughput screening and iterative optimization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized eigenvalue stabilization for immersed explicit dynamics</title>
<link>https://arxiv.org/abs/2509.07632</link>
<guid>https://arxiv.org/abs/2509.07632</guid>
<content:encoded><![CDATA[
<div> Stabilization, Immersed finite element discretizations, Generalized eigenvalue stabilization, Spectral basis functions, Finite cell method <br />
Summary: <br />
This article proposes a Generalized Eigenvalue Stabilization (GEVS) strategy for element mass matrices of cut elements in explicit time integration for immersed finite element discretizations. The use of spectral basis functions and the Finite Cell Method (FCM) ensures high-order convergence and definiteness of system matrices. The GEVS approach can be applied to various immersed boundary finite element methods. Numerical experiments show that the stabilization strategy achieves optimal convergence rates and restores critical time step sizes of boundary-conforming discretizations. It is effective even with weakly enforced Dirichlet boundary conditions using Nitsche's method or penalty formulations. <div>
arXiv:2509.07632v1 Announce Type: new 
Abstract: Explicit time integration for immersed finite element discretizations severely suffers from the influence of poorly cut elements. In this contribution, we propose a generalized eigenvalue stabilization (GEVS) strategy for the element mass matrices of cut elements to cure their adverse impact on the critical time step size of the global system. We use spectral basis functions, specifically $C^0$ continuous Lagrangian interpolation polynomials defined on Gauss-Lobatto-Legendre (GLL) points, which, in combination with its associated GLL quadrature rule, yield high-order convergent diagonal mass matrices for uncut elements. Moreover, considering cut elements, we combine the proposed GEVS approach with the finite cell method (FCM) to guarantee definiteness of the system matrices. However, the proposed GEVS stabilization can directly be applied to other immersed boundary finite element methods. Numerical experiments demonstrate that the stabilization strategy achieves optimal convergence rates and recovers critical time step sizes of equivalent boundary-conforming discretizations. This also holds in the presence of weakly enforced Dirichlet boundary conditions using either Nitsche's method or penalty formulations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards automatizing detection and quantification of intestinal metaplasia: a multi-expert comparative study</title>
<link>https://arxiv.org/abs/2509.06991</link>
<guid>https://arxiv.org/abs/2509.06991</guid>
<content:encoded><![CDATA[
<div> deep learning, gastric cancer, intestinal metaplasia, risk assessment, pathologists <br />
Summary: 
- The study introduces an automated method utilizing deep learning models to detect and quantify intestinal metaplasia in gastric samples for gastric cancer risk assessment.
- Deep learning models achieved high performance in classifying intestinal metaplasia, outperforming experienced pathologists.
- The best-performing model demonstrated F1-Score of 0.80 and AUC of 0.91 in classifying intestinal metaplasia.
- Pathologists' inter-observer agreement ranged from 0.61 to 0.75, while agreement between pathologists and the deep learning model ranged from 0.37 to 0.54.
- Deep learning models offer potential for more precise and reproducible detection and quantification of intestinal metaplasia, highlighting the variability in risk assessment when visually estimating intestinal metaplasia percentage. <br /> 
Summary: <div>
arXiv:2509.06991v1 Announce Type: cross 
Abstract: Current gastric cancer risk systems are prone to errors since they evaluate a visual estimation of intestinal metaplasia percentages to assign a risk. This study presents an automated method to detect and quantify intestinal metaplasia using deep learning models as well as a comparative analysis with visual estimations of three experienced pathologists. Gastric samples were collected from two different cohorts: 149 asymptomatic volunteers from a region with a high prevalence of GCa in Colombia and 56 patients from a third-level hospital. Deep learning models were selected and trained to classify intestinal metaplasia, and predictions were used to estimate the percentage of intestinal metaplasia and assign the risk score. Results were compared with independent blinded assessments performed by three experienced pathologists. The best-performing deep learning architecture classified intestinal metaplasia with F1-Score of 0.80 +- 0.01 and AUC of 0.91 +- 0.01. Among pathologists, inter-observer agreement by a Fleiss's Kappa score ranged from 0.61 to 0.75. In comparison, agreement between the pathologists and the best-performing model ranged from 0.37 to 0.54. Deep learning models show potential to detect and quantify the percentage of intestinal metaplasia with greater precision and reproducibility than experienced pathologists. Likewise, estimated risk shows high inter-observer variability when visually assigning the intestinal metaplasia percentage.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-strategy improved gazelle optimization algorithm for solving numerical optimization and engineering applications</title>
<link>https://arxiv.org/abs/2509.07211</link>
<guid>https://arxiv.org/abs/2509.07211</guid>
<content:encoded><![CDATA[
<div> iteration-based updating framework, adaptive parameter tuning, dominant population-based restart strategy, exploration, exploitation

Summary:
The article introduces a multi-strategy improved gazelle optimization algorithm (MSIGOA) to address the limitations of the basic GOA. The proposed algorithm utilizes an iteration-based updating framework to balance exploration and exploitation, enhancing convergence speed. Adaptive parameter tuning strategies improve applicability, while a dominant population-based restart strategy helps escape local optima. Evaluation on benchmark test sets shows MSIGOA outperforms basic GOA and other advanced algorithms, with a significant improvement in exploration and exploitation capabilities. Results indicate that MSIGOA performs well on various functions, demonstrating superiority over other methods. The algorithm's effectiveness is further confirmed through engineering design optimization problems, highlighting its extensibility for practical applications. The study showcases the potential of MSIGOA in enhancing optimization processes and solving complex problems efficiently and effectively. 

<br /><br />Summary: <div>
arXiv:2509.07211v1 Announce Type: cross 
Abstract: Aiming at the shortcomings of the gazelle optimization algorithm, such as the imbalance between exploration and exploitation and the insufficient information exchange within the population, this paper proposes a multi-strategy improved gazelle optimization algorithm (MSIGOA). To address these issues, MSIGOA proposes an iteration-based updating framework that switches between exploitation and exploration according to the optimization process, which effectively enhances the balance between local exploitation and global exploration in the optimization process and improves the convergence speed. Two adaptive parameter tuning strategies improve the applicability of the algorithm and promote a smoother optimization process. The dominant population-based restart strategy enhances the algorithms ability to escape from local optima and avoid its premature convergence. These enhancements significantly improve the exploration and exploitation capabilities of MSIGOA, bringing superior convergence and efficiency in dealing with complex problems. In this paper, the parameter sensitivity, strategy effectiveness, convergence and stability of the proposed method are evaluated on two benchmark test sets including CEC2017 and CEC2022. Test results and statistical tests show that MSIGOA outperforms basic GOA and other advanced algorithms. On the CEC2017 and CEC2022 test sets, the proportion of functions where MSIGOA is not worse than GOA is 92.2% and 83.3%, respectively, and the proportion of functions where MSIGOA is not worse than other algorithms is 88.57% and 87.5%, respectively. Finally, the extensibility of MSIGAO is further verified by several engineering design optimization problems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyHexTop: a compact Python code for topology optimization using hexagonal elements</title>
<link>https://arxiv.org/abs/2310.01968</link>
<guid>https://arxiv.org/abs/2310.01968</guid>
<content:encoded><![CDATA[
<div> Topology Optimization, Python, Hexagonal Elements, Educational, Compliance Minimization

Summary:
Python-based code "PyHexTop" is introduced as an alternative to MATLAB for topology optimization, featuring hexagonal elements for design domains without checkerboard issues. Developed from the MATLAB code "HoneyTop90," it utilizes NumPy and SciPy libraries, making it easy to understand for beginners in the field. The code focuses on compliance minimization with volume constraints, demonstrated with the Messerschmitt-Bolkow-Blohm beam problem and other variations. "PyHexTop" is a valuable educational tool shared openly for learning and exploration in topology optimization. <div>
arXiv:2310.01968v4 Announce Type: replace 
Abstract: Python serves as an open-source and cost-effective alternative to the MATLAB programming language. This paper introduces a concise topology optimization Python code, named ``\texttt{PyHexTop}," primarily intended for educational purposes. Code employs hexagonal elements to parameterize design domains as such elements provide checkerboard-free optimized design naturally. \texttt{PyHexTop} is developed based on the ``\texttt{HoneyTop90}" MATLAB code~\cite{kumar2023honeytop90} and uses the \texttt{NumPy} and \texttt{SciPy} libraries. Code is straightforward and easily comprehensible, proving a helpful tool that can help people new in the topology optimization field to learn and explore. \texttt{PyHexTop} is specifically tailored to address compliance minimization with specified volume constraints. The paper provides a detailed explanation of the code for solving the Messerschmitt-Bolkow-Blohm beam and extensions to solve problems different problems. The code is publicly shared at: https://github.com/PrabhatIn/PyHexTop
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOaCNN: Adaptive Convolutional Neural Network for Multidisciplinary Topology Optimization</title>
<link>https://arxiv.org/abs/2310.02069</link>
<guid>https://arxiv.org/abs/2310.02069</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive convolutional neural network, topology optimization, encoder-decoder networks, dense layers, compliance minimization problems

Summary:<br />
This paper introduces an adaptive convolutional neural network (CNN) architecture designed to automate a variety of topology optimization (TO) problems with different physical constraints. The network utilizes encoder-decoder networks with dense layers, incorporating an adaptive layer to capture complex geometric features. Trained on datasets from three open-source TO codes with varying physics, the CNN demonstrates robustness and success in compliance minimization tasks involving constant and design-dependent loads, as well as material bulk modulus optimization. Able to generate optimized designs quickly based on user input of volume fraction, the architecture closely matches results obtained from existing TO codes with minimal errors in performance and volume fraction. <div>
arXiv:2310.02069v2 Announce Type: replace 
Abstract: This paper presents an adaptive convolutional neural network (CNN) architecture that can automate diverse topology optimization (TO) problems having different underlying physics. The architecture uses the encoder-decoder networks with dense layers in the middle which includes an additional adaptive layer to capture complex geometrical features. The network is trained using the dataset obtained from the three open-source TO codes involving different physics. The robustness and success of the presented adaptive CNN are demonstrated on compliance minimization problems with constant and design-dependent loads and material bulk modulus optimization. The architecture takes the user's input of the volume fraction. It instantly generates optimized designs resembling their counterparts obtained via open-source TO codes with negligible performance and volume fraction error.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expanding Hardware-Efficiently Manipulable Hilbert Space via Hamiltonian Embedding</title>
<link>https://arxiv.org/abs/2401.08550</link>
<guid>https://arxiv.org/abs/2401.08550</guid>
<content:encoded><![CDATA[
<div> Quantum, Sparse Hamiltonian, Simulation, Hamiltonian Embedding, Quantum Applications <br />
Summary: 
The paper introduces the concept of Hamiltonian embedding as a technique for efficient quantum simulation of sparse Hamiltonians. This approach involves embedding the sparse Hamiltonian into a larger and more structured quantum system to enable more efficient simulation using hardware-efficient operations. Through a systematic study, the researchers demonstrate significant savings in computational resources, making it possible to implement quantum walks on complex graphs, quantum spatial search, and simulate real-space Schrödinger equations on current quantum platforms. The technique enhances the implementability of quantum advantages in the NISQ era by expanding the possibilities for quantum algorithms design. This advancement paves the way for practical implementation of quantum applications that depend on efficient sparse Hamiltonian simulation, offering a promising outlook for near-term quantum computing technology. <br /><br /> <div>
arXiv:2401.08550v2 Announce Type: replace-cross 
Abstract: Many promising quantum applications depend on the efficient quantum simulation of an exponentially large sparse Hamiltonian, a task known as sparse Hamiltonian simulation, which is fundamentally important in quantum computation. Although several theoretically appealing quantum algorithms have been proposed for this task, they typically require a black-box query model of the sparse Hamiltonian, rendering them impractical for near-term implementation on quantum devices.
  In this paper, we propose a technique named Hamiltonian embedding. This technique simulates a desired sparse Hamiltonian by embedding it into the evolution of a larger and more structured quantum system, allowing for more efficient simulation through hardware-efficient operations. We conduct a systematic study of this new technique and demonstrate significant savings in computational resources for implementing prominent quantum applications. As a result, we can now experimentally realize quantum walks on complicated graphs (e.g., binary trees, glued-tree graphs), quantum spatial search, and the simulation of real-space Schr\"odinger equations on current trapped-ion and neutral-atom platforms. Given the fundamental role of Hamiltonian evolution in the design of quantum algorithms, our technique markedly expands the horizon of implementable quantum advantages in the NISQ era.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Newton to Einstein: Axiom-Based Discovery via Game Design</title>
<link>https://arxiv.org/abs/2509.05448</link>
<guid>https://arxiv.org/abs/2509.05448</guid>
<content:encoded><![CDATA[
<div> machine learning, scientific discovery, axiom-based reasoning, rule-evolving system, interpretability

Summary: 
This position paper advocates for a shift in machine learning for scientific discovery from inductive pattern recognition to axiom-based reasoning. The proposed framework treats scientific inquiry as a rule-evolving system, where agents operate within environments governed by axioms and modify them to explain outlier observations. Unlike traditional machine learning approaches, this method allows for the discovery of new theoretical structures through systematic rule adaptation. Preliminary experiments in logic-based games demonstrate the feasibility of this approach by showing that agents can evolve axioms to solve previously unsolvable problems. This framework provides a basis for developing machine learning systems capable of creative, interpretable, and theory-driven discovery. <div>
arXiv:2509.05448v1 Announce Type: new 
Abstract: This position paper argues that machine learning for scientific discovery should shift from inductive pattern recognition to axiom-based reasoning. We propose a game design framework in which scientific inquiry is recast as a rule-evolving system: agents operate within environments governed by axioms and modify them to explain outlier observations. Unlike conventional ML approaches that operate within fixed assumptions, our method enables the discovery of new theoretical structures through systematic rule adaptation. We demonstrate the feasibility of this approach through preliminary experiments in logic-based games, showing that agents can evolve axioms that solve previously unsolvable problems. This framework offers a foundation for building machine learning systems capable of creative, interpretable, and theory-driven discovery.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUIDe: Generative and Uncertainty-Informed Inverse Design for On-Demand Nonlinear Functional Responses</title>
<link>https://arxiv.org/abs/2509.05641</link>
<guid>https://arxiv.org/abs/2509.05641</guid>
<content:encoded><![CDATA[
<div> Machine learning, generative models, inverse design, nonlinear systems, probabilistic modeling <br />
Summary: The article discusses the challenges of inverse design problems in engineering, particularly when dealing with nonlinear system responses. Traditional methods such as deep generative models and optimization-based approaches may yield unreliable solutions or incomplete coverage of the solution space. To overcome this, the Generative and Uncertainty-informed Inverse Design (GUIDe) framework is proposed, leveraging probabilistic machine learning and statistical inference. Unlike traditional inverse models, GUIDe uses a design-to-response strategy to generate designs with targeted nonlinear behaviors. By predicting each design's nonlinear functional response and evaluating the confidence that a design will meet the target, GUIDe enables the discovery of diverse feasible solutions, even for out-of-distribution targets. The method is validated through the design of interface properties for nacre-inspired composites to achieve target stress-strain responses. <div>
arXiv:2509.05641v1 Announce Type: new 
Abstract: Inverse design problems are pervasive in engineering, particularly when dealing with nonlinear system responses, such as in mechanical behavior or spectral analysis. The inherent intractability, non-existence, or non-uniqueness of their solutions, and the need for swift exploration of the solution space necessitate the adoption of machine learning and data-driven approaches, such as deep generative models. Here, we show that both deep generative model-based and optimization-based methods can yield unreliable solutions or incomplete coverage of the solution space. To address this, we propose the Generative and Uncertainty-informed Inverse Design (GUIDe) framework, leveraging probabilistic machine learning, statistical inference, and Markov chain Monte Carlo sampling to generate designs with targeted nonlinear behaviors. Unlike inverse models that directly map response to design, i.e., response $\mapsto$ design, we employ a design $\mapsto$ response strategy: a forward model that predicts each design's nonlinear functional response allows GUIDe to evaluate the confidence that a design will meet the target, conditioned on a target response with a user-specified tolerance level. Then, solutions are generated by sampling the solution space based on the confidence. We validate the method by designing the interface properties for nacre-inspired composites to achieve target stress-strain responses. Results show that GUIDe enables the discovery of diverse feasible solutions, including those outside the training data range, even for out-of-distribution targets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-based Topology Optimization</title>
<link>https://arxiv.org/abs/2509.05800</link>
<guid>https://arxiv.org/abs/2509.05800</guid>
<content:encoded><![CDATA[
<div> Transformer-based machine learning model, topology optimization, transfer learning, FFT encoding, auxiliary loss functions

Summary:
The study introduces a transformer-based machine learning model for topology optimization that incorporates critical boundary and loading conditions using a class token mechanism. Transfer learning and FFT encoding are utilized to enhance performance on dynamic datasets. The model includes auxiliary loss functions to improve the realism and manufacturability of generated designs. Performance evaluation shows that the model approaches the fidelity of diffusion-based models while eliminating the need for iterations. It achieves low compliance error, volume fraction error, floating material percentage, and load discrepancy error, demonstrating its potential for real-time, high-fidelity topology generation. <div>
arXiv:2509.05800v1 Announce Type: new 
Abstract: Topology optimization enables the design of highly efficient and complex structures, but conventional iterative methods, such as SIMP-based approaches, often suffer from high computational costs and sensitivity to initial conditions. Although machine learning methods have recently shown promise for accelerating topology generation, existing models either remain iterative or struggle to match ground-truth performance. In this work, we propose a transformer-based machine learning model for topology optimization that embeds critical boundary and loading conditions directly into the tokenized domain representation via a class token mechanism. We implement this model on static and dynamic datasets, using transfer learning and FFT encoding of dynamic loads to improve our performance on the dynamic dataset. Auxiliary loss functions are introduced to promote the realism and manufacturability of the generated designs. We conduct a comprehensive evaluation of the model's performance, including compliance error, volume fraction error, floating material percentage, and load discrepancy error, and benchmark it against state-of-the-art non-iterative and iterative generative models. Our results demonstrate that the proposed model approaches the fidelity of diffusion-based models while remaining iteration-free, offering a significant step toward real-time, high-fidelity topology generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distortion Minimization in Reverse Engineering for Additive Manufacturing: An Integrated 3D Scanning and Simulation Framework</title>
<link>https://arxiv.org/abs/2509.05857</link>
<guid>https://arxiv.org/abs/2509.05857</guid>
<content:encoded><![CDATA[
<div> Keywords: reverse engineering, additive manufacturing, 3D scanning, process simulation, geometric distortions

Summary:
This paper introduces a new framework for reverse engineering additively manufactured components, focusing on minimizing distortions. The framework combines 3D scanning with process simulation to predict geometric distortions and minimize errors between predicted and measured dimensions. The approach was demonstrated on Inconel-718 components produced using laser powder bed fusion additive manufacturing. The framework generates compensated STL and parametric CAD models, eliminating the need for experimental adjustments. The CAD-based method showed better accuracy, with an average absolute percent error of 0.087% between predicted and measured dimensions. This integrated approach offers a promising solution for reverse engineering and additive manufacturing processes, particularly for parts with complex geometries and high process-induced distortions. <div>
arXiv:2509.05857v1 Announce Type: new 
Abstract: Reverse engineering can be used to derive a 3D model of an existing physical part when such a model is not readily available. For parts that will be fabricated with subtractive and formative manufacturing processes, existing reverse engineering techniques can be readily applied, but parts produced with additive manufacturing can present new challenges due to the high level of process-induced distortions and unique part attributes. This paper introduces an integrated 3D scanning and process simulation data-driven framework to minimize distortions of reverse-engineered additively manufactured components. This framework employs iterative finite element simulations to predict geometric distortions to minimize errors between the predicted and measured geometrical deviations of the key dimensional characteristics of the part. The effectiveness of this approach is then demonstrated by reverse engineering two Inconel-718 components manufactured using laser powder bed fusion additive manufacturing. This paper presents a remanufacturing process that combines reverse engineering and additive manufacturing, leveraging geometric feature-based part compensation through process simulation. Our approach can generate both compensated STL and parametric CAD models, eliminating laborious experimentation during reverse engineering. We evaluate the merits of STL-based and CAD-based approaches by quantifying the errors induced at the different steps of the proposed approach and analyzing the influence of varying part geometries. Using the proposed CAD-based method, the average absolute percent error between simulation-predicted distorted dimensions and actual measured dimensions of the manufactured parts was 0.087%, with better accuracy than the STL-based method.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anticipating AMOC transitions via deep learning</title>
<link>https://arxiv.org/abs/2509.06450</link>
<guid>https://arxiv.org/abs/2509.06450</guid>
<content:encoded><![CDATA[
<div> AMOC, abrupt transitions, Earth system, convolutional neural network, early warning indicators
Summary: Key components of the Earth system, such as the Atlantic Meridional Overturning Circulation (AMOC), can undergo abrupt and potentially irreversible transitions when external forcing exceeds critical thresholds. This study explores the challenges of predicting such transitions, which can be induced by bifurcations, critical forcing rates, and noise. Traditional early warning indicators based on critical slowing down are unreliable in the stochastic regime of these transitions. To address this limitation, a convolutional neural network (CNN)-based approach is developed to identify statistical differences between transitioning and non-transitioning trajectories within ensemble simulations. This CNN-based indicator allows for real-time prediction of transition probabilities for individual trajectories prior to tipping points. The results demonstrate the effectiveness of this approach in providing early warnings for abrupt transitions of Earth system components, highlighting the importance of identifying safe operating spaces and early warning indicators under uncertainty. 
<br /><br />Summary: <div>
arXiv:2509.06450v1 Announce Type: new 
Abstract: Key components of the Earth system can undergo abrupt and potentially irreversible transitions when the magnitude or rate of external forcing exceeds critical thresholds. In this study, we use the example of the Atlantic Meridional Overturning Circulation (AMOC) to demonstrate the challenges associated with anticipating such transitions when the system is susceptible to bifurcation-induced, rate-induced, and noise-induced tipping. Using a calibrated AMOC box model, we conduct large ensemble simulations and show that transition behavior is inherently probabilistic: under identical freshwater forcing scenarios, some ensemble members exhibit transitions while others do not. In this stochastic regime, traditional early warning indicators based on critical slowing down are unreliable in predicting impending transitions. To address this limitation, we develop a convolutional neural network (CNN)-based approach that identifies higher-order statistical differences between transitioning and non-transitioning trajectories within the ensemble realizations. This method enables the real-time prediction of transition probabilities for individual trajectories prior to the onset of tipping. Our results show that the CNN-based indicator provides effective early warnings in a system where transitions can be induced by bifurcations, critical forcing rates, and noise. These findings underscore the potential in identifying safe operating spaces and early warning indicators for abrupt transitions of Earth system components under uncertainty.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reusable Surrogate Models for Distillation Columns</title>
<link>https://arxiv.org/abs/2509.06638</link>
<guid>https://arxiv.org/abs/2509.06638</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, chemical process engineering, distillation columns, ML-fueled modelfluid representation, entrainer distillation<br />
Summary:<br />
This article introduces a novel approach to surrogate modeling in chemical process engineering, aiming to create reusable models for distillation columns. By implementing a new ML-fueled modelfluid representation, the researchers were able to generate a vast dataset of over $1,000,000 samples, allowing the surrogate model to generalize across various column specifications and chemical compositions. The model's accuracy was validated, and it was successfully applied in a case study on entrainer distillation, where it efficiently screened and ranked candidate entrainers. This approach has the potential to significantly reduce computational efforts in optimization tasks compared to traditional flowsheet simulators, marking a paradigm shift towards more versatile and efficient surrogate models in chemical engineering.<br /> 
Summary: <div>
arXiv:2509.06638v1 Announce Type: new 
Abstract: Surrogate modeling is a powerful methodology in chemical process engineering, frequently employed to accelerate optimization tasks where traditional flowsheet simulators are computationally prohibitive. However, the state-of-the-art is dominated by surrogate models trained for a narrow range of fixed chemical systems and operating conditions, limiting their reusability. This work introduces a paradigm shift towards reusable surrogates by developing a single model for distillation columns that generalizes across a vast design space. The key enabler is a novel ML-fueled modelfluid representation which allows for the generation of datasets of more than $1,000,000$ samples. This allows the surrogate to generalize not only over column specifications but also over the entire chemical space of homogeneous ternary vapor-liquid mixtures. We validate the model's accuracy and demonstrate its practical utility in a case study on entrainer distillation, where it successfully screens and ranks candidate entrainers, significantly reducing the computational effort compared to rigorous optimization.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing</title>
<link>https://arxiv.org/abs/2509.05794</link>
<guid>https://arxiv.org/abs/2509.05794</guid>
<content:encoded><![CDATA[
<div> microservices, stateful services, migration, Kubernetes, optimization
Summary:<br />
- The paper addresses the challenge of migrating stateful microservices in Kubernetes, proposing an optimized scheme that integrates the MS2M framework with Kubernetes' FCC feature.
- Key enhancements include support for migrating StatefulSet-managed Pods and a Threshold-Based Cutoff Mechanism to manage high message rates.
- Evaluation results show that MS2M for individual Pods significantly reduces downtime by 96.986% compared to cold migration methods.
- The StatefulSet approach offers greater flexibility in managing stateful services within Kubernetes.
- The insights provided offer practical strategies for optimizing stateful microservice migration in cloud-native environments.<br />Summary: <div>
arXiv:2509.05794v1 Announce Type: cross 
Abstract: The widespread adoption of microservices architecture in modern software systems has emphasized the need for efficient management of distributed services. While stateless microservices enable straightforward migration, stateful microservices introduce added complexity due to the need to preserve in-memory state during migration. However, most container orchestrators, including Kubernetes, lack native support for live stateful service migration. This paper proposes an optimized migration scheme for stateful services in Kubernetes by integrating the Message-based Stateful Microservice Migration (MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC) feature. Key enhancements include support for migrating StatefulSet-managed Pods and the introduction of a Threshold-Based Cutoff Mechanism to handle high incoming message rates. Evaluation results demonstrate that MS2M for individual Pods reduces downtime by 96.986% compared to cold migration methods, while the StatefulSet approach provides greater flexibility in managing stateful services. These insights provide practical strategies for optimizing stateful microservice migration in cloud-native environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection</title>
<link>https://arxiv.org/abs/2509.06035</link>
<guid>https://arxiv.org/abs/2509.06035</guid>
<content:encoded><![CDATA[
<div> detection, transmission lines, UAVs, small defects, power grids  
TinyDef-DETR is a framework for small-defect detection in transmission lines using UAVs. It addresses challenges such as detail loss, weak boundary sensitivity, and lack of global context integration. The method includes a stride-free space-to-depth module for downsampling, edge-enhanced convolution for boundary awareness, and dual-domain multi-scale attention for global and local information capture. It also uses a Focaler-Wise-SIoU regression loss for improved localization of small objects. TinyDef-DETR outperforms competitive baselines in precision and recall, especially for small objects, with minimal computational overhead. Validation on the VisDrone benchmark confirms the approach's generalization capability for power grid inspections. Overall, the integration of detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression offers an efficient solution for UAV-based small-defect inspection in power grids.<br /><br />Summary: <div>
arXiv:2509.06035v1 Announce Type: cross 
Abstract: Automated inspection of transmission lines using UAVs is hindered by the difficulty of detecting small and ambiguous defects against complex backgrounds. Conventional detectors often suffer from detail loss due to strided downsampling, weak boundary sensitivity in lightweight backbones, and insufficient integration of global context with local cues. To address these challenges, we propose TinyDef-DETR, a DETR-based framework designed for small-defect detection. The method introduces a stride-free space-to-depth module for lossless downsampling, an edge-enhanced convolution for boundary-aware feature extraction, a cross-stage dual-domain multi-scale attention module to jointly capture global and local information, and a Focaler-Wise-SIoU regression loss to improve localization of small objects. Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR achieves substantial improvements in both precision and recall compared to competitive baselines, with particularly notable gains on small-object subsets, while incurring only modest computational overhead. Further validation on the VisDrone benchmark confirms the generalization capability of the proposed approach. Overall, the results indicate that integrating detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression provides a practical and efficient solution for UAV-based small-defect inspection in power grids.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation of CNN Ensemble Results for Enhanced Long-Term Prediction of the ENSO Phenomenon</title>
<link>https://arxiv.org/abs/2509.06227</link>
<guid>https://arxiv.org/abs/2509.06227</guid>
<content:encoded><![CDATA[
<div> Skill, ENSO forecasting, Deep learning, Ensemble members, Climate science  
Summary:  
- The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO) remains a challenge in climate science.  
- Operational systems often use the mean of all ensemble members assuming equal skill, but a subset of members show higher skill levels.  
- Study using a state-of-the-art ENSO forecast system found Top-5 subsets with significantly higher correlation and lower RMSE compared to the mean.  
- Improvement in skill is most pronounced at extreme lead times, crucial transition periods like SON and DJF, and season-dependent months like JJA and MJJ.  
- Identification of high-quality ensemble members could enhance forecasting skill and provide clues for future investigations.  

Summary: <div>
arXiv:2509.06227v1 Announce Type: cross 
Abstract: The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO) is still one of the biggest challenges in climate science. While it is true that short-to medium-range performance has been improved significantly using the advances in deep learning, statistical dynamical hybrids, most operational systems still use the simple mean of all ensemble members, implicitly assuming equal skill across members. In this study, we demonstrate, through a strictly a-posteriori evaluation , for any large enough ensemble of ENSO forecasts, there is a subset of members whose skill is substantially higher than that of the ensemble mean. Using a state-of-the-art ENSO forecast system cross-validated against the 1986-2017 observed Nino3.4 index, we identify two Top-5 subsets one ranked on lowest Root Mean Square Error (RMSE) and another on highest Pearson correlation. Generally across all leads, these outstanding members show higher correlation and lower RMSE, with the advantage rising enormously with lead time. Whereas at short leads (1 month) raises the mean correlation by about +0.02 (+1.7%) and lowers the RMSE by around 0.14 {\deg}C or by 23.3% compared to the All-40 mean, at extreme leads (23 months) the correlation is raised by +0.43 (+172%) and RMSE by 0.18 {\deg}C or by 22.5% decrease. The enhancements are largest during crucial ENSO transition periods such as SON and DJF, when accurate amplitude and phase forecasting is of greatest socio-economic benefit, and furthermore season-dependent e.g., mid-year months such as JJA and MJJ have incredibly large RMSE reductions. This study provides a solid foundation for further investigations to identify reliable clues for detecting high-quality ensemble members, thereby enhancing forecasting skill.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
<link>https://arxiv.org/abs/2509.06465</link>
<guid>https://arxiv.org/abs/2509.06465</guid>
<content:encoded><![CDATA[
<div> Keywords: Antibody binding site prediction, CAME-AB, multimodal representation, cross-modal reasoning, contrastive learning.

Summary: 
Antibody binding site prediction is crucial for computational immunology and antibody design. Existing methods often lack in representation and fail to identify antibody-specific binding sites. In this study, a novel framework called CAME-AB is introduced, which integrates multiple biologically grounded modalities to create a robust multimodal representation for antibody binding site prediction. The framework includes raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs. An adaptive modality fusion module dynamically weighs each modality for cross-modal reasoning, while a Transformer encoder with a Mixture-of-Experts module enhances feature specialization. Supervised contrastive learning shapes the latent space geometry for improved performance. Experimental results on antibody-antigen datasets show that CAME-AB outperforms existing baselines on various metrics. Ablation studies confirm the effectiveness of each architectural component and the benefits of multimodal feature integration. <div>
arXiv:2509.06465v1 Announce Type: cross 
Abstract: Antibody binding site prediction plays a pivotal role in computational immunology and therapeutic antibody design. Existing sequence or structure methods rely on single-view features and fail to identify antibody-specific binding sites on the antigens-a dual limitation in representation and prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding site prediction. CAME-AB integrates five biologically grounded modalities, including raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs-into a unified multimodal representation. To enhance adaptive cross-modal reasoning, we propose an adaptive modality fusion module that learns to dynamically weight each modality based on its global relevance and input-specific contribution. A Transformer encoder combined with an MoE module further promotes feature specialization and capacity expansion. We additionally incorporate a supervised contrastive learning objective to explicitly shape the latent space geometry, encouraging intra-class compactness and inter-class separability. To improve optimization stability and generalization, we apply stochastic weight averaging during training. Extensive experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB consistently outperforms strong baselines on multiple metrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further validate the effectiveness of each architectural component and the benefit of multimodal feature integration. The model implementation details and the codes are available on https://anonymous.4open.science/r/CAME-AB-C525
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A machine-learned expression for the excess Gibbs energy</title>
<link>https://arxiv.org/abs/2509.06484</link>
<guid>https://arxiv.org/abs/2509.06484</guid>
<content:encoded><![CDATA[
<div> neural network, Gibbs energy, thermodynamic properties, liquid mixtures, multi-component mixtures <br />
<br />Summary: 
HANNA, a neural network model, was developed to predict the excess Gibbs energy of multi-component liquid mixtures based on molecular structures. Physical laws were integrated as constraints to ensure thermodynamic consistency in predictions. The model was trained on a comprehensive experimental dataset and included a novel solver to incorporate liquid-liquid equilibrium data. A geometric projection method enabled accurate extrapolations to multi-component mixtures without the need for additional parameters. HANNA significantly outperformed existing methods in terms of accuracy and scope. The trained model and code are publicly available, with an interactive interface provided on the MLPROP website. <div>
arXiv:2509.06484v1 Announce Type: cross 
Abstract: The excess Gibbs energy plays a central role in chemical engineering and chemistry, providing a basis for modeling the thermodynamic properties of liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures solely from the molecular structures of their components is a long-standing challenge. In this work, we address this challenge by integrating physical laws as hard constraints within a flexible neural network. The resulting model, HANNA, was trained end-to-end on an extensive experimental dataset for binary mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent predictions. A novel surrogate solver developed in this work enabled the inclusion of liquid-liquid equilibrium data in the training process. Furthermore, a geometric projection method was applied to enable robust extrapolations to multi-component mixtures, without requiring additional parameters. We demonstrate that HANNA delivers excellent predictions, clearly outperforming state-of-the-art benchmark methods in accuracy and scope. The trained model and corresponding code are openly available, and an interactive interface is provided on our website, MLPROP.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Parallel Solver with Multiphysics Finite Element Method for Poroelasticity Coupled with Elasticity Model</title>
<link>https://arxiv.org/abs/2509.06673</link>
<guid>https://arxiv.org/abs/2509.06673</guid>
<content:encoded><![CDATA[
<div> pressure, poroelasticity, elasticity, Lagrange multiplier, parallel solver

Summary: 
This paper presents a parallel solver for the quasi-static linear poroelasticity and linear elasticity model in the Lagrange multiplier framework. The model is reformulated as a coupling of nearly incompressible elasticity and unsteady advection-diffusion equations, with new variables introduced to ensure normal stress continuity. Variational formulations and finite element methods are employed for each subdomain, with a parallel solver using the FETI method and FETI preconditioner for efficiency. Numerical tests demonstrate computational efficiency and convergence error order, and the model is validated using Barry-Mercer's model as a benchmark. The results show no oscillations in computed pressure, affirming the effectiveness of the proposed parallel solver for solving poroelasticity and elasticity models. 

<br /><br />Summary: <div>
arXiv:2509.06673v1 Announce Type: cross 
Abstract: In this paper, we propose a parallel solver for solving the quasi-static linear poroelasticity coupled with linear elasticity model in the Lagrange multiplier framework. Firstly, we reformulate the model into a coupling of the nearly incompressible elasticity and an unsteady affection-diffusion equations by setting new variable ``elastic pressure" and ``volumetric fluid content". And we introduce a Lagrange multiplier to guarantee the normal stress continuity on the interface. Then, we give the variational formulations in each subdomain and choose the $\boldsymbol{P}_k$-$P_1$-$P_1$ mixed finite element tuple for poroelasticity subdomain, and $\boldsymbol{P}_k$-$P_1$ finite element pair ($k=1,2$) for elasticity subdomain and the backward Euler scheme for time. Also, we propose a parallel solver for solving the fully discrete scheme at each time step-- the FETI method with a classical FETI preconditioner for solving the Lagrange multiplier and calculating the subproblems in each subdomain in parallel. And we show several numerical tests to validate the computational efficiency and the convergence error order, and we consider Barry-Mercer's model as the benchmark test to show that there no oscillation in the computed pressure. Finally, we draw conclusions to summarize the main results of this paper.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A simple and efficient hybrid discretization approach to alleviate membrane locking in isogeometric thin shells</title>
<link>https://arxiv.org/abs/2312.16944</link>
<guid>https://arxiv.org/abs/2312.16944</guid>
<content:encoded><![CDATA[
<div> membrane locking, isogeometric finite element, Kirchhoff-Love shells, hybrid discretization, stress recovery  
Summary:  
This work introduces a hybrid discretization technique to address membrane locking in isogeometric finite element models for Kirchhoff-Love shells. The method, compatible with existing isogeometric finite element codes, combines isogeometric and Lagrange-based surface discretizations without increasing the tangent matrix bandwidth or requiring additional degrees of freedom or static condensation. It proves effective for both linear and nonlinear problems, with simplified stress recovery. By incorporating quadratic NURBS surfaces, the approach significantly improves accuracy in membrane stresses relative to traditional methods. Rigorous analysis on various benchmark problems confirms the efficacy of the proposed technique in alleviating or eliminating membrane locking, suggesting potential extensions to other discretization types and constraints. <div>
arXiv:2312.16944v2 Announce Type: replace 
Abstract: This work presents a new hybrid discretization approach to alleviate membrane locking in isogeometric finite element formulations for Kirchhoff-Love shells. The approach is simple, and requires no additional dofs and no static condensation. It does not increase the bandwidth of the tangent matrix and is effective for both linear and nonlinear problems. It combines isogeometric surface discretizations with classical Lagrange-based surface discretizations, and can thus be run with existing isogeometric finite element codes. Also, the stresses can be recovered straightforwardly. The effectiveness of the proposed approach in alleviating, if not eliminating, membrane locking is demonstrated through the rigorous study of the convergence behavior of several classical benchmark problems. Accuracy gains are particularly large in the membrane stresses. The approach is formulated here for quadratic NURBS, but an extension to other discretization types can be anticipated. The same applies to other constraints and associated locking phenomena.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection-based model-order reduction via graph autoencoders suited for unstructured meshes</title>
<link>https://arxiv.org/abs/2407.13669</link>
<guid>https://arxiv.org/abs/2407.13669</guid>
<content:encoded><![CDATA[
<div> Graph autoencoder, projection-based model-order reduction, nonlinear manifold least-squares Petrov-Galerkin projection scheme, advection-dominated flows, unstructured meshes<br />
<br />
Summary: <br />
This paper introduces a graph autoencoder architecture, GD-LSPG, for projection-based model-order reduction in advection-dominated flow simulations on unstructured meshes. The architecture combines reduced graph hierarchy generation and message passing operations to emulate the filtering process of CNNs, allowing for improved flexibility and interpretability. GD-LSPG's latent state variables offer interpretable mode shapes similar to proper orthogonal decomposition modes. The framework is demonstrated on a one-dimensional Burgers' model with a structured mesh and two test cases for two-dimensional Euler equations using an unstructured mesh, showcasing its flexibility and accuracy compared to traditional affine projections. This approach provides a significant enhancement in accuracy for low-dimensional latent spaces and outperforms CNN-based autoencoders, making it a promising tool for model reduction in computational fluid dynamics. <br /> <div>
arXiv:2407.13669v4 Announce Type: replace 
Abstract: This paper presents the development of a graph autoencoder architecture capable of performing projection-based model-order reduction (PMOR) using a nonlinear manifold least-squares Petrov-Galerkin (LSPG) projection scheme. The architecture is particularly useful for advection-dominated flows modeled by unstructured meshes, as it provides a robust nonlinear mapping that can be leveraged in a PMOR setting. The presented graph autoencoder is constructed with a two-part process that consists of (1) generating a hierarchy of reduced graphs to emulate the compressive abilities of convolutional neural networks (CNNs) and (2) training a message passing operation at each step in the hierarchy of reduced graphs to emulate the filtering process of a CNN. The resulting framework provides improved flexibility over traditional CNN-based autoencoders because it is readily extendable to unstructured meshes. We provide an analysis of the interpretability of the graph autoencoder's latent state variables, where we find that the Jacobian of the decoder for the proposed graph autoencoder provides interpretable mode shapes akin to traditional proper orthogonal decomposition modes. To highlight the capabilities of the proposed framework, which is named geometric deep least-squares Petrov-Galerkin (GD-LSPG), we benchmark the method on a one-dimensional Burgers' model with a structured mesh and demonstrate the flexibility of GD-LSPG by deploying it on two test cases for two-dimensional Euler equations that use an unstructured mesh. The proposed framework is more flexible than using a traditional CNN-based autoencoder and provides considerable improvement in accuracy for very low-dimensional latent spaces in comparison with traditional affine projections.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed-precision numerics in scientific applications: survey and perspectives</title>
<link>https://arxiv.org/abs/2412.19322</link>
<guid>https://arxiv.org/abs/2412.19322</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, mixed-precision, scientific applications, algorithmic innovations, computational science
<br />
Summary: 
This article discusses the utilization of mixed-precision computations in artificial intelligence (AI) workloads and scientific applications. It highlights the potential for significant performance improvements up to 8x compared to double-precision in compute-intensive tasks. The survey covers various scientific domains like fluid dynamics, weather and climate, quantum chemistry, and computational genomics that have started implementing mixed-precision strategies. State-of-the-art algorithmic techniques such as iterative refinement and adaptive precision solvers are examined for their implications on accuracy, performance, and resource utilization. The article also discusses the emerging software ecosystem supporting mixed-precision methods at scale. Overall, the survey emphasizes the transformative impact of mixed-precision numerics in computational science, emphasizing the importance of aligning algorithms with evolving hardware capabilities. 
<br /><br /> <div>
arXiv:2412.19322v3 Announce Type: replace 
Abstract: The explosive demand for artificial intelligence (AI) workloads has led to a significant increase in silicon area dedicated to lower-precision computations on recent high-performance computing hardware designs. However, mixed-precision capabilities, which can achieve performance improvements of 8x compared to double-precision in extreme compute-intensive workloads, remain largely untapped in most scientific applications. A growing number of efforts have shown that mixed-precision algorithmic innovations can deliver superior performance without sacrificing accuracy. These developments should prompt computational scientists to seriously consider whether their scientific modeling and simulation applications could benefit from the acceleration offered by new hardware and mixed-precision algorithms. In this survey, we (1) review progress across diverse scientific domains -- including fluid dynamics, weather and climate, quantum chemistry, and computational genomics -- that have begun adopting mixed-precision strategies; (2) examine state-of-the-art algorithmic techniques such as iterative refinement, splitting and emulation schemes, and adaptive precision solvers; (3) assess their implications for accuracy, performance, and resource utilization; and (4) survey the emerging software ecosystem that enables mixed-precision methods at scale. We conclude with perspectives and recommendations on cross-cutting opportunities, domain-specific challenges, and the role of co-design between application scientists, numerical analysts and computer scientists. Collectively, this survey underscores that mixed-precision numerics can reshape computational science by aligning algorithms with the evolving landscape of hardware capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical optimization of aviation decarbonization scenarios: balancing traffic and emissions with maturing energy carriers and aircraft technology</title>
<link>https://arxiv.org/abs/2503.22435</link>
<guid>https://arxiv.org/abs/2503.22435</guid>
<content:encoded><![CDATA[
<div> Aviation, emissions, decarbonization, transportation, mitigation
<br />
Summary:<br />
- The article focuses on the role of aviation emissions in long-term climate mitigation in transportation.
- Low-carbon energy carriers and new aircraft deployment are modeled as technology-centered decarbonization policies.
- Supply constraints in targeted market segments are considered as demand-side policies.
- Shared Socioeconomic Pathways (SSPs) are used to estimate trend traffic demand and limit sectoral consumption of electricity and biomass.
- Emissions peak by 2040 in all scenarios, but meeting Paris Agreement goals requires targeted demand management or additional low-carbon energy supply.
- Gradient-based optimization in a multidisciplinary framework efficiently addresses nonlinear, high-dimensional problems while reducing implementation effort. 
<br />Summary: <div>
arXiv:2503.22435v2 Announce Type: replace 
Abstract: Despite being considered a hard-to-abate sector, aviation's emissions will play an important role in long-term climate mitigation of transportation. The introduction of low-carbon energy carriers and the deployment of new aircraft in the current fleet are modeled as technology-centered decarbonization policies, while supply constraints in targeted market segments are modeled as demand-side policies. Shared Socioeconomic Pathways (SSPs) are used to estimate trend traffic demand and to limit the sectoral consumption of electricity and biomass. Mitigation scenarios are formulated as optimization problems, and three applications are demonstrated: no-policy baselines, single-policy optimization, and scenario-robust policies. Results show that the choice of energy carrier is highly dependent on assumptions regarding aircraft technology and the background energy system. Across all SSP-based scenarios, emissions peak by around 2040, but achieving alignment with the Paris Agreement requires either targeted demand management or additional low-carbon energy supply. The use of gradient-based optimization within a multidisciplinary framework enables the efficient resolution of these nonlinear, high-dimensional problems while reducing implementation effort.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust blue-green urban flood risk management optimised with a genetic algorithm for multiple rainstorm return periods</title>
<link>https://arxiv.org/abs/2502.12174</link>
<guid>https://arxiv.org/abs/2502.12174</guid>
<content:encoded><![CDATA[
<div> optimisation, Blue-Green Infrastructure, flood risk, return periods, multi-objective

Summary:
This study introduces a novel methodology for optimising Blue-Green Infrastructure (BGI) designs to enhance flood risk management. By incorporating multiple return periods (10, 20, 30, 50, and 100 years) into a multi-objective optimisation framework, the study aims to improve the robustness of BGI schemes. Utilising a Non-dominated Sorting Genetic Algorithm II (NSGA-II) with a hydrodynamic model, the design process considers direct damage cost (DDC) and expected annual damage (EAD) as risk objective functions. Results highlight that a BGI design optimised for a single 100-year return period may not perform well for other return periods, indicating the importance of considering various storm magnitudes. The study demonstrates that a composite return period approach leads to improved performance metrics across all return periods, enhancing resilience to future climate extremes. This paradigm shift towards multi-return period-based designs in flood risk management can enhance adaptability and resilience in the face of changing climate conditions.

<br /><br />Summary: <div>
arXiv:2502.12174v2 Announce Type: replace-cross 
Abstract: Flood risk managers seek to optimise Blue-Green Infrastructure (BGI) designs to maximise return on investment. Current systems often use optimisation algorithms and detailed flood models to maximise benefit-cost ratios for single rainstorm return periods. However, these schemes may lack robustness in mitigating flood risks across different storm magnitudes. For example, a BGI scheme optimised for a 100-year return period may differ from one optimised for a 10-year return period. This study introduces a novel methodology incorporating five return periods (T = 10, 20, 30, 50, and 100 years) into a multi-objective BGI optimisation framework. The framework combines a Non-dominated Sorting Genetic Algorithm II (NSGA-II) with a fully distributed hydrodynamic model to optimise the spatial placement and combined size of BGI features. For the first time, direct damage cost (DDC) and expected annual damage (EAD), calculated for various building types, are used as risk objective functions, transforming a many-objective problem into a multi-objective one. Performance metrics such as Median Risk Difference (MedRD), Maximum Risk Difference (MaxRD), and Area Under Pareto Front (AUPF) reveal that a 100-year optimised BGI design performs poorly when evaluated for other return periods, particularly shorter ones. In contrast, a BGI design optimised using composite return periods enhances performance metrics across all return periods, with the greatest improvements observed in MedRD (22%) and AUPF (73%) for the 20-year return period, and MaxRD (23%) for the 50-year return period. Furthermore, climate uplift stress testing confirms the robustness of the proposed design to future rainfall extremes. This study advocates a paradigm shift in flood risk management, moving from single maximum to multiple rainstorm return period-based designs to enhance resilience and adaptability to future climate extremes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phase-field and lip-field approaches for fracture with extreme mesh deformation (X-Mesh): a one-dimensional study</title>
<link>https://arxiv.org/abs/2509.04971</link>
<guid>https://arxiv.org/abs/2509.04971</guid>
<content:encoded><![CDATA[
<div> Keywords: fracture, phase-field, lip-field, variational mesh study, X-Mesh

Summary:
The study examines a one-dimensional fracture problem using the phase-field or lip-field approach, focusing on optimizing incremental potential in relation to displacement and damage fields, as well as nodal coordinates of the mesh. Through variational mesh analysis, the research shows that as damage increases, the most damaged element decreases in size until it reaches zero, providing an accurate representation of the bar breaking. The optimized solution proves to be more precise than fixed mesh solutions. This work contributes to exploring the possibilities of extreme meshes in computational mechanics within the X-Mesh framework. 

<br /><br />Summary: <div>
arXiv:2509.04971v1 Announce Type: new 
Abstract: We consider a one-dimensional fracture problem modelled using either the phase-field or lip-field approach. In both cases, we optimise the incremental potential with respect to the displacement and damage fields and the nodal coordinates of the mesh. This is thus a variational mesh study. We observe that, as the damage reaches its maximum value, the optimisation drives the most damaged element to zero size as the damage reaches its maximum value. This peculiar element provides a precise displacement jump representation as the bar breaks. The overall solution is also shown to be much more accurate than the fixed mesh solution. This work forms part of an exploration into the capabilities of extreme meshes in computational mechanics (X-Mesh).
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Inverse Rosenblatt Transport for Structural Reliability Analysis</title>
<link>https://arxiv.org/abs/2509.05061</link>
<guid>https://arxiv.org/abs/2509.05061</guid>
<content:encoded><![CDATA[
<div> probability estimation, reliability analysis, solid mechanics, deep learning, high-dimensional spaces

Summary:
The article investigates the Deep Inverse Rosenblatt Transport (DIRT) framework for reliability analysis in solid mechanics. DIRT combines a TT decomposition with an inverse Rosenblatt transformation to efficiently estimate the probability of failure in high-dimensional settings. The framework scales linearly in input dimension and provides a compact and reusable surrogate of the target distribution. Through experimentation on various analytical and numerical examples, DIRT demonstrates lower estimator variance and accurate estimation of rare event probabilities compared to established methods like Bayesian updating with Subset Simulation (BUS-SuS). This research addresses the challenge of accurate failure probability estimation in engineering systems, particularly in high-dimensional settings with rare events. <div>
arXiv:2509.05061v1 Announce Type: new 
Abstract: Accurately estimating the probability of failure in engineering systems under uncertainty is a fundamental challenge, particularly in high-dimensional settings and for rare events. Conventional reliability analysis methods often become computationally intractable or exhibit high estimator variance when applied to problems with hundreds of uncertain parameters or highly concentrated failure regions. In this work, we investigate the use of the recently proposed Deep Inverse Rosenblatt Transport (DIRT) framework for reliability analysis in solid mechanics. DIRT combines a TT decomposition with an inverse Rosenblatt transformation to construct a low-rank approximation of the posterior distribution, enabling efficient sampling and probability estimation in high-dimensional spaces. By representing the optimal importance density in the TT format, DIRT scales linearly in the input dimension while maintaining a compact, reusable surrogate of the target distribution. We demonstrate the effectiveness of the DIRT framework on three analytical reliability problems and one numerical example with dimensionality ranging from 2 to 250. Compared to established methods such as Bayesian updating with Subset Simulation (BUS-SuS), DIRT seems to lower the estimator variance while accurately capturing rare event probabilities for the benchmark problems of this study.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Evaluation of Derivatives of Green's Functions Using Recurrences</title>
<link>https://arxiv.org/abs/2509.03687</link>
<guid>https://arxiv.org/abs/2509.03687</guid>
<content:encoded><![CDATA[
<div> Green's functions, derivatives, symbolic-numerical procedures, quadrature by expansion, higher-order accuracy. 
Summary: This article presents hybrid symbolic-numerical methods for efficiently computing higher-order derivatives of Green's functions, crucial in fast multipole methods and Barnes-Hut algorithms. The proposed procedures achieve an O(n) cost for computing n derivatives, offering significant computational savings. These methods are applicable to radially symmetric Green's functions and are general, requiring only knowledge of the relevant PDE. The algorithm guarantees controlled error levels, enhancing reliability. Additionally, the article introduces a rotation-based approach for target-specific evaluation in the Cartesian setting within the method of quadrature by expansion, which significantly reduces computational expenses compared to traditional symbolic methods. Numerical experiments validate the accuracy and efficiency of the proposed techniques. <div>
arXiv:2509.03687v1 Announce Type: new 
Abstract: High-order derivatives of Green's functions are a key ingredient in Taylor-based fast multipole methods, Barnes-Hut $n$-body algorithms, and quadrature by expansion (QBX). In these settings, derivatives underpin either the formation, evaluation, and/or translation of Taylor expansions.
  In this article, we provide hybrid symbolic-numerical procedures that generate recurrences to attain an $O(n)$ cost for the the computation of $n$ derivatives (i.e. $O(1)$ per derivative) for arbitrary radially symmetric Green's functions. These procedures are general--only requiring knowledge of the PDE that the Green's function solves. We show that the algorithm has controlled, theoretically-understood error.
  We apply these methods to the method of quadrature by expansion, a method for the evaluation of singular layer potentials, which requires higher-order derivatives of Green's functions. In doing so, we contribute a new rotation-based method for target-specific QBX evaluation in the Cartesian setting that attains dramatically lower cost than existing symbolic approaches.
  Numerical experiments support our claims of accuracy and cost.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamic Energy-Based Hysteresis Model for Pulsed-Operated Fast-Ramping Magnets</title>
<link>https://arxiv.org/abs/2509.04115</link>
<guid>https://arxiv.org/abs/2509.04115</guid>
<content:encoded><![CDATA[
<div> Keywords: ferromagnetic yokes, fast-ramping magnets, hysteresis description, eddy-current model, normal-conducting bending magnet<br />
<br />
Summary: 
This paper presents a dynamic ferromagnetic model that combines energy-based hysteresis description and a thin-sheet eddy-current model in the time domain. The study addresses the challenges of accurately analyzing fast-ramping magnets due to their strongly nonlinear behavior. Existing approaches often oversimplify the analysis using anhysteretic material descriptions and after-the-fact loss formulae. By utilizing a more comprehensive model, the research aims to provide a more precise calculation of losses and magnetic fields. The model's effectiveness was demonstrated through its application in analyzing a normal-conducting bending magnet. This work sheds light on the importance of considering dynamic ferromagnetic behavior in the numerical analysis of magnets, offering a more accurate representation of their performance. <div>
arXiv:2509.04115v1 Announce Type: new 
Abstract: Due to the strongly nonlinear behavior of ferromagnetic yokes, the numerical analysis of fast-ramping magnets is highly cumbersome and, therefore, in practice overly simplified by means of anhysteretic material descriptions and a posteriori loss formulae. This paper establishes the use of a dynamic ferromagnetic model combining a preconditioned energy-based hysteresis description and a thin-sheet eddy-current model in time-domain. The model was successfully employed in the analysis of a normal-conducting bending magnet in order to precisely calculate losses and fields.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COBRA: Multimodal Sensing Deep Learning Framework for Remote Chronic Obesity Management via Wrist-Worn Activity Monitoring</title>
<link>https://arxiv.org/abs/2509.04210</link>
<guid>https://arxiv.org/abs/2509.04210</guid>
<content:encoded><![CDATA[
<div> Keywords: Chronic obesity management, Deep learning, Behavioral monitoring, Multimodal sensors, Digital health systems

Summary: 
COBRA (Chronic Obesity Behavioral Recognition Architecture) is a novel deep learning framework designed for objective monitoring of energy balance behaviors in individuals with chronic obesity. It utilizes wrist-worn multimodal sensors and a hybrid D-Net architecture that incorporates spatial modeling, self-attention mechanisms, and temporal processing to classify daily activities related to obesity. Validation on the WISDM-Smart dataset shows high accuracy and outperformance of state-of-the-art baselines. The framework's optimal preprocessing strategy includes spectral-temporal feature extraction, enabling robust generalizability with low demographic variance. COBRA's success in accurately categorizing activities such as Food Intake, Physical Activity, Sedentary Behavior, and Daily Living showcases its potential for scalable deployment in personalized obesity interventions and continuous lifestyle monitoring.<br /><br />Summary: <div>
arXiv:2509.04210v1 Announce Type: new 
Abstract: Chronic obesity management requires continuous monitoring of energy balance behaviors, yet traditional self-reported methods suffer from significant underreporting and recall bias, and difficulty in integration with modern digital health systems. This study presents COBRA (Chronic Obesity Behavioral Recognition Architecture), a novel deep learning framework for objective behavioral monitoring using wrist-worn multimodal sensors. COBRA integrates a hybrid D-Net architecture combining U-Net spatial modeling, multi-head self-attention mechanisms, and BiLSTM temporal processing to classify daily activities into four obesity-relevant categories: Food Intake, Physical Activity, Sedentary Behavior, and Daily Living. Validated on the WISDM-Smart dataset with 51 subjects performing 18 activities, COBRA's optimal preprocessing strategy combines spectral-temporal feature extraction, achieving high performance across multiple architectures. D-Net demonstrates 96.86% overall accuracy with category-specific F1-scores of 98.55% (Physical Activity), 95.53% (Food Intake), 94.63% (Sedentary Behavior), and 98.68% (Daily Living), outperforming state-of-the-art baselines by 1.18% in accuracy. The framework shows robust generalizability with low demographic variance (<3%), enabling scalable deployment for personalized obesity interventions and continuous lifestyle monitoring.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and scalable deep Maxwell solvers using multilevel iterative methods</title>
<link>https://arxiv.org/abs/2509.03622</link>
<guid>https://arxiv.org/abs/2509.03622</guid>
<content:encoded><![CDATA[
<div> surrogate PDE solvers, neural networks, subdomain neural operator model, iterative algorithms, multilevel domain decomposition <br /> 
Summary: 
This article explores the use of neural networks as surrogate solvers for partial differential equations, addressing challenges in accuracy and scalability. By combining neural network surrogates with iterative algorithms, the study demonstrates the accurate solution of PDE problems with varying scales, resolutions, and boundary conditions. A subdomain neural operator model is developed to handle arbitrary Robin-type boundary conditions, serving as a flexible preconditioner for solving subdomain problems iteratively. The model also supports the construction of global coarse spaces for efficient large-scale PDE problem solving through multilevel domain decomposition. Using two-dimensional Maxwell's equations as a test case, a single neural network is trained to simulate diverse problem scenarios with varying sizes, resolutions, wavelengths, and media distributions. The study showcases the platform's effectiveness in accurately designing multi-wavelength nanophotonic devices through inverse design techniques.  <br /><br />Summary: <div>
arXiv:2509.03622v1 Announce Type: cross 
Abstract: Neural networks have promise as surrogate partial differential equation (PDE) solvers, but it remains a challenge to use these concepts to solve problems with high accuracy and scalability. In this work, we show that neural network surrogates can combine with iterative algorithms to accurately solve PDE problems featuring different scales, resolutions, and boundary conditions. We develop a subdomain neural operator model that supports arbitrary Robin-type boundary condition inputs, and we show that it can be utilized as a flexible preconditioner to iteratively solve subdomain problems with bounded accuracy. We further show that our subdomain models can facilitate the construction of global coarse spaces to enable accelerated, large scale PDE problem solving based on iterative multilevel domain decomposition. With two-dimensional Maxwell's equations as a model system, we train a single network to simulate large scale problems with different sizes, resolutions, wavelengths, and dielectric media distribution. We further demonstrate the utility of our platform in performing the accurate inverse design of multi-wavelength nanophotonic devices. Our work presents a promising path to building accurate and scalable multi-physics surrogate solvers for large practical problems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Linear Time Quantum Algorithm for Pairwise Sequence Alignment</title>
<link>https://arxiv.org/abs/2307.04479</link>
<guid>https://arxiv.org/abs/2307.04479</guid>
<content:encoded><![CDATA[
<div> Sequence Alignment, Quantum Algorithm, DNA sequences, Grover's search algorithm, optimal alignment

Summary: 
The paper introduces a Quantum Algorithm for sequence alignment, specifically for DNA sequences. By mapping the problem into a path-searching 2D graph and using a proposed oracle for profit calculation, the algorithm is able to find the optimal alignment in linear time. This surpasses classical deterministic algorithms in efficiency. By utilizing Grover's search algorithm, the proposed approach provides quadratic speeding up for unstructured search problems, ensuring optimal solutions deterministically. This is a significant improvement over existing randomized algorithms that often produce sub-optimal alignments. The quantum algorithm not only aligns sequences accurately but also guarantees finding the optimal solution, making it a promising tool for bioinformatics research. <br /><br />Summary: <div>
arXiv:2307.04479v2 Announce Type: replace-cross 
Abstract: Sequence Alignment is the process of aligning biological sequences in order to identify similarities between multiple sequences. In this paper, a Quantum Algorithm for finding the optimal alignment between DNA sequences has been demonstrated which works by mapping the sequence alignment problem into a path-searching problem through a 2D graph. The transition, which converges to a fixed path on the graph, is based on a proposed oracle for profit calculation. By implementing Grover's search algorithm, our proposed approach is able to align a pair of sequences and figure out the optimal alignment within linear time, which hasn't been attained by any classical deterministic algorithm. In addition to that, the proposed algorithm is capable of quadratic speeding up to any unstructured search problem by finding out the optimal paths accurately in a deterministic manner, in contrast to existing randomized algorithms that frequently sort out the sub-optimal alignments, therefore, don't always guarantee of finding out the optimal solutions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Use of Physicochemical Modification Methods for Producing Traditional and Nanomodified Polymeric Composites with Improved Operational Properties</title>
<link>https://arxiv.org/abs/2509.02572</link>
<guid>https://arxiv.org/abs/2509.02572</guid>
<content:encoded><![CDATA[
<div> surface modification, thermoplastic composite materials, interfacial interaction, ultrasonic processing, nanocomposites <br />
Summary: Various physical and physicochemical methods for modifying components of thermoplastic composite materials were analyzed. Improving the surface properties of fillers and the interaction between components of the composite is crucial for enhancing the reliability of the composite. Research focused on modifying the surface of reinforcing fibrous fillers and liquid polymer binders to improve contact properties within the composite. The effectiveness of low-frequency ultrasonic processing in enhancing interfacial interaction was highlighted. Cluster formation and physicochemical modification of epoxy polymers filled with dispersed fillers were discussed, with emphasis on ultrasonic cavitation for deagglomeration and nanoparticle distribution in nanocomposites. Experimental results showed improved technological and physicomechanical properties of sonicated epoxy matrices. The article also briefly touched on biological modifications of polymer components for functional applications. <br /><br />Summary: <div>
arXiv:2509.02572v1 Announce Type: new 
Abstract: Various aspects of the methods of physical and physicochemical modification of components of filled thermoplastic composite materials are analyzed, aimed at improving the surface properties of the fillers and the technological properties of the polymer matrix during their interaction. It is noted that the improvement of the interfacial interaction of the components of polymer reactoplastic composites, including adhesive strength, is a key factor for improving the reliability of the cured filled composite. As a promising area of research, a modification of the surface of the reinforcing fibrous filler and the technological characteristics of the liquid polymer binder, aimed at increasing their contact properties in the composite, was chosen. The effectiveness of the physical method of modifying the components of composites in the form of low-frequency ultrasonic processing is described. The peculiarities of cluster formation and physicochemical modification of epoxy polymers filled with dispersed fillers are analyzed. Attention is focused on the effectiveness of ultrasonic processing in the cavitation mode for deagglomeration and uniform distribution of nanoparticles in a liquid medium during the creation of nanocomposites. Experimentally confirmed is the improvement of the technological properties of liquid epoxy polymers, modified by ultrasound, used for the impregnation of oriented fibrous fillers, as well as the improvement of the physicomechanical properties of the sonicated epoxy matrices. Some issues of biological modifications of components of polymers for functional application are briefly reviewed.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Use ADAS Data to Predict Near-Miss Events: A Group-Based Zero-Inflated Poisson Approach</title>
<link>https://arxiv.org/abs/2509.02614</link>
<guid>https://arxiv.org/abs/2509.02614</guid>
<content:encoded><![CDATA[
<div> telematics, driving behavior, risk evaluation, usage-based insurance, zero-inflated Poisson framework  
Summary:  
The article discusses the utilization of driving behavior big data and telematics to understand how people drive and its applications in risk evaluation and insurance pricing. Traditional statistical models struggle to accurately analyze sparse and zero-inflated near-miss events captured by telematics. The study proposes zero-inflated Poisson frameworks that learn latent behavior groups and fit offset-based count models to improve weekly risk predictions. Using a naturalistic dataset from a fleet of commercial drivers, the results show significant improvements over prior models, with better calibration and lower information criteria values. Sensitivity analyses on the EM-based grouping demonstrate robust and interpretable gains, supporting context-aware ratemaking and fairer premiums by recognizing heterogeneous driving styles.<br /><br />Summary: <div>
arXiv:2509.02614v1 Announce Type: cross 
Abstract: Driving behavior big data leverages multi-sensor telematics to understand how people drive and powers applications such as risk evaluation, insurance pricing, and targeted intervention. Usage-based insurance (UBI) built on these data has become mainstream. Telematics-captured near-miss events (NMEs) provide a timely alternative to claim-based risk, but weekly NMEs are sparse, highly zero-inflated, and behaviorally heterogeneous even after exposure normalization. Analyzing multi-sensor telematics and ADAS warnings, we show that the traditional statistical models underfit the dataset. We address these challenges by proposing a set of zero-inflated Poisson (ZIP) frameworks that learn latent behavior groups and fit offset-based count models via EM to yield calibrated, interpretable weekly risk predictions. Using a naturalistic dataset from a fleet of 354 commercial drivers over a year, during which the drivers completed 287,511 trips and logged 8,142,896 km in total, our results show consistent improvements over baselines and prior telematics models, with lower AIC/BIC values in-sample and better calibration out-of-sample. We also conducted sensitivity analyses on the EM-based grouping for the number of clusters, finding that the gains were robust and interpretable. Practically, this supports context-aware ratemaking on a weekly basis and fairer premiums by recognizing heterogeneous driving styles.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Differentiation of Agent-Based Models</title>
<link>https://arxiv.org/abs/2509.03303</link>
<guid>https://arxiv.org/abs/2509.03303</guid>
<content:encoded><![CDATA[
<div> Agent-based models, automatic differentiation, computational burden, parameter calibration, variational inference <br />
<br />
Summary: <br />
Agent-based models (ABMs) are used to simulate complex systems by modeling individual agents and their interactions. However, the large number of agents in such systems requires significant computational resources and calibration of numerous parameters, limiting their widespread adoption. This paper demonstrates that automatic differentiation (AD) techniques can help alleviate these computational challenges by providing gradients of the simulator, making tasks like calibration and sensitivity analysis more efficient. By applying variational inference (VI) techniques enabled by AD, the study shows improved performance and computational savings in calibrating three different ABMs: Axtell's firm model, Sugarscape, and the SIR epidemiological model. This approach enhances the practicality and scalability of ABMs for studying complex systems. <br /> <div>
arXiv:2509.03303v1 Announce Type: cross 
Abstract: Agent-based models (ABMs) simulate complex systems by capturing the bottom-up interactions of individual agents comprising the system. Many complex systems of interest, such as epidemics or financial markets, involve thousands or even millions of agents. Consequently, ABMs often become computationally demanding and rely on the calibration of numerous free parameters, which has significantly hindered their widespread adoption. In this paper, we demonstrate that automatic differentiation (AD) techniques can effectively alleviate these computational burdens. By applying AD to ABMs, the gradients of the simulator become readily available, greatly facilitating essential tasks such as calibration and sensitivity analysis. Specifically, we show how AD enables variational inference (VI) techniques for efficient parameter calibration. Our experiments demonstrate substantial performance improvements and computational savings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape; and the SIR epidemiological model. Our approach thus significantly enhances the practicality and scalability of ABMs for studying complex systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems</title>
<link>https://arxiv.org/abs/2509.03340</link>
<guid>https://arxiv.org/abs/2509.03340</guid>
<content:encoded><![CDATA[
<div> machine learning, bifurcation phenomena, symmetries, flow matching, multistability 

Summary: 
This study addresses the challenge of capturing multiple stable solutions in nonlinear dynamical systems due to symmetry breaking and bifurcation phenomena. The proposed generative framework based on flow matching allows modeling the full probability distribution over bifurcation outcomes, enabling direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. A symmetric matching strategy aligns predicted and target outputs under group actions, facilitating accurate learning in equivariant settings. The method is validated on various systems, demonstrating superior performance in capturing multimodal distributions and symmetry-breaking bifurcations compared to non-probabilistic and variational approaches. Overall, flow matching offers a principled and scalable solution for modeling multistability in high-dimensional systems. <div>
arXiv:2509.03340v1 Announce Type: cross 
Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple coexisting stable solutions, particularly in the presence of symmetry breaking. Deterministic machine learning models struggle to capture this multiplicity, averaging over solutions and failing to represent lower-symmetry outcomes. In this work, we propose a generative framework based on flow matching to model the full probability distribution over bifurcation outcomes. Our method enables direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. We introduce a symmetric matching strategy that aligns predicted and target outputs under group actions, allowing accurate learning in equivariant settings. We validate our approach on a range of systems, from toy models to complex physical problems such as buckling beams and the Allen-Cahn equation. Our results demonstrate that flow matching significantly outperforms non-probabilistic and variational methods in capturing multimodal distributions and symmetry-breaking bifurcations, offering a principled and scalable solution for modeling multistability in high-dimensional systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Differentiable Boundary Element Solver for Hydrodynamic Sensitivity Analysis of Wave-Structure Interactions</title>
<link>https://arxiv.org/abs/2501.06988</link>
<guid>https://arxiv.org/abs/2501.06988</guid>
<content:encoded><![CDATA[
<div> wave-structure interactions, marine structures, boundary element method, linear potential flow theory, automatic differentiation

Summary:
Accurately predicting wave-structure interactions in marine structures is crucial for effective design and analysis. Current solvers using the boundary element method (BEM) rely on linear potential flow theory but lack the ability to provide sensitivities for system-level applications like design optimization. To address this limitation, a fully differentiable BEM solver has been developed, capable of estimating sensitivities. This advancement allows for precise estimation of wave-structure interaction sensitivity, which is crucial for optimizing designs. By incorporating automatic differentiation (AD) into BEM solvers, a more comprehensive understanding of wave-structure interactions can be achieved, enabling better design and analysis of marine structures. <div>
arXiv:2501.06988v2 Announce Type: replace 
Abstract: Accurately predicting wave-structure interactions is critical for the effective design and analysis of marine structures. This is typically achieved using solvers that employ the boundary element method (BEM), which relies on linear potential flow theory. Precise estimation of the sensitivity of these interactions is equally important for system-level applications such as design optimization. Current BEM solvers are unable to provide these sensitivities as they do not support automatic differentiation (AD). To address these challenges, we have developed a fully differentiable BEM solver
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Data Encoding and Variational Algorithms: A Framework for Hybrid Quantum Classical Machine Learning</title>
<link>https://arxiv.org/abs/2502.11951</link>
<guid>https://arxiv.org/abs/2502.11951</guid>
<content:encoded><![CDATA[
<div> QML, Quantum Machine Learning, classical data pipelines, hybrid quantum-classical models, CQ paradigm
<br />
Summary:
The article discusses the development of Quantum Machine Learning (QML) and its integration of quantum mechanics with classical machine learning. It proposes a broad architecture connecting classical data pipelines with quantum algorithms, emphasizing hybrid quantum-classical models for scalable quantum benefits. The Classical-Quantum (CQ) paradigm is highlighted, using classical encoding strategies to compress information into Hilbert space representations. Variational quantum circuits are explored to overcome device constraints. Experimental comparisons show that small quantum circuits can approximate probabilistic inference with competitive accuracy and robustness to noisy data. The article provides a roadmap for implementing quantum kernels, variational algorithms, and hybrid feedback loops in practice for optimization, computer vision, and medical diagnostics. It emphasizes the importance of strong data encoding and adaptive error protection in moving QML from theory to practice. 
<br /> <div>
arXiv:2502.11951v2 Announce Type: replace 
Abstract: The development of quantum computers has been the stimulus that enables the realization of Quantum Machine Learning (QML), an area that integrates the calculational framework of quantum mechanics with the adaptive properties of classical machine learning. This article suggests a broad architecture that allows the connection between classical data pipelines and quantum algorithms, hybrid quantum-classical models emerge as a promising route to scalable and near-term quantum benefit. At the core of this paradigm lies the Classical-Quantum (CQ) paradigm, in which the qubit states of high-dimensional classical data are encoded using sophisticated classical encoding strategies which encode the data in terms of amplitude and angle of rotation, along with superposition mapping. These techniques allow compression of information exponentially into Hilbert space representations, which, together with reduced sample complexity, allows greater feature expressivity. We also examine variational quantum circuits, quantum gates expressed as trainable variables that run with classical optimizers to overcome decoherence, noise, and gate-depth constraints of the existing Noisy Intermediate-Scale Quantum (NISQ) devices. Experimental comparisons with a Quantum Naive Bayes classifier prove that even small quantum circuits can approximate probabilistic inference with competitive accuracy compared to classical benchmarks, and have much better robustness to noisy data distributionsThis model does not only explain the algorithmic and architectural design of QML, it also offers a roadmap to the implementation of quantum kernels, variational algorithms, and hybrid feedback loops into practice, including optimization, computer vision, and medical diagnostics. The results support the idea that hybrid architectures with strong data encoding and adaptive error protection are key to moving QML out of theory to practice.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Metrics to Meaning: Time to Rethink Evaluation in Human-AI Collaborative Design</title>
<link>https://arxiv.org/abs/2402.07911</link>
<guid>https://arxiv.org/abs/2402.07911</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, human-AI collaboration, interactive design, evaluation methods, user engagement

Summary:
The study explores the interaction between humans and AI systems in the context of creative design. By analyzing the engagement of participants in a co-creative system called The Genetic Car Designer, the research highlights the importance of evaluating human-AI collaborative systems in a multidimensional manner. The system, which employs an interactive evolutionary algorithm, showed that exposure to design suggestions generated by the AI system led to enhanced cognitive and behavioral engagement, resulting in higher-quality design outcomes. The findings suggest that conventional evaluation methods focused solely on behavioral and design metrics may not capture the full extent of user engagement. It is proposed that evaluating human-AI systems holistically, considering emotional, behavioral, and cognitive states of the designer, is crucial for a comprehensive understanding of the user experience and the role of intelligent systems in creative design processes. <div>
arXiv:2402.07911v2 Announce Type: replace-cross 
Abstract: As AI systems increasingly shape decision making in creative design contexts, understanding how humans engage with these tools has become a critical challenge for interactive intelligent systems research. This paper contributes a challenge to rethink how to evaluate human--AI collaborative systems, advocating for a more nuanced and multidimensional approach. Findings from one of the largest field studies to date (n = 808) of a human--AI co-creative system, The Genetic Car Designer, complemented by a controlled lab study (n = 12) are presented. The system is based on an interactive evolutionary algorithm where participants were tasked with designing a simple two dimensional representation of a car. Participants were exposed to galleries of design suggestions generated by an intelligent system, MAP--Elites, and a random control. Results indicate that exposure to galleries generated by MAP--Elites significantly enhanced both cognitive and behavioural engagement, leading to higher-quality design outcomes. Crucially for the wider community, the analysis reveals that conventional evaluation methods, which often focus on solely behavioural and design quality metrics, fail to capture the full spectrum of user engagement. By considering the human--AI design process as a changing emotional, behavioural and cognitive state of the designer, we propose evaluating human--AI systems holistically and considering intelligent systems as a core part of the user experience -- not simply a back end tool.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel manifolds: nonlinear-augmentation dimensionality reduction using reproducing kernel Hilbert spaces</title>
<link>https://arxiv.org/abs/2509.00224</link>
<guid>https://arxiv.org/abs/2509.00224</guid>
<content:encoded><![CDATA[
<div> Kernel methods, Nonlinear dimensionality reduction, Quadratic manifold, Feature map, Reproducing kernel Hilbert space <br />
<br />
Summary: 
This paper introduces a novel approach to dimensionality reduction called kernel methods-based nonlinear-augmentation dimensionality reduction as an extension of quadratic manifold (QM) dimensionality reduction. The method involves augmenting linear dimensionality reduction with a nonlinear correction term in the reconstruction map to improve accuracy. Unlike previous methods that use least-squares optimal polynomial correction terms, this approach learns an optimal nonlinear correction from a reproducing kernel Hilbert space defined by the user. It allows for the imposition of various nonlinear structures on the correction term, including polynomial and radial basis function-based structures. The method is computationally efficient and exhibits decreasing error as the latent space dimension increases. Comparisons with other dimensionality reduction techniques like proper orthogonal decomposition and existing QM approaches demonstrate the effectiveness of the proposed method on various datasets. <div>
arXiv:2509.00224v1 Announce Type: new 
Abstract: This paper generalizes recent advances on quadratic manifold (QM) dimensionality reduction by developing kernel methods-based nonlinear-augmentation dimensionality reduction. QMs, and more generally feature map-based nonlinear corrections, augment linear dimensionality reduction with a nonlinear correction term in the reconstruction map to overcome approximation accuracy limitations of purely linear approaches. While feature map-based approaches typically learn a least-squares optimal polynomial correction term, we generalize this approach by learning an optimal nonlinear correction from a user-defined reproducing kernel Hilbert space. Our approach allows one to impose arbitrary nonlinear structure on the correction term, including polynomial structure, and includes feature map and radial basis function-based corrections as special cases. Furthermore, our method has relatively low training cost and has monotonically decreasing error as the latent space dimension increases. We compare our approach to proper orthogonal decomposition and several recent QM approaches on data from several example problems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I-FENN with DeepONets: accelerating simulations in coupled multiphysics problems</title>
<link>https://arxiv.org/abs/2509.00604</link>
<guid>https://arxiv.org/abs/2509.00604</guid>
<content:encoded><![CDATA[
<div> Keywords: DeepONet, Finite Element Method, multiphysics simulations, thermoelasticity, poroelasticity

Summary: 
This article introduces a novel framework that integrates DeepONet with the Finite Element Method to address coupled thermoelasticity and poroelasticity problems efficiently. The framework, called I-FENN, employs neural networks as PDE solvers within FEM, resulting in a hybrid staggered solver. By decoupling multiphysics interactions, the framework reduces computational costs while maintaining flexibility across various scenarios. The modified DeepONet architecture allows for multiple inputs and an efficient strategy for enforcing boundary conditions on distinct boundaries. Numerical examples demonstrate the computational efficiency, accuracy, and generalization capabilities of the proposed work, even under unseen loading conditions. The computational savings increase with model complexity while maintaining high accuracy levels in challenging regions of the domain. Overall, the framework shows promise in tackling high-dimensional, large-scale coupled multiphysics simulations. 

Summary: <div>
arXiv:2509.00604v1 Announce Type: new 
Abstract: Coupled multiphysics simulations for high-dimensional, large-scale problems can be prohibitively expensive due to their computational demands. This article presents a novel framework integrating a deep operator network (DeepONet) with the Finite Element Method (FEM) to address coupled thermoelasticity and poroelasticity problems. This integration occurs within the context of I-FENN, a framework where neural networks are directly employed as PDE solvers within FEM, resulting in a hybrid staggered solver. In this setup, the mechanical field is computed using FEM, while the other coupled field is predicted using a neural network (NN). By decoupling multiphysics interactions, the hybrid framework reduces computational cost by simplifying calculations and reducing the FEM unknowns, while maintaining flexibility across unseen scenarios. The proposed work introduces a new I-FENN architecture with extended generalizability due to the DeepONets ability to efficiently address several combinations of natural boundary conditions and body loads. A modified DeepONet architecture is introduced to accommodate multiple inputs, along with a streamlined strategy for enforcing boundary conditions on distinct boundaries. We showcase the applicability and merits of the proposed work through numerical examples covering thermoelasticity and poroelasticity problems, demonstrating computational efficiency, accuracy, and generalization capabilities. In all examples, the test cases involve unseen loading conditions. The computational savings scale with the model complexity while preserving an accuracy of more than 95\% in the non-trivial regions of the domain.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new definition of peridynamic damage for thermo-mechanical fracture modeling</title>
<link>https://arxiv.org/abs/2509.01079</link>
<guid>https://arxiv.org/abs/2509.01079</guid>
<content:encoded><![CDATA[
<div> Thermo-mechanical fracture modeling, thermal failure, heat conduction, continuum mechanics, peridynamic model<br />
Summary: A novel thermo-mechanical fracture modeling approach is proposed, combining classical continuum mechanics and peridynamic models to address thermal failure issues. The model incorporates a CCM/PD alternating solution for accurate calculations using finite element discretization. A new definition of PD damage is introduced, considering both the number and distribution of broken bonds for capturing damage in various directions. Validation against analytical solutions and simulations of crack propagation demonstrate the model's effectiveness in simulating complex thermal fractures and understanding initiation and propagation mechanisms. <div>
arXiv:2509.01079v1 Announce Type: new 
Abstract: A thermo-mechanical fracture modeling is proposed to address thermal failure issues, where the temperature field is calculated by a heat conduction model based on classical continuum mechanics (CCM), while the deformation field with discontinuities is calculated by the peridynamic (PD) model. The model is calculated by a CCM/PD alternating solution based on the finite element discretization, which ensures the calculation accuracy and facilitates engineering applications. The original PD model defines damage solely based on the number of broken bonds in the vicinity of the material point, neglecting the distribution of these bonds. To address this limitation, a new definition of the PD damage accounting for both the number of broken bonds and their specific distribution is proposed. As a result, damage in various directions can be captured, enabling more realistic thermal fracture simulations based on a unified mesh discretization. The effectiveness of the proposed model is validated by comparing numerical examples with analytical solutions. Moreover, simulation results of quasi-static and dynamic crack propagation demonstrate the model's ability to aid in understanding the initiation and propagation mechanisms of complex thermal fractures.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAMS: Residual-based adversarial-gradient moving sample method for scientific machine learning in solving partial differential equations</title>
<link>https://arxiv.org/abs/2509.01234</link>
<guid>https://arxiv.org/abs/2509.01234</guid>
<content:encoded><![CDATA[
<div> Neural networks, PDEs, PINNs, SciML, RAMS 
Summary: 
Physics-informed neural networks (PINNs) and neural operators are powerful tools for solving PDEs. Increasing the training sample size enhances network performance but increases computational costs. To address this trade-off, the residual-based adversarial-gradient moving sample (RAMS) method is proposed. RAMS moves samples based on the adversarial gradient direction to maximize the PDE residual, improving sampling efficiency for high-dimensional problems. It can be integrated into existing sampling methods. Extensive experiments show RAMS's effectiveness in PINNs for high-dimensional PDEs and operator learning tasks, making it the first efficient adaptive sampling approach for operator learning in the SciML field. <br /><br />Summary: <div>
arXiv:2509.01234v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) and neural operators, two leading scientific machine learning (SciML) paradigms, have emerged as powerful tools for solving partial differential equations (PDEs). Although increasing the training sample size generally enhances network performance, it also increases computational costs for physics-informed or data-driven training. To address this trade-off, different sampling strategies have been developed to sample more points in regions with high PDE residuals. However, existing sampling methods are computationally demanding for high-dimensional problems, such as high-dimensional PDEs or operator learning tasks. Here, we propose a residual-based adversarial-gradient moving sample (RAMS) method, which moves samples according to the adversarial gradient direction to maximize the PDE residual via gradient-based optimization. RAMS can be easily integrated into existing sampling methods. Extensive experiments, ranging from PINN applied to high-dimensional PDEs to physics-informed and data-driven operator learning problems, have been conducted to demonstrate the effectiveness of RAMS. Notably, RAMS represents the first efficient adaptive sampling approach for operator learning, marking a significant advancement in the SciML field.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A continuum multi-species biofilm model with a novel interaction scheme</title>
<link>https://arxiv.org/abs/2509.01274</link>
<guid>https://arxiv.org/abs/2509.01274</guid>
<content:encoded><![CDATA[
<div> biofilms, microorganisms, mathematical modeling, biofilm interactions, antibiotic agents 

Summary:
This article introduces a comprehensive multi-species continuum-based biofilm model that aims to understand the interactions between different species of microorganisms within biofilms. The model, derived using Hamilton's principle of stationary action, can replicate various biofilm interactions with an arbitrary number of species and incorporates the effects of nutrient sources and antibiotic agents on biofilm behavior. By combining mathematical modeling with in vitro and in vivo experiments, researchers can gain more insights into biofilm dynamics while reducing costs. The model demonstrates good quantitative agreement with biofilm behavior, showcasing its potential utility for researchers looking to study biofilm systems. <div>
arXiv:2509.01274v1 Announce Type: new 
Abstract: Biofilms are complex structures which are inhabited by numerous amount of different species of microorganisms. Due to their ubiquity, they influence human life on an everyday basis. It is therefore important to understand the interactions between different biofilm components and reactions to outside conditions. For this purpose, mathematical models and in silico experiments have proven themselves to be fundamental. In combination with in vitro and in vivo experiments, they can give more insights and focus researchers' attention, reducing costs in the process. In this work, a comprehensive multi-species continuum-based biofilm model is presented. This model is capable of replicating a variety of different biofilm interactions with an arbitrary number of species, while still being comprehensive to encourage usage by researchers less familiar with mathematical modeling. In addition to a nutrient source, antibiotic agents and their effect on the biofilm can also be depicted. The model is derived using Hamilton's principle of stationary action, ensuring thermodynamic consistency automatically. The results show good quantitative agreement with biofilm behavior.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Alpha Weighting with PPO: Enhancing Prompt-Based LLM-Generated Alphas in Quant Trading</title>
<link>https://arxiv.org/abs/2509.01393</link>
<guid>https://arxiv.org/abs/2509.01393</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, proximal policy optimization, large language model, formulaic alphas, stock trading strategies

Summary:
This paper presents a novel approach using reinforcement learning with Proximal Policy Optimization (PPO) to dynamically optimize the weights of multiple large language model-generated formulaic alphas for stock trading strategies. The study leverages a deepseek-r1-distill-llama-70b model to generate fifty alphas for five major stocks and applies PPO to adjust their weights in real time. The experimental results show that the PPO-optimized strategy outperforms an equal-weighted alpha portfolio and traditional benchmarks like Nikkei 225, S&amp;P 500, and Hang Seng Index. This highlights the significance of using reinforcement learning in the allocation of alpha weights and demonstrates the potential of combining large language model-generated signals with adaptive optimization for robust financial forecasting and trading. <div>
arXiv:2509.01393v1 Announce Type: new 
Abstract: This paper proposes a reinforcement learning framework that employs Proximal Policy Optimization (PPO) to dynamically optimize the weights of multiple large language model (LLM)-generated formulaic alphas for stock trading strategies. Formulaic alphas are mathematically defined trading signals derived from price, volume, sentiment, and other data. Although recent studies have shown that LLMs can generate diverse and effective alphas, a critical challenge lies in how to adaptively integrate them under varying market conditions. To address this gap, we leverage the deepseek-r1-distill-llama-70b model to generate fifty alphas for five major stocks: Apple, HSBC, Pepsi, Toyota, and Tencent, and then use PPO to adjust their weights in real time. Experimental results demonstrate that the PPO-optimized strategy achieves strong returns and high Sharpe ratios across most stocks, outperforming both an equal-weighted alpha portfolio and traditional benchmarks such as the Nikkei 225, S&amp;P 500, and Hang Seng Index. The findings highlight the importance of reinforcement learning in the allocation of alpha weights and show the potential of combining LLM-generated signals with adaptive optimization for robust financial forecasting and trading.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisit of Two-dimensional CEM on Crack Branching: from Single Crack-tip Tracking to Multiple Crack-tips Tracking</title>
<link>https://arxiv.org/abs/2509.01827</link>
<guid>https://arxiv.org/abs/2509.01827</guid>
<content:encoded><![CDATA[
<div> algorithm, crack tracking, fracture energy release rate, crack branching, GPU acceleration
Summary:
The article introduces the Multiple Crack-tips Tracking algorithm in two-dimensional Crack Element Model (MCT-2D-CEM) for predicting complex crack patterns in dynamic fracturing problems. The algorithm is developed to model advancements like crack branching and fragmentation. It utilizes a fracture energy release rate formulation for split elementary topology and includes benchmark examples to demonstrate its efficiency. The MCT-2D-CEM can also handle single crack propagation while introducing extra micro-cracks. The use of GPU acceleration in two-dimensional simulations ensures high computational efficiency, consistency, and accuracy. Overall, the algorithm offers a comprehensive solution for tracking and predicting advanced crack patterns in dynamic fracturing scenarios. 
<br /><br />Summary: <div>
arXiv:2509.01827v1 Announce Type: new 
Abstract: In this work, a Multiple Crack-tips Tracking algorithm in two-dimensional Crack Element Model (MCT-2D-CEM) is developed, aiming at modeling and predicting advanced and complicated crack patterns in two-dimensional dynamic fracturing problems, such as crack branching and fragmentation. Based on the developed fracture energy release rate formulation of split elementary topology, the Multiple Crack-tips Tracking algorithm is proposed and a series of benchmark examples are provided to validate effectiveness and efficiency in modeling crack branching and fragmentation. Besides, the proposed MCR-2D-CEM can still model single crack propagation but extra micro-cracks are introduced. GPU acceleration is employed in all two-dimensional simulations, providing high computational efficiency, consistency, and accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Fluid Dynamics Optimization of F1 Front Wing using Physics Informed Neural Networks</title>
<link>https://arxiv.org/abs/2509.01963</link>
<guid>https://arxiv.org/abs/2509.01963</guid>
<content:encoded><![CDATA[
<div> Neural Network, Computational Fluid Dynamics, Formula 1, Aerodynamics, Physics-Informed<br />
<br />
Summary: 
A Physics-Informed Neural Network (PINN) is proposed for fast prediction of Formula 1 front wing aerodynamic coefficients in response to new FIA regulations. The hybrid loss function combines CFD data with fluid dynamics principles to ensure accurate predictions while reducing computational time. With high R-squared values for drag and lift coefficient prediction, the PINN model offers F1 teams an efficient tool for design space exploration within the budget and time constraints. The physics-informed framework maintains adherence to fundamental aerodynamic principles, making it a valuable asset for aerodynamic development in the F1 industry. <div>
arXiv:2509.01963v1 Announce Type: new 
Abstract: In response to recent FIA regulations reducing Formula 1 team wind tunnel hours (from 320 hours for last-place teams to 200 hours for championship leaders) and strict budget caps of 135 million USD per year, more efficient aerodynamic development tools are needed by teams. Conventional computational fluid dynamics (CFD) simulations, though offering high fidelity results, require large computational resources with typical simulation durations of 8-24 hours per configuration analysis. This article proposes a Physics-Informed Neural Network (PINN) for the fast prediction of Formula 1 front wing aerodynamic coefficients. The suggested methodology combines CFD simulation data from SimScale with first principles of fluid dynamics through a hybrid loss function that constrains both data fidelity and physical adherence based on Navier-Stokes equations. Training on force and moment data from 12 aerodynamic features, the PINN model records coefficient of determination (R-squared) values of 0.968 for drag coefficient and 0.981 for lift coefficient prediction while lowering computational time. The physics-informed framework guarantees that predictions remain adherent to fundamental aerodynamic principles, offering F1 teams an efficient tool for the fast exploration of design space within regulatory constraints.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoder-based non-intrusive model order reduction in continuum mechanics</title>
<link>https://arxiv.org/abs/2509.02237</link>
<guid>https://arxiv.org/abs/2509.02237</guid>
<content:encoded><![CDATA[
<div> Autoencoder, reduced-order modeling, continuum mechanics, deep learning, surrogate models

Summary:

This article introduces a non-intrusive framework for reduced-order modeling in continuum mechanics using Autoencoder-based techniques. The framework comprises three stages: compression of high-dimensional finite element solutions into a latent space, mapping problem parameters to latent codes through regression networks, and reconstructing full-field solutions from input parameters. Two key extensions, including a force-augmented variant and a multi-field architecture, enhance the framework's capabilities, allowing for accurate predictions in complex scenarios such as thermo-mechanical coupling. The proposed method is validated on various benchmark problems and demonstrates accurate reconstructions of high-fidelity solutions without intruding on the original model. By combining deep learning with dimensionality reduction, this approach offers an efficient and extensible solution for building surrogate models in continuum mechanics. The publicly available implementation offers a foundation for integrating data-driven model order reduction into various applications, including uncertainty quantification, optimization, and digital twins.<br /><br />Summary: <div>
arXiv:2509.02237v1 Announce Type: new 
Abstract: We propose a non-intrusive, Autoencoder-based framework for reduced-order modeling in continuum mechanics. Our method integrates three stages: (i) an unsupervised Autoencoder compresses high-dimensional finite element solutions into a compact latent space, (ii) a supervised regression network maps problem parameters to latent codes, and (iii) an end-to-end surrogate reconstructs full-field solutions directly from input parameters.
  To overcome limitations of existing approaches, we propose two key extensions: a force-augmented variant that jointly predicts displacement fields and reaction forces at Neumann boundaries, and a multi-field architecture that enables coupled field predictions, such as in thermo-mechanical systems. The framework is validated on nonlinear benchmark problems involving heterogeneous composites, anisotropic elasticity with geometric variation, and thermo-mechanical coupling. Across all cases, it achieves accurate reconstructions of high-fidelity solutions while remaining fully non-intrusive.
  These results highlight the potential of combining deep learning with dimensionality reduction to build efficient and extensible surrogate models. Our publicly available implementation provides a foundation for integrating data-driven model order reduction into uncertainty quantification, optimization, and digital twin applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning-Fueled Modelfluid for Flowsheet Optimization</title>
<link>https://arxiv.org/abs/2509.02242</link>
<guid>https://arxiv.org/abs/2509.02242</guid>
<content:encoded><![CDATA[
<div> machine learning, process optimization, thermodynamic data, distillation, entrainer selection

Summary:
This article discusses the use of machine learning techniques to predict thermodynamic mixture properties for process optimization in chemical engineering. The authors present a novel modelfluid representation that integrates machine learning predicted data directly into flowsheet optimization, specifically tailored for distillation processes. The approach is built on physically interpretable features derived from vapor-liquid equilibrium phenomena, ensuring compatibility with existing simulation tools and optimization methods. The study demonstrates the accuracy and efficiency of this machine learning-fueled modelfluid by applying it to the problem of entrainer selection for azeotropic separation. Results show that the framework successfully identifies optimal entrainers with high fidelity compared to traditional models. This work provides a practical way to incorporate large-scale property prediction into process design, overcoming limitations of traditional thermodynamic models and complex equations of state.

<br /><br />Summary: <div>
arXiv:2509.02242v1 Announce Type: new 
Abstract: Process optimization in chemical engineering may be hindered by the limited availability of reliable thermodynamic data for fluid mixtures.
  Remarkable progress is being made in predicting thermodynamic mixture properties by machine learning techniques. The vast information provided by these prediction methods enables new possibilities in process optimization.
  This work introduces a novel modelfluid representation that is designed to seamlessly integrate these ML-predicted data directly into flowsheet optimization. Tailored for distillation, our approach is built on physically interpretable and continuous features derived from core vapor liquid equilibrium phenomena. This ensures compatibility with existing simulation tools and gradient-based optimization. We demonstrate the power and accuracy of this ML-fueled modelfluid by applying it to the problem of entrainer selection for an azeotropic separation. The results show that our framework successfully identifies optimal, thermodynamically consistent entrainers with high fidelity compared to conventional models.
  Ultimately, this work provides a practical pathway to incorporate large-scale property prediction into efficient process design and optimization, overcoming the limitations of both traditional thermodynamic models and complex molecular-based equations of state.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Electromechanical computational model of the human stomach</title>
<link>https://arxiv.org/abs/2509.02486</link>
<guid>https://arxiv.org/abs/2509.02486</guid>
<content:encoded><![CDATA[
<div> Keywords: gastric electromechanics, computational framework, peristalsis, motility disorders, organ scale

Summary:
The article presents a new computational framework for modeling human gastric electromechanics to study digestion and related motility disorders. It addresses the limitations of existing approaches by incorporating spatial heterogeneity, anisotropic deformations, and active-strain dynamics. The framework combines a rotation-free shell formulation with a constrained mixture material model, allowing for realistic simulation of gastric peristalsis. It can reproduce key features of gastric motility such as slow-wave entrainment, conduction velocity gradients, and peristaltic contractions. This new tool enables robust simulations of the entire stomach at the organ scale, offering promise for in-depth studies of both normal physiology and pathological conditions affecting gastric motility. <div>
arXiv:2509.02486v1 Announce Type: new 
Abstract: The stomach plays a central role in digestion through coordinated muscle contractions, known as gastric peristalsis, driven by slow-wave electrophysiology. Understanding this process is critical for treating motility disorders such as gastroparesis, dyspepsia, and gastroesophageal reflux disease. Computer simulations can be a valuable tool to deepen our understanding of these disorders and help to develop new therapies. However, existing approaches often neglect spatial heterogeneity, fail to capture large anisotropic deformations, or rely on computationally expensive three-dimensional formulations. We present here a computational framework of human gastric electromechanics, that combines a nonlinear, rotation-free shell formulation with a constrained mixture material model. The formulation incorporates active-strain, constituent-specific prestress, and spatially non-uniform parameter fields. Numerical examples demonstrate that the framework can reproduce characteristic features of gastric motility, including slow-wave entrainment, conduction velocity gradients, and large peristaltic contractions with physiologically realistic amplitudes. The proposed framework enables robust electromechanical simulations of the whole stomach at the organ scale. It thus provides a promising basis for future in silico studies of both physiological function and pathological motility disorders.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized Renewable Energy Planning MDP for Socially-Equitable Electricity Coverage in the US</title>
<link>https://arxiv.org/abs/2509.00008</link>
<guid>https://arxiv.org/abs/2509.00008</guid>
<content:encoded><![CDATA[
<div> optimization, renewable energy, social equity, electricity distribution, clean energy 

Summary: 
The study introduces a Markov Decision Process framework to optimize renewable energy allocation in the electricity distribution system while addressing social equity concerns. By considering budget constraints, energy demand variability, and social vulnerability indicators across major U.S. cities, the model evaluates policy alternatives for achieving equitable clean energy transitions. Numerical experiments show that an equity-focused approach can increase renewable energy penetration by 32.9% and reduce underserved low-income populations by 55% compared to conventional methods. The expert policy performed the best, while the Monte Carlo Tree Search baseline showed competitive performance with lower budget utilization. This study highlights that fair distribution of clean energy resources is possible without compromising system performance, offering a pathway to integrate social equity considerations with climate goals and provide inclusive access to clean power infrastructure. 

<br /><br />Summary: <div>
arXiv:2509.00008v1 Announce Type: cross 
Abstract: Traditional power grid infrastructure presents significant barriers to renewable energy integration and perpetuates energy access inequities, with low-income communities experiencing disproportionately longer power outages. This study develops a Markov Decision Process (MDP) framework to optimize renewable energy allocation while explicitly addressing social equity concerns in electricity distribution. The model incorporates budget constraints, energy demand variability, and social vulnerability indicators across eight major U.S. cities to evaluate policy alternatives for equitable clean energy transitions. Numerical experiments compare the MDP-based approach against baseline policies including random allocation, greedy renewable expansion, and expert heuristics. Results demonstrate that equity-focused optimization can achieve 32.9% renewable energy penetration while reducing underserved low-income populations by 55% compared to conventional approaches. The expert policy achieved the highest reward, while the Monte Carlo Tree Search baseline provided competitive performance with significantly lower budget utilization, demonstrating that fair distribution of clean energy resources is achievable without sacrificing overall system performance and providing ways for integrating social equity considerations with climate goals and inclusive access to clean power infrastructure.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Parameter Fields in Multi-Physics PDEs from Scarce Measurements</title>
<link>https://arxiv.org/abs/2509.00203</link>
<guid>https://arxiv.org/abs/2509.00203</guid>
<content:encoded><![CDATA[
<div> parameterized partial differential equations, parameter estimation methods, sparse measurements, Neptune, parameter fields<br />
<br />
Summary: Neptune is a novel method for inferring parameter fields from sparse measurements of system responses in parameterized partial differential equations. It addresses challenges in accurately estimating parameters with nonlinear and spatiotemporal variations. Neptune outperforms existing methods like sparse identification and PINNs by reducing parameter estimation errors significantly and improving dynamic response prediction accuracy. It achieves reliable parameter inference from a small number of observations and exhibits superior extrapolation capabilities compared to PINNs. Neptune's performance in various physical and biomedical problems makes it a promising tool for applications in engineering and healthcare, offering data-efficient and robust parameter estimation for complex systems. <div>
arXiv:2509.00203v1 Announce Type: cross 
Abstract: Parameterized partial differential equations (PDEs) underpin the mathematical modeling of complex systems in diverse domains, including engineering, healthcare, and physics. A central challenge in using PDEs for real-world applications is to accurately infer the parameters, particularly when the parameters exhibit non-linear and spatiotemporal variations. Existing parameter estimation methods, such as sparse identification and physics-informed neural networks (PINNs), struggle in such cases, especially with nonlinear dynamics, multiphysics interactions, or limited observations of the system response. To address these challenges, we introduce Neptune, a general-purpose method capable of inferring parameter fields from sparse measurements of system responses. Neptune employs independent coordinate neural networks to continuously represent each parameter field in physical space or in state variables. Across various physical and biomedical problems, where direct parameter measurements are prohibitively expensive or unattainable, Neptune significantly outperforms existing methods, achieving robust parameter estimation from as few as 50 observations, reducing parameter estimation errors by two orders of magnitude and dynamic response prediction errors by a factor of ten compared to PINNs. Furthermore, Neptune exhibits superior extrapolation capabilities, enabling accurate predictions in regimes beyond training data where PINN fail. By facilitating reliable and data-efficient parameter inference, Neptune promises broad transformative impacts in engineering, healthcare, and beyond.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilization techniques for immersogeometric analysis of plate and shell problems in explicit dynamics</title>
<link>https://arxiv.org/abs/2509.00522</link>
<guid>https://arxiv.org/abs/2509.00522</guid>
<content:encoded><![CDATA[
<div> Finite element, plate, shell, immersed finite element, lumped mass matrix <br />
<br />
Summary: <br />
The article discusses the challenges faced in dynamic analyses of slender structures using finite element plate and shell formulations due to high order partial differential equations and badly cut elements in immersed finite element discretizations. The critical time step in explicit dynamics is constrained, and lumping the mass matrix, while increasing the critical time step, can lead to spurious oscillations. The authors extend their previous work to enable stable immersogeometric analysis of plate and shell problems with lumped mass matrices by using polynomial extensions. This technique restores accuracy comparable to boundary-fitted discretizations, providing a solution to the issues faced in dynamic analysis of slender structures. <div>
arXiv:2509.00522v1 Announce Type: cross 
Abstract: Finite element plate and shell formulations are ubiquitous in structural analysis for modeling all kinds of slender structures, both for static and dynamic analyses. The latter are particularly challenging as the high order nature of the underlying partial differential equations and the slenderness of the structures all impose a stringent constraint on the critical time step in explicit dynamics. Unfortunately, badly cut elements in immersed finite element discretizations further aggravate the issue. While lumping the mass matrix often increases the critical time step, it might also trigger spurious oscillations in the approximate solution thereby compromising the numerical solution. In this article, we extend our previous work in \cite{voet2025stabilization} to allow stable immersogeometric analysis of plate and shell problems with lumped mass matrices. This technique is based on polynomial extensions and restores a level of accuracy comparable to boundary-fitted discretizations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures</title>
<link>https://arxiv.org/abs/2509.00633</link>
<guid>https://arxiv.org/abs/2509.00633</guid>
<content:encoded><![CDATA[
<div> thermal vulnerabilities, memory wall, HBM architectures, performance degradation attacks, thermal wave<br />
<br />
Summary: 3D-stacked High Bandwidth Memory (HBM) architectures address the memory wall challenge but are vulnerable to thermal attacks due to vertical adjacency. Adversaries could exploit this by injecting intense heat pulses from adjacent memory banks, creating a convergent thermal wave that delays victim applications. These attacks do not access out-of-range memory, bypassing security tests and memory management policies. Detection is difficult as the attack mimics legitimate workloads. <div>
arXiv:2509.00633v1 Announce Type: cross 
Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance memory interactions to address the well-known performance challenge, namely the memory wall. However, these architectures are susceptible to thermal vulnerabilities due to the inherent vertical adjacency that occurs during the manufacturing process of HBM architectures. We anticipate that adversaries may exploit the intense vertical and lateral adjacency to design and develop thermal performance degradation attacks on the memory banks that host data/instructions from victim applications. In such attacks, the adversary manages to inject short and intense heat pulses from vertically and/or laterally adjacent memory banks, creating a convergent thermal wave that maximizes impact and delays the victim application from accessing its data/instructions. As the attacking application does not access any out-of-range memory locations, it can bypass both design-time security tests and the operating system's memory management policies. In other words, since the attack mimics legitimate workloads, it will be challenging to detect.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resting-state fMRI Analysis using Quantum Time-series Transformer</title>
<link>https://arxiv.org/abs/2509.00711</link>
<guid>https://arxiv.org/abs/2509.00711</guid>
<content:encoded><![CDATA[
<div> fMRI, Quantum Time-series Transformer, neural biomarkers, computational complexity, interpretability analysis <br />
Summary:<br />
The article introduces a Quantum Time-series Transformer, a novel quantum-enhanced transformer architecture for analyzing resting-state fMRI data. It addresses the limitations of classical transformer models by reducing computational complexity, parameter counts, and data requirements. The Quantum Time-series Transformer outperforms traditional models in predictive performance, especially in small-sample scenarios, and reliably identifies neural biomarkers related to ADHD. By leveraging quantum techniques like Linear Combination of Unitaries and Quantum Singular Value Transformation, this approach shows promise in efficiently modeling complex brain dynamics and enhancing clinical interpretability in computational neuroscience. <div>
arXiv:2509.00711v1 Announce Type: cross 
Abstract: Resting-state functional magnetic resonance imaging (fMRI) has emerged as a pivotal tool for revealing intrinsic brain network connectivity and identifying neural biomarkers of neuropsychiatric conditions. However, classical self-attention transformer models--despite their formidable representational power--struggle with quadratic complexity, large parameter counts, and substantial data requirements. To address these barriers, we introduce a Quantum Time-series Transformer, a novel quantum-enhanced transformer architecture leveraging Linear Combination of Unitaries and Quantum Singular Value Transformation. Unlike classical transformers, Quantum Time-series Transformer operates with polylogarithmic computational complexity, markedly reducing training overhead and enabling robust performance even with fewer parameters and limited sample sizes. Empirical evaluation on the largest-scale fMRI datasets from the Adolescent Brain Cognitive Development Study and the UK Biobank demonstrates that Quantum Time-series Transformer achieves comparable or superior predictive performance compared to state-of-the-art classical transformer models, with especially pronounced gains in small-sample scenarios. Interpretability analyses using SHapley Additive exPlanations further reveal that Quantum Time-series Transformer reliably identifies clinically meaningful neural biomarkers of attention-deficit/hyperactivity disorder (ADHD). These findings underscore the promise of quantum-enhanced transformers in advancing computational neuroscience by more efficiently modeling complex spatio-temporal dynamics and improving clinical interpretability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Modeling for Personalized Transcranial Electrical Stimulation: Theory, Tools, and Applications</title>
<link>https://arxiv.org/abs/2509.01192</link>
<guid>https://arxiv.org/abs/2509.01192</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized tES, computational modeling, individualized stimulation optimization, head modeling, optimization algorithms<br />
Summary:<br />
The review focuses on the advancements in personalized transcranial electrical stimulation (tES) through computational modeling. It emphasizes the importance of individualized stimulation optimization due to the significant variability in brain anatomy and physiology among individuals. The review systematically examines recent developments in forward and inverse modeling techniques to simulate personalized electric fields and optimize stimulation parameters. It discusses the progress in constructing subject-specific head conductor models, utilizing optimization algorithms, and integrating multimodal brain data. Recent advancements have led to dynamic and individualized stimulation planning, moving away from traditional trial-and-error approaches. The review highlights the challenges, opportunities, and future directions in achieving precision neuromodulation in research and clinical settings. <div>
arXiv:2509.01192v1 Announce Type: cross 
Abstract: Objective. Personalized transcranial electrical stimulation (tES) has gained growing attention due to the substantial inter-individual variability in brain anatomy and physiology. While previous reviews have discussed the physiological mechanisms and clinical applications of tES, there remains a critical gap in up-to-date syntheses focused on the computational modeling frameworks that enable individualized stimulation optimization. Approach. This review presents a comprehensive overview of recent advances in computational techniques supporting personalized tES. We systematically examine developments in forward modeling for simulating individualized electric fields, as well as inverse modeling approaches for optimizing stimulation parameters. We critically evaluate progress in head modeling pipelines, optimization algorithms, and the integration of multimodal brain data. Main results. Recent advances have substantially accelerated the construction of subject-specific head conductor models and expanded the landscape of optimization methods, including multi-objective optimization and brain network-informed optimization. These advances allow for dynamic and individualized stimulation planning, moving beyond empirical trial-and-error approaches.Significance. By integrating the latest developments in computational modeling for personalized tES, this review highlights current challenges, emerging opportunities, and future directions for achieving precision neuromodulation in both research and clinical contexts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Bifurcation Handling in Physics-Based Reduced-Order Vascular Hemodynamic Models</title>
<link>https://arxiv.org/abs/2508.21165</link>
<guid>https://arxiv.org/abs/2508.21165</guid>
<content:encoded><![CDATA[
<div> machine learning, cardiovascular flows, reduced-order models, bifurcation coefficients, numerical framework

Summary:
- The study presents a numerical framework that combines machine learning-predicted bifurcation coefficients with zero-dimensional (0D) hemodynamic reduced-order models (ROMs) to enhance accuracy while maintaining computational efficiency.
- A resistor-resistor-inductor (RRI) model utilizing neural networks is developed to predict pressure-flow relationships based on bifurcation geometry, incorporating linear and quadratic resistances and inductive effects.
- Non-dimensionalization is used to reduce training data requirements, and a priori flow split prediction is employed for better bifurcation characterization.
- The RRI model is integrated into a 0D model using an optimization-based solution strategy, demonstrating significant accuracy improvements, especially at high Reynolds numbers and in complex vascular networks.
- The enhanced 0D models enable real-time hemodynamic modeling for clinical decision support, uncertainty quantification, and digital twins in cardiovascular biomedical engineering.

<br /><br />Summary: <div>
arXiv:2508.21165v1 Announce Type: new 
Abstract: Three-dimensional (3D) finite-element simulations of cardiovascular flows provide high-fidelity predictions to support cardiovascular medicine, but their high computational cost limits clinical practicality. Reduced-order models (ROMs) offer computationally efficient alternatives but suffer reduced accuracy, particularly at vessel bifurcations where complex flow physics are inadequately captured by standard Poiseuille flow assumptions. We present an enhanced numerical framework that integrates machine learning-predicted bifurcation coefficients into zero-dimensional (0D) hemodynamic ROMs to improve accuracy while maintaining computational efficiency. We develop a resistor-resistor-inductor (RRI) model that uses neural networks to predict pressure-flow relationships from bifurcation geometry, incorporating linear and quadratic resistances along with inductive effects. The method employs non-dimensionalization to reduce training data requirements and apriori flow split prediction for improved bifurcation characterization. We incorporate the RRI model into a 0D model using an optimization-based solution strategy. We validate the approach in isolated bifurcations and vascular trees, across Reynolds numbers from 0 to 5,500, defining ROM accuracy by comparison to 3D finite element simulation. Results demonstrate substantial accuracy improvements: averaged across all trees and Reynolds numbers, the RRI method reduces inlet pressure errors from 54 mmHg (45%) for standard 0D models to 25 mmHg (17%), while a simplified resistor-inductor (RI) variant achieves 31 mmHg (26%) error. The enhanced 0D models show particular effectiveness at high Reynolds numbers and in extensive vascular networks. This hybrid numerical approach enables accurate, real-time hemodynamic modeling for clinical decision support, uncertainty quantification, and digital twins in cardiovascular biomedical engineering.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A hyperreduced manifold learning approach to nonlinear model order reduction for the homogenisation of hyperelastic RVEs</title>
<link>https://arxiv.org/abs/2508.21527</link>
<guid>https://arxiv.org/abs/2508.21527</guid>
<content:encoded><![CDATA[
<div> Keywords: graph-based manifold learning, nonlinear Galerkin-reduction, model order reduction, hyperreduction methods, online computational costs <br />
Summary: 
The article presents a graph-based manifold learning scheme for nonlinear Galerkin-reduction in quasi-static solid mechanical problems. This approach allows for the creation of nonlinear approximation spaces that closely represent nonlinear solution manifolds. By integrating hyperreduction methods, the scheme significantly reduces online computational costs while maintaining high accuracy. The algorithmic complexity is independent of the original system size, and improvements to the local online linearization scheme enhance performance and robustness. In an example problem, the model order reduction scheme accelerates computations by over two orders of magnitude with minimal training data and negligible loss of accuracy. The approach outperforms alternative methods in the accuracy-runtime trade-off, showcasing its efficiency and effectiveness in reducing computational costs in nonlinear solid mechanics problems. <br /><br />Summary: <div>
arXiv:2508.21527v1 Announce Type: new 
Abstract: In a recent work, we proposed a graph-based manifold learning scheme for the nonlinear Galerkin-reduction of quasi-static solid mechanical problems [1]. The resulting nonlinear approximation spaces can closely and flexibly represent nonlinear solution manifolds. The present work discusses how this nonlinear model order reduction (MOR) approach can be employed to reduce online computational costs by multiple orders of magnitude while retaining high levels of accuracy. We integrate two popular hyperreduction methods into the nonlinear MOR framework and discuss how we achieve an algorithmic complexity which is independent from the original system size. Furthermore, improvements are made to the local online linearisation scheme for the sake of performance and robustness. On an example RVE problem, the MOR scheme accelerates computations by more than two orders of magnitude with little training data and negligible loss of accuracy. Additionally, the algorithm Pareto-dominates alternative approaches in the trade-off between accuracy and runtime on the considered example.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIPPO: A Graph-based, Iterative, Printing-Path Optimization Slicer for Architected Lattices</title>
<link>https://arxiv.org/abs/2508.21694</link>
<guid>https://arxiv.org/abs/2508.21694</guid>
<content:encoded><![CDATA[
<div> Graph-based, Iterative, Printing-Path Optimization, lattice structures, 3D printing, mechanical properties, open-source slicing platform, fabrication

Summary:
GIPPO is an open-source slicing platform that optimizes printing trajectories for complex lattice designs using a modified version of Prim's algorithm. It improves shape fidelity, reduces local thickness deviations, eliminates missing struts, and minimizes excess material deposition. GIPPO outperforms conventional slicing software in fabricating architected lattice structures made of thermoplastic polyurethane through fused deposition modeling. The optimization of printing paths directly affects the mechanical responses of the structures under different loading conditions. GIPPO accommodates planar and non-planar printing geometries and allows for the fabrication of objects with varying infill patterns per layer. This platform addresses limitations in commercial slicing software and enables high-fidelity fabrication of intricate architected materials. 

<br /><br />Summary: <div>
arXiv:2508.21694v1 Announce Type: new 
Abstract: Architected materials of significant geometric complexity offer exceptional mechanical properties that often surpass those of their constituent materials. However, their fabrication through extrusion-based 3D printing remains hindered by suboptimal printing trajectories, which is inherent to commercial slicing software. They produce multiple non-continuous paths that compromise fabrication time, shape fidelity, and structural integrity, particularly for thin-walled lattice structures. To address this issue, we introduce GIPPO (Graph-based, Iterative, Printing-Path Optimization), an open-source slicing platform that transforms complex lattice designs into optimized printing trajectories. Lattices are converted to graph networks to derive the optimal printing trajectories through a modified version of Prim's algorithm. The resulting paths are translated back to Euclidean coordinates and exported as a ready-to-use G-code. We validated GIPPO's performance against conventional slicing software across six architected lattice geometries fabricated from thermoplastic polyurethane using fused deposition modeling. GIPPO-optimized constructs demonstrated superior shape fidelity with reduced local thickness deviations, no missing struts, and minimized excess material deposition compared to conventionally printed controls. Mechanical testing revealed that printing path optimization directly influences both uniaxial and out-of-plane mechanical responses, with different optimization strategies yielding distinct performance characteristics suited to specific loading conditions. Moreover, the platform accommodates both planar and non-planar printing geometries and enables fabrication of objects with varying infill patterns per layer. Our work addresses critical limitations in commercial slicing software and opens new opportunities for high-fidelity fabrication of complex architected materials.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Financial Brain Scan of the LLM</title>
<link>https://arxiv.org/abs/2508.21285</link>
<guid>https://arxiv.org/abs/2508.21285</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, economic forecasts, sentiment, risk-averse, biases

Summary:
Using advanced computer science techniques, researchers can analyze large language models (LLMs) to understand the concepts guiding their economic forecasts. This method allows for the identification of key factors such as sentiment, technical analysis, and timing without compromising performance. The approach also enables researchers to manipulate the models to be more or less risk-averse, optimistic, or pessimistic, providing opportunities to correct or simulate biases in the forecasts. Importantly, this method is transparent, easy to implement, and replicable, making it a valuable tool for empirical research in the social sciences.<br /><br />Summary: <div>
arXiv:2508.21285v1 Announce Type: cross 
Abstract: Emerging techniques in computer science make it possible to "brain scan" large language models (LLMs), identify the plain-English concepts that guide their reasoning, and steer them while holding other factors constant. We show that this approach can map LLM-generated economic forecasts to concepts such as sentiment, technical analysis, and timing, and compute their relative importance without reducing performance. We also show that models can be steered to be more or less risk-averse, optimistic, or pessimistic, which allows researchers to correct or simulate biases. The method is transparent, lightweight, and replicable for empirical research in the social sciences.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality</title>
<link>https://arxiv.org/abs/2508.21736</link>
<guid>https://arxiv.org/abs/2508.21736</guid>
<content:encoded><![CDATA[
<div> Keywords: Microbiomes, mathematical modeling, spatiotemporal data, virtual reality, data analysis

Summary: <br /><br />Microbiomes play a crucial role in the human body, but studying them experimentally is challenging, leading to more research in mathematical modeling. Current tools for simulating microbial communities lack interactive functionalities and are complex to use. To address these limitations, a user-friendly tool called MicroLabVR has been developed. It transfers spatial data into virtual reality (VR) and allows users to explore spatiotemporal simulation data interactively. Users can import datasets containing population growth, substance concentration development, and metabolic flux distribution data. This tool aims to improve data analysis by enabling the exploration of microbiome data in their spatial context. <div>
arXiv:2508.21736v1 Announce Type: cross 
Abstract: Microbiomes are a vital part of the human body, engaging in tasks like food digestion and immune defense. Their structure and function must be understood in order to promote host health and facilitate swift recovery during disease. Due to the difficulties in experimentally studying these systems in situ, more research is being conducted in the field of mathematical modeling. Visualizing spatiotemporal data is challenging, and current tools that simulate microbial communities' spatial and temporal development often only provide limited functionalities, often requiring expert knowledge to generate useful results. To overcome these limitations, we provide a user-friendly tool to interactively explore spatiotemporal simulation data, called MicroLabVR, which transfers spatial data into virtual reality (VR) while following guidelines to enhance user experience (UX). With MicroLabVR, users can import CSV datasets containing population growth, substance concentration development, and metabolic flux distribution data. The implemented visualization methods allow users to evaluate the dataset in a VR environment interactively. MicroLabVR aims to improve data analysis for the user by allowing the exploration of microbiome data in their spatial context.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Hypergraph Diffusion for Crystal Structure Prediction</title>
<link>https://arxiv.org/abs/2501.18850</link>
<guid>https://arxiv.org/abs/2501.18850</guid>
<content:encoded><![CDATA[
<div> Keywords: Crystal Structure Prediction, Generative Models, Hypergraphs, EH-Diff, Symmetry-preserving Properties

Summary: 
Crystal Structure Prediction (CSP) is a challenging task crucial for developing new materials. Traditional graph-based models struggle to capture complex high-order interactions in crystal structures. This study introduces a novel approach using hypergraphs to represent crystal structures, allowing for the modeling of multi-way atomic interactions. The Equivariant Hypergraph Diffusion Model (EH-Diff) is proposed as a generative model that leverages the symmetry-preserving properties of hypergraphs to accurately predict crystal structures. Experimental results on benchmark datasets show that EH-Diff outperforms existing CSP methods with just one sample. This approach offers an efficient and accurate method for crystal structure prediction, emphasizing the importance of symmetry and high-order relationships in accurately characterizing crystal structures.<br /><br />Summary: <div>
arXiv:2501.18850v2 Announce Type: replace 
Abstract: Crystal Structure Prediction (CSP) remains a fundamental challenge with significant implications for the development of new materials and the advancement of various scientific disciplines. Recent developments have shown that generative models, particularly diffusion models, hold great promise for CSP. However, traditional graph-based representations, where atomic bonds are modeled as pairwise graph edges, fail to fully capture the intricate high-order interactions essential for accurately representing crystal structures. In this work, we propose a novel approach that utilizes hypergraphs to represent crystal structures, providing a more expressive abstraction for modeling multi-way atomic interactions. By adopting hypergraphs, we can effectively capture complex high-order relationships and symmetries, such as permutation and periodic translation invariance, which are crucial for characterizing crystal structures. In this work, we propose the \textbf{E}quivariant \textbf{H}ypergraph \textbf{Diff}usion Model (\textbf{EH-Diff}), a generative model designed to take advantage of the symmetry-preserving properties of hypergraphs. EH-Diff exploits these features to offer an efficient and accurate method for predicting crystal structures with a strong theoretical justification to preserve invariance properties. Empirically, we conduct extensive experiments on four benchmark datasets, and the results demonstrate that EH-Diff outperforms state-of-the-art CSP methods with only one sample.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mass conservation analysis of extrusion-based 3D printing simulations based on the level-set method</title>
<link>https://arxiv.org/abs/2508.20617</link>
<guid>https://arxiv.org/abs/2508.20617</guid>
<content:encoded><![CDATA[
<div> conservative level-set method, mass conservation, extrusion-based printing, numerical simulations, cross-sectional area<br />
Summary:<br />
The article investigates the mass conservation properties of the conservative level-set method in extrusion-based 3D printing. It focuses on tracking evolving material boundaries accurately to avoid mismatches between the extruded and simulated shapes. The study analyzes the impact of level set parameters on mass conservation accuracy, specifically looking at the cross-sectional area of deposited strands. Results show that reducing reinitialization and interface thickness parameters decreases errors in cross-sectional area calculations but may increase computational costs. Selecting an appropriate interface thickness can also reduce strong mesh requirements. Comparing simulated cross-sectional areas with ideal areas from a mass balance at steady state indicates good agreement, validating the method's accuracy. The research contributes valuable insights into improving mass conservation in extrusion-based 3D printing simulations. <br /><br />Summary: <div>
arXiv:2508.20617v1 Announce Type: new 
Abstract: Numerical simulations of extrusion-based printing require tracking evolving material bound- aries, a challenging task due to possible topological changes and mass conservation issues. Inaccurate conservation of mass can lead to a mismatch between the extruded and simulated shapes, and generally to unreliable predictions of the actual ink behavior. This work investigates the mass conservation properties of the conservative level-set method in extrusion-based 3D printing applications. We analyze the effects of the level set parameters on the accuracy of mass conservation using the cross-sectional area of the deposited strand. We compare the cross- sectional areas obtained in the simulation with the ideal areas obtained from a mass balance when the system reaches a steady-state condition. The numerical results indicate that reducing the reinitialization and the interface thickness parameters decreases the errors in the cross-sectional area obtained. However, the reductions in error tend to decline and could lead to excessive computational cost. Furthermore, we also found that the typical strong mesh requirements can be lessened by selecting an adequate interface thickness. Finally, we obtained the cross-sectional areas from simulations with different printing settings and found that they show good agreement with the simulated and experimental data published in previous work.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can News Predict the Direction of Oil Price Volatility? A Language Model Approach with SHAP Explanations</title>
<link>https://arxiv.org/abs/2508.20707</link>
<guid>https://arxiv.org/abs/2508.20707</guid>
<content:encoded><![CDATA[
<div> Keywords: financial markets, crude oil, news analysis, predictive modeling, sentiment analysis 

Summary:<br /><br />Financial markets are influenced by news, sentiment, and economic indicators, impacting asset price fluctuations. This study focuses on crude oil price volatility prediction using news data exclusively, comparing it to traditional market data methods. Utilizing a decade-long Eikon dataset, an ensemble learning framework incorporating sentiment analysis techniques and language models is developed. The model's performance is compared to the HAR model through the McNemar test, with raw news count being a significant predictor. FastText emerges as the most effective embedding technique for forecasting price movements. SHAP-based interpretation at the word level reveals evolving predictive drivers during different market regimes. Pre-pandemic factors included supply-demand and economic terms, early pandemic emphasized uncertainty and macroeconomic instability, post-shock focused on long-term recovery indicators, and war-period considered geopolitical and regional oil market disruptions. These findings highlight the potential of news-driven features and explainable NLP in financial forecasting. <div>
arXiv:2508.20707v1 Announce Type: new 
Abstract: Financial markets can be highly sensitive to news, investor sentiment, and economic indicators, leading to important asset price fluctuations. In this study we focus on crude oil, due to its crucial role in commodity markets and the global economy. Specifically, we are interested in understanding the directional changes of oil price volatility, and for this purpose we investigate whether news alone -- without incorporating traditional market data -- can effectively predict the direction of oil price movements. Using a decade-long dataset from Eikon (2014-2024), we develop an ensemble learning framework to extract predictive signals from financial news. Our approach leverages diverse sentiment analysis techniques and modern language models, including FastText, FinBERT, Gemini, and LLaMA, to capture market sentiment and textual patterns. We benchmark our model against the Heterogeneous Autoregressive (HAR) model and assess statistical significance using the McNemar test. While most sentiment-based indicators do not consistently outperform HAR, the raw news count emerges as a robust predictor. Among embedding techniques, FastText proves most effective for forecasting directional movements. Furthermore, SHAP-based interpretation at the word level reveals evolving predictive drivers across market regimes: pre-pandemic emphasis on supply-demand and economic terms; early pandemic focus on uncertainty and macroeconomic instability; post-shock attention to long-term recovery indicators; and war-period sensitivity to geopolitical and regional oil market disruptions. These findings highlight the predictive power of news-driven features and the value of explainable NLP in financial forecasting.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-consistent clustering analysis for homogenisation of heterogeneous plates</title>
<link>https://arxiv.org/abs/2508.20446</link>
<guid>https://arxiv.org/abs/2508.20446</guid>
<content:encoded><![CDATA[
<div> plate structures, reduced-order model, periodic micro-structures, self-consistent clustering analysis, Lippmann-Schwinger equation

Summary: 
This study presents a novel reduced-order model for plate structures with periodic micro-structures. By combining self-consistent clustering analysis (SCA) with the Lippmann-Schwinger equation, the model allows for fast multiscale homogenization of heterogeneous plates. A plate-specific SCA scheme is developed, incorporating an offline-online strategy utilizing Green's functions and k-means data compression, as well as an online self-consistent update leveraging the weak sensitivity of the reference medium. The framework is applicable to both linear and nonlinear problems in classical plate theory and first-order shear deformation theory, demonstrating its accuracy on various plate configurations. The proposed model matches the precision of FFT-based direct numerical simulation while significantly reducing computational cost. Examples include linear isotropic perforated plates, woven composites, and nonlinear elasto-plastic perforated plates with damage. This innovative approach enables efficient analysis of complex plate structures with periodic micro-structures. <div>
arXiv:2508.20446v1 Announce Type: cross 
Abstract: This work introduces a reduced-order model for plate structures with periodic micro-structures by coupling self-consistent clustering analysis (SCA) with the Lippmann-Schwinger equation, enabling rapid multiscale homogenisation of heterogeneous plates. A plate-specific SCA scheme is derived for the first time and features two key elements: (i) an offline-online strategy that combines Green's functions with k-means data compression, and (ii) an online self-consistent update that exploits the weak sensitivity of the reference medium. The framework handles both linear and nonlinear problems in classical plate theory and first-order shear deformation theory, and its performance is verified on linear isotropic perforated plates and woven composites, as well as on non-linear elasto-plastic perforated plates and woven composites with damage. Across all cases the proposed model matches the accuracy of FFT-based direct numerical simulation while reducing computational cost by over an order of magnitude.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation</title>
<link>https://arxiv.org/abs/2508.20806</link>
<guid>https://arxiv.org/abs/2508.20806</guid>
<content:encoded><![CDATA[
<div> epistemic uncertainty, Epistemic Support-Point Filter, possibility theory, ordinal logic, Choquet integral <br /> 
Summary: The Epistemic Support-Point Filter (ESPF) introduces a non-Bayesian filtering framework that addresses the limitations of traditional state estimation methods. ESPF is grounded in possibility theory and emphasizes epistemic humility by redefining belief evolution using compatibility-weighted support updates and surprisal-aware pruning. It adapts belief support through adaptive dispersion via sparse grid quadrature and employs the Choquet integral for multi-model inference. ESPF does not seek a posterior distribution but maintains a structured region of plausibility, updating using ordinal logic. This approach allows for dynamic contraction or expansion of belief support based on information structure without requiring prior statistical calibration. The framework supports robust estimation in sparse or adversarial sensing environments where priors are unavailable, misleading, or epistemically unjustified. <br /> 
Summary: <div>
arXiv:2508.20806v1 Announce Type: cross 
Abstract: Traditional state estimation methods rely on probabilistic assumptions that often collapse epistemic uncertainty into scalar beliefs, risking overconfidence in sparse or adversarial sensing environments. We introduce the Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework fully grounded in possibility theory and epistemic humility. ESPF redefines the evolution of belief over state space using compatibility-weighted support updates, surprisalaware pruning, and adaptive dispersion via sparse grid quadrature. Unlike conventional filters, ESPF does not seek a posterior distribution, but rather maintains a structured region of plausibility or non-rejection, updated using ordinal logic rather than integration. For multi-model inference, we employ the Choquet integral to fuse competing hypotheses based on a dynamic epistemic capacity function, generalizing classical winner-take-all strategies. The result is an inference engine capable of dynamically contracting or expanding belief support in direct response to information structure, without requiring prior statistical calibration. This work presents a foundational shift in how inference, evidence, and ignorance are reconciled, supporting robust estimation where priors are unavailable, misleading, or epistemically unjustified.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scale-invariant Monte Carlo and multilevel Monte Carlo estimation of mean and variance: An application to simulation of linear elastic bone tissue</title>
<link>https://arxiv.org/abs/2106.13723</link>
<guid>https://arxiv.org/abs/2106.13723</guid>
<content:encoded><![CDATA[
<div> scale-invariant, error estimators, Monte Carlo, multilevel Monte Carlo, mechanical simulation

Summary:
The article introduces novel scale-invariant error estimators for Monte Carlo and multilevel Monte Carlo methods used in estimating mean and variance. These estimators optimize computation costs across different grid levels for linear transformations of the quantity of interest, remaining robust to distribution variations. The proposed algorithms are demonstrated in a mechanical simulation of linear elastic bone tissue, incorporating material uncertainty with heterogeneity and random anisotropy in the constitutive law. The new error estimators are fully dimensionless and offer improved efficiency in estimating mean and variance, making them suitable for a wide range of applications where accurate estimation of uncertainty is essential.<br /><br />Summary: <div>
arXiv:2106.13723v3 Announce Type: replace-cross 
Abstract: We propose novel scale-invariant error estimators for the Monte Carlo and multilevel Monte Carlo estimation of mean and variance. For any linear transformation of the distribution of the quantity of interest, the computation cost across grid levels is optimized using a normalized error estimate, which is not only fully dimensionless but also remains robust to variation in characteristics of the distribution. We demonstrate the effectiveness of the algorithms through application to a mechanical simulation of linear elastic bone tissue, where material uncertainty incorporating both heterogeneity and random anisotropy is considered in the constitutive law.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infrastructure-enabled risk assessment of hazardous road conditions on rural roads during inclement weather</title>
<link>https://arxiv.org/abs/2508.19444</link>
<guid>https://arxiv.org/abs/2508.19444</guid>
<content:encoded><![CDATA[
<div> Keywords: Rural roadways, Commercial Motor Vehicle drivers, hazardous conditions, roadway hazard risk assessment, safe advisory speeds

Summary: 
The study addresses the lack of real-time reporting of hazardous conditions on rural roadways and limited infrastructure, increasing the risk of crashes for Commercial Motor Vehicle (CMV) drivers. The framework presented quantifies the probability and severity of crash occurrences due to specific roadway hazards, providing a comprehensive approach to assess combined driving risks. A synthetic dataset was used for a case study, confirming the coherence of the risk profile generated by the combined ProbabilitySeverity scoring. The results validate the practicality of the risk assessment approach and suggest implementing graduated safety measures in real-world roadway operations.<br /><br />Summary: <div>
arXiv:2508.19444v1 Announce Type: new 
Abstract: Rural roadways often expose Commercial Motor Vehicle (CMV) drivers to hazardous conditions, such as heavy fog, rain, snow, black ice, and flash floods, many of which remain unreported in real time. This lack of timely information, coupled with limited infrastructure in rural areas, significantly increases the risk of crashes. Although various sensing technologies exist to monitor individual hazards like low visibility or surface friction, they rarely assess the combined driving risk posed by multiple simultaneous hazards, nor do they provide actionable recommendations such as safe advisory speeds. To address this critical gap, in this study, we present a roadway hazard risk assessment framework that provides an approach to quantify the probability and severity of crash occurrences due to specific roadway hazards. To evaluate this framework, we presented a case study by constructing a synthetic "year-long" dataset that encompasses every possible pairing of road surface and visibility conditions. Our analysis confirms that the combined ProbabilitySeverity scoring yields a coherent, stepwise risk profile across all hazard scenarios. These results validate the practicality of our risk assessment approach and provide a foundation for deploying graduated safety measures in real-world roadway operations.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An assessment of estimation models and investment gaps for the deployment of high-speed broadband networks in NUTS3 regions to meet the objectives of the European Gigabit Society</title>
<link>https://arxiv.org/abs/2508.19921</link>
<guid>https://arxiv.org/abs/2508.19921</guid>
<content:encoded><![CDATA[
<div> European Union, high speed broadband networks, investment, European Gigabit Society, estimation model <br />
Summary: 
This paper examines the deployment of high speed broadband networks in the European Union, specifically focusing on the investment required to achieve the targets set by the European Commission for 2025 as part of the European Gigabit Society. The analysis includes assessing the availability and adoption of high capacity fixed and wireless networks in urban and rural areas. The estimation model used in the study incorporates data at the local level to determine the investment gap for each EGS objective. Three scenarios based on technology mixes are considered. The paper compares its methodology with existing literature and provides a dynamic view of the investment gap evolution from 2017 to 2019. The analysis proves the usefulness of the estimation models in evaluating the investment needed for high speed broadband infrastructure in the EU. <br /><br /> <div>
arXiv:2508.19921v1 Announce Type: new 
Abstract: This paper analyses the deployment of high speed broadband networks in the European Union (EU). Its aim is to assess the investment required to meet the targets set by the European Commission (EC) for 2025, within the framework of the European Gigabit Society (EGS). This plan aims to ensure the availability and take up of very high capacity fixed and wireless networks, in both urban and rural areas, among households and the main socioeconomic drivers. The estimation model presented here uses a methodology supported by data at the local (NUTS3) level to give a bottom up estimation of the investment gap for each of the EGS objectives, using three different scenarios depending on the mix of wired and wireless technologies offered. The methodology and estimation model used in the paper are examined against other examples and assumptions available in the literature. We also offer a dynamic perspective on the analysis of the evolution of this investment gap over the years 2017 2019, which includes an assessment of the usefulness of these estimation models.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-field decomposed hyper-reduced order modeling of damage-plasticity simulations</title>
<link>https://arxiv.org/abs/2508.19957</link>
<guid>https://arxiv.org/abs/2508.19957</guid>
<content:encoded><![CDATA[
<div> DEIM, ECSW, multi-field decomposed approach, hyper-reduced order modeling, gradient-extended damage-plasticity simulations <br />
Summary: <br />
This paper introduces a new approach for hyper-reduced order modeling to address the limitations of traditional model reduction techniques in gradient-extended damage-plasticity simulations. The method involves extending the discrete empirical interpolation method (DEIM) and the energy-conserving sampling and weighting method (ECSW) to accommodate the multi-field nature of the problem. By applying these methods, stable reduced order simulations are achieved while significantly reducing computational costs compared to full-order simulations. Through two numerical examples, the proposed approaches' performance and limitations are demonstrated. The decomposed ECSW method proves to have higher accuracy and lower computational cost than the decomposed DEIM method, showcasing its superiority in hyper-reduced order modeling for complex simulations. <div>
arXiv:2508.19957v1 Announce Type: new 
Abstract: This paper presents a multi-field decomposed approach for hyper-reduced order modeling to overcome the limitations of traditional model reduction techniques for gradient-extended damage-plasticity simulations. The discrete empirical interpolation method (DEIM) and the energy-conserving sampling and weighting method (ECSW) are extended to account for the multi-field nature of the problem. Both methods yield stable reduced order simulations, while significantly reducing the computational cost compared to full-order simulations. Two numerical examples are presented to demonstrate the performance and limitations of the proposed approaches. The decomposed ECSW method has overall higher accuracy and lower computational cost than the decomposed DEIM method.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From stand-up to start-up: exploring entrepreneurship competences and STEM womens intention</title>
<link>https://arxiv.org/abs/2508.20091</link>
<guid>https://arxiv.org/abs/2508.20091</guid>
<content:encoded><![CDATA[
<div> STEM, entrepreneurship competencies, intention, gender differences, European Commission

Summary: 
The study examines the relationship between entrepreneurship competencies and intention among potential STEM entrepreneurs. Contrary to the assumption, there is no significant difference in entrepreneurship intention between men and women. Gender does not act as a moderating factor in the relationship between competencies and intention. The analysis, based on the Entrepreneurship Competences Framework by the European Commission, highlights a positive correlation between competencies and entrepreneurship intention. Self-perceived competences show minor variations based on gender. These findings debunk the belief that women have lower rates of entrepreneurship intention due to perceived lack of competence. The study's results provide valuable insights for entrepreneurship education and business creation initiatives. <div>
arXiv:2508.20091v1 Announce Type: new 
Abstract: This study seeks to explore the relationship between entrepreneurship competencies and intention (EI) of a sample of potential STEM entrepreneurs in order to assess the conventional assumption on women exhibiting lower rates of entrepreneurship intention than men and that the lack of competence perceived is a higher barrier to be an entrepreneur for them. The model used for the analysis takes as reference the Entrepreneurship Competences Framework (EntreComp) proposed by the European Commission (EC) as a common guide to inspire entrepreneurship education. Data gathering is based on a structured questionnaire. The conducted analysis uses Students t test means comparison and factor analysis to define the model of competences, and a multiple regression model to study the relationship between competences and skill factors in EI. Findings do not validate the hypothesis that women have fewer entrepreneurship intentions than men. Also, slight differences on the self-perceived competences are obtained by gender. In addition, the study confirms the hypothesis of a positive relationship between competences and EI, but here gender is not a moderating factor. Results are expected to contribute to the entrepreneurship competences debate and provide useful insights of application in entrepreneurship education with orientation towards the business creation.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Trade-Off: An Analysis of the Operational Breakdown and Ontological Limits of "Certainty-Scope" in AI</title>
<link>https://arxiv.org/abs/2508.19304</link>
<guid>https://arxiv.org/abs/2508.19304</guid>
<content:encoded><![CDATA[
<div> Keywords: Floridi's conjecture, artificial intelligence, certainty, scope, operationalization

Summary:<br /><br />
Floridi's conjecture on the trade-off between certainty and scope in artificial intelligence systems is discussed in this paper. The conjecture, while conceptually sound, faces challenges in practical implementation due to its reliance on incomputable constructs and its assumption of AI systems as self-contained entities. This hinders its ability to inform the design, deployment, and governance of real-world AI systems in complex human-centric domains. The paper argues that these limitations prevent the conjecture from being actionable and verifiable in real-world scenarios. The authors propose a re-framing of Floridi's epistemic challenge to address the epistemic burdens of AI within dynamic socio-technical environments, aiming to bridge the gap between theoretical insights and practical applications in AI engineering and regulation. <div>
arXiv:2508.19304v1 Announce Type: cross 
Abstract: Floridi's conjecture offers a compelling intuition about the fundamental trade-off between certainty and scope in artificial intelligence (AI) systems. This exploration remains crucial, not merely as a philosophical exercise, but as a potential compass for guiding AI investments, particularly in safety-critical industrial domains where the level of attention will surely be higher in the future. However, while intellectually coherent, its formalization ultimately freezes this insight into a suspended epistemic truth, resisting operationalization within real-world systems. This paper is a result of an analysis arguing that the conjecture's ambition to provide insights to engineering design and regulatory decision-making is constrained by two critical factors: first, its reliance on incomputable constructs - rendering it practically unactionable and unverifiable; second, its underlying ontological assumption of AI systems as self-contained epistemic entities - separating it from the intricate and dynamic socio-technical environments in which knowledge is co-constructed. We conclude that this dual breakdown - an epistemic closure deficit and an embeddedness bypass - prevents the conjecture from transitioning into a computable and actionable framework suitable for informing the design, deployment, and governance of real-world AI hybrid systems. In response, we propose a contribution to the framing of Floridi's epistemic challenge, addressing the inherent epistemic burdens of AI within complex human-centric domains.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Topology Optimization for Self-Supporting Structures in Additive Manufacturing</title>
<link>https://arxiv.org/abs/2508.19169</link>
<guid>https://arxiv.org/abs/2508.19169</guid>
<content:encoded><![CDATA[
<div> machine learning, topology optimization, self-supporting structures, additive manufacturing, stress constraints
<br />
The paper introduces a novel machine learning-based framework for optimizing the topology of self-supporting structures for additive manufacturing. The framework utilizes a graph neural network (GNN) to predict material distributions over a finite element mesh, ensuring printability through an integrated AM filter. By minimizing structural compliance under volume and stress constraints, the framework generates stress-constrained manufacturable designs in various loading conditions. The stress constraint is enforced using a differentiable p-norm aggregation of von Mises stress to enhance mechanical reliability. The approach features a fully differentiable architecture, eliminating the need for explicit sensitivity derivation in the optimization loop. Numerical experiments demonstrate the efficacy of the framework in producing high-performance designs suitable for additive manufacturing with reduced post-processing requirements.
<br /><br />Summary: <div>
arXiv:2508.19169v1 Announce Type: new 
Abstract: This paper presents a machine learning-based framework for topology optimization of self-supporting structures, specifically tailored for additive manufacturing (AM). By employing a graph neural network (GNN) that acts as a neural field over the finite element mesh, the framework effectively learns and predicts continuous material distributions. An integrated AM filter ensures printability by eliminating unsupported overhangs, while the optimization process minimizes structural compliance under volume and stress constraints. The stress constraint is enforced using a differentiable p-norm aggregation of von Mises stress, promoting mechanical reliability in the optimized designs. A key advantage of the approach lies in its fully differentiable architecture, which leverages automatic differentiation throughout the optimization loop--eliminating the need for explicit sensitivity derivation for both the filter and the stress constraint. Numerical experiments demonstrate the ability of the framework to generate stress-constrained manufacturable topologies under various loading and boundary conditions, offering a practical pathway toward AM-ready high-performance designs with reduced post-processing requirements.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance</title>
<link>https://arxiv.org/abs/2508.19138</link>
<guid>https://arxiv.org/abs/2508.19138</guid>
<content:encoded><![CDATA[
<div> nanoscale electronic devices, nanoribbon field-effect transistors, NEGF, DFT, electron-electron interactions<br />
<br />
Summary:<br />
Designing nanoscale electronic devices like nanoribbon field-effect transistors (NRFETs) requires advanced quantum mechanical modeling tools. Current approaches combine NEGF and DFT, but with ultra-small device dimensions, electron-electron interactions become crucial. The NEGF+GW scheme presented here extends existing solvers to handle NRFET geometries with dimensions comparable to experiments. The QuaTrEx package utilizes a novel domain decomposition scheme, can handle devices with up to 84,480 atoms, and scales efficiently on supercomputers like Alps and Frontier. It achieves exascale FP64 performance on 42,240 atoms, reaching 1.15 Eflop/s. <div>
arXiv:2508.19138v1 Announce Type: cross 
Abstract: Designing nanoscale electronic devices such as the currently manufactured nanoribbon field-effect transistors (NRFETs) requires advanced modeling tools capturing all relevant quantum mechanical effects. State-of-the-art approaches combine the non-equilibrium Green's function (NEGF) formalism and density functional theory (DFT). However, as device dimensions do not exceed a few nanometers anymore, electrons are confined in ultra-small volumes, giving rise to strong electron-electron interactions. To account for these critical effects, DFT+NEGF solvers should be extended with the GW approximation, which massively increases their computational intensity. Here, we present the first implementation of the NEGF+GW scheme capable of handling NRFET geometries with dimensions comparable to experiments. This package, called QuaTrEx, makes use of a novel spatial domain decomposition scheme, can treat devices made of up to 84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80% weak scaling efficiency), and sustains an exascale FP64 performance on 42,240 atoms (1.15 Eflop/s).
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOFLUX: A Differentiable Topology Optimization Framework for Multiphysics Fluidic Problems</title>
<link>https://arxiv.org/abs/2508.17564</link>
<guid>https://arxiv.org/abs/2508.17564</guid>
<content:encoded><![CDATA[
<div> Topology Optimization, fluidic devices, automatic differentiation, JAX library, TOFLUX

Summary:
TOFLUX is a new framework for fluid devices that utilizes automatic differentiation for efficient design optimization. The complexity of fluid-based systems, with multiphysics nonlinear interactions, often hinders researchers, but TOFLUX aims to simplify the process. By using the JAX library, the framework enables rapid exploration of various objectives and constraints, even in challenging scenarios like thermo-fluidic coupling and fluid-structure interaction. The integration with neural networks and machine learning enhances scientific computing capabilities. TOFLUX provides a foundational resource to accelerate research and innovation in fluid-based Topology Optimization. The accompanying software can be accessed on GitHub at github.com/UW-ERSL/TOFLUX. <br /><br />Summary: <div>
arXiv:2508.17564v1 Announce Type: new 
Abstract: Topology Optimization (TO) holds the promise of designing next-generation compact and efficient fluidic devices. However, the inherent complexity of fluid-based TO systems, characterized by multiphysics nonlinear interactions, poses substantial barriers to entry for researchers.
  Beyond the inherent intricacies of forward simulation models, design optimization is further complicated by the difficulty of computing sensitivities, i.e., gradients. Manual derivation and implementation of sensitivities are often laborious and prone to errors, particularly for non-trivial objectives, constraints, and material models. An alternative solution is automatic differentiation (AD). Although AD has been previously demonstrated for simpler TO problems, extending its use to complex nonlinear multiphysics systems, specifically in fluidic optimization, is key to reducing the entry barrier.
  To this end, we introduce TOFLUX, a TO framework for fluid devices leveraging the JAX library for high-performance automatic differentiation. The flexibility afforded by AD enables the rapid exploration and evaluation of various objectives and constraints. We illustrate this capability through challenging examples encompassing thermo-fluidic coupling, fluid-structure interaction, and non-Newtonian flows. Additionally, we demonstrate the seamless integration of our framework with neural networks and machine learning methodologies, enabling modern approaches to scientific computing. Ultimately, the framework aims to provide a foundational resource to accelerate research and innovation in fluid-based TO. The software accompanying this educational paper can be accessed at github.com/UW-ERSL/TOFLUX.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing the exploration-exploitation trade-off in active learning for surrogate model-based reliability analysis via multi-objective optimization</title>
<link>https://arxiv.org/abs/2508.18170</link>
<guid>https://arxiv.org/abs/2508.18170</guid>
<content:encoded><![CDATA[
<div> active learning, reliability assessment, surrogate model, multi-objective optimization, sample acquisition

Summary: 
The article introduces a new approach for reliability assessment of engineering systems by using active learning to iteratively refine a surrogate model. This approach aims to reduce the number of expensive simulations by balancing exploration and exploitation through a multi-objective optimization (MOO) formulation. Traditional strategies like U and Expected Feasibility Function (EFF) are compared with the MOO approach, which explicitly considers the trade-off between exploration and exploitation. The MOO framework provides a unifying perspective and allows for the selection of samples based on a quantifiable exploration-exploitation trade-off. Different sample selection strategies, such as knee point and compromise solution, are evaluated across benchmark limit-state functions. Results show that the MOO approach is generally effective, with an adaptive strategy maintaining high reliability estimates and low relative errors. <div>
arXiv:2508.18170v1 Announce Type: new 
Abstract: Reliability assessment of engineering systems is often hindered by the need to evaluate limit-state functions through computationally expensive simulations, rendering standard sampling impractical. An effective solution is to approximate the limit-state function with a surrogate model iteratively refined through active learning, thereby reducing the number of expensive simulations. At each iteration, an acquisition strategy selects the next sample by balancing two competing goals: exploration, to reduce global predictive uncertainty, and exploitation, to improve accuracy near the failure boundary. Classical strategies, such as the U-function and the Expected Feasibility Function (EFF), implicitly condense exploration and exploitation into a scalar score derived from the surrogate predictive mean and variance, concealing the trade-off and biasing sampling. We introduce a multi-objective optimization (MOO) formulation for sample acquisition in reliability analysis, where exploration and exploitation are explicit, competing objectives. Within our framework, U and EFF correspond to specific Pareto-optimal solutions, providing a unifying perspective that connects classical and Pareto-based approaches. Solving the MOO problem discards dominated candidates, yielding a compact Pareto set, with samples representing a quantifiable exploration-exploitation trade-off. To select samples from the Pareto set, we adopt the knee point and the compromise solution, and further propose a strategy that adjusts the trade-off according to reliability estimates. Across benchmark limit-state functions, we assess the sample efficiency and active learning performance of all strategies. Results show that U and EFF exhibit case-dependent performance, knee and compromise are generally effective, and the adaptive strategy is robust, consistently reaching strict targets and maintaining relative errors below 0.1%.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience</title>
<link>https://arxiv.org/abs/2508.16836</link>
<guid>https://arxiv.org/abs/2508.16836</guid>
<content:encoded><![CDATA[
<div> Neural symbolic approach, complex networks, resilience prediction, physical information method, industrial chain<br />
<br />
Summary: 
This paper introduces a novel physically informative neural symbolic approach for predicting the resilience of complex networks, focusing on industrial chains. The approach integrates physical entity dynamics with spatiotemporal network evolution to enhance predictive accuracy. By jointly learning physical symbol dynamics and network topology, the model demonstrates superior prediction capabilities for industrial chain resilience. The experimental results showcase the effectiveness of the proposed approach in accurately and effectively predicting the elasticity of industrial chains. This advancement is crucial for sustainable development and has significant implications for the industry. <div>
arXiv:2508.16836v1 Announce Type: cross 
Abstract: Industrial chain plays an increasingly important role in the sustainable development of national economy. However, as a typical complex network, data-driven deep learning is still in its infancy in describing and analyzing the resilience of complex networks, and its core is the lack of a theoretical framework to describe the system dynamics. In this paper, we propose a physically informative neural symbolic approach to describe the evolutionary dynamics of complex networks for resilient prediction. The core idea is to learn the dynamics of the activity state of physical entities and integrate it into the multi-layer spatiotemporal co-evolution network, and use the physical information method to realize the joint learning of physical symbol dynamics and spatiotemporal co-evolution topology, so as to predict the industrial chain resilience. The experimental results show that the model can obtain better results and predict the elasticity of the industry chain more accurately and effectively, which has certain practical significance for the development of the industry.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.17086</link>
<guid>https://arxiv.org/abs/2508.17086</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial markets, Trade-based manipulation, Spoofing, Limit Order Book, Anomaly detection 

Summary: 
Financial markets are essential for global economic stability but are often undermined by trade-based manipulation (TBM), including deceptive strategies like spoofing. Detecting these anomalies in the rich information of the Limit Order Book (LOB) is challenging due to its high dimensionality and noise. To address this, a representation learning framework combining a cascaded LOB representation pipeline with supervised contrastive learning is proposed. Extensive experiments show improved detection performance across various models, with Transformer-based architectures achieving state-of-the-art results. Systematic analyses and ablation studies are conducted to investigate multilevel anomalies and the contributions of key components, providing insights into representation learning and anomaly detection for complex sequential data. The code for the framework will be released later at the provided URL. 

<br /><br />Summary: <div>
arXiv:2508.17086v1 Announce Type: cross 
Abstract: Financial markets are critical to global economic stability, yet trade-based manipulation (TBM) often undermines their fairness. Spoofing, a particularly deceptive TBM strategy, exhibits multilevel anomaly patterns that have not been adequately modeled. These patterns are usually concealed within the rich, hierarchical information of the Limit Order Book (LOB), which is challenging to leverage due to high dimensionality and noise. To address this, we propose a representation learning framework combining a cascaded LOB representation pipeline with supervised contrastive learning. Extensive experiments demonstrate that our framework consistently improves detection performance across diverse models, with Transformer-based architectures achieving state-of-the-art results. In addition, we conduct systematic analyses and ablation studies to investigate multilevel anomalies and the contributions of key components, offering broader insights into representation learning and anomaly detection for complex sequential data. Our code will be released later at this URL.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Easy Acceleration with Distributed Arrays</title>
<link>https://arxiv.org/abs/2508.17493</link>
<guid>https://arxiv.org/abs/2508.17493</guid>
<content:encoded><![CDATA[
<div> Keywords: high level programming languages, GPU accelerators, distributed arrays, memory bandwidth, scalability<br />
<br />
Summary: <br />
High level programming languages and GPU accelerators play a crucial role in enabling a wide range of applications. To achieve scalable vertical, horizontal, and temporal performance, effective abstractions are needed. Distributed arrays serve as one such abstraction, enabling high level programming to achieve highly scalable performance by deriving parallelism from data locality. Using the STREAM memory bandwidth benchmark on various hardware, this paper demonstrates scalable performance within and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across multiple nodes showed linear performance. The study also compared hardware improvements for memory bandwidth over decades, showing significant increases in CPU core, CPU node, and GPU node bandwidth. Finally, running on hundreds of MIT SuperCloud nodes simultaneously achieved a sustained bandwidth of over 1 PB/s. <br /><br />Summary: <div>
arXiv:2508.17493v1 Announce Type: cross 
Abstract: High level programming languages and GPU accelerators are powerful enablers for a wide range of applications. Achieving scalable vertical (within a compute node), horizontal (across compute nodes), and temporal (over different generations of hardware) performance while retaining productivity requires effective abstractions. Distributed arrays are one such abstraction that enables high level programming to achieve highly scalable performance. Distributed arrays achieve this performance by deriving parallelism from data locality, which naturally leads to high memory bandwidth efficiency. This paper explores distributed array performance using the STREAM memory bandwidth benchmark on a variety of hardware. Scalable performance is demonstrated within and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across multiple nodes was linear. The hardware used spans decades and allows a direct comparison of hardware improvements for memory bandwidth over this time range; showing a 10x increase in CPU core bandwidth over 20 years, 100x increase in CPU node bandwidth over 20 years, and 5x increase in GPU node bandwidth over 5 years. Running on hundreds of MIT SuperCloud nodes simultaneously achieved a sustained bandwidth $>$1 PB/s.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boltzina: Efficient and Accurate Virtual Screening via Docking-Guided Binding Prediction with Boltz-2</title>
<link>https://arxiv.org/abs/2508.17555</link>
<guid>https://arxiv.org/abs/2508.17555</guid>
<content:encoded><![CDATA[
<div> high-accuracy, computational efficiency, virtual screening, drug discovery, molecular docking<br />
Summary:
Boltzina is introduced as a novel framework in structure-based drug discovery to enhance the computational efficiency of high-accuracy binding affinity prediction. By omitting the structure prediction step and directly predicting affinity from AutoDock Vina docking poses, Boltzina achieves improved screening performance compared to traditional methods like AutoDock Vina and GNINA. While Boltzina falls slightly below Boltz-2 in accuracy, it offers significant speed enhancements, up to 11.8 times faster, through optimized iterations and batch processing. The study explores multi-pose selection strategies and proposes a two-stage screening approach combining Boltzina and Boltz-2 for increased accuracy and efficiency tailored to specific application requirements. This work marks the first successful integration of Boltz-2's accurate predictions into practical-scale screening, providing a comprehensive pipeline that balances accuracy and efficiency in computational biology.<br /><br /> <div>
arXiv:2508.17555v1 Announce Type: cross 
Abstract: In structure-based drug discovery, virtual screening using conventional molecular docking methods can be performed rapidly but suffers from limitations in prediction accuracy. Recently, Boltz-2 was proposed, achieving extremely high accuracy in binding affinity prediction, but requiring approximately 20 seconds per compound per GPU, making it difficult to apply to large-scale screening of hundreds of thousands to millions of compounds. This study proposes Boltzina, a novel framework that leverages Boltz-2's high accuracy while significantly improving computational efficiency. Boltzina achieves both accuracy and speed by omitting the rate-limiting structure prediction from Boltz-2's architecture and directly predicting affinity from AutoDock Vina docking poses. We evaluate on eight assays from the MF-PCBA dataset and show that while Boltzina performs below Boltz-2, it provides significantly higher screening performance compared to AutoDock Vina and GNINA. Additionally, Boltzina achieved up to 11.8$\times$ faster through reduced recycling iterations and batch processing. Furthermore, we investigated multi-pose selection strategies and two-stage screening combining Boltzina and Boltz-2, presenting optimization methods for accuracy and efficiency according to application requirements. This study represents the first attempt to apply Boltz-2's high-accuracy predictions to practical-scale screening, offering a pipeline that combines both accuracy and efficiency in computational biology. The Boltzina is available on github; https://github.com/ohuelab/boltzina.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation</title>
<link>https://arxiv.org/abs/2508.17568</link>
<guid>https://arxiv.org/abs/2508.17568</guid>
<content:encoded><![CDATA[
<div> Keywords: Metamaterials, MetaDSL, MetaDB, MetaBench, structure-representation-property relationships 

Summary: 
MetaDSL is introduced as a domain-specific language for capturing diverse metamaterial designs in a human-readable and machine-parsable form. MetaDB serves as a repository with a vast collection of parameterized MetaDSL programs and their derivatives, providing detailed information on geometry, renderings, and elastic properties. MetaBench offers benchmark suites for testing vision-language metamaterial assistants' core capabilities like structure reconstruction, inverse design driven by properties, and performance prediction. The study establishes baselines by fine-tuning advanced vision-language models and deploying an omni-model within an interactive CAD-like interface. Through case studies, the framework demonstrates a significant advancement in integrated design and comprehension of structure-representation-property relationships. 

<br /><br />Summary: <div>
arXiv:2508.17568v1 Announce Type: cross 
Abstract: Metamaterials are micro-architected structures whose geometry imparts highly tunable-often counter-intuitive-bulk properties. Yet their design is difficult because of geometric complexity and a non-trivial mapping from architecture to behaviour. We address these challenges with three complementary contributions. (i) MetaDSL: a compact, semantically rich domain-specific language that captures diverse metamaterial designs in a form that is both human-readable and machine-parsable. (ii) MetaDB: a curated repository of more than 150,000 parameterized MetaDSL programs together with their derivatives-three-dimensional geometry, multi-view renderings, and simulated elastic properties. (iii) MetaBench: benchmark suites that test three core capabilities of vision-language metamaterial assistants-structure reconstruction, property-driven inverse design, and performance prediction. We establish baselines by fine-tuning state-of-the-art vision-language models and deploy an omni-model within an interactive, CAD-like interface. Case studies show that our framework provides a strong first step toward integrated design and understanding of structure-representation-property relationships.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral-Prior Guided Multistage Physics-Informed Neural Networks for Highly Accurate PDE Solutions</title>
<link>https://arxiv.org/abs/2508.17902</link>
<guid>https://arxiv.org/abs/2508.17902</guid>
<content:encoded><![CDATA[
<div> PINNs, physics-informed neural networks, spectral prior, multistage strategy, accuracy improvement <br />
Summary:<br /> 
- This paper introduces two methods, SI-MSPINNs and RFF-MSPINNs, to enhance the accuracy of Physics-Informed Neural Networks (PINNs) by incorporating spectral information.
- SI-MSPINNs extract dominant spectral patterns to guide network initialization and use a multistage strategy to optimize resolution accuracy.
- RFF-MSPINNs combine random Fourier features with spectral weighting to prioritize learning high-energy physical modes based on residual power spectral density.
- Experimental verification on the Burgers equation and Helmholtz equation demonstrates significant accuracy improvements compared to traditional PINNs. 
- The proposed methods offer a practical solution to enhance the accuracy and performance of PINNs for solving high-dimensional problems efficiently. 
Summary: <div>
arXiv:2508.17902v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINNs) are becoming a popular method for solving PDEs, due to their mesh-free nature and their ability to handle high-dimensional problems where traditional numerical solvers often struggle. Despite their promise, the practical application of PINNs is still constrained by several fac- tors, a primary one being their often-limited accuracy. This paper is dedicated to enhancing the accuracy of PINNs by introducing spectral-prior guided multistage strategy. We propose two methods: Spectrum- Informed Multistage Physics-Informed Neural Networks (SI-MSPINNs) and Multistage Physics-Informed Neural Networks with Spectrum Weighted Random Fourier Features (RFF-MSPINNs). The SI-MSPINNs integrate the core mechanism of Spectrum-Informed Multistage Neural Network (SI-MSNNs) and PINNs, in which we extract the Dominant Spectral Pattern (DSP) of residuals by the discrete Fourier transform. This DSP guides the network initialization to alleviate spectral bias, and gradually optimizes the resolution accuracy using a multistage strategy. The RFF-MSPINNs combines random Fourier features with spectral weighting methods, dynamically adjusting the frequency sampling distribution based on the residual power spectral density, allowing the network to prioritize learning high-energy physical modes. Through experimental verification of the Burgers equation and the Helmholtz equation, we show that both models significantly improve the accuracy of the original PINNs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermodynamically Consistent Hybrid and Permutation-Invariant Neural Yield Functions for Anisotropic Plasticity</title>
<link>https://arxiv.org/abs/2508.15923</link>
<guid>https://arxiv.org/abs/2508.15923</guid>
<content:encoded><![CDATA[
<div> architecturally-constrained neural networks, plastic anisotropy, yield criteria, anisotropic yield function, data-driven frameworks
<br />
Summary:
The study addresses the challenge of modeling plastic anisotropy in metals by utilizing architecturally-constrained neural networks. Two data-driven frameworks are developed: one that combines the Hill yield criterion with Input Convex Neural Networks for anisotropic yield function representation, and another that uses a permutation-invariant input convex neural network to embed anisotropy through linear stress transformations. Calibration on an Al-7079 extrusion experimental dataset shows that the permutation-invariant input convex neural network frameworks outperform existing methods in terms of generalization capabilities. These frameworks accurately predict yield loci and Lankford ratios with minimal data, demonstrating the potential for rapid and thermodynamically consistent constitutive models for advanced forming simulations and microstructure-informed design in the future. 
<br /> <div>
arXiv:2508.15923v1 Announce Type: new 
Abstract: Plastic anisotropy in metals remains challenging to model. This is partly because conventional phenomenological yield criteria struggle to combine a highly descriptive, flexible representation with constraints, such as convexity, dictated by thermodynamic consistency. To address this gap, we employ architecturally-constrained neural networks and develop two data-driven frameworks: (i) a hybrid model that augments the Hill yield criterion with an Input Convex Neural Network (ICNN) to get an anisotropic yield function representation in the six-dimensional stress space and (ii) a permutation-invariant input convex neural network (PI-ICNN) that learns an isotropic yield function representation in the principal stress space and embeds anisotropy through linear stress transformations. We calibrate the proposed frameworks on a sparse Al-7079 extrusion experimental dataset comprising 12 uniaxial samples with measured yield stresses and Lankford ratios. To test the robustness of each framework, nine datasets were generated using k-fold cross-validation. These datasets were then used to quantitatively compare Hill-48, Yld2004-18p, pure ICNNs, the hybrid approach, and the PI-ICNN frameworks. While ICNNs and hybrid approaches can almost perfectly fit the training data, they exhibit significant over-fitting, resulting in high validation and test losses. In contrast, both PI-ICNN frameworks demonstrate better generalization capabilities, even outperforming Yld2004-18p on the validation and test data. These results demonstrate that PI-ICNNs unify physics-based constraints with the flexibility of neural networks, enabling the accurate prediction of both yield loci and Lankford ratios from minimal data. The approach opens a path toward rapid, thermodynamically consistent constitutive models for advanced forming simulations and future exploration of coupled hardening or microstructure-informed design.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise, Adaptation, and Strategy: Assessing LLM Fidelity in Decision-Making</title>
<link>https://arxiv.org/abs/2508.15926</link>
<guid>https://arxiv.org/abs/2508.15926</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, social science simulations, decision-making, variability, adaptability <br />
Summary: 
Large language models (LLMs) are being used in social science simulations, but their ability to simulate human decision-making variability and adaptability is not well understood. A new evaluation framework with progressive interventions was proposed to examine LLM agents' adaptability under different levels of external guidance and human-derived noise. The framework was validated on two classic economics tasks, highlighting behavioral gaps between LLMs and humans. By default, LLMs tend to converge on stable and conservative strategies that differ from human behaviors. Risk-framed instructions influence LLM behavior but do not capture human-like diversity. Incorporating human data through in-context learning helps narrow the gap but still falls short of replicating human subjects' strategic variability. These results underscore the need for more realistic evaluations of LLMs in dynamic decision-making tasks, providing guidance for their application in synthetic data for social science research. <br /><br /> <div>
arXiv:2508.15926v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in social science simulations. While their performance on reasoning and optimization tasks has been extensively evaluated, less attention has been paid to their ability to simulate human decision-making's variability and adaptability. We propose a process-oriented evaluation framework with progressive interventions (Intrinsicality, Instruction, and Imitation) to examine how LLM agents adapt under different levels of external guidance and human-derived noise. We validate the framework on two classic economics tasks, irrationality in the second-price auction and decision bias in the newsvendor problem, showing behavioral gaps between LLMs and humans.
  We find that LLMs, by default, converge on stable and conservative strategies that diverge from observed human behaviors. Risk-framed instructions impact LLM behavior predictably but do not replicate human-like diversity. Incorporating human data through in-context learning narrows the gap but fails to reach human subjects' strategic variability. These results highlight a persistent alignment gap in behavioral fidelity and suggest that future LLM evaluations should consider more process-level realism. We present a process-oriented approach for assessing LLMs in dynamic decision-making tasks, offering guidance for their application in synthetic data for social science research.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUEENS: An Open-Source Python Framework for Solver-Independent Analyses of Large-Scale Computational Models</title>
<link>https://arxiv.org/abs/2508.16316</link>
<guid>https://arxiv.org/abs/2508.16316</guid>
<content:encoded><![CDATA[
<div> Keywords: QUEENS, uncertainty quantification, simulation management, distributed computing, Bayesian analysis

Summary:
QUEENS is a Python framework designed to facilitate the analysis of large-scale computational models, specifically patient-specific digital twins of diseased human organs. It aims to streamline simulation management with arbitrary solvers on distributed systems, offering a range of state-of-the-art algorithms for convergence studies, optimization, uncertainty quantification, and Bayesian inverse analysis. The framework supports both deterministic and probabilistic analysis, featuring multi-fidelity uncertainty quantification and Bayesian analysis. With a modular architecture, QUEENS allows researchers to easily switch between different types of analyses and build sophisticated algorithms. The open-source repository for QUEENS is available on GitHub, providing researchers with access to cutting-edge research in probabilistic machine learning and efficient analysis methods. <div>
arXiv:2508.16316v1 Announce Type: new 
Abstract: A growing challenge in research and industrial engineering applications is the need for repeated, systematic analysis of large-scale computational models, for example, patient-specific digital twins of diseased human organs: The analysis requires efficient implementation, data, resource management, and parallelization, possibly on distributed systems. To tackle these challenges and save many researchers from annoying, time-consuming tasks, we present QUEENS (Quantification of Uncertain Effects in Engineering Systems), an open-source Python framework for composing and managing simulation analyses with arbitrary (physics-based) solvers on distributed computing infrastructures. Besides simulation management capabilities, QUEENS offers a comprehensive collection of efficiently implemented state-of-the-art algorithms ranging from routines for convergence studies and common optimization algorithms to more advanced sampling algorithms for uncertainty quantification and Bayesian inverse analysis. Additionally, we provide our latest cutting-edge research in multi-fidelity uncertainty quantification, efficient multi-fidelity Bayesian inverse analysis, and probabilistic machine learning. QUEENS adopts a Bayesian, probabilistic mindset but equally supports standard deterministic analysis without requiring prior knowledge of probability theory. The modular architecture allows rapid switching between common types of analyses and facilitates building sophisticated hierarchical algorithms. Encouraging natural incremental steps and scaling towards complexity allows researchers to consider the big picture while building towards it through smaller, manageable steps. The open-source repository is available at https://github.com/queens-py/queens.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Linear to Hierarchical: Evolving Tree-structured Thoughts for Efficient Alpha Mining</title>
<link>https://arxiv.org/abs/2508.16334</link>
<guid>https://arxiv.org/abs/2508.16334</guid>
<content:encoded><![CDATA[
<div> Alpha mining, Large Language Models, Tree-structured thought Evolution, hierarchical reasoning, automatic quantitative investment <br />
Summary: 

This paper introduces Tree-structured thought Evolution (TreEvo) as a solution to alpha mining using Large Language Models (LLMs). The goal is to automatically discover signals that predict asset returns without depending on handcrafted features or arithmetic operators. TreEvo evolves hierarchical reasoning ideas solely at the thought level, addressing the hierarchical tree structures of alphas. Experiments on real-market datasets show that TreEvo can generate better alphas in less computational time and with fewer expert efforts compared to traditional methods. The tree-structured thoughts and compatible evolutionary operators play a crucial role in achieving this improvement, demonstrating the importance of considering hierarchical structures in alpha mining. <br /><br />Summary: <div>
arXiv:2508.16334v1 Announce Type: new 
Abstract: Alpha mining, which discovers signals that predict asset returns, has long been attractive for automatic quantitative investment. This problem is typically formulated as a tree-based symbolic regression with handcrafted market data features and arithmetic operators. Unfortunately, existing symbolic methods are concerned with computational inefficiency and dependence on prior knowledge. Recent implementation of Large Language Models (LLMs) show that they can automatically generate executable codes for various tasks efficiently, thus can be considered as a new promising way for alpha mining. Specifically, LLMs-driven methods evolve a set of heuristics, including thoughts and codes, where the thoughts are usually represented as plain-text prompts of codes. Unfortunately, trivially adopting them in alpha mining ignores the fact that alphas are with hierarchical tree structures. This paper introduces Tree-structured thought Evolution (TreEvo), which evolves hierarchical reasoning ideas solely at the thought level. Experiments on four real-market datasets demonstrate that TreEvo can obtain better alphas with much less computational time and human expert efforts. And this superiority hardly holds without the tree-structured thoughts and the compatible evolutionary operators.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-optimized replacement strategies for water electrolysis systems affected by degradation</title>
<link>https://arxiv.org/abs/2508.16370</link>
<guid>https://arxiv.org/abs/2508.16370</guid>
<content:encoded><![CDATA[
<div> renewable hydrogen, electrolyzer stacks, degradation modeling, levelized cost, optimization<br />
<br />
Summary:<br />
A study was conducted to analyze the economics of green hydrogen production using water electrolysis systems. The focus was on minimizing the degradation of electrolyzer stacks to reduce project costs and increase stack lifetime. A linear optimization approach was used to calculate the levelized cost of hydrogen based on varying degradation thresholds, determining the optimal time for stack replacement. The study considered uncertainties such as degradation scale, load-dependency of degradation and energy demand, and electrolyzer costs. The findings showed that the optimal time for stack replacement could differ by up to 9 years depending on degradation scale. Understanding the impact of degradation is crucial for reducing project costs and supporting the growth of the hydrogen market. <div>
arXiv:2508.16370v1 Announce Type: new 
Abstract: A key factor in reducing the cost of green hydrogen production projects using water electrolysis systems is to minimize the degradation of the electrolyzer stacks, as this impacts the lifetime of the stacks and therefore the frequency of their replacement. To create a better understanding of the economics of stack degradation, we present a linear optimization approach minimizing the costs of a green hydrogen supply chain including an electrolyzer with degradation modeling. By calculating the levelized cost of hydrogen depending on a variable degradation threshold, the cost optimal time for stack replacement can be identified. We further study how this optimal time of replacement is affected by uncertainties such as the degradation scale, the load-dependency of both degradation and energy demand, and the costs of the electrolyzer. The variation of the identified major uncertainty degradation scale results in a difference of up to 9 years regarding the cost optimal time for stack replacement, respectively lifetime of the stacks. Therefore, a better understanding of the degradation impact is imperative for project cost reductions, which in turn would support a proceeding hydrogen market ramp-up.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment-Aware Mean-Variance Portfolio Optimization for Cryptocurrencies</title>
<link>https://arxiv.org/abs/2508.16378</link>
<guid>https://arxiv.org/abs/2508.16378</guid>
<content:encoded><![CDATA[
<div> Keywords: cryptocurrency, portfolio optimization, technical indicators, sentiment analysis, investment decision-making

Summary:
This paper introduces a dynamic cryptocurrency portfolio optimization strategy that combines technical indicators and sentiment analysis for better investment decision-making. The method uses the RSI and SMA to capture market momentum and sentiment scores from news articles with the VADER model. Google Gemini is employed to verify sentiment scores and make investment decisions. These signals are integrated into expected return estimates for mean-variance optimization with asset weight constraints. The strategy is tested through a rolling-window backtest on cryptocurrency market data, outperforming benchmarks of Bitcoin and an equal-weighted portfolio in cumulative return and Sharpe ratio. However, it also shows higher short-term downside risk. The results demonstrate the potential of integrating sentiment and technical signals to enhance cryptocurrency portfolio performance while emphasizing the importance of managing risk exposure in volatile markets.<br /><br />Summary: <div>
arXiv:2508.16378v1 Announce Type: new 
Abstract: This paper presents a dynamic cryptocurrency portfolio optimization strategy that integrates technical indicators and sentiment analysis to enhance investment decision-making. The proposed method employs the 14-day Relative Strength Index (RSI) and 14-day Simple Moving Average (SMA) to capture market momentum, while sentiment scores are extracted from news articles using the VADER (Valence Aware Dictionary and sEntiment Reasoner) model, with compound scores quantifying overall market tone. The large language model Google Gemini is used to further verify the sentiment scores predicted by VADER and give investment decisions. These technical indicator and sentiment signals are incorporated into the expected return estimates before applying mean-variance optimization with constraints on asset weights. The strategy is evaluated through a rolling-window backtest over cryptocurrency market data, with Bitcoin (BTC) and an equal-weighted portfolio of selected cryptocurrencies serving as benchmarks. Experimental results show that the proposed approach achieves a cumulative return of 38.72, substantially exceeding Bitcoin's 8.85 and the equal-weighted portfolio's 21.65 over the same period, and delivers a higher Sharpe ratio (1.1093 vs. 0.8853 and 1.0194, respectively). However, the strategy exhibits a larger maximum drawdown (-18.52%) compared to Bitcoin (-4.48%) and the equal-weighted portfolio (-11.02%), indicating higher short-term downside risk. These results highlight the potential of combining sentiment and technical signals to improve cryptocurrency portfolio performance, while also emphasizing the need to address risk exposure in volatile markets.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain</title>
<link>https://arxiv.org/abs/2508.15776</link>
<guid>https://arxiv.org/abs/2508.15776</guid>
<content:encoded><![CDATA[
<div> Keywords: pharmaceutical supply chain, cybersecurity, zero trust architecture, data protection, resilience

Summary:
The pharmaceutical supply chain is facing increasing cybersecurity challenges that threaten patient safety and operational continuity. This paper explores the potential of zero trust architecture in enhancing security and resilience in this critical ecosystem. By implementing principles such as continuous verification, least-privilege access, and data-centric security, organizations can strengthen security measures and protect sensitive data. Real-world case studies demonstrate the successful implementation of zero trust in pharmaceutical supply chains. One crucial area where zero trust can be effectively applied is in managing narcotics and high-health-risk drugs to ensure drug safety throughout the production process. By adopting zero trust principles, the pharmaceutical industry can safeguard its supply chain from evolving cyber threats, guaranteeing the reliability of critical medical operations.<br /><br />Summary: <div>
arXiv:2508.15776v1 Announce Type: cross 
Abstract: The pharmaceutical supply chain faces escalating cybersecurity challenges threatening patient safety and operational continuity. This paper examines the transformative potential of zero trust architecture for enhancing security and resilience within this critical ecosystem. We explore the challenges posed by data breaches, counterfeiting, and disruptions and introduce the principles of continuous verification, least-privilege access, and data-centric security inherent in zero trust. Real-world case studies illustrate successful implementations. Benefits include heightened security, data protection, and adaptable resilience. As recognized by researchers and industrialists, a reliable drug tracing system is crucial for ensuring drug safety throughout the pharmaceutical production process. One of the most pivotal domains within the pharmaceutical industry and its associated supply chains where zero trust can be effectively implemented is in the management of narcotics, high-health-risk drugs, and abusable substances. By embracing zero trust, the pharmaceutical industry fortifies its supply chain against constantly changing cyber threats, ensuring the trustworthiness of critical medical operations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms</title>
<link>https://arxiv.org/abs/2508.15963</link>
<guid>https://arxiv.org/abs/2508.15963</guid>
<content:encoded><![CDATA[
<div> Measurement System, Wheel Flange Wear, Machine Learning Algorithm, Infinite Impulse Response Filter, Rail Safety<br />
Summary:<br /> 
This paper presents an innovative onboard measurement system for monitoring wheel flange wear depth in railway systems. The system uses displacement and temperature sensors to accurately measure wear depth and surrounding temperature fluctuations. Machine learning algorithms based on regression models are trained dynamically using collected data, achieving an accuracy of 96.5%. An infinite impulse response filter (IIR) is designed to mitigate vehicle dynamics and sensor noise, further enhancing accuracy to 98.2%. The system also integrates with Internet of Things devices for real-time monitoring of wheel flange wear and track conditions. Overall, this advanced monitoring system ensures increased safety and efficiency in railway operations. <div>
arXiv:2508.15963v1 Announce Type: cross 
Abstract: Rail and wheel interaction functionality is pivotal to the railway system safety, requiring accurate measurement systems for optimal safety monitoring operation. This paper introduces an innovative onboard measurement system for monitoring wheel flange wear depth, utilizing displacement and temperature sensors. Laboratory experiments are conducted to emulate wheel flange wear depth and surrounding temperature fluctuations in different periods of time. Employing collected data, the training of machine learning algorithms that are based on regression models, is dynamically automated. Further experimentation results, using standards procedures, validate the system's efficacy. To enhance accuracy, an infinite impulse response filter (IIR) that mitigates vehicle dynamics and sensor noise is designed. Filter parameters were computed based on specifications derived from a Fast Fourier Transform analysis of locomotive simulations and emulation experiments data. The results show that the dynamic machine learning algorithm effectively counter sensor nonlinear response to temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime. The real-time noise reduction via IIR filter enhances the accuracy up to 98.2 %. Integrated with railway communication embedded systems such as Internet of Things devices, this advanced monitoring system offers unparalleled real-time insights into wheel flange wear and track irregular conditions that cause it, ensuring heightened safety and efficiency in railway systems operations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Scattering Matrix Synthesis: Independent Region Decomposition for Hybrid Antenna--Scatterer Systems</title>
<link>https://arxiv.org/abs/2503.17616</link>
<guid>https://arxiv.org/abs/2503.17616</guid>
<content:encoded><![CDATA[
<div> Keywords: generalized scattering matrix, hybrid electromagnetic systems, vector spherical wavefunctions, modular region decomposition, efficient analysis <br />
Summary: 
This paper introduces a unified formulation for synthesizing the generalized scattering matrix (GS-matrix) of hybrid electromagnetic systems consisting of various antennas and scatterers. The method utilizes a modular region decomposition framework to analyze electromagnetic interactions between separate structures, assuming they are separable by a plane. By using the addition theorem of vector spherical wavefunctions (VSWFs), a compact matrix representation is created to combine the GS- and S-matrices of individual components for the overall system response. This approach extends previous methods for multiple scattering or antenna array analysis, making it suitable for configurations where substructures can be repositioned or reused. Numerical examples demonstrate the accuracy and flexibility of the method, including cases with closely spaced components and rotational variations in substructure layout. <div>
arXiv:2503.17616v2 Announce Type: replace 
Abstract: This paper presents a unified formulation for synthesizing the generalized scattering matrix (GS-matrix) of hybrid electromagnetic systems comprising arbitrary numbers of antennas and scatterers. The proposed method provides a modular region decomposition framework that enables efficient analysis of electromagnetic interactions between distinct structures, under the relaxed geometric condition that the constituents are separable by a plane. By leveraging the addition theorem of vector spherical wavefunctions (VSWFs), a compact matrix representation is derived to assemble the GS- and S-matrices of individual components into the overall system response. This formulation generalizes and extends prior methods developed for either multiple scattering or antenna array analysis, and is particularly suited to configurations where substructures may be repositioned or reused. Numerical examples are provided to validate the accuracy and versatility of the method, including scenarios involving tightly spaced components and rotational variations in substructure layout.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa</title>
<link>https://arxiv.org/abs/2508.15110</link>
<guid>https://arxiv.org/abs/2508.15110</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, insurance sector, African insurance market, inclusive solutions
<br />
Summary: 
Artificial Intelligence, particularly Large Language Models (LLMs) and agentic AI, offer transformative potential in the insurance sector. Rapid performance improvements, open-source access, and decreasing deployment costs present unique opportunities and challenges for insurers. There is a need to address the complexity of LLM and agentic AI frameworks in the insurance industry. In the African insurance market, critical gaps exist, but there are also local efforts, players, and partnership opportunities that can be leveraged. It is essential for actuaries, insurers, regulators, and tech leaders to collaborate in creating inclusive, sustainable, and equitable AI strategies and solutions that cater to the specific needs of Africans.
<br /><br />Summary: <div>
arXiv:2508.15110v1 Announce Type: new 
Abstract: In this work, we highlight the transformative potential of Artificial Intelligence (AI), particularly Large Language Models (LLMs) and agentic AI, in the insurance sector. We consider and emphasize the unique opportunities, challenges, and potential pathways in insurance amid rapid performance improvements, increased open-source access, decreasing deployment costs, and the complexity of LLM or agentic AI frameworks. To bring it closer to home, we identify critical gaps in the African insurance market and highlight key local efforts, players, and partnership opportunities. Finally, we call upon actuaries, insurers, regulators, and tech leaders to a collaborative effort aimed at creating inclusive, sustainable, and equitable AI strategies and solutions: by and for Africans.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating profitable price bounds for prescriptive price optimization</title>
<link>https://arxiv.org/abs/2508.15248</link>
<guid>https://arxiv.org/abs/2508.15248</guid>
<content:encoded><![CDATA[
<div> bootstrap procedure, confidence intervals, Nelder-Mead simplex method, black-box optimization, total revenue

Summary:
The article discusses the importance of pricing in maximizing business profits, focusing on prescriptive price optimization. Two methods for estimating price bounds in prescriptive price optimization are proposed: one using the bootstrap procedure to estimate confidence intervals for optimal prices, and the other employing the Nelder-Mead simplex method for black-box price bounds optimization. Experimental results with synthetic price-demand datasets show that these methods successfully narrow down the price range while maintaining high revenues, especially with a small number of items or low demand noise levels. Additionally, the comparative advantage of these methods increases as more data accumulates. <div>
arXiv:2508.15248v1 Announce Type: cross 
Abstract: Pricing of products and services, which has a significant impact on consumer demand, is one of the most important factors in maximizing business profits. Prescriptive price optimization is a prominent data-driven pricing methodology consisting of two phases: demand forecasting and price optimization. In the practice of prescriptive price optimization, the price of each item is typically set within a predetermined range defined by lower and upper bounds. Narrow price ranges can lead to missed opportunities, while wide price ranges run the risk of proposing unrealistic prices; therefore, determining profitable price bounds while maintaining the reliability of the suggested prices is a critical challenge that directly affects the effectiveness of prescriptive price optimization. We propose two methods for estimating price bounds in prescriptive price optimization so that future total revenue derived from the optimized prices will be maximized. Our first method for price bounds estimation uses the bootstrap procedure to estimate confidence intervals for optimal prices. Our second method uses the Nelder--Mead simplex method for black-box price bounds optimization that maximizes total revenue estimated through $K$-fold cross-validation. Experimental results with synthetic price--demand datasets demonstrate that our methods successfully narrowed down the price range while maintaining high revenues, particularly when the number of items was small or the demand noise level was low. Moreover, as more data accumulated, the comparative advantage of our methods further increased.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEAS: Hierarchical Evolutionary Agent Simulation Framework for Cross-Scale Modeling and Multi-Objective Search</title>
<link>https://arxiv.org/abs/2508.15555</link>
<guid>https://arxiv.org/abs/2508.15555</guid>
<content:encoded><![CDATA[
<div> Framework, Agent-based modeling, Evolutionary optimization, Python, Hierarchical

Summary:
Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that integrates layered agent-based modeling with evolutionary optimization and tournament evaluation. HEAS organizes models as hierarchies of processes scheduled in layers, facilitating explicit cross-scale couplings. The framework provides a compact API for simulating, optimizing, and evaluating single- and multi-objective evolution, with PyTorch policy integration. HEAS standardizes evaluation metrics, persists data, and offers plotting tools for analysis. It emphasizes separating mechanism from orchestration, enabling easy composition and swapping of components. The framework is versatile and applicable for forward simulation, optimization, and comparisons across studies. Two example applications demonstrate the utility of HEAS in ecological systems and enterprise decision-making scenarios. HEAS serves as a reliable foundation for interdisciplinary, multi-level investigations, delivering reproducible results. 

<br /><br />Summary: <div>
arXiv:2508.15555v1 Announce Type: cross 
Abstract: Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that unifies layered agent-based modeling with evolutionary optimization and tournament evaluation in a single, reproducible workflow. HEAS represents models as hierarchies of lightweight processes ("streams") scheduled in deterministic layers that read and write a shared context, making cross-scale couplings explicit and auditable. A compact API and CLI-simulate, optimize, evaluate-expose single- and multi-objective evolution, PyTorch policy integration via parameter flattening/unflattening, and general tournament tooling with user-defined scoring and voting rules. The framework standardizes evaluation through uniform per-step and episode metrics, persists seeds, logbooks, and hall-of-fame archives, and provides plotting helpers for traces, Pareto fronts, and comparative outcomes, reducing glue code and improving comparability across studies. HEAS emphasizes separation of mechanism from orchestration, allowing exogenous drivers, endogenous agents, and aggregators to be composed and swapped without refactoring, while the same model can be used for forward simulation, optimization, or systematic comparison. We illustrate usage with two compact examples-an ecological system and an enterprise decision-making setting. HEAS offers a practical foundation for cross-disciplinary, multi-level inquiry, yielding reliable, reproducible results.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliability comparison of vessel trajectory prediction models via Probability of Detection</title>
<link>https://arxiv.org/abs/2508.14198</link>
<guid>https://arxiv.org/abs/2508.14198</guid>
<content:encoded><![CDATA[
<div> deep learning, vessel trajectory prediction, traffic complexity, model performance, reliability analysis

Summary:<br />
This study examines vessel trajectory prediction using deep learning approaches, focusing on evaluating model performance in various traffic complexities. Unlike previous models, this research considers specific traffic situations and assesses reliability through a probability of detection analysis. The models are tested on different traffic scenarios, with performance metrics and reliability estimates calculated for each category. The results provide insights into the strengths and limitations of the prediction approaches and their reliability in ensuring safe forecasts over different prediction horizons. By understanding these aspects, future developments can lead to more reliable vessel trajectory prediction methods, ultimately enhancing safety and efficiency in inland waterway navigation. <br /> <div>
arXiv:2508.14198v1 Announce Type: cross 
Abstract: This contribution addresses vessel trajectory prediction (VTP), focusing on the evaluation of different deep learning-based approaches. The objective is to assess model performance in diverse traffic complexities and compare the reliability of the approaches. While previous VTP models overlook the specific traffic situation complexity and lack reliability assessments, this research uses a probability of detection analysis to quantify model reliability in varying traffic scenarios, thus going beyond common error distribution analyses. All models are evaluated on test samples categorized according to their traffic situation during the prediction horizon, with performance metrics and reliability estimates obtained for each category. The results of this comprehensive evaluation provide a deeper understanding of the strengths and weaknesses of the different prediction approaches, along with their reliability in terms of the prediction horizon lengths for which safe forecasts can be guaranteed. These findings can inform the development of more reliable vessel trajectory prediction approaches, enhancing safety and efficiency in future inland waterways navigation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel geometric predictive algorithm for assessing Compressive Elastic Modulus in MEX additive processes, based on part nonlinearities and layers stiffness,validated with PETG and PLA materials</title>
<link>https://arxiv.org/abs/2508.13164</link>
<guid>https://arxiv.org/abs/2508.13164</guid>
<content:encoded><![CDATA[
<div> Algorithm, plastic materials, MEX, elastic modulus, compressive loads

Summary:
The paper introduces a new predictive algorithm developed by researchers for determining the elastic modulus and mechanical behavior of plastic materials manufactured using MEX under compressive loads. This algorithm requires input of the compressive elastic modulus of the material filament and MEX manufacturing parameters. It calculates layer stiffness based on the number of holes in the projected area and has been validated using PETG and PLA materials on test specimens and a variable topology case study. The algorithm is applicable to various print patterns and manufacturing directions, offering versatility for different plastic polymers suitable for MEX. It eliminates the need for costly mechanical analysis software or extensive experimental validations for complex component geometries under uniaxial compression loads. 

<br /><br />Summary: <div>
arXiv:2508.13164v1 Announce Type: new 
Abstract: The paper presents an innovative methodology based on the use of a new predictive algorithm created by the researchers capable of obtaining the elastic modulus of a plastic material manufactured with MEX and its mechanical behaviour in the elastic zone under compressive loads. The predictive algorithm only needs as input the compressive elastic modulus of the isotropic plastic material filament and the manufacturing parameters of the MEX process. The smart developed algorithm calculates the stiffness of each layer considering the number of holes in the projected area. The innovative predictive algorithm has been experimentally and numerically validated using PETG Polyethylene Terephthalate Glycol material and PLA Polylactic Acid on test specimens and on a case study of variable topology. The predictive algorithm is valid for each print pattern and manufacturing direction. The new algorithm improves the existing state of the art significantly since this algorithm extends its utility to most plastic polymer materials suitable for MEX 3D printing, provided that the mechanical and elastic properties of the filament are known. Its versatility extends to complex component geometries subjected to uniaxial compression loads, eliminating the need for mechanical analysis software or expensive experimental validations.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models</title>
<link>https://arxiv.org/abs/2508.13491</link>
<guid>https://arxiv.org/abs/2508.13491</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, FinCDM, CPA examination, Financial skills, Cognitive diagnosis evaluation

Summary:
FinCDM introduces a new cognitive diagnosis evaluation framework tailored for financial Large Language Models (LLMs). It allows for the evaluation of LLMs at the knowledge-skill level, uncovering hidden knowledge gaps and identifying under-tested areas such as tax and regulatory reasoning. The framework is supported by CPA-QKA, a dataset derived from the Certified Public Accountant examination, providing comprehensive coverage of real-world accounting and financial skills. The dataset is rigorously annotated by domain experts for fine-grained knowledge labels. Through extensive experiments on various LLMs, FinCDM reveals behavioral clusters among models, enabling interpretable, skill-aware diagnosis for more targeted model development. The approach supports trustworthy and targeted model development in the financial domain and aims to improve the overall performance and understanding of LLMs in high-stakes applications.<br /><br />Summary: <div>
arXiv:2508.13491v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promise for financial applications, yet their suitability for this high-stakes domain remains largely unproven due to inadequacies in existing benchmarks. Existing benchmarks solely rely on score-level evaluation, summarizing performance with a single score that obscures the nuanced understanding of what models truly know and their precise limitations. They also rely on datasets that cover only a narrow subset of financial concepts, while overlooking other essentials for real-world applications. To address these gaps, we introduce FinCDM, the first cognitive diagnosis evaluation framework tailored for financial LLMs, enabling the evaluation of LLMs at the knowledge-skill level, identifying what financial skills and knowledge they have or lack based on their response patterns across skill-tagged tasks, rather than a single aggregated number. We construct CPA-QKA, the first cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, with comprehensive coverage of real-world accounting and financial skills. It is rigorously annotated by domain experts, who author, validate, and annotate questions with high inter-annotator agreement and fine-grained knowledge labels. Our extensive experiments on 30 proprietary, open-source, and domain-specific LLMs show that FinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax and regulatory reasoning overlooked by traditional benchmarks, and uncovers behavioral clusters among models. FinCDM introduces a new paradigm for financial LLM evaluation by enabling interpretable, skill-aware diagnosis that supports more trustworthy and targeted model development, and all datasets and evaluation scripts will be publicly released to support further research.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Discovery of Multi-Dimensional Breakage Population Balance Equations</title>
<link>https://arxiv.org/abs/2508.13763</link>
<guid>https://arxiv.org/abs/2508.13763</guid>
<content:encoded><![CDATA[
<div> Sparse regression, multi-dimensional breakage, population balance equation, data-driven, Dynamic Mode Decomposition

Summary: 
The article introduces the Multi-Dimensional Breakage Population Balance Equation Identification (mPBE ID) algorithm, which aims to discover multi-dimensional breakage population balance equations directly from data. Current inverse solution techniques are limited to one-dimensional cases and require prior system knowledge, constraining their applicability. The mPBE-ID incorporates breakage-informed constrained sparse regression, constructs candidate library functions based on Dynamic Mode Decomposition (DMD) insights, and handles noisy/limited data through ensembling. The DMD is crucial for identifying dominant breakage dynamics and guiding the inclusion of candidate terms. The algorithm successfully discovers various forms of mPBE, even with noisy and limited data, offering a foundational framework for future extensions to generalize the discovery of multi-dimensional PBEs for high-dimensional particulate phenomena.<br /><br />Summary: <div>
arXiv:2508.13763v1 Announce Type: new 
Abstract: Multi-dimensional breakage is a ubiquitous phenomenon in natural systems, yet the systematic discovery of underlying governing equations remains a long-standing challenge. Current inverse solution techniques are restricted to one-dimensional cases and typically depend on the availability of a priori system knowledge, thus limiting their applicability. By leveraging advances in data-driven sparse regression techniques, we develop the Multi-Dimensional Breakage Population Balance Equation Identification (mPBE ID) algorithm for discovering multi-dimensional breakage population balance equations (mPBEs) directly from data. Our mPBE-ID enables tractable identification of mPBEs by incorporating several key strategies, namely, a breakage-informed constrained sparse regression, targeted candidate library functions construction via insights from Dynamic Mode Decomposition (DMD), and robust handling of noisy/limited data through ensembling (bagging/bragging). Notably, we demonstrate how the DMD is indispensable for distilling dominant breakage dynamics which can then be used to facilitate the systematic inclusion of candidate library terms. We showcase the ability of the mPBE-ID to discover different forms of mPBE (including those with discontinuous stoichiometric kernels) even when tested against noisy and limited data. We anticipate that the mPBE-ID will serve as a foundational framework for future extensions to generalize the discovery of multi-dimensional PBEs for various high-dimensional particulate phenomena.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Modelling of Infrastructure Asset Performance Deterioration -- a bounded gamma process approach</title>
<link>https://arxiv.org/abs/2508.13359</link>
<guid>https://arxiv.org/abs/2508.13359</guid>
<content:encoded><![CDATA[
<div> flexible deterioration model, infrastructure asset management systems, gamma process, bounded transformed gamma process, infrastructure performance deterioration

Summary: 
The article discusses the importance of a flexible deterioration model in infrastructure asset management systems and introduces a new bounded transformed gamma process (BTGP) model. This model is compared to a bounded nonstationary gamma process (BNGP) model in terms of deterioration modelling and asset management decision-making. An empirical study using real-world bridge condition data showcases the flexibility and significance of the proposed BTGP model. The BTGP model is deeply rooted in traditional regression modeling, providing a more flexible approach to characterizing different deterioration patterns in infrastructure systems. This study highlights the advantages of the BTGP model over existing alternatives and emphasizes its potential for improving infrastructure asset management practices. <div>
arXiv:2508.13359v1 Announce Type: cross 
Abstract: Infrastructure asset management systems require a flexible deterioration model that can handle various degradation patterns in a unified way. Owing to its appealing monotonic sample paths, independent increments and mathematical tractability, gamma process has been widely employed as an infrastructure performance deterioration model. This model was recently enhanced by introducing an upper bound to satisfy a practical modelling need that many infrastructure performance deterioration processes are constrained by physical or managerial limits. Several bounded transformed gamma process (BTGP) alternatives had been proposed; however, they lacked due flexibility to characterize different deterioration patterns. This paper proposed a new BTGP model that is deeply grounded upon the traditional regression modelling tradition in infrastructure asset management systems. Qualitative and quantitative comparisons were carried out between the proposed BTGP and a bounded nonstationary gamma process (BNGP) model from both deterioration modelling and asset management decision-making perspectives. An empirical study using the real-world historical bridge condition data was performed to examine the flexibility of the BTGP against the BNGP and six other BTGP alternatives. The results confirmed the flexibility and significance of the proposed BTGP model for infrastructure systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistence is All You Need -- A Topological Lens on Microstructural Characterization</title>
<link>https://arxiv.org/abs/2508.11967</link>
<guid>https://arxiv.org/abs/2508.11967</guid>
<content:encoded><![CDATA[
<div> Keywords: microstructure, materials, energy engineering, computational topology, deep neural network

Summary:
This study presents a novel approach to accurately design materials for energy and chemical engineering technologies by extracting key microstructural descriptors. By combining computational topology with assembly-learning-based regression, the researchers created a workflow that successfully predicted eight important microstructural features. Using a dataset of synthetic three-dimensional microstructures and a deep neural network trained on persistence images, the model achieved high accuracy in predicting the descriptors. The results showed an average R^2 of ~0.84 and Pearson r of ~0.92 in an independent test set, demonstrating both precision and generality of the approach. This unified and scalable tool offers a rapid characterization method for functional porous materials, potentially leading to improved design and performance in various applications. 

<br /><br />Summary: <div>
arXiv:2508.11967v1 Announce Type: new 
Abstract: The microstructure critically governs the properties of materials used in energy and chemical engineering technologies, from catalysts and filters to thermal insulators and sensors. Therefore, accurate design is based on quantitative descriptors of microstructural features. Here we show that eight key descriptors can be extracted by a single workflow that fuses computational topology with assembly-learning-based regression. First, 1312 synthetic three-dimensional microstructures were generated and evaluated using established algorithms, and a labeled data set of ground-truth parameters was built. Converting every structure into a persistence image allowed us to train a deep neural network that predicts the eight descriptors. In an independent test set, the model achieved on average R^2 ~ 0.84 and Pearson r ~ 0.92, demonstrating both precision and generality. The approach provides a unified and scalable tool for rapid characterization of functional porous materials.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Porous Convection in the Discrete Exterior Calculus with Geometric Multigrid</title>
<link>https://arxiv.org/abs/2508.12501</link>
<guid>https://arxiv.org/abs/2508.12501</guid>
<content:encoded><![CDATA[
<div> DEC, Discrete Exterior Calculus, Decapodes.jl, CombinatorialSpaces.jl, porous convection, geometric multigrid solver

Summary:
The article introduces the use of Discrete Exterior Calculus (DEC) in solving porous convection equations through the Decapodes.jl embedded domain-specific language. This approach is implemented using CombinatorialSpaces.jl, a Julia library that applies DEC over simplicial complexes and includes a geometric multigrid solver for maps between subdivided simplicial complexes. The study showcases numerical results of multigrid solvers for both the Poisson problem and porous convection problem, serving as a standalone solver or a preconditioner for open-source Julia iterative methods libraries. The DEC framework ensures the preservation of properties from the exterior calculus, making it a robust choice for solving multiphysics problems with efficiency and accuracy. <div>
arXiv:2508.12501v1 Announce Type: new 
Abstract: The discrete exterior calculus (DEC) defines a family of discretized differential operators which preserve certain desirable properties from the exterior calculus. We formulate and solve the porous convection equations in the DEC via the Decapodes.jl embedded domain-specific language (eDSL) for multiphysics problems discretized via CombinatorialSpaces.jl. CombinatorialSpaces.jl is an open-source Julia library which implements the DEC over simplicial complexes, and now offers a geometric multigrid solver over maps between subdivided simplicial complexes. We demonstrate numerical results of multigrid solvers for the Poisson problem and porous convection problem, both as a standalone solver and as a preconditioner for open-source Julia iterative methods libraries.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensured Energy: A simulation game to elicit preferences around Swiss energy transition pathways</title>
<link>https://arxiv.org/abs/2508.12799</link>
<guid>https://arxiv.org/abs/2508.12799</guid>
<content:encoded><![CDATA[
<div> Keywords: Paris Agreement, energy transition, serious game, public acceptance, sustainability <br />
Summary: 
The article discusses the analysis of Switzerland's energy and climate strategy towards achieving the objectives set in the 2015 Paris Agreement. Researchers examine different scenarios for transitioning towards renewable energy sources and assessing the impacts on society, environment, and economy. To gauge public acceptance of energy policies, a population survey was complemented with an online serious game that simulates the current and future energy provision, allowing players to make informed decisions. The game successfully attracted participants from various societal groups, highlighting the challenge of balancing complexity and entertainment. This approach provides valuable insights into public opinion and offers a more engaging way for stakeholders and policymakers to understand and address the challenges of transitioning to a sustainable energy future. <br /><br />Summary: <div>
arXiv:2508.12799v1 Announce Type: new 
Abstract: The 2015 Paris Agreement on global warming specifies national objectives for the reduction of greenhouse gas emissions. In support of Switzerland's energy and climate strategy for 2050, researchers investigate scenarios for the transition of energy systems towards a higher share of renewables, assessing their social, environmental and economic impact. Their results guide stakeholders and policy makers in designing resilient and sustainable systems. Political scientists use surveys to quantify public acceptance of energy policy, but the complexity and long time horizon of the subject creates difficulties, both for researchers in posing contextually relevant questions, and for respondents in assimilating enough information to give meaningful answers. A population survey was therefore augmented with an online serious game in which players experience an accurate simulation of current and future energy provision and manage transition towards a sustainable future. This interactive environment allows better informed and engaged decisions, and provides richer information on public opinion. In this paper we motivate and describe the design of the game and report initial findings on player characteristics and engagement. We show that a serious game can successfully attract participants from diverse societal groups and highlight the challenge of balancing complexity and entertainment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising diffusion models for inverse design of inflatable structures with programmable deformations</title>
<link>https://arxiv.org/abs/2508.13097</link>
<guid>https://arxiv.org/abs/2508.13097</guid>
<content:encoded><![CDATA[
<div> Keywords: Programmable structures, Inflatable structures, Generative design, Denoising diffusion probabilistic models, Pressure-driven actuation

Summary:
- The article discusses the inverse design of elastic structures undergoing large, nonlinear deformations under pressure-driven actuation.
- A generative design framework based on denoising diffusion probabilistic models (DDPMs) is presented for designing programmable structures.
- The method formulates the inverse design as a conditional generation task, using geometric descriptors of target deformed states as inputs.
- The framework quickly produces diverse undeformed configurations that achieve desired deformations when inflated, enabling parallel exploration of design candidates.
- Numerical experiments show the framework can accommodate complex constraints and efficiently explore viable design options. 

<br /><br />Summary: <div>
arXiv:2508.13097v1 Announce Type: new 
Abstract: Programmable structures are systems whose undeformed geometries and material property distributions are deliberately designed to achieve prescribed deformed configurations under specific loading conditions. Inflatable structures are a prominent example, using internal pressurization to realize large, nonlinear deformations in applications ranging from soft robotics and deployable aerospace systems to biomedical devices and adaptive architecture. We present a generative design framework based on denoising diffusion probabilistic models (DDPMs) for the inverse design of elastic structures undergoing large, nonlinear deformations under pressure-driven actuation. The method formulates the inverse design as a conditional generation task, using geometric descriptors of target deformed states as inputs and outputting image-based representations of the undeformed configuration. Representing these configurations as simple images is achieved by establishing a pre- and postprocessing pipeline that involves a fixed image processing, simulation setup, and descriptor extraction methods. Numerical experiments with scalar and higher-dimensional descriptors show that the framework can quickly produce diverse undeformed configurations that achieve the desired deformations when inflated, enabling parallel exploration of viable design candidates while accommodating complex constraints.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites</title>
<link>https://arxiv.org/abs/2508.12029</link>
<guid>https://arxiv.org/abs/2508.12029</guid>
<content:encoded><![CDATA[
<div> antibody-binding sites, epitopes, vaccine design, antigen sequences, convolutional neural networks 

Summary:
- Accurate prediction of antibody-binding sites is critical for various applications in immunology.
- In silico methods have limitations in predicting conformational epitopes effectively.
- A new conformer-based model leveraging CNNs and Transformers is proposed for epitope prediction.
- CNNs enhance the prediction of linear epitopes, while Transformers improve conformational epitope prediction.
- Experimental results show the model outperforms existing baselines in various performance metrics. 

<br /><br />Summary: <div>
arXiv:2508.12029v1 Announce Type: cross 
Abstract: Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and for advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose a conformer-based model trained on antigen sequences derived from 1,080 antigen-antibody complexes, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of PCC, ROC-AUC, PR-AUC, and F1 scores on conformational epitopes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems</title>
<link>https://arxiv.org/abs/2508.12569</link>
<guid>https://arxiv.org/abs/2508.12569</guid>
<content:encoded><![CDATA[
<div> framework, metriplectic, entropy, self-supervised learning, stochastic <br />
Summary: 
The article discusses a framework for machine learning coarse-grained dynamics of multiscale systems using the metriplectic bracket formalism. This framework preserves properties such as dissipative, history-dependent, and stochastic emergent physics, ensuring conservation laws and fluctuation-dissipation balance. A novel self-supervised learning strategy is introduced to identify emergent structural variables when labels are unavailable. The method is validated on benchmark systems and applied to challenging examples like coarse-graining star polymers and learning models from high-speed video of colloidal suspensions. Open-source implementations in PyTorch and LAMMPS are provided, enabling large-scale inference and applicability to various particle-based systems. <br /> <div>
arXiv:2508.12569v1 Announce Type: cross 
Abstract: Multiscale systems are ubiquitous in science and technology, but are notoriously challenging to simulate as short spatiotemporal scales must be appropriately linked to emergent bulk physics. When expensive high-dimensional dynamical systems are coarse-grained into low-dimensional models, the entropic loss of information leads to emergent physics which are dissipative, history-dependent, and stochastic. To machine learn coarse-grained dynamics from time-series observations of particle trajectories, we propose a framework using the metriplectic bracket formalism that preserves these properties by construction; most notably, the framework guarantees discrete notions of the first and second laws of thermodynamics, conservation of momentum, and a discrete fluctuation-dissipation balance crucial for capturing non-equilibrium statistics. We introduce the mathematical framework abstractly before specializing to a particle discretization. As labels are generally unavailable for entropic state variables, we introduce a novel self-supervised learning strategy to identify emergent structural variables. We validate the method on benchmark systems and demonstrate its utility on two challenging examples: (1) coarse-graining star polymers at challenging levels of coarse-graining while preserving non-equilibrium statistics, and (2) learning models from high-speed video of colloidal suspensions that capture coupling between local rearrangement events and emergent stochastic dynamics. We provide open-source implementations in both PyTorch and LAMMPS, enabling large-scale inference and extensibility to diverse particle-based systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shapley Values: Paired-Sampling Approximations</title>
<link>https://arxiv.org/abs/2508.12947</link>
<guid>https://arxiv.org/abs/2508.12947</guid>
<content:encoded><![CDATA[
<div> Shapley values, cooperative game theory, machine learning, sampling approximations, KernelSHAP, PermutationSHAP <br />
<br />
Summary: <br />
Shapley values, originally from cooperative game theory, are widely used in explaining machine learning predictions by assigning credit to input components based on their contribution. This study provides novel contributions by proving the asymptotic normality of sampling approximations like KernelSHAP and PermutationSHAP. Paired-sampling approaches offer exact results for interactions of maximal order two. Additionally, the paired-sampling PermutationSHAP exhibits the additive recovery property, while the kernel counterpart does not. These findings enhance the understanding and computation of Shapley values in explaining prediction outcomes. <div>
arXiv:2508.12947v1 Announce Type: cross 
Abstract: Originally introduced in cooperative game theory, Shapley values have become a very popular tool to explain machine learning predictions. Based on Shapley's fairness axioms, every input (feature component) gets a credit how it contributes to an output (prediction). These credits are then used to explain the prediction. The only limitation in computing the Shapley values (credits) for many different predictions is of computational nature. There are two popular sampling approximations, sampling KernelSHAP and sampling PermutationSHAP. Our first novel contributions are asymptotic normality results for these sampling approximations. Next, we show that the paired-sampling approaches provide exact results in case of interactions being of maximal order two. Furthermore, the paired-sampling PermutationSHAP possesses the additive recovery property, whereas its kernel counterpart does not.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Analysis in Frequency Domain: A Survey of Open Challenges, Opportunities and Benchmarks</title>
<link>https://arxiv.org/abs/2504.07099</link>
<guid>https://arxiv.org/abs/2504.07099</guid>
<content:encoded><![CDATA[
<div> Keywords: Frequency-domain analysis, Spectral methods, Causal structure preservation, Uncertainty quantification, Geometric deep learning<br />
Summary: 
Frequency-domain analysis is a powerful paradigm for time series analysis, offering advantages over traditional approaches. This survey covers classical Fourier analysis to modern neural operators, highlighting three key challenges: preserving causal structure during spectral transformations, quantifying uncertainty in learned frequency representations, and analyzing non-Euclidean data structures with topology awareness. Over 100 studies were reviewed to develop a unified taxonomy bridging conventional spectral techniques with machine learning approaches. The survey identifies knowledge gaps in geometric deep learning and quantum-enhanced spectral analysis. It provides a systematic framework for method selection and implementation and charts promising directions for future research in this rapidly evolving domain. <div>
arXiv:2504.07099v3 Announce Type: replace 
Abstract: Frequency-domain analysis has emerged as a powerful paradigm for time series analysis, offering unique advantages over traditional time-domain approaches while introducing new theoretical and practical challenges. This survey provides a comprehensive examination of spectral methods from classical Fourier analysis to modern neural operators, systematically summarizing three open challenges in current research: (1) causal structure preservation during spectral transformations, (2) uncertainty quantification in learned frequency representations, and (3) topology-aware analysis for non-Euclidean data structures. Through rigorous reviewing of over 100 studies, we develop a unified taxonomy that bridges conventional spectral techniques with cutting-edge machine learning approaches, while establishing standardized benchmarks for performance evaluation. Our work identifies key knowledge gaps in the field, particularly in geometric deep learning and quantum-enhanced spectral analysis. The survey offers practitioners a systematic framework for method selection and implementation, while charting promising directions for future research in this rapidly evolving domain.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantformer: from attention to profit with a quantitative transformer trading strategy</title>
<link>https://arxiv.org/abs/2404.00424</link>
<guid>https://arxiv.org/abs/2404.00424</guid>
<content:encoded><![CDATA[
<div> Transformer, Quantformer, investment factors, sentiment analysis, quantitative trading<br />
<br />
Summary:<br />
Quantitative trading faces challenges in capturing market variables for profit. Quantformer, a neural network based on transformer, uses transfer learning from sentiment analysis to build investment factors. It excels in modeling complex data relationships and forecasting stock trends accurately. With data from 2010 to 2023, Quantformer outperforms other quantitative strategies in predicting stock trends. Its innovative use of transformer-like models combined with market sentiment information enhances trading signal accuracy, promising advancements in quantitative trading strategies. <div>
arXiv:2404.00424v3 Announce Type: replace-cross 
Abstract: In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Fully capturing various market variables, including long-term information, as well as essential signals that may lead to profit remains a difficult task for learning algorithms. In order to tackle this challenge, this paper introduces quantformer, an enhanced neural network architecture based on transformer, to build investment factors. By transfer learning from sentiment analysis, quantformer not only exploits its original inherent advantages in capturing long-range dependencies and modeling complex data relationships, but is also able to solve tasks with numerical inputs and accurately forecast future returns over a given period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2023. The results of this study demonstrate the model's superior performance in predicting stock trends compared with other 100-factor-based quantitative strategies. Notably, the model's innovative use of transformer-like model to establish factors, in conjunction with market sentiment information, has been shown to enhance the accuracy of trading signals significantly, thereby offering promising implications for the future of quantitative trading strategies.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News</title>
<link>https://arxiv.org/abs/2508.10927</link>
<guid>https://arxiv.org/abs/2508.10927</guid>
<content:encoded><![CDATA[
<div> Keywords: company risk factors, news articles, machine learning models, supply chain, regulations<br />
Summary:<br />
- Importance of identifying company risk factors in financial market for investors and overall well-being.
- Computational framework developed to automatically extract risk factors from news articles.
- Schema comprising seven aspects such as supply chain, regulations, competitions.
- Experiment shows fine-tuned pre-trained language models performing better in identifying risk factors compared to zero-shot and few-shot prompting LLMs.
- Analysis of over 277K Bloomberg news articles demonstrates insight into company and industry operations through identification of risk factors.
<br /><br />Summary: Identifying company risk factors is crucial for investors and financial market stability. A computational framework was created to extract these factors from news articles, focusing on aspects like supply chain and regulations. Fine-tuned language models outperformed zero-shot and few-shot models in identifying risk factors. Analysis of Bloomberg news articles revealed valuable insights into company and industry operations through this approach. <div>
arXiv:2508.10927v1 Announce Type: cross 
Abstract: Identifying risks associated with a company is important to investors and the well-being of the overall financial market. In this study, we build a computational framework to automatically extract company risk factors from news articles. Our newly proposed schema comprises seven distinct aspects, such as supply chain, regulations, and competitions. We sample and annotate 744 news articles and benchmark various machine learning models. While large language models have achieved huge progress in various types of NLP tasks, our experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs (e.g. LLaMA-2) can only achieve moderate to low performances in identifying risk factors. And fine-tuned pre-trained language models are performing better on most of the risk factors. Using this model, we analyze over 277K Bloomberg news articles and demonstrate that identifying risk factors from news could provide extensive insight into the operations of companies and industries.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressive Meta-Learning</title>
<link>https://arxiv.org/abs/2508.11090</link>
<guid>https://arxiv.org/abs/2508.11090</guid>
<content:encoded><![CDATA[
<div> Keywords: compressive learning, parameter-learning, neural networks, meta-learning, data structure<br />
<br />
Summary: 
The article discusses the need for fast and efficient parameter-learning techniques due to the exponential growth of new datasets. Compressive learning is introduced as a framework that utilizes random, non-linear features to project large-scale databases onto compact representations, enabling efficient processing without access to the original samples. However, current compressive learning methods lack data structure exploitation. The proposed Compressive Meta-Learning framework meta-learns both encoding and decoding stages using neural networks, offering faster and more accurate systems. Various applications of the framework, such as compressive PCA, compressive ridge regression, and compressive k-means, are explored. This approach shows promise for improving the efficiency and privacy-friendliness of parameter learning in the face of growing dataset sizes. <br /><br />Summary: <div>
arXiv:2508.11090v1 Announce Type: cross 
Abstract: The rapid expansion in the size of new datasets has created a need for fast and efficient parameter-learning techniques. Compressive learning is a framework that enables efficient processing by using random, non-linear features to project large-scale databases onto compact, information-preserving representations whose dimensionality is independent of the number of samples and can be easily stored, transferred, and processed. These database-level summaries are then used to decode parameters of interest from the underlying data distribution without requiring access to the original samples, offering an efficient and privacy-friendly learning framework. However, both the encoding and decoding techniques are typically randomized and data-independent, failing to exploit the underlying structure of the data. In this work, we propose a framework that meta-learns both the encoding and decoding stages of compressive learning methods by using neural networks that provide faster and more accurate systems than the current state-of-the-art approaches. To demonstrate the potential of the presented Compressive Meta-Learning framework, we explore multiple applications -- including neural network-based compressive PCA, compressive ridge regression, compressive k-means, and autoencoders.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance</title>
<link>https://arxiv.org/abs/2508.11395</link>
<guid>https://arxiv.org/abs/2508.11395</guid>
<content:encoded><![CDATA[
<div> inflection point, stablecoins, "Banking 2.0", institutional adoption, macroeconomic imbalances <br />
<br />
Summary: The article discusses how stablecoins are revolutionizing the global financial system by seamlessly integrating cryptocurrency innovation with traditional banking infrastructure. It highlights the significance of stablecoins in addressing vulnerabilities in modern fiat currencies and tackling macroeconomic imbalances such as the inflation-productivity gap. The increasing institutional adoption of stablecoins, as evidenced by U.S. legislation and initiatives from major industry players like JPMorgan and PayPal, underscores their transformative potential. Stablecoins offer enhanced stability, reduced fraud risk, and facilitate unified global transactions that transcend national boundaries. By providing more robust and diversified backing mechanisms, stablecoins pave the way for a more interconnected international financial system while enabling deregulation and efficiency gains. The article provides real-world examples and current market data to support the argument that stablecoins are poised to reshape banking as we know it. <div>
arXiv:2508.11395v1 Announce Type: cross 
Abstract: The global financial system stands at an inflection point. Stablecoins represent the most significant evolution in banking since the abandonment of the gold standard, positioned to enable "Banking 2.0" by seamlessly integrating cryptocurrency innovation with traditional finance infrastructure. This transformation rivals artificial intelligence as the next major disruptor in the financial sector. Modern fiat currencies derive value entirely from institutional trust rather than physical backing, creating vulnerabilities that stablecoins address through enhanced stability, reduced fraud risk, and unified global transactions that transcend national boundaries. Recent developments demonstrate accelerating institutional adoption: landmark U.S. legislation including the GENIUS Act of 2025, strategic industry pivots from major players like JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive "Pay with Crypto" service. Widespread stablecoin implementation addresses critical macroeconomic imbalances, particularly the inflation-productivity gap plaguing modern monetary systems, through more robust and diversified backing mechanisms. Furthermore, stablecoins facilitate deregulation and efficiency gains, paving the way for a more interconnected international financial system. This whitepaper comprehensively explores how stablecoins are poised to reshape banking, supported by real-world examples, current market data, and analysis of their transformative potential.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models</title>
<link>https://arxiv.org/abs/2508.11542</link>
<guid>https://arxiv.org/abs/2508.11542</guid>
<content:encoded><![CDATA[
<div> approach, Operator Inference, reduced-order models, dynamic systems, snapshot data  
Summary:  
This paper presents a data-driven, nested Operator Inference (OpInf) approach for learning physics-informed reduced-order models (ROMs) from snapshot data of high-dimensional dynamical systems. The approach utilizes a hierarchy within the reduced space to iteratively construct initial guesses prioritizing interactions of dominant modes. The initial guess for any target reduced dimension yields a ROM with smaller or equal snapshot reconstruction error compared to standard OpInf. The nested OpInf algorithm supports warm-starting from previous models, allowing for dynamic basis and model form updates. Demonstrations on a cubic heat conduction problem showed nested OpInf achieved significantly smaller errors than standard OpInf with comparable offline time. Application to a large-scale Greenland ice sheet model produced a ROM with an average error of 3% and a computational speed-up factor exceeding 19,000. <div>
arXiv:2508.11542v1 Announce Type: cross 
Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach for learning physics-informed reduced-order models (ROMs) from snapshot data of high-dimensional dynamical systems. The approach exploits the inherent hierarchy within the reduced space to iteratively construct initial guesses for the OpInf learning problem that prioritize the interactions of the dominant modes. The initial guess computed for any target reduced dimension corresponds to a ROM with provably smaller or equal snapshot reconstruction error than with standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from previously learned models, enabling versatile application scenarios involving dynamic basis and model form updates. We demonstrate the performance of our algorithm on a cubic heat conduction problem, with nested OpInf achieving a four times smaller error than standard OpInf at a comparable offline time. Further, we apply nested OpInf to a large-scale, parameterized model of the Greenland ice sheet where, despite model form approximation errors, it learns a ROM with, on average, 3% error and computational speed-up factor above 19,000.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CarAT: Carbon Atom Tracing across Industrial Chemical Value Chains via Chemistry Language Models</title>
<link>https://arxiv.org/abs/2508.10216</link>
<guid>https://arxiv.org/abs/2508.10216</guid>
<content:encoded><![CDATA[
<div> Carbon Atom Tracker, biogenic carbon content, sustainability reporting, industrial value chains, sustainability.

Summary:
The chemical industry is focusing on sustainability and reducing carbon footprints. The Together for Sustainability consortium will soon require reporting of biogenic carbon content (BCC) in chemical products. Carbon-14 is impractical for continuous monitoring, so a new automated methodology called CarAT has been developed. CarAT uses Enterprise Resource Planning data to calculate BCC across industrial value chains by mapping carbon atoms in chemical reactions and applying a linear program. The methodology was validated on a toluene diisocyanate value chain with different scenarios. Results were visualized with Sankey diagrams, showing the flow of carbon attributes. CarAT enables real-time BCC calculation, supports compliance with reporting mandates, and facilitates the transition to sustainable manufacturing by tracking carbon sources transparently and empowering data-driven decisions. <br /><br />Summary: <div>
arXiv:2508.10216v1 Announce Type: new 
Abstract: The chemical industry is increasingly prioritising sustainability, with a focus on reducing carbon footprints to achieve net zero. By 2026, the Together for Sustainability (TfS) consortium will require reporting of biogenic carbon content (BCC) in chemical products, posing a challenge as BCC depends on feedstocks, value chain configuration, and process-specific variables. While carbon-14 isotope analysis can measure BCC, it is impractical for continuous industrial monitoring. This work presents CarAT (Carbon Atom Tracker), an automated methodology for calculating BCC across industrial value chains, enabling dynamic and accurate sustainability reporting. The approach leverages existing Enterprise Resource Planning data in three stages: (1) preparing value chain data, (2) performing atom mapping in chemical reactions using chemistry language models, and (3) applying a linear program to calculate BCC given known inlet compositions. The methodology is validated on a 27-node industrial toluene diisocyanate value chain. Three scenarios are analysed: a base case with fossil feedstocks, a case incorporating a renewable feedstock, and a butanediol value chain with a recycle stream. Results are visualised with Sankey diagrams showing the flow of carbon attributes across the value chain. The key contribution is a scalable, automated method for real-time BCC calculation under changing industrial conditions. CarAT supports compliance with upcoming reporting mandates and advances carbon neutrality goals by enabling systematic fossil-to-biogenic substitution. Through transparent, auditable tracking of carbon sources in production networks, it empowers data-driven decisions to accelerate the transition to sustainable manufacturing.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOBACO: Topology Optimization via Band-limited Coordinate Networks for Compositionally Graded Alloys</title>
<link>https://arxiv.org/abs/2508.10320</link>
<guid>https://arxiv.org/abs/2508.10320</guid>
<content:encoded><![CDATA[
<div> Keywords: Compositionally Graded Alloys, Additive Manufacturing, Topology Optimization, Coordinate Neural Network, Spatial Gradation<br />
Summary: <br />
Compositionally Graded Alloys (CGAs) offer design flexibility through spatial composition variations for stronger and lighter components. Advances in additive manufacturing (AM) have made CGA fabrication feasible, but manufacturing constraints on spatial gradation exist. This paper presents a topology optimization (TO) framework for optimized CGA designs with controlled compositional gradation. A band-limited coordinate neural network represents the composition distribution, ensuring compliance with gradation limits without explicit constraints. The approach benefits from TO advantages like mesh independence and high-resolution design extraction. Demonstrations in elastic and thermo-elastic TO examples showcase the framework's effectiveness. <div>
arXiv:2508.10320v1 Announce Type: new 
Abstract: Compositionally Graded Alloys (CGAs) offer unprecedented design flexibility by enabling spatial variations in composition; tailoring material properties to local loading conditions. This flexibility leads to components that are stronger, lighter, and more cost-effective than traditional monolithic counterparts. The fabrication of CGAs have become increasingly feasible owing to recent advancements in additive manufacturing (AM), particularly in multi-material printing and improved precision in material deposition. However, AM of CGAs requires imposition of manufacturing constraints; in particular limits on the maximum spatial gradation of composition.
  This paper introduces a topology optimization (TO) based framework for designing optimized CGA components with controlled compositional gradation. In particular, we represent the constrained composition distribution using a band-limited coordinate neural network. By regulating the network's bandwidth, we ensure implicit compliance with gradation limits, eliminating the need for explicit constraints. The proposed approach also benefits from the inherent advantages of TO using coordinate networks, including mesh independence, high-resolution design extraction, and end-to-end differentiability. The effectiveness of our framework is demonstrated through various elastic and thermo-elastic TO examples.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chem3DLLM: 3D Multimodal Large Language Models for Chemistry</title>
<link>https://arxiv.org/abs/2508.10696</link>
<guid>https://arxiv.org/abs/2508.10696</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D molecular structures, Chem3DLLM, multimodal, protein-conditioned, drug discovery

Summary:<br /><br />Chem3DLLM is introduced as a unified protein-conditioned multimodal large language model to generate 3D molecular structures. It addresses challenges faced by autoregressive-based language models in handling 3D molecular conformation. The model utilizes a reversible text encoding technique for 3D molecular structures that enables integration with protein pocket features. Reinforcement learning with stability-based rewards is employed to optimize chemical validity, and a lightweight protein embedding projector is incorporated for end-to-end training. Experimental results demonstrate state-of-the-art performance in structure-based drug design with a Vina score of -7.21, showcasing the efficacy of the unified multimodal approach for practical drug discovery applications. <div>
arXiv:2508.10696v1 Announce Type: new 
Abstract: In the real world, a molecule is a 3D geometric structure. Compared to 1D SMILES sequences and 2D molecular graphs, 3D molecules represent the most informative molecular modality. Despite the rapid progress of autoregressive-based language models, they cannot handle the generation of 3D molecular conformation due to several challenges: 1) 3D molecular structures are incompatible with LLMs' discrete token space, 2) integrating heterogeneous inputs like proteins, ligands, and text remains difficult within a unified model, and 3) LLMs lack essential scientific priors, hindering the enforcement of physical and chemical constraints during generation. To tackle these issues, we present Chem3DLLM, a unified protein-conditioned multimodal large language model. Our approach designs a novel reversible text encoding for 3D molecular structures using run-length compression, achieving 3x size reduction while preserving complete structural information. This enables seamless integration of molecular geometry with protein pocket features in a single LLM architecture. We employ reinforcement learning with stability-based rewards to optimize chemical validity and incorporate a lightweight protein embedding projector for end-to-end training. Experimental results on structure-based drug design demonstrate state-of-the-art performance with a Vina score of -7.21, validating our unified multimodal approach for practical drug discovery applications.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model</title>
<link>https://arxiv.org/abs/2508.10492</link>
<guid>https://arxiv.org/abs/2508.10492</guid>
<content:encoded><![CDATA[
<div> AI, clinical diagnosis, full-process, DxDirector-7B, deep thinking<br />
Summary: In the article, the authors propose a paradigm shift in which AI, specifically the DxDirector-7B large language model, takes on the primary role in driving the full-process clinical diagnosis, with human physicians as assistants. This model is equipped with advanced deep thinking capabilities and establishes an accountability framework for misdiagnoses. DxDirector-7B outperforms existing medical and general-purpose language models in accuracy and significantly reduces physician workload. It is evaluated across rare, complex, and real-world cases, showing potential to serve as a viable substitute for medical specialists. The shift towards AI driving the diagnostic process marks a new era in healthcare, with the potential to enhance diagnostic efficiency and reduce physicians' workload. <div>
arXiv:2508.10492v1 Announce Type: cross 
Abstract: Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians' workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians' assistant, now drives the entire diagnostic process to drastically reduce physicians' workload, indicating an efficient and accurate diagnostic solution.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Sensing for Solder Layer Degradation and Temperature Monitoring in IGBT Modules</title>
<link>https://arxiv.org/abs/2508.10515</link>
<guid>https://arxiv.org/abs/2508.10515</guid>
<content:encoded><![CDATA[
<div> solder degradation, IGBT modules, machine learning, virtual sensing, temperature estimation
Summary:
Machine learning-based virtual sensing is explored in this study to monitor solder degradation in IGBT modules, crucial for power electronic system reliability. With limited physical sensors, accurate estimation of degraded solder area (1.17% error) and surface temperature (max 4.56% relative error) is achieved. This approach offers a promising alternative to direct measurement of internal component degradation indicators, overcoming physical inaccessibility challenges in harsh environments. <div>
arXiv:2508.10515v1 Announce Type: cross 
Abstract: Monitoring the degradation state of Insulated Gate Bipolar Transistor (IGBT) modules is essential for ensuring the reliability and longevity of power electronic systems, especially in safety-critical and high-performance applications. However, direct measurement of key degradation indicators - such as junction temperature, solder fatigue or delamination - remains challenging due to the physical inaccessibility of internal components and the harsh environment. In this context, machine learning-based virtual sensing offers a promising alternative by bridging the gap from feasible sensor placement to the relevant but inaccessible locations. This paper explores the feasibility of estimating the degradation state of solder layers, and the corresponding full temperature maps based on a limited number of physical sensors. Based on synthetic data of a specific degradation mode, we obtain a high accuracy in the estimation of the degraded solder area (1.17% mean absolute error), and are able to reproduce the surface temperature of the IGBT with a maximum relative error of 4.56% (corresponding to an average relative error of 0.37%).
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Deep Contrast Source Inversion: A Unified Framework for Inverse Scattering Problems</title>
<link>https://arxiv.org/abs/2508.10555</link>
<guid>https://arxiv.org/abs/2508.10555</guid>
<content:encoded><![CDATA[
<div> inverse scattering problems, electromagnetic imaging, medical diagnostics, deep learning, contrast source inversion

Summary:
This paper proposes a physics-informed deep contrast source inversion framework (DeepCSI) for accurate medium reconstruction in inverse scattering problems. The approach utilizes a residual multilayer perceptron (ResMLP) to model current distributions under different transmitter excitations, linearizing the problem and reducing computational costs. By treating medium parameters as learnable tensors and employing a hybrid loss function, DeepCSI enables joint optimization of network parameters and medium properties. The framework is capable of handling diverse measurement scenarios, including phase-less and multi-frequency observation, offering simplicity and universal modeling capabilities compared to traditional methods. Simulations and experiments show that DeepCSI outperforms conventional contrast source inversion methods, providing high-precision and robust reconstruction in complex inverse scattering problems. <div>
arXiv:2508.10555v1 Announce Type: cross 
Abstract: Inverse scattering problems are critical in electromagnetic imaging and medical diagnostics but are challenged by their nonlinearity and diverse measurement scenarios. This paper proposes a physics-informed deep contrast source inversion framework (DeepCSI) for fast and accurate medium reconstruction across various measurement conditions. Inspired by contrast source inversion (CSI) and neural operator methods, a residual multilayer perceptron (ResMLP) is employed to model current distributions in the region of interest under different transmitter excitations, effectively linearizing the nonlinear inverse scattering problem and significantly reducing the computational cost of traditional full-waveform inversion. By modeling medium parameters as learnable tensors and utilizing a hybrid loss function that integrates state equation loss, data equation loss, and total variation regularization, DeepCSI establishes a fully differentiable framework for joint optimization of network parameters and medium properties. Compared with conventional methods, DeepCSI offers advantages in terms of simplicity and universal modeling capabilities for diverse measurement scenarios, including phase-less and multi-frequency observation. Simulations and experiments demonstrate that DeepCSI achieves high-precision, robust reconstruction under full-data, phaseless data, and multifrequency conditions, outperforming traditional CSI methods and providing an efficient and universal solution for complex inverse scattering problems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impacts of DEM Type and Resolution on Deep Learning-Based Flood Inundation Mapping</title>
<link>https://arxiv.org/abs/2309.13360</link>
<guid>https://arxiv.org/abs/2309.13360</guid>
<content:encoded><![CDATA[
<div> Keywords: flood mapping, deep learning, DEM resolution, flood prediction accuracy, data-scarce regions 

Summary:
- The study examines how DEM type and resolution impact flood prediction accuracy using a deep learning method.
- Synthetic hydrographs are used as training input with water depth data from a hydrodynamic model as target data.
- DSMs and DTMs derived from a 1 m LIDAR-based DTM were compared at resolutions from 15 to 30 m in the city of Carlisle, UK.
- Using a 30 m DTM outperformed a 30 m DSM by 21% in flood depth prediction accuracy during peak stages.
- Increasing DTM resolution to 15 m resulted in a minimum 50% increase in RMSE and a 20% increase in fit index across all flood stages.
- Coarser resolution DEMs may impact accuracy, but even a slight improvement in data resolution in data-scarce regions can enhance flood risk management.

<br /><br />Summary: <div>
arXiv:2309.13360v4 Announce Type: replace 
Abstract: The increasing availability of hydrological and physiographic spatiotemporal data has boosted machine learning's role in rapid flood mapping. Yet, data scarcity, especially high-resolution DEMs, challenges regions with limited access. This paper examines how DEM type and resolution affect flood prediction accuracy, utilizing a cutting-edge deep learning (DL) method called 1D convolutional neural network (CNN). It utilizes synthetic hydrographs as training input and water depth data obtained from LISFLOOD-FP, a 2D hydrodynamic model, as target data. This study investigates digital surface models (DSMs) and digital terrain models (DTMs) derived from a 1 m LIDAR-based DTM, with resolutions from 15 to 30 m. The methodology is applied and assessed in an established benchmark, the city of Carlisle, UK. The models' performance is then evaluated and compared against an observed flood event using RMSE, Bias, and Fit indices. Leveraging the insights gained from this region, the paper discusses the applicability of the methodology to address the challenges encountered in a data-scarce flood-prone region, exemplified by Pakistan. Results indicated that utilizing a 30 m DTM outperformed a 30 m DSM in terms of flood depth prediction accuracy by about 21% during the flood peak stage, highlighting the superior performance of DTM at lower resolutions. Increasing the resolution of DTM to 15 m resulted in a minimum 50% increase in RMSE and a 20% increase in fit index across all flood stages. The findings emphasize that while a coarser resolution DEM may impact the accuracy of machine learning models, it remains a viable option for rapid flood prediction. However, even a slight improvement in data resolution in data-scarce regions would provide significant added value, ultimately enhancing flood risk management.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Neural ODEs for Gene Regulatory Network Discovery under Perturbations</title>
<link>https://arxiv.org/abs/2501.02409</link>
<guid>https://arxiv.org/abs/2501.02409</guid>
<content:encoded><![CDATA[
<div> neural ODEs, gene regulatory network, perturbations, trajectory prediction, cell state<br />
Summary:<br />
The article focuses on developing PerturbODE, a framework that uses neural ordinary differential equations (ODEs) to model cell state trajectories and infer gene regulatory networks (GRNs) from large-scale perturbation datasets. By incorporating biologically informative neural ODEs, PerturbODE addresses the limitations of existing GRN inference models in terms of expressivity and scalability. It aims to capture causal gene regulatory relationships and account for the dynamic nature of biological processes such as cellular differentiation. The efficacy of PerturbODE is demonstrated through trajectory prediction and GRN inference on simulated and real over-expression datasets. The framework shows promising results in accurately predicting cell state trajectories under perturbations and deriving causal GRNs. <div>
arXiv:2501.02409v3 Announce Type: replace-cross 
Abstract: Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Differentiable causal graphical models have been proposed to infer a gene regulatory network (GRN) from large scale interventional datasets, capturing the causal gene regulatory relationships from genetic perturbations. However, existing models are limited in their expressivity and scalability while failing to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Topology Optimisation of Time-dependent Thermal Conduction Using Space-Time Finite Elements and a Parallel Space-Time Multigrid Preconditioner</title>
<link>https://arxiv.org/abs/2508.09589</link>
<guid>https://arxiv.org/abs/2508.09589</guid>
<content:encoded><![CDATA[
<div> Keywords: space-time topology optimisation, thermal conduction, finite element method, parallel computing, scalability

Summary:
This paper introduces a novel space-time topology optimization framework for time-dependent thermal conduction problems. Time is treated as an additional spatial dimension, and the governing equations are discretized using a stabilised continuous Galerkin space-time finite element method. A parallel-in-time method is implemented, demonstrating excellent scalability on a distributed-memory supercomputer. The framework offers up to 52x speed-up compared to traditional time-stepping approaches, with only moderate increases in total computational cost. Validation on benchmark problems with varying designs and material properties show the flexibility of the method. The proposed space-time method proves to be a promising approach for large-scale time-dependent topology optimization in thermal applications. 

<br /><br />Summary: <div>
arXiv:2508.09589v1 Announce Type: new 
Abstract: This paper presents a novel space-time topology optimisation framework for time-dependent thermal conduction problems, aiming to significantly reduce the time-to-solution. By treating time as an additional spatial dimension, we discretise the governing equations using a stabilised continuous Galerkin space-time finite element method. The resulting large all-at-once system is solved using an iterative Krylov solver preconditioned with a parallel space-time multigrid method employing a semi-coarsening strategy. Implemented in a fully parallel computing framework, the method yields a parallel-in-time method that demonstrates excellent scalability on a distributed-memory supercomputer, solving problems up to 4.2 billion degrees of freedom. Comparative studies show up to 52x speed-up over traditional time-stepping approaches, with only moderate increases in total computational cost in terms of core-hours. The framework is validated on benchmark problems with both time-constant and time-varying designs, and its flexibility is demonstrated through variations in material properties. These results establish the proposed space-time method as a promising approach for large-scale time-dependent topology optimisation in thermal applications.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisFinEval: A Scenario-Driven Chinese Multimodal Benchmark for Holistic Financial Understanding</title>
<link>https://arxiv.org/abs/2508.09641</link>
<guid>https://arxiv.org/abs/2508.09641</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, financial analysis, VisFinEval, Chinese benchmark, error analysis

Summary: 
Multimodal large language models (MLLMs) are promising for automating complex financial analysis. VisFinEval is a large-scale Chinese benchmark that covers various financial tasks using different image modalities. The benchmark includes 15,848 annotated question-answer pairs organized into three financial scenario depths. 21 state-of-the-art MLLMs were evaluated in a zero-shot setting, with the top model achieving 76.3% overall accuracy. However, it still trails behind financial experts. An error analysis revealed six recurring failure modes, indicating areas for future research such as cross-modal misalignment and lapses in business-process reasoning. VisFinEval aims to advance the development of domain-tailored MLLMs capable of integrating textual and visual financial information seamlessly.

<br /><br />Summary: <div>
arXiv:2508.09641v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) hold great promise for automating complex financial analysis. To comprehensively evaluate their capabilities, we introduce VisFinEval, the first large-scale Chinese benchmark that spans the full front-middle-back office lifecycle of financial tasks. VisFinEval comprises 15,848 annotated question-answer pairs drawn from eight common financial image modalities (e.g., K-line charts, financial statements, official seals), organized into three hierarchical scenario depths: Financial Knowledge & Data Analysis, Financial Analysis & Decision Support, and Financial Risk Control & Asset Optimization. We evaluate 21 state-of-the-art MLLMs in a zero-shot setting. The top model, Qwen-VL-max, achieves an overall accuracy of 76.3%, outperforming non-expert humans but trailing financial experts by over 14 percentage points. Our error analysis uncovers six recurring failure modes-including cross-modal misalignment, hallucinations, and lapses in business-process reasoning-that highlight critical avenues for future research. VisFinEval aims to accelerate the development of robust, domain-tailored MLLMs capable of seamlessly integrating textual and visual financial information. The data and the code are available at https://github.com/SUFE-AIFLM-Lab/VisFinEval.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finetuning Large Language Model as an Effective Symbolic Regressor</title>
<link>https://arxiv.org/abs/2508.09897</link>
<guid>https://arxiv.org/abs/2508.09897</guid>
<content:encoded><![CDATA[
<div> Keywords: Symbolic Regression, Large Language Models, Fine-tuning, SymbArena, SymbolicChat 

Summary: 
The article introduces the concept of Symbolic Regression (SR) and highlights the limitations of current Large Language Models (LLMs) in solving SR tasks efficiently. To address this, the authors propose fine-tuning LLMs for enhanced SR capability. However, the lack of dedicated datasets for SR-focused fine-tuning poses a challenge. To overcome this, the authors introduce SymbArena, a benchmark comprising a diverse set of equations for LLM training. SymbArena also introduces a new metric for evaluating form-level consistency in SR tasks. Through experiments, the authors demonstrate the effectiveness of SymbolicChat, a new LLM-based SR model that outperforms traditional numerical methods in both numerical precision and symbolic form accuracy. SymbolicChat achieves significant improvements in R2 score and form-level consistency score compared to other LLM baselines. This research paves the way for utilizing LLMs in SR tasks more effectively. 

<br /><br />Summary: <div>
arXiv:2508.09897v1 Announce Type: new 
Abstract: Deriving governing equations from observational data, known as Symbolic Regression (SR), is a cornerstone of scientific discovery. Large Language Models (LLMs) have shown promise in this task by leveraging their vast cross-disciplinary scientific knowledge. However, existing LLM-based methods primarily rely on direct inference or prompt engineering, often requiring excessive inference iterations to converge on correct formulas or failing to treating complex equation targets. These limitations in effectiveness and generalization stem from an inherent tension between pre-trained LLMs' proficiency in approximate reasoning and the high-precision demands of SR tasks. To bridge this gap, we propose to fine-tune LLMs for enhanced SR capability. Yet, the absence of dedicated datasets for SR-oriented fine-tuning remains a critical barrier. We thus introduce SymbArena, specifically engineered to optimize LLMs for SR. This benchmark comprises 148,102 diverse equations formulated as corpora of 1.83 billion tokens for LLM utilization, enabling effective training and inference. Further, SymbArena proposes a heuristics metric to precisely quantify form-level consistency, going beyond existing SR numerical-oriented evaluation strategies. With this benchmark, we explore mainstream LLM fine-tuning techniques for SR tasks and establish SymbolicChat, a simple yet effective LLM-based SR strong baseline. Experimental results validate SymbolicChat as the first LLM to exceed traditional numerical methods in both numerical precision and symbolic form accuracy, outperforming the second-best LLM baseline with improvements of 2-fold gains in R2 score and 8.37% in form-level consistency score.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction</title>
<link>https://arxiv.org/abs/2508.09227</link>
<guid>https://arxiv.org/abs/2508.09227</guid>
<content:encoded><![CDATA[
<div> Graph Attention Network, Recurrent Neural Network, Trajectory Prediction, Intelligent Transportation Systems, Task Corrector

Summary:
The proposed GSMT model integrates a Graph Attention Network (GAT) with a Recurrent Neural Network (RNN) for accurate trajectory prediction of buses in urban environments. A task corrector refines predictions by clustering historical trajectories and identifying distinct motion patterns. GSMT fuses dynamic bus and static station data through embedded networks for prediction and utilizes the corrector for further refinement. The approach allows for multi-node trajectory prediction in dense urban traffic conditions. Experimental results on a Kuala Lumpur dataset show superior performance compared to existing methods in both short-term and long-term prediction tasks. The GSMT model offers enhanced accuracy and efficiency in trajectory prediction for buses, especially in regions with limited multimodal data access.<br /><br />Summary: <div>
arXiv:2508.09227v1 Announce Type: cross 
Abstract: Accurate trajectory prediction for buses is crucial in intelligent transportation systems, particularly within urban environments. In developing regions where access to multimodal data is limited, relying solely on onboard GPS data remains indispensable despite inherent challenges. To address this problem, we propose GSMT, a hybrid model that integrates a Graph Attention Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and incorporates a task corrector capable of extracting complex behavioral patterns from large-scale trajectory data. The task corrector clusters historical trajectories to identify distinct motion patterns and fine-tunes the predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus information and static station information through embedded hybrid networks to perform trajectory prediction, and applies the task corrector for secondary refinement after the initial predictions are generated. This two-stage approach enables multi-node trajectory prediction among buses operating in dense urban traffic environments under complex conditions. Experiments conducted on a real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method significantly outperforms existing approaches, achieving superior performance in both short-term and long-term trajectory prediction tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</title>
<link>https://arxiv.org/abs/2508.09811</link>
<guid>https://arxiv.org/abs/2508.09811</guid>
<content:encoded><![CDATA[
<div> 3D scene geometry, appearance, physical information, dynamic multi-view videos, physics-informed losses

Summary:
The paper introduces a new framework named TRACE for modeling complex dynamic 3D scenes solely from multi-view videos without human labels. By treating each 3D point as a rigid particle with size and orientation, the method learns a translation rotation dynamics system for individual particles, accurately estimating physical parameters governing their motion. This approach enables the modeling of complex motion physics without requiring additional labels. Extensive experiments on various datasets demonstrate superior performance in future frame extrapolation compared to existing methods. Additionally, the framework allows for easy segmentation of multiple objects or parts by clustering the learned physical parameters. The method showcases remarkable capabilities in capturing 3D scene geometry, appearance, and physical information from dynamic multi-view videos, showcasing its potential for various applications in computer vision and scene understanding. 

<br /><br />Summary: <div>
arXiv:2508.09811v1 Announce Type: cross 
Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural nets, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. We propose a new framework named TRACE to model the motion physics of complex dynamic 3D scenes. The key novelty of our method is that, by formulating each 3D point as a rigid particle with size and orientation in space, we directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particle's motion over time. Extensive experiments on three existing dynamic datasets and one newly created challenging synthetic datasets demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Volume Element: Existence and Extent in Cracked Heterogeneous Medium</title>
<link>https://arxiv.org/abs/2508.08320</link>
<guid>https://arxiv.org/abs/2508.08320</guid>
<content:encoded><![CDATA[
<div> microscale failure, composites, multiscale modelling, representative volume element, periodic boundary conditions

Summary:
This paper addresses the growing demand for composites in engineering by focusing on microscale failure and improving composites through multiscale modelling. It aims to enhance the representativeness of volume elements by reducing mesh and size sensitivities in representative volume element (RVE) modelling. A technique is proposed to equalize fracture energy in computational analysis with real phenomena to mitigate mesh sensitivity. Modified periodic boundary conditions (MPBCs) are introduced to reduce size dependency in RVE modelling, validated through analysis of 1200 RVE samples under transverse loading. The study also examines factors influencing damage initiation in 2D composite RVEs, finding that the arrangement of closely spaced fibers can promote damage in the region between them. <div>
arXiv:2508.08320v1 Announce Type: new 
Abstract: Acknowledging the ever-increasing demand for composites in the engineering industry, this paper focuses on the failure of composites at the microscale and augmenting the use of multiscale modelling techniques to make them better for various applications. This work aims to increase the representativeness of the volume element by attenuating the mesh and size sensitivities in representative volume element (RVE) modelling. A technique to alleviate mesh sensitivity in RVE modelling is proposed, which equalises the fracture energy observed from computational analysis with the real phenomenon, thereby keeping the response independent of the bandwidth of strain localisation. Based on the hypothesis that ensuring periodicity of strain, in addition to displacement periodicity across the domain boundary and supplementing the capability of periodic boundary conditions (PBCs) to attenuate the size dependency in RVE modelling, a set of modified PBCs (MPBCs) are formulated. One thousand two hundred RVE samples falling into combinations of five fibre volume fractions and four RVE sizes are analysed under transverse loading, and the ability of MPBCs to attenuate the effect of RVE size on the precision of material response, particularly in the inelastic regime, is verified. This work also focuses on various factors affecting damage initiation in 2D composite RVEs. The arrangement of a pair of fibres with their members placed close to each other, such that the angle between the direction of loading and an imaginary line drawn between their centres is less, is observed to make the region between them more favourable to damage.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms</title>
<link>https://arxiv.org/abs/2508.08325</link>
<guid>https://arxiv.org/abs/2508.08325</guid>
<content:encoded><![CDATA[
<div> algorithm, pricing, advertising, competition, collusion
<br />
Summary:<br />
The study examines the impact of AI learning algorithms on product pricing and advertising decisions in online marketplaces. It investigates concerns about tacit collusion among sellers using these algorithms. The research shows that under certain conditions, algorithms can lead to mutually beneficial outcomes for consumers, sellers, and platforms by coordinating on lower prices. This coordination is achieved through lower advertising costs, resulting in decreased prices. Analysis of a large dataset from Amazon.com reveals varying consumer search costs across different product keywords. The study also identifies a negative relationship between consumer search costs and algorithm usage, indicating beneficial collusion. In terms of platform response, profit increases are seen through commission adjustments rather than reserve price changes. Overall, the findings suggest that competing learning algorithms may not have harmful effects and can assist in decision-making for sellers, platforms, and policymakers. 
 <div>
arXiv:2508.08325v1 Announce Type: cross 
Abstract: Online sellers have been adopting AI learning algorithms to automatically make product pricing and advertising decisions on e-commerce platforms. When sellers compete using such algorithms, one concern is that of tacit collusion - the algorithms learn to coordinate on higher than competitive. We empirically investigate whether these concerns are valid when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. Our empirical strategy is to analyze competition with multi-agent reinforcement learning, which we calibrate to a large-scale dataset collected from Amazon.com products. Our first contribution is to find conditions under which learning algorithms can facilitate win-win-win outcomes that are beneficial for consumers, sellers, and even the platform, when consumers have high search costs. In these cases the algorithms learn to coordinate on prices that are lower than competitive prices. The intuition is that the algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices. Our second contribution is an analysis of a large-scale, high-frequency keyword-product dataset for more than 2 million products on Amazon.com. Our estimates of consumer search costs show a wide range of costs for different product keywords. We generate an algorithm usage and find a negative interaction between the estimated consumer search costs and the algorithm usage index, providing empirical evidence of beneficial collusion. Finally, we analyze the platform's strategic response. We find that reserve price adjustments will not increase profits for the platform, but commission adjustments will. Our analyses help alleviate some worries about the potentially harmful effects of competing learning algorithms, and can help sellers, platforms and policymakers to decide on whether to adopt or regulate such algorithms.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Can Understand Spectra: A Multimodal Model for Molecular Structure Elucidation</title>
<link>https://arxiv.org/abs/2508.08441</link>
<guid>https://arxiv.org/abs/2508.08441</guid>
<content:encoded><![CDATA[
<div> Keywords: Structure elucidation, spectroscopic modalities, SpectraLLM, multi-modal reasoning, molecular structure

Summary: 
SpectraLLM is a novel language model designed to support multi-modal spectroscopic joint reasoning for structure elucidation. It can process single or multiple spectroscopic inputs and perform end-to-end structure elucidation by integrating continuous and discrete spectroscopic modalities. SpectraLLM learns to uncover substructural patterns that are consistent and complementary across spectra, enabling precise molecular structure elucidation. Pretrained and fine-tuned in small molecule domain, SpectraLLM achieves state-of-the-art performance on standardized chemical datasets. The model demonstrates strong robustness and generalization, even for single-spectrum inference, with its multi-modal reasoning capability further improving structural prediction accuracy.<br /><br />Summary: <div>
arXiv:2508.08441v1 Announce Type: cross 
Abstract: Structure elucidation is a fundamental technique for understanding the microscopic composition of matter and is widely applied across various disciplines in the natural sciences and engineering. However, existing methods often rely heavily on prior databases or known structural information, making it difficult to resolve unknown structures. In addition, complex structures typically require the joint analysis of multiple spectroscopic modalities. This process heavily depends on expert domain knowledge and is often accompanied by high costs in terms of both time and instrumentation. To address these challenges, we propose SpectraLLM, the first large language model designed to support multi-modal spectroscopic joint reasoning. SpectraLLM is capable of processing either single or multiple spectroscopic inputs and performing end-to-end structure elucidation. By integrating continuous and discrete spectroscopic modalities into a shared semantic space, SpectraLLM learns to uncover substructural patterns that are consistent and complementary across spectra, enabling precise molecular structure elucidation. We pretrain and fine-tune SpectraLLM in the domain of small molecules, and evaluate it on six standardized, publicly available chemical datasets. The model achieves state-of-the-art performance, significantly outperforming existing approaches trained on single modalities. Notably, SpectraLLM demonstrates strong robustness and generalization even for single-spectrum inference, while its multi-modal reasoning capability further improves the accuracy of structural prediction.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection-based multifidelity linear regression for data-scarce applications</title>
<link>https://arxiv.org/abs/2508.08517</link>
<guid>https://arxiv.org/abs/2508.08517</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, multifidelity methods, linear regression, high-dimensional outputs, data-limited applications

Summary:
Surrogate modeling for systems with high-dimensional outputs is challenging when training data are limited. This study introduces multifidelity methods for multiple-input multiple-output linear regression in such data-limited scenarios. Two projection-based approaches are proposed using principal component basis vectors for dimensionality reduction. These approaches combine high-fidelity and low-fidelity data through direct data augmentation and data augmentation with explicit linear corrections. The regression model is trained using weighted least squares with fidelity-specific weights, exploring various weighting schemes. The methods are applied to approximating the surface pressure field of a hypersonic vehicle, showing 3% - 12% improvement in median accuracy compared to single-fidelity methods with comparable computational cost in a low-data regime of up to ten high-fidelity samples.

<br /><br />Summary: <div>
arXiv:2508.08517v1 Announce Type: cross 
Abstract: Surrogate modeling for systems with high-dimensional quantities of interest remains challenging, particularly when training data are costly to acquire. This work develops multifidelity methods for multiple-input multiple-output linear regression targeting data-limited applications with high-dimensional outputs. Multifidelity methods integrate many inexpensive low-fidelity model evaluations with limited, costly high-fidelity evaluations. We introduce two projection-based multifidelity linear regression approaches that leverage principal component basis vectors for dimensionality reduction and combine multifidelity data through: (i) a direct data augmentation using low-fidelity data, and (ii) a data augmentation incorporating explicit linear corrections between low-fidelity and high-fidelity data. The data augmentation approaches combine high-fidelity and low-fidelity data into a unified training set and train the linear regression model through weighted least squares with fidelity-specific weights. Various weighting schemes and their impact on regression accuracy are explored. The proposed multifidelity linear regression methods are demonstrated on approximating the surface pressure field of a hypersonic vehicle in flight. In a low-data regime of no more than ten high-fidelity samples, multifidelity linear regression achieves approximately 3% - 12% improvement in median accuracy compared to single-fidelity methods with comparable computational cost.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Portable Multi-GPU Solver for Collisional Plasmas with Coulombic Interactions</title>
<link>https://arxiv.org/abs/2508.06771</link>
<guid>https://arxiv.org/abs/2508.06771</guid>
<content:encoded><![CDATA[
<div> Keywords: particle-in-cell methods, low-temperature plasmas, GPU acceleration, collisions, Python-based HPC tools

Summary: 
Particle-in-cell (PIC) methods for low-temperature plasmas (LTPs) are studied in this work, focusing on GPU acceleration of algorithms for velocity-space interactions, particularly collisions involving electrons with neutrals, ions, and electrons. The research explores both algorithmic analysis and the feasibility of implementing algorithms using Python-based HPC tools, specifically PyKokkos. Common PIC kernels are discussed, and performance results for NVIDIA Volta V100 and AMD MI250X GPUs are presented, with the MI250X showing slightly faster performance overall but being more sensitive to register pressure. Scaling results for a distributed memory implementation on up to 16 MPI ranks are also reported. This study contributes valuable insights into optimizing PIC methods for LTP simulations and highlights the potential of GPU acceleration in enhancing computational efficiency for studying plasma physics phenomena. 

<br /><br />Summary: <div>
arXiv:2508.06771v1 Announce Type: new 
Abstract: We study parallel particle-in-cell (PIC) methods for low-temperature plasmas (LTPs), which discretize kinetic formulations that capture the time evolution of the probability density function of particles as a function of position and velocity. We use a kinetic description for electrons and a fluid approximation for heavy species. In this paper, we focus on GPU acceleration of algorithms for velocity-space interactions and in particular, collisions of electrons with neutrals, ions, and electrons. Our work has two thrusts. The first is algorithmic exploration and analysis. The second is examining the viability of rapid-prototyping implementations using Python-based HPC tools, in particular PyKokkos. We discuss several common PIC kernels and present performance results on NVIDIA Volta V100 and AMD MI250X GPUs. Overall, the MI250X is slightly faster for most kernels but shows more sensitivity to register pressure. We also report scaling results for a distributed memory implementation on up to 16 MPI ranks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of association rule mining to assess forest species distribution in Italy considering abiotic and biotic factors</title>
<link>https://arxiv.org/abs/2508.07076</link>
<guid>https://arxiv.org/abs/2508.07076</guid>
<content:encoded><![CDATA[
<div> Association Rule Mining, biodiversity monitoring, forest community composition, ecological rules, Picea abies<br />
<br />Summary: The study explores the relationships among co-occurring plant species and environmental conditions using Association Rule Mining in forest biodiversity monitoring. By analyzing data from 6,784 plots in Italy, the Frequent Pattern Growth algorithm identified ecological rules, such as the strong correlation between temperature seasonality and precipitation seasonality with Picea abies. This tree species showed a specific association with cold, seasonal environments, indicating its ecological specificity. Some plant species acted as community "hubs," suggesting ties to particular environmental or biotic conditions. The findings provide valuable insights for future research in similar environmental settings, emphasizing the importance of accessible ecological data in understanding forest dynamics and biodiversity conservation.<br /> <div>
arXiv:2508.07076v1 Announce Type: new 
Abstract: Biodiversity monitoring represents a pressing global priority, and assessing forest community composition plays a crucial role due to its influence on ecosystem functions. The spatial distribution of forest species becomes essential for understanding biodiversity dynamics, territorial planning, aiding nature conservation and enhancing ecosystem resilience amid global change. Association Rule Mining, commonly applied to other scientific contexts, is now innovatively adopted in the ecological field to explore the relationships among co-occurring plant species and extract hidden interpretable patterns, also with abiotic and biotic conditions. Multiple heterogeneous data sources were integrated through data preprocessing into a unique dataset, including georeferenced information about 151 plant species monitored within 6,784 plots across Italy and several bioclimatic indices, soil-related factors, and variables from earth observations. The Frequent Pattern Growth algorithm, used for association rule mining, provided interesting and encouraging findings, suggesting ecological rules among plant species and environmental conditions. Indeed, temperature seasonality between 650-700 and precipitation seasonality between 45-50 resulted very correlated with Picea abies (confidence = 90.9%, lift = 7.13). Patterns detected for Picea abies highlighted its ecological specificity, indicating a strong association with cold, highly seasonal environments, and particular plant communities. Some species appeared acting as community "hubs", frequently co-occurring with other species, suggesting ties to specific environmental or biotic conditions. These findings represent a valuable resource for future research, especially in regions with similar environmental settings and when prior ecological knowledge exists, also underlining the importance of publicly accessible, high-quality ecological data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harmonic balance-automatic differentiation method: an out-of-the-box and efficient solver for general nonlinear dynamics simulation</title>
<link>https://arxiv.org/abs/2508.07309</link>
<guid>https://arxiv.org/abs/2508.07309</guid>
<content:encoded><![CDATA[
<div> method, Harmonic Balance-Alternating Frequency-Time domain, high-dimensional complex systems, Automatic Differentiation, computational efficiency

Summary:
The paper introduces the Harmonic Balance-Automatic Differentiation (HB-AD) method, which aims to enhance the dynamic response analysis of nonlinear systems, particularly in high-dimensional complex systems. HB-AD integrates Automatic Differentiation (AD) with the harmonic balance framework to eliminate manual derivations of Jacobian matrices, making it more efficient and accurate. The implementation of HB-AD leverages deep learning frameworks for parallel computing and CUDA acceleration, combined with arc-length continuation for high efficiency. Computational experiments on rotor systems demonstrate HB-AD's capability in handling complex nonlinear expressions with automated Jacobian calculations. Compared to traditional methods, HB-AD achieves significantly higher computational efficiency, making it a valuable tool for the dynamic characterization of high-dimensional engineering systems. <div>
arXiv:2508.07309v1 Announce Type: new 
Abstract: The Harmonic Balance-Alternating Frequency-Time domain (HB-AFT) method is extensively employed for dynamic response analysis of nonlinear systems. However, its application to high-dimensional complex systems is constrained by the manual derivation of Jacobian matrices during Newton-Raphson iterations, which become computationally intractable or error-prone for intricate nonlinearities. The Harmonic Balance-Automatic Differentiation (HB-AD) method is proposed to address this limitation, in which AD is integrated with the harmonic balance framework. This approach eliminates all manual derivations by leveraging AD to compute exact Jacobians numerically, enabling generic and efficient analysis of high-dimensional complex nonlinear systems. The implementation utilizes advanced deep learning frameworks for native parallel computing and CUDA acceleration, and combines AD with arc-length continuation, establishing an out-of-the-box and high efficiency computational architecture. Users need only supply the system's dynamic equations, HB-AD then autonomously trace the complete panorama of periodic responses -- including stable/unstable solution branches. Computational experiments on a rotor system with squeeze-film damper (SFD) demonstrate HB-AD's capability in handling complex nonlinear expressions with automated Jacobian calculations. For a high-dimensional aero-engine rotor-bearing-casing system with complex bearing nonlinearities, HB-AD achieves 17-fold higher efficiency than traditional HB-AFT and 144-fold acceleration over the Newmark method. The HB-AD method is a synergistic merger of computational mechanics and machine learning primitives, delivers an easy to use, general-purpose, high efficiency platform for high-fidelity dynamic characterization of high-dimensional engineering systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cardiotensor: A Python Library for Orientation Analysis and Tractography in 3D Cardiac Imaging</title>
<link>https://arxiv.org/abs/2508.07476</link>
<guid>https://arxiv.org/abs/2508.07476</guid>
<content:encoded><![CDATA[
<div> Keywords: cardiotensor, 3D cardiomyocyte orientation, structure tensor analysis, high-performance computing, tractography

Summary:
cardiotensor is a new open-source Python package designed to quantify 3D cardiomyocyte orientation in high-resolution imaging datasets of the human heart. It utilizes structure tensor analysis to extract directional metrics such as helical angle, intrusion angle, and fractional anisotropy. The package supports large teravoxel-scale datasets and is optimized for high-performance computing environments. In addition to providing detailed structural mapping of cardiac tissue, cardiotensor also includes tractography functionality to reconstruct continuous cardiomyocyte trajectories, allowing for multi-scale myoaggregate visualization down to the myocyte level. These capabilities enable the assessment of anatomical continuity and regional organization in the heart, providing valuable insights into its microstructural architecture. 

<br /><br />Summary: <div>
arXiv:2508.07476v1 Announce Type: new 
Abstract: Understanding the architecture of the human heart requires analysis of its microstructural organization across scales. With the advent of high-resolution imaging techniques such as synchrotron-based tomography, it has become possible to visualize entire hearts at micron-scale resolution. However, translating these large, complex volumetric datasets into interpretable, quantitative descriptors of cardiac organization remains a major challenge. Here we present cardiotensor, an open-source Python package designed to quantify 3D cardiomyocyte orientation in whole- or partial-heart imaging datasets. It provides efficient, scalable implementations of structure tensor analysis, enabling extraction of directional metrics such as helical angle (HA), intrusion angle (IA), and fractional anisotropy (FA). The package supports datasets reaching teravoxel-scale and is optimized for high-performance computing environments, including parallel and chunk-based processing pipelines. In addition, cardiotensor includes tractography functionality to reconstruct continuous cardiomyocyte trajectories. This enables multi-scale myoaggregate visualization down to the myocyte level, depending on resolution. These capabilities enable detailed structural mapping of cardiac tissue, supporting the assessment of anatomical continuity and regional organization.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Material Fingerprinting: A shortcut to material model discovery without solving optimization problems</title>
<link>https://arxiv.org/abs/2508.07831</link>
<guid>https://arxiv.org/abs/2508.07831</guid>
<content:encoded><![CDATA[
<div> fingerprinting, mechanical material models, hyperelastic materials, pattern recognition algorithm, experimental setups  
Summary:  
Material Fingerprinting is introduced as a novel method for rapidly discovering mechanical material models without solving complex optimization problems. The method is based on the assumption that each material has a unique response under standardized experimental conditions, creating a "fingerprint" that encodes mechanical characteristics. By establishing a database of fingerprints and corresponding models, unseen materials can be quickly characterized by matching their fingerprints. The study demonstrates that Material Fingerprinting is effective for model discovery in experiments with homogeneous and heterogeneous deformation fields, avoiding the challenges of non-convex optimization. This approach is shown to be applicable across different experimental setups and material behaviors, providing a versatile framework for rapid material model identification. This innovation holds promise for future developments in material characterization.  
<br /><br />Summary: <div>
arXiv:2508.07831v1 Announce Type: new 
Abstract: We propose Material Fingerprinting, a new method for the rapid discovery of mechanical material models from direct or indirect data that avoids solving potentially non-convex optimization problems. The core assumption of Material Fingerprinting is that each material exhibits a unique response when subjected to a standardized experimental setup. We can interpret this response as the material's fingerprint, essentially a unique identifier that encodes all pertinent information about the material's mechanical characteristics. Consequently, once we have established a database containing fingerprints and their corresponding mechanical models during an offline phase, we can rapidly characterize an unseen material in an online phase. This is accomplished by measuring its fingerprint and employing a pattern recognition algorithm to identify the best matching fingerprint in the database. In our study, we explore this concept in the context of hyperelastic materials, demonstrating the applicability of Material Fingerprinting across different experimental setups. Initially, we examine Material Fingerprinting through experiments involving homogeneous deformation fields, which provide direct strain-stress data pairs. We then extend this concept to experiments involving complexly shaped specimens with heterogeneous deformation fields, which provide indirect displacement and reaction force measurements. We show that, in both cases, Material Fingerprinting is an efficient tool for model discovery, bypassing the challenges of potentially non-convex optimization. We believe that Material Fingerprinting provides a powerful and generalizable framework for rapid material model identification across a wide range of experimental designs and material behaviors, paving the way for numerous future developments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Engineering Student Perceptions of Introductory CS Courses in an Indian Context</title>
<link>https://arxiv.org/abs/2508.06563</link>
<guid>https://arxiv.org/abs/2508.06563</guid>
<content:encoded><![CDATA[
<div> Keywords: student perceptions, assessment practices, computer science education, inclusive learning, programming course<br />
Summary: 
- The study focuses on engineering students' perceptions of assessment practices in an introductory computer science course and its associated lab in an Indian engineering institute.
- A survey involving 318 first-year students revealed that lab assignments were seen as effective, while exams and projects were viewed as authentic and skill-enhancing.
- Instructors played a significant role in shaping course content, and teaching assistants were found to be approachable and helpful despite some inconsistencies.
- Variations in academic performance and assessment perceptions were noted based on factors like prior programming experience, technology familiarity, gender, and academic branch.
- The study challenges common assumptions in grade modeling as the performance data did not follow a Gaussian distribution. A comparative analysis with European cohorts highlighted universal patterns and contextual differences, offering insights for designing inclusive assessment strategies in programming education.<br /><br />Summary: <div>
arXiv:2508.06563v1 Announce Type: cross 
Abstract: Understanding student perceptions of assessment is vital for designing inclusive and effective learning environments, especially in technical education. This study explores engineering students' perceptions of assessment practices in an introductory computer science/ programming course, and its associated laboratory within an Indian engineering institute context. A total of 318 first-year Bachelor of Technology students participated in a weekly 25-statement Likert-scale survey conducted over nine weeks. Using descriptive statistics and non-parametric tests (Mann-Whitney U and Kruskal-Wallis), the analysis reveals that students largely perceive lab assignments as effective learning activities and view exams and projects as authentic and skill-enhancing. Students appreciated the role of instructors in shaping course content and found teaching assistants to be approachable and helpful, despite some inconsistencies. The study also finds significant variations in students' academic performance and assessment perceptions based on prior programming experience, technology familiarity, gender, and academic branch. Notably, the performance data did not follow a Gaussian distribution, challenging common assumptions in grade modeling. A comparative analysis with European cohorts highlights both universal patterns and contextual differences, offering valuable insights for designing inclusive and equitable assessment strategies in programming education.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovery Learning accelerates battery design evaluation</title>
<link>https://arxiv.org/abs/2508.06985</link>
<guid>https://arxiv.org/abs/2508.06985</guid>
<content:encoded><![CDATA[
<div> Discovery Learning, batteries, machine learning, lifetime prediction, rapid feedback<br />
<br />
Summary: <br />
The article introduces Discovery Learning (DL), a scientific machine-learning approach that combines active learning, physics-guided learning, and zero-shot learning to efficiently evaluate novel battery designs. DL leverages historical data to predict battery lifetime for untested material-design combinations without the need for additional data labeling or extensive prototyping. Testing DL on a set of large-format lithium-ion pouch cells demonstrates its effectiveness in predicting cycle life with a 7.2% test error, leading to significant time and energy savings compared to traditional industrial practices. This approach showcases the potential of leveraging past designs to accelerate the development of next-generation battery technologies, making data-driven modeling more efficient and aiding scientific discovery and engineering innovation. <div>
arXiv:2508.06985v1 Announce Type: cross 
Abstract: Fast and reliable validation of novel designs in complex physical systems such as batteries is critical to accelerating technological innovation. However, battery research and development remain bottlenecked by the prohibitively high time and energy costs required to evaluate numerous new design candidates, particularly in battery prototyping and life testing. Despite recent progress in data-driven battery lifetime prediction, existing methods require labeled data of target designs to improve accuracy and cannot make reliable predictions until after prototyping, thus falling far short of the efficiency needed to enable rapid feedback for battery design. Here, we introduce Discovery Learning (DL), a scientific machine-learning paradigm that integrates active learning, physics-guided learning, and zero-shot learning into a human-like reasoning loop, drawing inspiration from learning theories in educational psychology. DL can learn from historical battery designs and actively reduce the need for prototyping, thus enabling rapid lifetime evaluation for unobserved material-design combinations without requiring additional data labeling. To test DL, we present 123 industrial-grade large-format lithium-ion pouch cells, spanning eight material-design combinations and diverse cycling protocols. Trained solely on public datasets of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting the average cycle life under unknown device variability. This results in savings of 98% in time and 95% in energy compared to industrial practices. This work highlights the potential of uncovering insights from historical designs to inform and accelerate the development of next-generation battery technologies. DL represents a key advance toward efficient data-driven modeling and helps realize the promise of machine learning for accelerating scientific discovery and engineering innovation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Enhanced Time-Series Forecasting via Large Language Models</title>
<link>https://arxiv.org/abs/2508.07697</link>
<guid>https://arxiv.org/abs/2508.07697</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, large language models, semantic enhancement, anomalous characteristics, Transformer-based models

Summary: 
- The article proposes a Semantic-Enhanced Large Language Model (SE-LLM) that integrates the periodicity and anomalies of time series data into a semantic space to enhance token embedding.
- By enhancing token interpretability for LLMs, SE-LLM effectively bridges the gap between linguistic knowledge structures and time series data patterns, improving semantic representation.
- A plugin module embedded within self-attention is introduced to enable LLMs to capture both long-term dependencies and short-term anomalies in time series data, enhancing their adaptability to temporal sequence analysis.
- The approach freezes the LLM model and reduces the dimensionality of token sequences, leading to significant computational efficiency gains.
- Experimental results demonstrate that SE-LLM outperforms existing state-of-the-art methods for time series forecasting. 

<br /><br />Summary: <div>
arXiv:2508.07697v1 Announce Type: cross 
Abstract: Time series forecasting plays a significant role in finance, energy, meteorology, and IoT applications. Recent studies have leveraged the generalization capabilities of large language models (LLMs) to adapt to time series forecasting, achieving promising performance. However, existing studies focus on token-level modal alignment, instead of bridging the intrinsic modality gap between linguistic knowledge structures and time series data patterns, greatly limiting the semantic representation. To address this issue, we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent periodicity and anomalous characteristics of time series to embed into the semantic space to enhance the token embedding. This process enhances the interpretability of tokens for LLMs, thereby activating the potential of LLMs for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel at capturing long-range dependencies but are weak at modeling short-term anomalies in time-series data. Hence, we propose a plugin module embedded within self-attention that models long-term and short-term dependencies to effectively adapt LLMs to time-series analysis. Our approach freezes the LLM and reduces the sequence dimensionality of tokens, greatly reducing computational consumption. Experiments demonstrate the superiority performance of our SE-LLM against the state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lagrangian method for solving the spherical shallow water equations using power diagrams</title>
<link>https://arxiv.org/abs/2508.08129</link>
<guid>https://arxiv.org/abs/2508.08129</guid>
<content:encoded><![CDATA[
<div> Eulerian viewpoint, Lagrangian viewpoint, spherical power cells, optimal transport problem, momentum conservation <br />
Summary: 
The article presents a new Lagrangian method for simulating the atmosphere using spherical power cells. Mass conservation is ensured through solving an optimal transport problem, while a semi-implicit time stepping procedure is used for time advancement. Artificial viscosity is not required for stabilization. The efficiency of computing spherical Voronoi diagrams is demonstrated, with calculations of 100 million sites completed in under 2 minutes. The new method is evaluated on benchmark tests, showing comparable momentum and energy conservation to the latest Lagrangian approach for the spherical shallow water equations. The study suggests that this Lagrangian approach can offer a competitive alternative to Eulerian simulations for global atmospheric simulations. <br /> <div>
arXiv:2508.08129v1 Announce Type: cross 
Abstract: Numerical simulations of the air in the atmosphere and water in the oceans are essential for numerical weather prediction. The state-of-the-art for performing these fluid simulations relies on an Eulerian viewpoint, in which the fluid domain is discretized into a mesh, and the governing equations describe the fluid motion as it passes through each cell of the mesh. However, it is unclear whether a Lagrangian viewpoint, in which the fluid is discretized by a collection of particles, can outperform Eulerian simulations in global atmospheric simulations. To date, Lagrangian approaches have shown promise, but tend to produce smoother solutions. In this work, a new Lagrangian method is developed to simulate the atmosphere in which particles are represented with spherical power cells. We introduce an efficient algorithm for computing these cells which are then used to discretize the spherical shallow water equations. Mass conservation is enforced by solving a semi-discrete optimal transport problem and a semi-implicit time stepping procedure is used to advance the solution in time. We note that, in contrast to previous work, artificial viscosity is not needed to stabilize the simulation. The performance of the spherical Voronoi diagram calculation is first assessed, which shows that spherical Voronoi diagrams of 100 million sites can be computed in under 2 minutes on a single machine. The new simulation method is then evaluated on standard benchmark test cases, which shows that momentum and energy conservation of this new method is comparable to the latest Lagrangian approach for simulating the spherical shallow water equations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Organizations, teams, and job mobility: A social microdynamics approach</title>
<link>https://arxiv.org/abs/2503.24117</link>
<guid>https://arxiv.org/abs/2503.24117</guid>
<content:encoded><![CDATA[
<div> social connections, worker mobility, organizational teams, job reunions, job change

Summary:
Workers in large organizations are influenced by preferring to reunite with past coworkers when changing jobs, with a significant percentage of job moves leading to worker reunions. The study introduces a new framework to describe organizations as composites of teams with specific tasks and social connections. The importance of worker reunions in determining job moves is highlighted, surpassing labor supply and demand considerations. Time spent together and team size are factors influencing the likelihood of reunions, indicating the role of familiarity and trust in job mobility. The study underscores the significance of teams structures and social ties in shaping internal job change within large organizations. <br /><br />Summary: <div>
arXiv:2503.24117v2 Announce Type: replace 
Abstract: Most of the modeling approaches used to understand organizational worker mobility are highly stylized, using idealizations such as structureless organizations, indistinguishable workers, and a lack of social bonding of the workers. In this article, aided by a decade of precise, temporally resolved data of a large civilian organization of the US Army in which employees can change jobs in a similar way to many private organizations, we introduce a new framework to describe organizations as composites of teams within which individuals perform specific tasks and where social connections develop. By tracking the personnel composition of organizational teams, we find that workers who change jobs are highly influenced by preferring to reunite with past coworkers. In this organization, 34% of all moves across temporally stable teams (and 32% of the totality of moves) lead to worker reunions, percentages that have not been reported and are well-above intuitive expectation. To assess the importance of worker reunions in determining job moves, we compare them to labor supply and demand with or without occupational specialization. The comparison shows that the most consistent information about job change is provided by reunions. We find that the greater the time workers spend together or the smaller the team they share both increase their likelihood to reunite, supporting the notion of increased familiarity and trust behind such reunions and the dominant role of social capital in the evolution of large organizations. Our study of this organization supports the idea that to correctly forecast job mobility inside large organizations, their teams structures and the social ties formed in those teams play a key role in shaping internal job change.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection</title>
<link>https://arxiv.org/abs/2403.06534</link>
<guid>https://arxiv.org/abs/2403.06534</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthetic Aperture Radar, object detection, dataset, SAR, Multi-Stage with Filter Augmentation<br />
<br />
Summary: 
This research introduces a new benchmark dataset, SARDet-100K, for large-scale Synthetic Aperture Radar (SAR) object detection. The dataset is a combination of 10 existing SAR detection datasets and is the first of its kind with multi-class objects at a COCO-level scale. The study identifies a challenge in SAR object detection related to the disparities between pretraining on RGB datasets and finetuning on SAR datasets. In response, a novel pretraining framework called Multi-Stage with Filter Augmentation (MSFA) is proposed to address these gaps in data domain and model structure. The MSFA method significantly improves the performance of SAR object detection models and demonstrates versatility across various models. By providing the SARDet-100K dataset and open-source code, this work aims to advance research in SAR object detection. <div>
arXiv:2403.06534v3 Announce Type: replace-cross 
Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at https://github.com/zcablii/SARDet_100K.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Sioux Falls Network: Insights from Path-Driven Higher-Order Network Analysis</title>
<link>https://arxiv.org/abs/2508.06234</link>
<guid>https://arxiv.org/abs/2508.06234</guid>
<content:encoded><![CDATA[
<div> Keywords: Benchmark scenarios, Higher-order network models, Mobility behavior, Structural complexity, Path diversity

Summary: 
The study introduces a mathematical framework based on higher-order network models to evaluate benchmark scenarios in transportation research, focusing on the Sioux Falls scenario. The framework aims to quantify the representativeness of benchmark networks by assessing structural and functional patterns. Results show that the classical Sioux Falls network has limited path diversity, rapid structural fragmentation, and weak alignment with empirical routing behavior. Higher-order network models are proposed as a way to bridge the gap between simulation-based and real-world mobility analysis, providing more accurate and generalizable insights in transportation research. This study highlights the importance of considering memory-aware network representations to improve the fidelity of benchmark scenarios in evaluating routing algorithms, infrastructure interventions, and new technologies in transportation research. 

Summary:<br /><br />Keywords: Benchmark scenarios, Higher-order network models, Mobility behavior, Structural complexity, Path diversity<br /><br />The study examines the representativeness of benchmark networks in transportation research using a mathematical framework based on higher-order network models. The analysis focuses on the widely used Sioux Falls scenario, revealing limited path diversity, rapid structural fragmentation, and weak alignment with empirical routing behavior in the classical Sioux Falls network. The study suggests that higher-order network models can enhance the accuracy and generalizability of simulation results, bridging the gap between simulation-based and real-world mobility analysis. By considering memory-aware network representations, researchers can improve the fidelity of benchmark scenarios and gain more meaningful insights in evaluating routing algorithms, infrastructure interventions, and new technologies in transportation research. <div>
arXiv:2508.06234v1 Announce Type: new 
Abstract: Benchmark scenarios are widely used in transportation research to evaluate routing algorithms, simulate infrastructure interventions, and test new technologies under controlled conditions. However, the structural and behavioral fidelity of these benchmarks remains largely unquantified, raising concerns about the external validity of simulation results. In this study, we introduce a mathematical framework based on higher-order network models to evaluate the representativeness of benchmark networks, focusing on the widely used Sioux Falls scenario. Higher-order network models encode empirical and simulated trajectory data into memory-aware network representations, which we use to quantify sequential dependencies in mobility behavior and assess how well benchmark networks capture real-world structural and functional patterns. Applying this framework to the Sioux Falls network, as well as real-world trajectory data, we quantify structural complexity, optimal memory length, link prediction accuracy, and centrality alignment. Our results show and statistically quantify that the classical Sioux Falls network exhibits limited path diversity, rapid structural fragmentation at higher orders, and weak alignment with empirical routing behavior. These results illustrate the potential of higher-order network models to bridge the gap between simulation-based and real-world mobility analysis, providing a robust foundation for more accurate and generalizable insights in transportation research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading</title>
<link>https://arxiv.org/abs/2508.06312</link>
<guid>https://arxiv.org/abs/2508.06312</guid>
<content:encoded><![CDATA[
<div> factor mining, quantitative trading, Large Language Models, automated, alpha discovery

Summary: 
Chain-of-Alpha introduces a novel framework for automated alpha mining in quantitative trading using Large Language Models (LLMs). The method utilizes a dual-chain architecture comprising a Factor Generation Chain and a Factor Optimization Chain to iteratively generate, evaluate, and refine alpha factors without human intervention. By leveraging market data, backtest feedback, and prior optimization knowledge, Chain-of-Alpha offers a high degree of automation, generality, and efficiency in alpha discovery. The framework outperforms existing baselines in real-world A-share benchmarks, highlighting its scalability and promising potential for LLM-driven quantitative research. <div>
arXiv:2508.06312v1 Announce Type: new 
Abstract: Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Farm Economics and Landscape Ecology for Global Sustainability through Hierarchical and Bayesian Optimization</title>
<link>https://arxiv.org/abs/2508.06386</link>
<guid>https://arxiv.org/abs/2508.06386</guid>
<content:encoded><![CDATA[
<div> Optimization, Agricultural landscapes, Biodiversity, Connectivity, Agri-environmental policies
Summary: 
The article introduces a novel hierarchical optimization framework addressing the challenge of sustaining food production while reversing biodiversity loss in agricultural landscapes. The framework consists of an Ecological Intensification (EI) model determining optimal allocation of land to margin and habitat interventions at the farm level, an Ecological Connectivity (EC) model arranging interventions across the landscape to enhance connectivity while maintaining profitability, and a Bayesian Optimization (BO) approach translating spatial outcomes into policy instruments. By applying this framework to a Canadian agricultural landscape, the study demonstrates improved connectivity under economic constraints. The approach aligns farm incentives with biodiversity goals, offering an effective tool for developing economically viable and ecologically sound agri-environmental policies. 
<br /><br />Summary: <div>
arXiv:2508.06386v1 Announce Type: new 
Abstract: Agricultural landscapes face the dual challenge of sustaining food production while reversing biodiversity loss. Agri-environmental policies often fall short of delivering ecological functions such as landscape connectivity, in part due to a persistent disconnect between farm-level economic decisions and landscape-scale spatial planning. We introduce a novel hierarchical optimization framework that bridges this gap. First, an Ecological Intensification (EI) model determines the economically optimal allocation of land to margin and habitat interventions at the individual farm level. These farm-specific intervention levels are then passed to an Ecological Connectivity (EC) model, which spatially arranges them across the landscape to maximize connectivity while preserving farm-level profitability. Finally, we introduce a Bayesian Optimization (BO) approach that translates these spatial outcomes into simple, cost effective, and scalable policy instruments, such as subsidies and eco-premiums, using non-spatial, farm-level policy parameters. Applying the framework to a Canadian agricultural landscape, we demonstrate how it enhances connectivity under real-world economic constraints. Our approach provides a globally relevant tool for aligning farm incentives with biodiversity goals, advancing the development of agri-environmental policies that are economically viable and ecologically effective.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Collocation Point Strategies For Physics Informed Neural Networks via the QR Discrete Empirical Interpolation Method</title>
<link>https://arxiv.org/abs/2501.07700</link>
<guid>https://arxiv.org/abs/2501.07700</guid>
<content:encoded><![CDATA[
<div> adaptive collocation point selection, physics-informed neural networks, partial differential equations, QR Discrete Empirical Interpolation Method, adaptive mesh refinement <br />
<br />
Summary: 
This article explores the impact of collocation point sampling on the performance of Physics-Informed Neural Networks (PINNs) for solving problems related to partial differential equations (PDEs). Traditional fixed sampling methods can be limited in capturing high-gradient regions, impacting the accuracy of PINNs for complex PDEs. The proposed adaptive collocation point selection strategies leverage the QR Discrete Empirical Interpolation Method (QR-DEIM) to dynamically update collocation points during training. Results on benchmark PDEs showcase that the QR-DEIM-based approaches enhance PINN accuracy compared to existing methods. This research opens up a promising pathway for improving the efficiency and effectiveness of adaptive collocation point strategies in the context of PINNs. <br /> <div>
arXiv:2501.07700v4 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) have gained significant attention for solving forward and inverse problems related to partial differential equations (PDEs). While advancements in loss functions and network architectures have improved PINN accuracy, the impact of collocation point sampling on their performance remains underexplored. Fixed sampling methods, such as uniform random sampling and equispaced grids, can fail to capture critical regions with high solution gradients, limiting their effectiveness for complex PDEs. Adaptive methods, inspired by adaptive mesh refinement from traditional numerical methods, address this by dynamically updating collocation points during training but may overlook residual dynamics between updates, potentially losing valuable information. To overcome this limitation, we propose two adaptive collocation point selection strategies utilizing the QR Discrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling technique for efficiently approximating nonlinear functions. Our results on benchmark PDEs demonstrate that our QR-DEIM-based approaches improve PINN accuracy compared to existing methods, offering a promising direction for adaptive collocation point strategies.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriceFM: Foundation Model for Probabilistic Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2508.04875</link>
<guid>https://arxiv.org/abs/2508.04875</guid>
<content:encoded><![CDATA[
<div> spatiotemporal forecasting, electricity price, Europe, deep learning, graph-based

Summary:
The paper introduces PriceFM, a spatiotemporal foundation model for electricity price forecasting in Europe that leverages graph-based inductive biases to capture spatial interdependencies across interconnected power markets. The model utilizes a comprehensive dataset spanning 24 European countries and 38 regions over a three-year period. PriceFM is designed for multi-region, multi-timestep, and multi-quantile probabilistic forecasting, outperforming competitive baselines in extensive experiments. The study highlights the importance of incorporating spatial context in electricity market forecasting. The dataset and code are available on GitHub for further research and development. <br /><br />Summary: <div>
arXiv:2508.04875v1 Announce Type: new 
Abstract: Electricity price forecasting in Europe presents unique challenges due to the continent's increasingly integrated and physically interconnected power market. While recent advances in deep learning and foundation models have led to substantial improvements in general time series forecasting, most existing approaches fail to capture the complex spatial interdependencies and uncertainty inherent in electricity markets. In this paper, we address these limitations by introducing a comprehensive and up-to-date dataset across 24 European countries (38 regions), spanning from 2022-01-01 to 2025-01-01. Building on this groundwork, we propose PriceFM, a spatiotemporal foundation model that integrates graph-based inductive biases to capture spatial interdependencies across interconnected electricity markets. The model is designed for multi-region, multi-timestep, and multi-quantile probabilistic electricity price forecasting. Extensive experiments and ablation studies confirm the model's effectiveness, consistently outperforming competitive baselines and highlighting the importance of spatial context in electricity markets. The dataset and code can be found at https://github.com/runyao-yu/PriceFM.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment-Aware Stock Price Prediction with Transformer and LLM-Generated Formulaic Alpha</title>
<link>https://arxiv.org/abs/2508.04975</link>
<guid>https://arxiv.org/abs/2508.04975</guid>
<content:encoded><![CDATA[
<div> Keywords: alpha decay, large language models, stock price prediction, financial data, interpretability

Summary:
Automating the generation of alpha decay strategies in trading and quantitative analysis is now possible through the integration of large language models (LLMs) with Transformer models. This novel framework utilizes structured inputs like historical stock features, technical indicators, and sentiment scores to generate diverse and adaptive alphas. These formulaic alphas serve as high-level features capturing complex dependencies within financial data. The alphas, not directly used for trading, are inputted into prediction models like Transformer, LSTM, TCN, SVR, and Random Forest to forecast future stock prices. Experimental results show that LLM-generated alphas significantly enhance predictive accuracy. Additionally, the natural language reasoning provided by the LLM enhances interpretability and transparency in financial decision-making. <br /><br />Summary: <div>
arXiv:2508.04975v1 Announce Type: new 
Abstract: Traditionally, traders and quantitative analysts address alpha decay by manually crafting formulaic alphas, mathematical expressions that identify patterns or signals in financial data, through domain expertise and trial-and-error. This process is often time-consuming and difficult to scale. With recent advances in large language models (LLMs), it is now possible to automate the generation of such alphas by leveraging the reasoning capabilities of LLMs. This paper introduces a novel framework that integrates a prompt-based LLM with a Transformer model for stock price prediction. The LLM first generates diverse and adaptive alphas using structured inputs such as historical stock features (Close, Open, High, Low, Volume), technical indicators, sentiment scores of both target and related companies. These alphas, instead of being used directly for trading, are treated as high-level features that capture complex dependencies within the financial data. To evaluate the effectiveness of these LLM-generated formulaic alphas, the alpha features are then fed into prediction models such as Transformer, LSTM, TCN, SVR, and Random Forest to forecast future stock prices. Experimental results demonstrate that the LLM-generated alphas significantly improve predictive accuracy. Moreover, the accompanying natural language reasoning provided by the LLM enhances the interpretability and transparency of the predictions, supporting more informed financial decision-making.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy Decisions on Fluid Instabilities: Autoencoder-Based Reconstruction meets Rule-Based Anomaly Classification</title>
<link>https://arxiv.org/abs/2508.05418</link>
<guid>https://arxiv.org/abs/2508.05418</guid>
<content:encoded><![CDATA[
<div> Keywords: shockwave classification, shadowgraph imaging, hybrid framework, unsupervised autoencoder, fuzzy inference system

Summary: 
This study introduces a novel approach for shockwave classification in shadowgraph imaging, addressing the challenges posed by limited labeled data and complex flow structures. The hybrid framework combines unsupervised autoencoder models with a fuzzy inference system to generate and interpret anomaly maps. Among the methods evaluated, the hybrid $\beta$-VAE autoencoder with a fuzzy rule-based system proves to be the most effective in capturing coherent shock features and integrating spatial context for enhanced anomaly classification. This approach allows for interpretable, unsupervised classification of flow disruptions, paving the way for real-time, physics-informed diagnostics in experimental and industrial fluid applications. The proposed methodology holds promise for improving understanding and analysis of shockwave phenomena in various fluid dynamics scenarios. 

<br /><br />Summary: <div>
arXiv:2508.05418v1 Announce Type: new 
Abstract: Shockwave classification in shadowgraph imaging is challenging due to limited labeled data and complex flow structures. This study presents a hybrid framework that combines unsupervised autoencoder models with a fuzzy inference system to generate and interpret anomaly maps. Among the evaluated methods, the hybrid $\beta$-VAE autoencoder with a fuzzy rule-based system most effectively captured coherent shock features, integrating spatial context to enhance anomaly classification. The resulting approach enables interpretable, unsupervised classification of flow disruptions and lays the groundwork for real-time, physics-informed diagnostics in experimental and industrial fluid applications.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorising SME Bank Transactions with Machine Learning and Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2508.05425</link>
<guid>https://arxiv.org/abs/2508.05425</guid>
<content:encoded><![CDATA[
<div> Financial, Small and Medium Enterprises, Cash flow lending, Synthetic data generation, Classification model<br />
Summary:<br />
This article explores the challenges faced by Small and Medium Enterprises (SMEs) in securing traditional financing due to information asymmetries. Cash flow lending is proposed as an alternative, but its effectiveness relies on accurate modeling of transaction-level data. The main obstacle in analyzing SME transactions is the unstructured nature of textual descriptions, characterized by abbreviations and imbalanced label distributions. To address these challenges, the authors propose a bank categorization pipeline leveraging synthetic data generation to enrich transaction datasets. Their approach consists of a synthetic data generation module, a fine-tuned classification model, and a calibration methodology. Experimental results show the model achieves high accuracy and robust generalization across different SMEs and transaction types, making it suitable for practical deployment in cash-flow lending applications. This framework offers a practical solution to data challenges in SME lending contexts, addressing scarcity, noise, and imbalance. <br /> <div>
arXiv:2508.05425v1 Announce Type: new 
Abstract: Despite their significant economic contributions, Small and Medium Enterprises (SMEs) face persistent barriers to securing traditional financing due to information asymmetries. Cash flow lending has emerged as a promising alternative, but its effectiveness depends on accurate modelling of transaction-level data. The main challenge in SME transaction analysis lies in the unstructured nature of textual descriptions, characterised by extreme abbreviations, limited context, and imbalanced label distributions. While consumer transaction descriptions often show significant commonalities across individuals, SME transaction descriptions are typically nonstandard and inconsistent across businesses and industries. To address some of these challenges, we propose a bank categorisation pipeline that leverages synthetic data generation to augment existing transaction data sets. Our approach comprises three core components: (1) a synthetic data generation module that replicates transaction properties while preserving context and semantic meaning; (2) a fine-tuned classification model trained on this enriched dataset; and (3) a calibration methodology that aligns model outputs with real-world label distributions. Experimental results demonstrate that our approach achieves 73.49% (+-5.09) standard accuracy on held-out data, with high-confidence predictions reaching 90.36% (+-6.52) accuracy. The model exhibits robust generalisation across different types of SMEs and transactions, which makes it suitable for practical deployment in cash-flow lending applications. By addressing core data challenges, namely, scarcity, noise, and imbalance, our framework provides a practical solution to build robust classification systems in data-sparse SME lending contexts.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deconstructing the Crystal Ball: From Ad-Hoc Prediction to Principled Startup Evaluation with the SAISE Framework</title>
<link>https://arxiv.org/abs/2508.05491</link>
<guid>https://arxiv.org/abs/2508.05491</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, startup evaluation, systematic literature review, prediction models, SAISE Framework

Summary: 
The article discusses the integration of Artificial Intelligence (AI) into startup evaluation and highlights the fragmented nature of existing academic research in this field. It points out the inconsistencies in definitions of success, lack of theoretical foundations, and insufficient validation methods in current predictive models. The study includes a systematic literature review of 57 empirical studies to understand the features, algorithms, data sources, and evaluation practices used in AI-driven startup prediction. The research identifies key weaknesses in the field, such as fragmented definition of success, lack of theory-driven feature engineering, inadequate model validation, and limited focus on data ethics and explainability. In response to these findings, the authors propose the Systematic AI-driven Startup Evaluation (SAISE) Framework, a five-stage roadmap aimed at guiding researchers towards a more principled and rigorous evaluation methodology. By emphasizing problem definition, data synthesis, feature engineering, validation, and interpretation, the SAISE framework aims to enhance the comparability, robustness, and practical relevance of research in this rapidly evolving domain.

Summary: <br /><br /> <div>
arXiv:2508.05491v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) into startup evaluation represents a significant technological shift, yet the academic research underpinning this transition remains methodologically fragmented. Existing studies often employ ad-hoc approaches, leading to a body of work with inconsistent definitions of success, atheoretical features, and a lack of rigorous validation. This fragmentation severely limits the comparability, reliability, and practical utility of current predictive models.
  To address this critical gap, this paper presents a comprehensive systematic literature review of 57 empirical studies. We deconstruct the current state-of-the-art by systematically mapping the features, algorithms, data sources, and evaluation practices that define the AI-driven startup prediction landscape. Our synthesis reveals a field defined by a central paradox: a strong convergence on a common toolkit -- venture databases and tree-based ensembles -- but a stark divergence in methodological rigor. We identify four foundational weaknesses: a fragmented definition of "success," a divide between theory-informed and data-driven feature engineering, a chasm between common and best-practice model validation, and a nascent approach to data ethics and explainability.
  In response to these findings, our primary contribution is the proposal of the Systematic AI-driven Startup Evaluation (SAISE) Framework. This novel, five-stage prescriptive roadmap is designed to guide researchers from ad-hoc prediction toward principled evaluation. By mandating a coherent, end-to-end methodology that emphasizes stage-aware problem definition, theory-informed data synthesis, principled feature engineering, rigorous validation, and risk-aware interpretation, the SAISE framework provides a new standard for conducting more comparable, robust, and practically relevant research in this rapidly maturing domain
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Space Diffusion for Topology Optimization</title>
<link>https://arxiv.org/abs/2508.05624</link>
<guid>https://arxiv.org/abs/2508.05624</guid>
<content:encoded><![CDATA[
<div> latent diffusion models, variational autoencoders, topology optimization, material distribution, generative process

Summary:<br />
This study introduces a novel framework that combines latent diffusion models (LDMs) with variational autoencoders (VAEs) for efficient topology optimization. The method allows for fast generation of optimized structures by conditioning the generative process on physically meaningful fields such as von Mises stress, strain energy density, volume fraction, and loading information. To improve design quality, auxiliary loss functions are introduced to penalize floating material, load imbalance, and volume fraction deviation, promoting realistic and manufacturable designs. Numerical experiments on a synthetic dataset demonstrate the framework's superior performance in compliance accuracy, volume control, and structural connectivity compared to existing diffusion-based methods. This approach offers a scalable alternative to traditional gradient-based methods, addressing issues of scalability and dimensionality in topology optimization. <br /><br /> <div>
arXiv:2508.05624v1 Announce Type: new 
Abstract: Topology optimization enables the automated design of efficient structures by optimally distributing material within a defined domain. However, traditional gradient-based methods often scale poorly with increasing resolution and dimensionality due to the need for repeated finite element analyses and sensitivity evaluations. In this work, we propose a novel framework that combines latent diffusion models (LDMs) with variational autoencoders (VAEs) to enable fast, conditional generation of optimized topologies. Unlike prior approaches, our method conditions the generative process on physically meaningful fields, specifically von Mises stress, strain energy density, volume fraction, and loading information, embedded as dense input channels. To further guide the generation process, we introduce auxiliary loss functions that penalize floating material, load imbalance, and volume fraction deviation, thereby encouraging physically realistic and manufacturable designs. Numerical experiments on a large synthetic dataset demonstrate that our VAE-LDM framework outperforms existing diffusion-based methods in compliance accuracy, volume control, and structural connectivity, providing a robust and scalable alternative to conventional
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle</title>
<link>https://arxiv.org/abs/2508.04755</link>
<guid>https://arxiv.org/abs/2508.04755</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, dynamic treatment regimes, large language models, implicit prior knowledge, clinical heuristics<br />
Summary: This study evaluates the use of large language models (LLMs) as dynamic insulin dosing agents in Type 1 diabetes treatment. LLMs demonstrate comparable performance to neural network-based agents when provided with carefully designed prompts. However, LLMs show limitations such as aggressive dosing due to reasoning errors like arithmetic hallucination and temporal misinterpretation. Explicit reasoning about latent states does not significantly improve performance. The study suggests cautious integration of LLMs into clinical workflows, emphasizing the need for targeted prompt engineering and validation. Hybrid approaches combining linguistic reasoning with structured physiological modeling may offer more effective decision-support systems.<br /><br />Summary: <div>
arXiv:2508.04755v1 Announce Type: cross 
Abstract: Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold promise for automating complex clinical decision-making, yet their practical deployment remains hindered by the intensive engineering required to inject clinical knowledge and ensure patient safety. Recent advancements in large language models (LLMs) suggest a complementary approach, where implicit prior knowledge and clinical heuristics are naturally embedded through linguistic prompts without requiring environment-specific training. In this study, we rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in silico Type 1 diabetes simulator, comparing their zero-shot inference performance against small neural network-based RL agents (SRAs) explicitly trained for the task. Our results indicate that carefully designed zero-shot prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or superior clinical performance relative to extensively trained SRAs, particularly in stable patient cohorts. However, LLMs exhibit notable limitations, such as overly aggressive insulin dosing when prompted with chain-of-thought (CoT) reasoning, highlighting critical failure modes including arithmetic hallucination, temporal misinterpretation, and inconsistent clinical logic. Incorporating explicit reasoning about latent clinical states (e.g., meals) yielded minimal performance gains, underscoring the current model's limitations in capturing complex, hidden physiological dynamics solely through textual inference. Our findings advocate for cautious yet optimistic integration of LLMs into clinical workflows, emphasising the necessity of targeted prompt engineering, careful validation, and potentially hybrid approaches that combine linguistic reasoning with structured physiological modelling to achieve safe, robust, and clinically effective decision-support systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations</title>
<link>https://arxiv.org/abs/2508.05020</link>
<guid>https://arxiv.org/abs/2508.05020</guid>
<content:encoded><![CDATA[
<div> AMR, Regent, Legion programming model, high-order solvers, compressible flows<br />
Summary:<br />
This study focuses on developing an adaptive mesh refinement (AMR) numerical solver using Regent, a high-level programming language designed for the Legion programming model. The implementation addresses challenges such as dynamic data structures for patch refinement/coarsening, mesh validity enforcement, and reducing task launch overhead through task fusion. Experimental results demonstrate significant speedups achieved with task fusion and automated GPU kernel generation using simple annotations. The approach is validated through simulations of two compressible flow problems governed by the Euler equations. Overall, the study showcases the effectiveness of using Regent and AMR for high-order solvers in scientific applications. <div>
arXiv:2508.05020v1 Announce Type: cross 
Abstract: High-order solvers for compressible flows are vital in scientific applications. Adaptive mesh refinement (AMR) is a key technique for reducing computational cost by concentrating resolution in regions of interest. In this work, we develop an AMR-based numerical solver using Regent, a high-level programming language for the Legion programming model. We address several challenges associated with implementing AMR in Regent. These include dynamic data structures for patch refinement/coarsening, mesh validity enforcement, and reducing task launch overhead via task fusion. Experimental results show that task fusion achieves 18x speedup, while automated GPU kernel generation via simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate our approach through simulations of two canonical compressible flow problems governed by the Euler equations.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo State Networks for Bitcoin Time Series Prediction</title>
<link>https://arxiv.org/abs/2508.05416</link>
<guid>https://arxiv.org/abs/2508.05416</guid>
<content:encoded><![CDATA[
<div> Keywords: stock prices, cryptocurrency prices, Echo State Networks, chaos analysis, machine learning methods

Summary:
Forecasting stock and cryptocurrency prices is a challenging task due to their high volatility and non-stationarity, influenced by various factors such as economic changes and market sentiment. This study investigates the use of Echo State Networks (ESNs) for predicting cryptocurrency prices, particularly during periods of extreme volatility. Results show that ESNs outperform other machine learning methods significantly, especially during chaotic periods, as reflected in the Lyapunov exponent analysis. The research demonstrates the robustness of ESNs during turbulent market conditions and their superior performance compared to Boosting and Naïve methods. The findings suggest that ESNs are well-suited for capturing nonlinear patterns in dynamic data and can be effective tools for short-term forecasting of both stock and cryptocurrency prices. 

Summary: <div>
arXiv:2508.05416v1 Announce Type: cross 
Abstract: Forecasting stock and cryptocurrency prices is challenging due to high volatility and non-stationarity, influenced by factors like economic changes and market sentiment. Previous research shows that Echo State Networks (ESNs) can effectively model short-term stock market movements, capturing nonlinear patterns in dynamic data. To the best of our knowledge, this work is among the first to explore ESNs for cryptocurrency forecasting, especially during extreme volatility. We also conduct chaos analysis through the Lyapunov exponent in chaotic periods and show that our approach outperforms existing machine learning methods by a significant margin. Our findings are consistent with the Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods and excel under high chaos compared to Boosting and Na\"ive methods.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Variance Efficient Collaborative Filtering for Stock Recommendation</title>
<link>https://arxiv.org/abs/2306.06590</link>
<guid>https://arxiv.org/abs/2306.06590</guid>
<content:encoded><![CDATA[
<div> efficient collaborative filtering, stock recommendations, mean-variance, portfolio theory, personalized recommendations 
Summary:
The article introduces a novel mean-variance efficient collaborative filtering (MVECF) model for personalized stock recommendations that consider both user preferences and the risk-return characteristics of stocks. Traditional recommendation methods often overlook user preferences and focus solely on high-return stocks or diversified portfolios. The MVECF model aims to optimize the trade-off between risk and return by incorporating uncertainties in stock prices through regularization techniques. By enhancing the mean-variance efficiency of suggested portfolios, the MVECF model demonstrates improved performance while maintaining high average precision and recall. The model is designed for computational efficiency and can easily integrate with graph-based ranking models, making it a valuable tool for financial services in the era of FinTech. <br /><br />Summary: <div>
arXiv:2306.06590v2 Announce Type: replace-cross 
Abstract: The rise of FinTech has transformed financial services onto online platforms, yet stock investment recommender systems have received limited attention compared to other industries. Personalized stock recommendations can significantly impact customer engagement and satisfaction within the industry. However, traditional investment recommendations focus on high-return stocks or highly diversified portfolios based on the modern portfolio theory, often neglecting user preferences. On the other hand, collaborative filtering (CF) methods also may not be directly applicable to stock recommendations, because it is inappropriate to just recommend stocks that users like. The key is to optimally blend users preference with the portfolio theory. However, research on stock recommendations within the recommender system domain remains comparatively limited, and no existing model considers both the preference of users and the risk-return characteristics of stocks. In this regard, we propose a mean-variance efficient collaborative filtering (MVECF) model for stock recommendations that consider both aspects. Our model is specifically designed to improve the pareto optimality (mean-variance efficiency) in a trade-off between the risk (variance of return) and return (mean return) by systemically handling uncertainties in stock prices. Such improvements are incorporated into the MVECF model using regularization, and the model is restructured to fit into the ordinary matrix factorization scheme to boost computational efficiency. Experiments on real-world fund holdings data show that our model can increase the mean-variance efficiency of suggested portfolios while sacrificing just a small amount of mean average precision and recall. Finally, we further show MVECF is easily applicable to the state-of-the-art graph-based ranking models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tunable Plasmonic Absorption in Metal-Dielectric Multilayers via FDTD Simulations and an Explainable Machine Learning Approach</title>
<link>https://arxiv.org/abs/2508.04014</link>
<guid>https://arxiv.org/abs/2508.04014</guid>
<content:encoded><![CDATA[
<div> plasmonic devices, nanophotonics, machine learning, absorption behavior, multilayer systems
Summary:
- The study combines finite-difference time-domain simulations with machine learning to predict absorbed power behavior in multilayer plasmonic stacks.
- Varying Au and Ag thicknesses across a spectral range, spatial absorption maps and power metrics are generated.
- A multilayer perceptron and convolutional neural network models absorption behavior with high accuracy.
- Plasmonic layer thickness and excitation wavelength are identified as dominant contributors to absorption.
- Gold exhibits broader and sustained absorption compared to silver, with efficiency peaking between 450 and 850 nm.
<br /><br />Summary: <div>
arXiv:2508.04014v1 Announce Type: new 
Abstract: Plasmonic devices, fundamental to modern nanophotonics, exploit resonant interactions between light and free electrons in metals to achieve enhanced light trapping and electromagnetic field confinement. However, modeling their complex, nonlinear optical responses remains computationally intensive. In this work, we combine finite-difference time-domain simulations with machine learning to simulate and predict absorbed power behavior in multilayer plasmonic stacks composed of SiO2, gold, silver, and indium tin oxide. By varying Au and Ag thicknesses (10-50nm) across a spectral range of 300-1500nm, we generate spatial absorption maps and integrated power metrics from full-wave solutions to Maxwell's equations. A multilayer perceptron models global absorption behavior with a mean absolute error of 0.0953, while a convolutional neural network predicts spatial absorption distributions with an MAE of 0.0101. SHapley Additive exPlanations identify plasmonic layer thickness and excitation wavelength as dominant contributors to absorption, which peaks between 450 and 850~nm. Gold demonstrates broader and more sustained absorption compared to silver, although both metals exhibit reduced efficiency outside the resonance window. This integrated FDTD-ML framework offers a fast, explainable, and accurate approach for investigating tunable plasmonic behavior in multilayer systems, with applications in optical sensing, photovoltaics, and nanophotonic device design.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A GPU-Accelerated Three-Dimensional Crack Element Method for Transient Dynamic Fracture Simulation</title>
<link>https://arxiv.org/abs/2508.04076</link>
<guid>https://arxiv.org/abs/2508.04076</guid>
<content:encoded><![CDATA[
<div> Crack Element Method, dynamic crack propagation, quasi-brittle materials, element-splitting algorithm, fracture energy release rate<br />
Summary: <br />
This work introduces a novel three-dimensional Crack Element Method (CEM) for efficiently modeling transient dynamic crack propagation in quasi-brittle materials. The CEM features an advanced element-splitting algorithm that allows for element-wise crack growth and branching. A new formulation for calculating the fracture energy release rate in three dimensions is developed based on the evolving topology of split elements. The proposed 3D CEM is demonstrated to accurately simulate both single crack propagation and complex crack branching scenarios through a series of benchmark examples. Additionally, all three-dimensional simulations are GPU-accelerated, ensuring high levels of computational efficiency, consistency, and accuracy. <div>
arXiv:2508.04076v1 Announce Type: new 
Abstract: This work presents a novel three-dimensional Crack Element Method (CEM) designed to model transient dynamic crack propagation in quasi-brittle materials efficiently. CEM introduces an advanced element-splitting algorithm that enables element-wise crack growth, including crack branching. Based on the evolving topology of split elements, an original formulation for computing the fracture energy release rate in three dimensions is derived. A series of benchmark examples is conducted to demonstrate that the proposed 3D CEM accurately simulates both single crack propagation and complex crack branching scenarios. Furthermore, all three-dimensional simulations are GPU-accelerated, achieving high levels of computational efficiency, consistency, and accuracy.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional autoencoders for the reconstruction of three-dimensional interfacial multiphase flows</title>
<link>https://arxiv.org/abs/2508.04084</link>
<guid>https://arxiv.org/abs/2508.04084</guid>
<content:encoded><![CDATA[
<div> Keywords: autoencoders, reduced-order modeling, multiphase flows, convolutional architecture, interface representation

Summary:<br />
This study explores the use of autoencoders for reduced-order modeling of three-dimensional multiphase flows. The accuracy of reconstructing multiphase flow volume and mass fractions using a standard convolutional architecture is investigated, considering different interface representations such as diffuse, sharp, and level set. The research utilizes synthetic data with complex interface topologies and high-resolution simulation data of multiphase homogeneous isotropic turbulence for training and validation purposes. The findings establish best practices for reducing the dimensionality of multiphase flows with autoencoders, paving the way for separate training of accurate reconstruction and temporal or input/output models on the lower-dimensional latent space. This presents significant implications for the multiphase flow community and beyond, enabling advancements in modeling and understanding complex fluid dynamics efficiently. 

<br /><br />Summary: <div>
arXiv:2508.04084v1 Announce Type: new 
Abstract: In this work, we perform a comprehensive investigation of autoencoders for reduced-order modeling of three-dimensional multiphase flows. Focusing on the accuracy of reconstructing multiphase flow volume/mass fractions with a standard convolutional architecture, we examine the advantages and disadvantages of different interface representation choices (diffuse, sharp, level set). We use a combination of synthetic data with non-trivial interface topologies and high-resolution simulation data of multiphase homogeneous isotropic turbulence for training and validation. This study clarifies the best practices for reducing the dimensionality of multiphase flows via autoencoders. Consequently, this paves the path for uncoupling the training of autoencoders for accurate reconstruction and the training of temporal or input/output models such as neural operators (e.g., FNOs, DeepONets) and neural ODEs on the lower-dimensional latent space given by the autoencoders. As such, the implications of this study are significant and of interest to the multiphase flow community and beyond.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generic Framework for Optimization in Blockchain Simulators</title>
<link>https://arxiv.org/abs/2508.04157</link>
<guid>https://arxiv.org/abs/2508.04157</guid>
<content:encoded><![CDATA[
<div> Keywords: blockchain, simulation, optimization, warm starting technique, concurrent multiprocessing<br />
Summary: 
The paper introduces the Generic Framework for Optimization in Blockchain Simulators (GFOBS), a tool created to standardize and optimize blockchain simulations. GFOBS is designed to support various optimization algorithms, variables, and objectives, catering to a wide range of blockchain research needs. The key contributions of the paper include the development of GFOBS as a versatile tool, an innovative optimization method utilizing warm starting technique, and a novel concurrent multiprocessing technique for simultaneous simulation processes. These advancements aim to enhance the efficiency, replicability, and standardization of blockchain simulation experiments. The authors address the challenge of diverse and non-standardized simulation parameters that hinder the replicability and comparability of research methodologies in the rapidly evolving blockchain technology landscape. GFOBS provides a flexible platform for researchers to conduct blockchain simulations more effectively and efficiently. 

<br /><br />Summary: <div>
arXiv:2508.04157v1 Announce Type: new 
Abstract: As blockchain technology rapidly evolves, researchers face a significant challenge due to diverse and non-standardized simulation parameters, which hinder the replicability and comparability of research methodologies. This paper introduces a Generic Framework for Optimization in Blockchain Simulators (GFOBS), a comprehensive and adaptable solution designed to standardize and optimize blockchain simulations. GFOBS provides a flexible platform that supports various optimization algorithms, variables, and objectives, thereby catering to a wide range of blockchain research needs. The paper's key contributions are threefold: the development of GFOBS as a versatile tool for blockchain simulation optimization; the introduction of an innovative optimization method using warm starting technique; and the proposition of a novel concurrent multiprocessing technique for simultaneous simulation processes. These advancements collectively enhance the efficiency, replicability, and standardization of blockchain simulation experiments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Method-Based Reasoning for Large Language Models: Extraction, Reuse, and Continuous Improvement</title>
<link>https://arxiv.org/abs/2508.04289</link>
<guid>https://arxiv.org/abs/2508.04289</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, reasoning process, method-based model, continual learning, logical consistency

Summary: 
Large language models (LLMs) have shown impressive capabilities in various language tasks but are limited by their reliance on statistical patterns. To address this, a method-based model is proposed, augmenting LLMs with explicit procedures extracted from training data, responses, and user interactions. These methods are stored externally, ranked based on feedback, and retrieved to guide the LLM's response to new queries. The model enables continual learning, method reuse, and logical consistency beyond token prediction. Experimental results show improved factual verification and generalization in complex prompts. Furthermore, newly learned methods can surpass earlier ones through user-driven refinement. The approach shows promise in enhancing the reasoning abilities of LLMs by incorporating reusable procedures to handle novel problems and improve logical reasoning. 

<br /><br />Summary: <div>
arXiv:2508.04289v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive capabilities across a wide range of language tasks. However, their reasoning process is primarily guided by statistical patterns in training data, which limits their ability to handle novel problems and perform consistent logical reasoning. In this paper, we propose a method-based model that enhances LLMs with explicit, reusable procedures extracted from training content, generated responses, and user interactions. Each method is represented as a pair consisting of a problem and its corresponding solution, stored externally and ranked based on feedback. When a new query is received, the system retrieves and applies the most relevant methods to guide the LLM's response. Our model enables continual learning, method reuse, and logical consistency beyond next-token prediction. Experimental results demonstrate that the system improves factual verification and generalization in complex prompts, and that newly learned methods can outperform earlier ones through user-driven refinement.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extreme Event Precursor Prediction in Turbulent Dynamical Systems via CNN-Augmented Recurrence Analysis</title>
<link>https://arxiv.org/abs/2508.04301</link>
<guid>https://arxiv.org/abs/2508.04301</guid>
<content:encoded><![CDATA[
<div> framework, predict, precursors, extreme events, turbulent dynamical systems 

Summary:
- A general framework is presented to predict precursors to extreme events in turbulent dynamical systems.
- The approach combines phase-space reconstruction techniques with recurrence matrices and convolutional neural networks.
- Three testbed systems were evaluated: a triad turbulent interaction model, a stochastic anisotropic turbulent flow, and the Kolmogorov flow.
- The method offers a threshold-free classification strategy, efficient training with a small number of recurrence matrices, and generalizability to unseen systems.
- Results show robust predictive performance with detection rates of 96% for the triad model, 96% for the anisotropic turbulent flow, and 93% for the Kolmogorov flow, with varying mean lead times. 

<br /><br />Summary: <div>
arXiv:2508.04301v1 Announce Type: new 
Abstract: We present a general framework to predict precursors to extreme events in turbulent dynamical systems. The approach combines phase-space reconstruction techniques with recurrence matrices and convolutional neural networks to identify precursors to extreme events. We evaluate the framework across three distinct testbed systems: a triad turbulent interaction model, a prototype stochastic anisotropic turbulent flow, and the Kolmogorov flow. This method offers three key advantages: (1) a threshold-free classification strategy that eliminates subjective parameter tuning, (2) efficient training using only $\mathcal{O}(100)$ recurrence matrices, and (3) ability to generalize to unseen systems. The results demonstrate robust predictive performance across all test systems: 96\% detection rate for the triad model with a mean lead time of 1.8 time units, 96\% for the anisotropic turbulent flow with a mean lead time of 6.1 time units, and 93\% for the Kolmogorov flow with a mean lead time of 22.7 units.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation</title>
<link>https://arxiv.org/abs/2508.04306</link>
<guid>https://arxiv.org/abs/2508.04306</guid>
<content:encoded><![CDATA[
<div> Keywords: literature reviews, large language models, automated systems, Multi-Agent Taskforce Collaboration, benchmark dataset 

Summary: 
The article discusses the importance of literature reviews in scientific research and the role of large language models (LLMs) in automating the literature review process. The Multi-Agent Taskforce Collaboration (MATC) framework is proposed to address challenges of compounding errors in the review workflow. MATC consists of a manager agent and four executor agents for different tasks. Three collaboration paradigms are introduced to organize agents effectively and mitigate errors. Experimental results show that MATC outperforms existing benchmarks and a new dataset with diverse topics is introduced for literature review generation. The framework aims to improve the faithfulness and quality of automated literature reviews. <div>
arXiv:2508.04306v1 Announce Type: new 
Abstract: Literature reviews play an important role in scientific research. Recent advances in large language models (LLMs) have boosted the development of automated systems for the entire literature review workflow, from retrieval to manuscript drafting. However, a key challenge is that mistakes made in early stages can propagate and amplify in subsequent steps, leading to compounding errors that undermine the faithfulness of the final review. To tackle this issue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework, which consists of a manager agent and four executor agents for literature searching, outline generation, fact localization, and manuscript drafting. We propose three novel collaboration paradigms, forming exploration, exploitation, and experience taskforces, to effectively organize agents and mitigate compounding errors both between and within executor agents. Experimental results show that MATC achieves state-of-the-art performance on existing benchmarks. We further propose a new benchmark dataset featuring more diverse topics for faithful literature review generation.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing Large Language Models with PCA Without Performance Loss</title>
<link>https://arxiv.org/abs/2508.04307</link>
<guid>https://arxiv.org/abs/2508.04307</guid>
<content:encoded><![CDATA[
<div> Keywords: Principal Component Analysis, extreme compression, neural models, transformer, token sequences

Summary:<br />
- The study demonstrates that applying Principal Component Analysis (PCA) in a structured manner to polar-transformed images or token sequences allows for extreme compression of neural models while maintaining performance.
- A one-layer classifier trained on PCA-compressed polar MNIST achieves over 98 percent accuracy with only 840 parameters.
- A two-layer transformer utilizing 70-dimensional PCA-reduced MiniLM embeddings achieves 76.62 percent accuracy on the 20 Newsgroups dataset with just 81000 parameters.
- A decoder-only transformer generates coherent token sequences from 70-dimensional PCA embeddings, preserving over 97 percent cosine similarity with full MiniLM representations while using less than 17 percent of the parameter count of GPT-2.
- These findings underscore the effectiveness of PCA-based input compression as a strategy to align model capacity with information content, facilitating the development of lightweight architectures across various modalities.

<br /><br />Summary: <div>
arXiv:2508.04307v1 Announce Type: new 
Abstract: We demonstrate that Principal Component Analysis (PCA), when applied in a structured manner, either to polar-transformed images or segment-wise to token sequences, enables extreme compression of neural models without sacrificing performance. Across three case studies, we show that a one-layer classifier trained on PCA-compressed polar MNIST achieves over 98 percent accuracy using only 840 parameters. A two-layer transformer trained on 70-dimensional PCA-reduced MiniLM embeddings reaches 76.62 percent accuracy on the 20 Newsgroups dataset with just 81000 parameters. A decoder-only transformer generates coherent token sequences from 70-dimensional PCA embeddings while preserving over 97 percent cosine similarity with full MiniLM representations, using less than 17 percent of the parameter count of GPT-2. These results highlight PCA-based input compression as a general and effective strategy for aligning model capacity with information content, enabling lightweight architectures across multiple modalities.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Simulation and Experiment: A Self-Supervised Domain Adaptation Framework for Concrete Damage Classification</title>
<link>https://arxiv.org/abs/2508.04538</link>
<guid>https://arxiv.org/abs/2508.04538</guid>
<content:encoded><![CDATA[
<div> Keywords: concrete degradation, coda wave signals, domain adaptation, ultrasonic wave propagation simulations, neural networks

Summary: 
The study introduces a self-supervised domain adaptation framework for accurate concrete damage classification using coda wave signals. A virtual testing platform is developed for generating large-scale labeled synthetic data to reduce reliance on costly experimental labeling. The framework integrates domain adversarial training, minimum class confusion loss, and the BYOL strategy to bridge the domain gap between simulation and experimental data. Extensive experiments demonstrate notable performance improvements, with an accuracy of 0.7762 and a macro F1 score of 0.7713, outperforming baseline methods and domain adaptation techniques. The framework exhibits high robustness and minimal additional computational cost, showcasing its practical potential for structural health monitoring applications. 

<br /><br />Summary: <div>
arXiv:2508.04538v1 Announce Type: new 
Abstract: Reliable assessment of concrete degradation is critical for ensuring structural safety and longevity of engineering structures. This study proposes a self-supervised domain adaptation framework for robust concrete damage classification using coda wave signals. To support this framework, an advanced virtual testing platform is developed, combining multiscale modeling of concrete degradation with ultrasonic wave propagation simulations. This setup enables the generation of large-scale labeled synthetic data under controlled conditions, reducing the dependency on costly and time-consuming experimental labeling. However, neural networks trained solely on synthetic data often suffer from degraded performance when applied to experimental data due to domain shifts. To bridge this domain gap, the proposed framework integrates domain adversarial training, minimum class confusion loss, and the Bootstrap Your Own Latent (BYOL) strategy. These components work jointly to facilitate effective knowledge transfer from the labeled simulation domain to the unlabeled experimental domain, achieving accurate and reliable damage classification in concrete. Extensive experiments demonstrate that the proposed method achieves notable performance improvements, reaching an accuracy of 0.7762 and a macro F1 score of 0.7713, outperforming both the plain 1D CNN baseline and six representative domain adaptation techniques. Moreover, the method exhibits high robustness across training runs and introduces only minimal additional computational cost. These findings highlight the practical potential of the proposed simulation-driven and label-efficient framework for real-world applications in structural health monitoring.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Stablecoins for Digital Transformation -- Design, Metrics, and Application with Real World Asset Tokenization as a Case Study</title>
<link>https://arxiv.org/abs/2508.02403</link>
<guid>https://arxiv.org/abs/2508.02403</guid>
<content:encoded><![CDATA[
<div> taxonomy, stablecoin systems, performance evaluation framework, digital systems, Real World Asset tokenization

Summary: 
This study addresses the fragmented academic research on stablecoins by providing a unified taxonomy based on custodial structure, stabilization mechanism, and governance. It also introduces a comprehensive performance evaluation framework tailored to diverse stakeholder needs and offers transparency through an open-source benchmarking pipeline. Additionally, a case study on Real World Asset tokenization demonstrates how stablecoins function as programmable monetary infrastructure in cross-border digital systems. By combining conceptual theory with empirical tools, the paper contributes to the development of trusted, inclusive, and transparent digital monetary infrastructure. This research aims to bridge the gap between economics, law, and computer science in the study of stablecoins and provide a solid foundation for future research in this area. <br /><br />Summary: <div>
arXiv:2508.02403v1 Announce Type: cross 
Abstract: Stablecoins have become a foundational component of the digital asset ecosystem, with their market capitalization exceeding 230 billion USD as of May 2025. As fiat-referenced and programmable assets, stablecoins provide low-latency, globally interoperable infrastructure for payments, decentralized finance, DeFi, and tokenized commerce. Their accelerated adoption has prompted extensive regulatory engagement, exemplified by the European Union's Markets in Crypto-assets Regulation, MiCA, the US Guiding and Establishing National Innovation for US Stablecoins Act, GENIUS Act, and Hong Kong's Stablecoins Bill. Despite this momentum, academic research remains fragmented across economics, law, and computer science, lacking a unified framework for design, evaluation, and application. This study addresses that gap through a multi-method research design. First, it synthesizes cross-disciplinary literature to construct a taxonomy of stablecoin systems based on custodial structure, stabilization mechanism, and governance. Second, it develops a performance evaluation framework tailored to diverse stakeholder needs, supported by an open-source benchmarking pipeline to ensure transparency and reproducibility. Third, a case study on Real World Asset tokenization illustrates how stablecoins operate as programmable monetary infrastructure in cross-border digital systems. By integrating conceptual theory with empirical tools, the paper contributes: a unified taxonomy for stablecoin design; a stakeholder-oriented performance evaluation framework; an empirical case linking stablecoins to sectoral transformation; and reproducible methods and datasets to inform future research. These contributions support the development of trusted, inclusive, and transparent digital monetary infrastructure.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification</title>
<link>https://arxiv.org/abs/2508.03750</link>
<guid>https://arxiv.org/abs/2508.03750</guid>
<content:encoded><![CDATA[
<div> Keywords: glaucoma, risk prediction, multimodal, interpretability, XGBoost

Summary: 
GlaBoost is a novel multimodal gradient boosting framework designed for early and accurate detection of glaucoma. It integrates structured clinical features, fundus image embeddings, and textual descriptions for glaucoma risk prediction. GlaBoost utilizes pretrained convolutional encoders for visual representation extraction from retinal fundus photos and transformer-based language models for encoding neuroretinal rim assessments. By combining these heterogeneous signals with manually assessed risk scores and ophthalmic indicators, GlaBoost creates a unified feature space for classification using an enhanced XGBoost model. Experimental results on a real-world dataset show that GlaBoost outperforms baseline models with a validation accuracy of 98.71%. Feature importance analysis highlights the significant contributions of cup-to-disc ratio, rim pallor, and textual embeddings in model decisions. GlaBoost offers a transparent and scalable solution for interpretable glaucoma diagnosis and has the potential for extension to other ophthalmic disorders.<br /><br />Summary: <div>
arXiv:2508.03750v1 Announce Type: cross 
Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible vision loss. However, existing methods often rely on unimodal data and lack interpretability, limiting their clinical utility. In this paper, we present GlaBoost, a multimodal gradient boosting framework that integrates structured clinical features, fundus image embeddings, and expert-curated textual descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual representations from retinal fundus photographs using a pretrained convolutional encoder and encodes free-text neuroretinal rim assessments using a transformer-based language model. These heterogeneous signals, combined with manually assessed risk scores and quantitative ophthalmic indicators, are fused into a unified feature space for classification via an enhanced XGBoost model. Experiments conducted on a real-world annotated dataset demonstrate that GlaBoost significantly outperforms baseline models, achieving a validation accuracy of 98.71%. Feature importance analysis reveals clinically consistent patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings contributing most to model decisions. GlaBoost offers a transparent and scalable solution for interpretable glaucoma diagnosis and can be extended to other ophthalmic disorders.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations</title>
<link>https://arxiv.org/abs/2508.03839</link>
<guid>https://arxiv.org/abs/2508.03839</guid>
<content:encoded><![CDATA[
<div> surrogate model, trainable by parts, parameterized nonlinear PDEs, encoder, latent space

Summary: 
The article introduces a new trainable-by-parts surrogate model for solving forward and inverse parameterized nonlinear partial differential equations. The model consists of an encoder to reduce input dimensionality, a neural network to map to the solution space, and a decoder for reconstruction. The key innovation is the independent training of the model components, leading to decreased time and energy requirements compared to existing models like FNO and DeepONet. The model, named VAE-DNN, is evaluated on solving the nonlinear diffusion equation for groundwater flow, demonstrating higher efficiency and accuracy in both forward and inverse solutions. The separable training approach through a variational autoencoder framework enhances the overall performance of the model. <div>
arXiv:2508.03839v1 Announce Type: cross 
Abstract: We propose a trainable-by-parts surrogate model for solving forward and inverse parameterized nonlinear partial differential equations. Like several other surrogate and operator learning models, the proposed approach employs an encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct $h(\bm{x},t)$. The innovative aspect of our model is its ability to train its three components independently. This approach leads to a substantial decrease in both the time and energy required for training when compared to leading operator learning models such as FNO and DeepONet. The separable training is achieved by training the encoder as part of the variational autoencoder (VAE) for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet models for obtaining forward and inverse solutions to the nonlinear diffusion equation governing groundwater flow in an unconfined aquifer. Our findings indicate that VAE-DNN not only demonstrates greater efficiency but also delivers superior accuracy in both forward and inverse solutions compared to the FNO and DeepONet models.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging</title>
<link>https://arxiv.org/abs/2508.04625</link>
<guid>https://arxiv.org/abs/2508.04625</guid>
<content:encoded><![CDATA[
<div> benchmark, financial reasoning, multimodal, numerical reasoning, large language models

Summary:
- FinMMR is a new benchmark designed to evaluate the reasoning abilities of multimodal large language models in financial numerical tasks.
- It introduces multimodality by incorporating images in addition to text-based questions, covering 14 categories in the financial domain.
- The benchmark is comprehensive, spanning 14 financial subdomains such as corporate finance and banking, surpassing existing benchmarks.
- The challenge lies in requiring models to perform precise numerical reasoning by combining financial knowledge with understanding complex financial images and text.
- The best-performing model achieves 53.0% accuracy on difficult problems, indicating the need for advancement in this area.

<br /><br />Summary: <div>
arXiv:2508.04625v1 Announce Type: cross 
Abstract: We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation</title>
<link>https://arxiv.org/abs/2307.07694</link>
<guid>https://arxiv.org/abs/2307.07694</guid>
<content:encoded><![CDATA[
<div> algorithm, portfolio optimisation, deep reinforcement learning, Kelly criterion, market impact

Summary:
- The study assessed various deep reinforcement learning algorithms for portfolio optimization using simulated data.
- The simulator utilized correlated geometric Brownian motion with the Bertsimas-Lo market impact model to generate data.
- By applying the Kelly criterion as the objective, the optimal policy without market impact was analytically derived, serving as a performance benchmark.
- Off-policy algorithms like DDPG, TD3, and SAC struggled to learn the correct $Q$-function due to noisy rewards, leading to inferior performance.
- On-policy algorithms PPO and A2C, alongside generalised advantage estimation, effectively managed noise and generated policies close to optimal. The clipping variant of PPO was crucial in maintaining policy convergence. In a more complex setting with changing GBM parameters, PPO combined with a hidden Markov model successfully learned and adapted policies to different regimes. However, the algorithms demonstrated high sample complexity, requiring substantial steps for effective learning in real data applications. <br /><br />Summary: <div>
arXiv:2307.07694v3 Announce Type: replace 
Abstract: We evaluate benchmark deep reinforcement learning algorithms on the task of portfolio optimisation using simulated data. The simulator to generate the data is based on correlated geometric Brownian motion with the Bertsimas-Lo market impact model. Using the Kelly criterion (log utility) as the objective, we can analytically derive the optimal policy without market impact as an upper bound to measure performance when including market impact. We find that the off-policy algorithms DDPG, TD3 and SAC are unable to learn the right $Q$-function due to the noisy rewards and therefore perform poorly. The on-policy algorithms PPO and A2C, with the use of generalised advantage estimation, are able to deal with the noise and derive a close to optimal policy. The clipping variant of PPO was found to be important in preventing the policy from deviating from the optimal once converged. In a more challenging environment where we have regime changes in the GBM parameters, we find that PPO, combined with a hidden Markov model to learn and predict the regime context, is able to learn different policies adapted to each regime. Overall, we find that the sample complexity of these algorithms is too high for applications using real data, requiring more than 2m steps to learn a good policy in the simplest setting, which is equivalent to almost 8,000 years of daily prices.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Insight into Eco-Choices: Simulating the Fast Fashion Shift</title>
<link>https://arxiv.org/abs/2407.18814</link>
<guid>https://arxiv.org/abs/2407.18814</guid>
<content:encoded><![CDATA[
<div> Keywords: fashion, fast fashion, Spain, consumer behavior, Agent-Based Modeling

Summary: 
The study focuses on the impact of fast fashion on the environment and society, particularly in Spain. It highlights the detrimental effects of fast fashion, such as waste and human rights abuses, while also acknowledging its economic significance. Through Agent-Based Modeling, the research examines individual decision-making processes in purchasing fast fashion and the influence of awareness on sustainable fashion practices. The study emphasizes the role of government interventions in shaping consumer behavior, with campaigns playing a crucial role in driving progress. However, the success of such interventions is dependent on factors like social media influence and public polarization. The research suggests that a balanced approach by the government, along with targeted social media strategies, can lead to more significant shifts in consumer habits towards sustainable fashion choices. <div>
arXiv:2407.18814v2 Announce Type: replace 
Abstract: Fashion is a powerful force in the modern world. It is one of the most accessible means of self-expression, thereby playing a significant role in our society. Yet, it is plagued by well-documented issues of waste and human rights abuses. Fast fashion in particular, characterized by its disposable nature, contributes extensively to environmental degradation and CO$_2$ emissions, surpassing the combined outputs of France, Germany, and the UK, but its economic contributions have somewhat shielded it from criticism. In this paper, we examine the demand for fast fashion, with a focus on Spain. We explore the individual decision-making process involved in choosing to buy fast fashion and the role of awareness regarding working conditions, environmental consequences, and education on sustainable fashion in influencing consumer behavior. By employing Agent-Based Modeling, we investigate the factors influencing garment consumption patterns and how shifts in public opinion can be achieved through peer pressure, social media influence, and government interventions. Our study revealed that government interventions are pivotal, with the state's campaigns setting the overall tone for progress, although its success is conditioned by social media and polarization levels of the population. Importantly, the state does not need to adopt an extremely proactive stance or continue the campaigns indefinitely to achieve optimal results, as excessive interventions yield diminishing returns.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A CFL condition for the finite cell method</title>
<link>https://arxiv.org/abs/2502.13675</link>
<guid>https://arxiv.org/abs/2502.13675</guid>
<content:encoded><![CDATA[
<div> boundary-conforming mesh generation, finite element methods, critical time step size, explicit time integration, finite cell method
<br />
Summary: 
The study focuses on the finite cell method's effect on the critical time step size for explicit time integration in immersed wave propagation simulations. By analyzing a one-degree-of-freedom model, the influence of α-stabilization on the maximum eigenvalue and critical time step size for corner and sliver cuts was systematically studied. It was found that the critical time step size does not decrease below a limit even as the cut fraction tends to zero, with the lower bound controlled by α. Sliver cuts were identified as more detrimental than corner cuts in higher dimensions. Increasing polynomial degree had minimal impact on degradation. An estimate of the minimum critical time step size as a function of α was derived to propose a modified CFL condition for the finite cell method, validated on a two-dimensional perforated plate example. <div>
arXiv:2502.13675v2 Announce Type: replace 
Abstract: Immersed boundary finite element methods allow the user to bypass the potentially troublesome task of boundary-conforming mesh generation. When combined with explicit time integration, poorly cut elements with little support in the physical domain lead to a severely reduced critical time step size, posing a major challenge for immersed wave propagation simulations. The finite cell method stabilizes cut elements by defining the weak form of the problem also in the fictitious domain, but scaled by a small value $\alpha$. This paper investigates the effect of the finite cell method on the critical time step size for explicit time integration. Starting with an analytical one-degree-of-freedom model, we systematically study the influence of $\alpha$-stabilization on the maximum eigenvalue, and thus on the critical time step size, for corner and sliver cuts. The analysis is complemented by a numerical study of an example with one element and increasing polynomial degree, confirming that the critical time step size does not decrease below a certain limit, even as the cut fraction tends to zero. This lower bound is controlled by the choice of $\alpha$. In higher dimensions, sliver cuts are found to be more detrimental than corner cuts, thus determining the minimum critical time step size. Increasing the polynomial degree has only little effect on this degradation. Based on these observations, we derive an estimate of the minimum critical time step size as a function of $\alpha$, which we use to propose a modified CFL condition for the finite cell method. The validity of this condition is demonstrated on a two-dimensional perforated plate example.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical Errors in Quantitative System Analysis With Decision Diagrams</title>
<link>https://arxiv.org/abs/2508.02673</link>
<guid>https://arxiv.org/abs/2508.02673</guid>
<content:encoded><![CDATA[
<div> DDs, state-space explosion problem, probabilistic systems, quantum systems, floating-point numbers <br />
<br />
Matrix-vector multiplication with multi-terminal binary decision diagrams (MTBDDs) is crucial for computing successor states in probabilistic and quantum systems. This paper delves into the numerical stability of this algorithm, as floating-point computations can introduce errors affecting result correctness and DD compression effectiveness. The study demonstrates that the MTBDD matrix-vector multiplication algorithm can be made numerically stable under specific conditions, though real-world MTBDD implementations often fail to meet these criteria. A case study on quantum circuit simulation reveals varying degrees of numerical errors in practice. The research underscores the importance of addressing numerical stability challenges in DD-based approaches for handling probabilistic and quantum systems effectively. <br /><br />Summary: <div>
arXiv:2508.02673v1 Announce Type: new 
Abstract: Decision diagrams (DDs) are a powerful data structure that is used to tackle the state-space explosion problem, not only for discrete systems, but for probabilistic and quantum systems as well. While many of the DDs used in the probabilistic and quantum domains make use of floating-point numbers, this is not without challenges. Floating-point computations are subject to small rounding errors, which can affect both the correctness of the result and the effectiveness of the DD's compression. In this paper, we investigate the numerical stability, i.e. the robustness of an algorithm to small numerical errors, of matrix-vector multiplication with multi-terminal binary decision diagrams (MTBDDs). Matrix-vector multiplication is of particular interest because it is the function that computes successor states for both probabilistic and quantum systems. We prove that the MTBDD matrix-vector multiplication algorithm can be made numerically stable under certain conditions, although in many practical implementations of MTBDDs these conditions are not met. Additionally, we provide a case study of the numerical errors in the simulation of quantum circuits, which shows that the extent of numerical errors in practice varies greatly between instances.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming the Loss Conditioning Bottleneck in Optimization-Based PDE Solvers: A Novel Well-Conditioned Loss Function</title>
<link>https://arxiv.org/abs/2508.02692</link>
<guid>https://arxiv.org/abs/2508.02692</guid>
<content:encoded><![CDATA[
<div> optimization, PDE solvers, loss function, Stabilized Gradient Residual, convergence

Summary:
The article introduces a new Stabilized Gradient Residual (SGR) loss for optimization-based PDE solvers that aims to address the slow convergence issues associated with the mean squared error (MSE) loss. The SGR loss allows for flexible modulation of the condition number, leading to faster convergence compared to the MSE loss. By systematically benchmarking the performance of the SGR loss in both the ODIL and PINNs frameworks, the study demonstrates significant improvements in convergence speed and optimization stability. The SGR loss outperforms the MSE loss in various numerical experiments on benchmark problems within the ODIL framework and shows consistent better performance within the PINNs framework despite high nonlinearity. These findings emphasize the importance of loss conditioning in the design of more efficient PDE solvers, bridging the performance gap between classical iterative solvers and optimization-based solvers. <br /><br />Summary: <div>
arXiv:2508.02692v1 Announce Type: new 
Abstract: Optimization-based PDE solvers that minimize scalar loss functions have gained increasing attention in recent years. These methods either define the loss directly over discrete variables, as in Optimizing a Discrete Loss (ODIL), or indirectly through a neural network surrogate, as in Physics-Informed Neural Networks (PINNs). However, despite their promise, such methods often converge much more slowly than classical iterative solvers and are commonly regarded as inefficient. This work provides a theoretical insight, attributing the inefficiency to the use of the mean squared error (MSE) loss, which implicitly forms the normal equations, squares the condition number, and severely impairs optimization. To address this, we propose a novel Stabilized Gradient Residual (SGR) loss. By tuning a weight parameter, it flexibly modulates the condition number between the original system and its normal equations, while reducing to the MSE loss in the limiting case. We systematically benchmark the convergence behavior and optimization stability of the SGR loss within both the ODIL framework and PINNs-employing either numerical or automatic differentiation-and compare its performance against classical iterative solvers. Numerical experiments on a range of benchmark problems demonstrate that, within the ODIL framework, the proposed SGR loss achieves orders-of-magnitude faster convergence than the MSE loss. Further validation within the PINNs framework shows that, despite the high nonlinearity of neural networks, SGR consistently outperforms the MSE loss. These theoretical and empirical findings help bridge the performance gap between classical iterative solvers and optimization-based solvers, highlighting the central role of loss conditioning, and provide key insights for the design of more efficient PDE solvers.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using numerical-experimental analysis to evaluate rPET mechanical behavior under compressive stresses and FFF additive manufacturing for new sustainable designs</title>
<link>https://arxiv.org/abs/2508.02728</link>
<guid>https://arxiv.org/abs/2508.02728</guid>
<content:encoded><![CDATA[
<div> Keywords: recycled polymer, compressive stresses, mechanical behavior modeling, sustainable design, numerical-experimental study

Summary:<br />
The study focuses on investigating the mechanical behavior modeling of recycled polyethylene terephthalate (rPET) manufactured using a deposition FFF process under compressive stresses for sustainable designs. Experimental tests revealed that rPET behaves linearly until the elastic limit along manufacturing axes. Numerical analyses based on experimental data validated the design's structural safety and confirmed rPET could be configured as isotropic in simulation software without material modeling modifications. The results support the use of recycled rPET for sustainable product production using MEX technology under compressive stress. Major design companies are now incorporating recycled plastic materials in their designs. The validation results, presented through experimental testing and numerical simulations, demonstrate the feasibility and efficacy of using recycled rPET in real-world applications.<br /><br />Summary: <div>
arXiv:2508.02728v1 Announce Type: new 
Abstract: The purpose of this study is to investigate the numerical-experimental mechanical behavior modeling of the recycled polymer, that is, recyclable polyethylene terephthalate (rPET), manufactured by a deposition FFF process under compressive stresses for new sustainable designs. In all, 42 test specimens were manufactured and analyzed according to the ASTM D695-15 standards. Eight numerical analyzes were performed on a real design manufactured with rPET using Young's compression modulus from the experimental tests. Finally, eight additional experimental tests under uniaxial compression loads were performed on the real sustainable design for validating its mechanical behavior versus computational numerical tests. As a result of the experimental tests, rPET behaves linearly until it reaches the elastic limit, along each manufacturing axis. The results of this study confirmed the design's structural safety by the load scenario and operating boundary conditions. Experimental and numerical results show a difference of 0.001-0.024 mm, allowing for the rPET to be configured as isotropic in numerical simulation software without having to modify its material modeling equations. The results obtained are of great help to industry, designers and researchers because they validate the use of recycled rPET for the ecological production of real-sustainable products using MEX technology under compressive stress and its configuration for numerical simulations. Major design companies are now using recycled plastic materials in their high-end designs. Validation results have been presented on test specimens and real items, comparing experimental material configuration values with numerical results.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fluid--peridynamic structure model of deformation and damage of microchannels</title>
<link>https://arxiv.org/abs/2508.02875</link>
<guid>https://arxiv.org/abs/2508.02875</guid>
<content:encoded><![CDATA[
<div> fluid-structure interaction, microchannels, peridynamic formulation, failure scenarios, wave propagation <br />
<br />
Summary: 
This study investigates fluid-structure interaction in soft-walled microchannels, applying a nonlocal mechanical theory to model the compliant top wall's behavior, including potential failure scenarios. A computational model coupling viscous flow and a peridynamic Euler-Bernoulli beam formulation is developed to analyze steady and time-dependent responses. Through dispersion analysis, the study reveals that increasing nonlocal influence leads to a gradual suppression of phase velocity in wave propagation. The research identifies a dividing curve in a parameter space, based on the Strouhal number and compliance number, distinguishing potential failure scenarios during transient and steady loads. This work lays the foundation for understanding and predicting failure modes in soft-walled microchannels under hydrodynamic forces. <br /> <div>
arXiv:2508.02875v1 Announce Type: new 
Abstract: Soft-walled microchannels arise in many applications, ranging from organ-on-a-chip platforms to soft-robotic actuators. However, despite extensive research on their static and dynamic response, the potential failure of these devices has not been addressed. To this end, we explore fluid--structure interaction in microchannels whose compliant top wall is governed by a nonlocal mechanical theory capable of simulating both deformation and material failure. We develop a one-dimensional model by coupling viscous flow under the lubrication approximation to a state-based peridynamic formulation of an Euler--Bernoulli beam. The peridynamic formulation enables the wall to be modeled as a genuinely nonlocal beam, and the integral form of its equation of motion remains valid whether the deformation field is smooth or contains discontinuities. Through the proposed computational model, we explore the steady and time-dependent behaviors of this fluid--peridynamic structure interaction. We rationalize the wave and damping dynamics observed in the simulations through a dispersion (linearized) analysis of the coupled system, finding that, with increasing nonlocal influence, wave propagation exhibits a clear departure from classical behavior, characterized by a gradual suppression of the phase velocity. The main contribution of our study is to outline the potential failure scenarios of the microchannel's soft wall under the hydrodynamic load of the flow. Specifically, we find a dividing curve in the space spanned by the dimensionless Strouhal number (quantifying unsteady inertia of the beam) and the compliance number (quantifying the strength of the fluid--structure coupling) separating scenarios of potential failure during transient conditions from potential failure at the steady load.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closed-Loop Multi-Agent Framework for Aerodynamics-Aware Automotive Styling Design</title>
<link>https://arxiv.org/abs/2508.03370</link>
<guid>https://arxiv.org/abs/2508.03370</guid>
<content:encoded><![CDATA[
<div> Keywords: automotive design, aerodynamic performance, multi-agent framework, concept generation, rapid engineering validation<br />
Summary:<br />
The article introduces a new approach to automotive exterior design that combines aesthetics with aerodynamic performance using a multi-agent framework driven by LLM. The framework automates the workflow from ambiguous requirements to 3D concept model validation in two stages: conceptual generation and performance validation. In the conceptual generation stage, agents collaborate to interpret design requirements and produce concept sketches and renderings using diffusion models. The renderings are then converted to 3D point clouds for rapid validation using a Drag Prediction Agent based on a lightweight surrogate model. This approach seamlessly integrates creative exploration with engineering validation in an automated system, offering a new paradigm for balancing design creativity with engineering constraints in the early stages of automotive design. <br /><br />Summary: <div>
arXiv:2508.03370v1 Announce Type: new 
Abstract: The core challenge in automotive exterior design is balancing subjective aesthetics with objective aerodynamic performance while dramatically accelerating the development cycle. To address this, we propose a novel, LLM-driven multi-agent framework that automates the end-to-end workflow from ambiguous requirements to 3D concept model performance validation. The workflow is structured in two stages: conceptual generation and performance validation. In the first stage, agents collaborate to interpret fuzzy design requirements, generate concept sketches, and produce photorealistic renderings using diffusion models. In the second stage, the renderings are converted to 3D point clouds, where a Drag Prediction Agent, built upon a lightweight surrogate model, provides near-instantaneous predictions of the drag coefficient and pressure fields, replacing time-consuming CFD simulations. The primary contribution of this work is the seamless integration of creative generation with a rapid engineering validation loop within a unified, automated system, which provides a new paradigm for efficiently balancing creative exploration with engineering constraints in the earliest stages of design.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Incentivize: LLM-Empowered Contract for AIGC Offloading in Teleoperation</title>
<link>https://arxiv.org/abs/2508.03464</link>
<guid>https://arxiv.org/abs/2508.03464</guid>
<content:encoded><![CDATA[
<div> service providers, incentive mechanisms, AI-generated content, online learning, contract design
<br />
Summary:
This paper addresses the challenge of designing incentive mechanisms for edge AI-generated content service providers (ASPs) with information asymmetry. The study focuses on bonus design between a teleoperator and an ASP, where the teleoperator cannot observe the ASP's private settings and actions. The problem is formulated as an online learning contract design issue, divided into ASP's settings inference and contract derivation subproblems. A large language model (LLM)-empowered framework is introduced to tackle the NP-hard setting-inference problem. By leveraging the LLM's domain expertise, the framework refines a seed solver iteratively. Subsequently, the contract derivation problem is addressed using convex optimization techniques to obtain a near-optimal contract. Simulation results on a Unity-based teleoperation platform demonstrate that the proposed method significantly increases the teleoperator's utility compared to benchmarks, while maintaining positive incentives for the ASP. The code for this study is available on GitHub for further exploration. 
 <div>
arXiv:2508.03464v1 Announce Type: new 
Abstract: With the rapid growth in demand for AI-generated content (AIGC), edge AIGC service providers (ASPs) have become indispensable. However, designing incentive mechanisms that motivate ASPs to deliver high-quality AIGC services remains a challenge, especially in the presence of information asymmetry. In this paper, we address bonus design between a teleoperator and an edge ASP when the teleoperator cannot observe the ASP's private settings and chosen actions (diffusion steps). We formulate this as an online learning contract design problem and decompose it into two subproblems: ASP's settings inference and contract derivation. To tackle the NP-hard setting-inference subproblem with unknown variable sizes, we introduce a large language model (LLM)-empowered framework that iteratively refines a naive seed solver using the LLM's domain expertise. Upon obtaining the solution from the LLM-evolved solver, we directly address the contract derivation problem using convex optimization techniques and obtain a near-optimal contract. Simulation results on our Unity-based teleoperation platform show that our method boosts the teleoperator's utility by $5 \sim 40\%$ compared to benchmarks, while preserving positive incentives for the ASP. The code is available at https://github.com/Zijun0819/llm4contract.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Dynamically Changing Domains</title>
<link>https://arxiv.org/abs/2508.02697</link>
<guid>https://arxiv.org/abs/2508.02697</guid>
<content:encoded><![CDATA[
<div> planning, first-order logic, bounded planning, sequential generalized planning, conformant planning
Summary:
In the article, the authors challenge the Domain Closure Assumption commonly made in classical and conformant planning by introducing a dynamic object set that can change during actions. They formulate the planning problem in first-order logic, considering a finite consistent set of fluent literals as the initial theory. By imposing a finite integer bound on plan length and organizing search over grounded action sequences, they ensure soundness and completeness in solving bounded planning problems without DCA. Their approach combines elements of sequential generalized planning and conformant planning, without the use of disjunction over fluent literals. A proof-of-concept implementation of the planner is discussed, showcasing its practical application in scenarios where object sets evolve dynamically. <div>
arXiv:2508.02697v1 Announce Type: cross 
Abstract: In classical planning and conformant planning, it is assumed that there are finitely many named objects given in advance, and only they can participate in actions and in fluents. This is the Domain Closure Assumption (DCA). However, there are practical planning problems where the set of objects changes dynamically as actions are performed; e.g., new objects can be created, old objects can be destroyed. We formulate the planning problem in first-order logic, assume an initial theory is a finite consistent set of fluent literals, discuss when this guarantees that in every situation there are only finitely many possible actions, impose a finite integer bound on the length of the plan, and propose to organize search over sequences of actions that are grounded at planning time. We show the soundness and completeness of our approach. It can be used to solve the bounded planning problems without DCA that belong to the intersection of sequential generalized planning (without sensing actions) and conformant planning, restricted to the case without the disjunction over fluent literals. We discuss a proof-of-the-concept implementation of our planner.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model</title>
<link>https://arxiv.org/abs/2508.02734</link>
<guid>https://arxiv.org/abs/2508.02734</guid>
<content:encoded><![CDATA[
<div> Keywords: Location-Based Service, human mobility, activity sequences, Variable Selection Network, Insertion Transformer

Summary: 
The study addresses the challenge of incomplete trip and activity sequences in Location-Based Service (LBS) data, proposing a novel solution named Variable Selection Network-fused Insertion Transformer (VSNIT). VSNIT combines the flexibility of the Insertion Transformer with the dynamic covariate handling capability of the Variable Selection Network to recover missing segments in activity sequences at the individual level. Results show that VSNIT generates more diverse and realistic activity patterns, effectively restoring disrupted activity transitions. Compared to baseline models, VSNIT outperforms in accuracy and diversity metrics, showcasing its potential to enhance LBS data utility for mobility analysis. The proposed approach offers a promising framework for future research and applications in location-based studies.<br /><br />Summary: <div>
arXiv:2508.02734v1 Announce Type: cross 
Abstract: Location-Based Service (LBS) data provides critical insights into human mobility, yet its sparsity often yields incomplete trip and activity sequences, making accurate inferences about trips and activities difficult. We raise a research problem: Can we use activity sequences derived from high-quality LBS data to recover incomplete activity sequences at the individual level? This study proposes a new solution, the Variable Selection Network-fused Insertion Transformer (VSNIT), integrating the Insertion Transformer's flexible sequence construction with the Variable Selection Network's dynamic covariate handling capability, to recover missing segments in incomplete activity sequences while preserving existing data. The findings show that VSNIT inserts more diverse, realistic activity patterns, more closely matching real-world variability, and restores disrupted activity transitions more effectively aligning with the target. It also performs significantly better than the baseline model across all metrics. These results highlight VSNIT's superior accuracy and diversity in activity sequence recovery tasks, demonstrating its potential to enhance LBS data utility for mobility analysis. This approach offers a promising framework for future location-based research and applications.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreditARF: A Framework for Corporate Credit Rating with Annual Report and Financial Feature Integration</title>
<link>https://arxiv.org/abs/2508.02738</link>
<guid>https://arxiv.org/abs/2508.02738</guid>
<content:encoded><![CDATA[
<div> Keywords: corporate credit rating, FinBERT, annual reports, non-financial data, dataset

Summary:
Corporate credit rating is essential for the market economy, maintaining economic order. This paper introduces a framework that combines financial data with features extracted from annual reports using FinBERT. It aims to leverage the value of unstructured text data typically overlooked in credit rating models. The Comprehensive Corporate Rating Dataset (CCRD) incorporates both financial and textual data, enhancing the accuracy of rating predictions by 8-12%. By integrating non-financial data into the credit rating process, the method proposed in this study improves the effectiveness and reliability of corporate credit ratings. <div>
arXiv:2508.02738v1 Announce Type: cross 
Abstract: Corporate credit rating serves as a crucial intermediary service in the market economy, playing a key role in maintaining economic order. Existing credit rating models rely on financial metrics and deep learning. However, they often overlook insights from non-financial data, such as corporate annual reports. To address this, this paper introduces a corporate credit rating framework that integrates financial data with features extracted from annual reports using FinBERT, aiming to fully leverage the potential value of unstructured text data. In addition, we have developed a large-scale dataset, the Comprehensive Corporate Rating Dataset (CCRD), which combines both traditional financial data and textual data from annual reports. The experimental results show that the proposed method improves the accuracy of the rating predictions by 8-12%, significantly improving the effectiveness and reliability of corporate credit ratings.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTBench: Cryptocurrency Time Series Generation Benchmark</title>
<link>https://arxiv.org/abs/2508.02758</link>
<guid>https://arxiv.org/abs/2508.02758</guid>
<content:encoded><![CDATA[
<div> benchmark, cryptocurrency, time series generation, trading, forecasting

Summary:
The article introduces CTBench, a Time Series Generation (TSG) benchmark specifically tailored for cryptocurrency markets. It addresses the limitations of existing TSG methods by considering the unique characteristics of crypto trading, such as 24/7 trading, extreme volatility, and rapid regime shifts. CTBench evaluates TSG models across 13 metrics in key dimensions including forecasting accuracy, rank fidelity, trading performance, risk assessment, and computational efficiency. A dual-task evaluation framework is introduced, measuring both Predictive Utility for forecasting and Statistical Arbitrage for trading signals. The benchmark analyzes eight models from five methodological families across four market regimes, revealing trade-offs between statistical fidelity and real-world profitability. CTBench provides actionable guidance for selecting and deploying TSG models in cryptocurrency analytics and strategy development. <br /><br />Summary: <div>
arXiv:2508.02758v1 Announce Type: cross 
Abstract: Synthetic time series are essential tools for data augmentation, stress testing, and algorithmic prototyping in quantitative finance. However, in cryptocurrency markets, characterized by 24/7 trading, extreme volatility, and rapid regime shifts, existing Time Series Generation (TSG) methods and benchmarks often fall short, jeopardizing practical utility. Most prior work (1) targets non-financial or traditional financial domains, (2) focuses narrowly on classification and forecasting while neglecting crypto-specific complexities, and (3) lacks critical financial evaluations, particularly for trading applications. To address these gaps, we introduce \textsf{CTBench}, the first comprehensive TSG benchmark tailored for the cryptocurrency domain. \textsf{CTBench} curates an open-source dataset from 452 tokens and evaluates TSG models across 13 metrics spanning 5 key dimensions: forecasting accuracy, rank fidelity, trading performance, risk assessment, and computational efficiency. A key innovation is a dual-task evaluation framework: (1) the \emph{Predictive Utility} task measures how well synthetic data preserves temporal and cross-sectional patterns for forecasting, while (2) the \emph{Statistical Arbitrage} task assesses whether reconstructed series support mean-reverting signals for trading. We benchmark eight representative models from five methodological families over four distinct market regimes, uncovering trade-offs between statistical fidelity and real-world profitability. Notably, \textsf{CTBench} offers model ranking analysis and actionable guidance for selecting and deploying TSG models in crypto analytics and strategy development.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal wall pressure forecast of a rectangular cylinder with physics-aware DeepUFNet</title>
<link>https://arxiv.org/abs/2508.03183</link>
<guid>https://arxiv.org/abs/2508.03183</guid>
<content:encoded><![CDATA[
<div> deep learning, wall pressure, spatiotemporal, Fourier neural network, forecasting

Summary:
The study introduces the DeepUFNet model to predict spatiotemporal wall pressure generated by fluid flow past a rectangular cylinder. The model combines UNet structure and Fourier neural network while embedding physical high-frequency loss control in model training. Wind tunnel testing provides data for training and testing the DeepUFNet model, demonstrating its accurate forecast of wall pressure information. Results show agreement with experimental data in statistical information, temporal pressure variation, spatial distribution, and spatiotemporal correlation. Incorporating a physical high-frequency loss control coefficient improves the model's performance, particularly in forecasting high-order frequency fluctuation and wall pressure variance. Additionally, the model exhibits a satisfactory extrapolation capability, even with sparse spatial information input. <div>
arXiv:2508.03183v1 Announce Type: cross 
Abstract: The wall pressure is of great importance in understanding the forces and structural responses induced by fluid. Recent works have investigated the potential of deep learning techniques in predicting mean pressure coefficients and fluctuating pressure coefficients, but most of existing deep learning frameworks are limited to predicting a single snapshot using full spatial information. To forecast spatiotemporal wall pressure of flow past a rectangular cylinder, this study develops a physics-aware DeepU-Fourier neural Network (DeepUFNet) deep learning model. DeepUFNet comprises the UNet structure and the Fourier neural network, with physical high-frequency loss control embedded in the model training stage to optimize model performance, where the parameter $\beta$ varies with the development of the training epoch. Wind tunnel testing is performed to collect wall pressures of a two-dimensional rectangular cylinder with a side ratio of 1.5 at an angle of attack of zero using high-frequency pressure scanning, thereby constructing a database for DeepUFNet training and testing. The DeepUFNet model is found to forecast spatiotemporal wall pressure information with high accuracy. The comparison between forecast results and experimental data presents agreement in statistical information, temporal pressure variation, power spectrum density, spatial distribution, and spatiotemporal correlation. It is also found that embedding a physical high-frequency loss control coefficient $\beta$ in the DeepUFNet model can significantly improve model performance in forecasting spatiotemporal wall pressure information, in particular, in forecasting high-order frequency fluctuation and wall pressure variance. Furthermore, the DeepUFNet extrapolation capability is tested with sparse spatial information input, and the model presents a satisfactory extrapolation ability
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A neural network machine-learning approach for characterising hydrogen trapping parameters from TDS experiments</title>
<link>https://arxiv.org/abs/2508.03371</link>
<guid>https://arxiv.org/abs/2508.03371</guid>
<content:encoded><![CDATA[
<div> machine learning, trapping behaviour, thermal desorption spectroscopy, parameter identification, neural network

Summary:
This study presents a novel approach to analyze the hydrogen trapping behavior of metallic alloys using machine learning and Thermal Desorption Spectroscopy (TDS). Traditional methods struggle to extract key trapping parameters accurately, but this work introduces a multi-Neural Network (NN) model trained on synthetic data to predict these parameters directly from experimental TDS spectra. The model consists of two NNs - a classification model to predict trap types and a regression model to determine trap densities and binding energies. Through optimization of architecture, hyperparameters, and data pre-processing, the model demonstrated high predictive accuracy when applied to three tempered martensitic steels with different compositions. The code developed for this model is openly available for use. This innovative approach shows promise in enhancing the efficiency and accuracy of hydrogen trapping analysis in metallic alloys. 

Summary: <div>
arXiv:2508.03371v1 Announce Type: cross 
Abstract: The hydrogen trapping behaviour of metallic alloys is generally characterised using Thermal Desorption Spectroscopy (TDS). However, as an indirect method, extracting key parameters (trap binding energies and densities) remains a significant challenge. To address these limitations, this work introduces a machine learning-based scheme for parameter identification from TDS spectra. A multi-Neural Network (NN) model is developed and trained exclusively on synthetic data to predict trapping parameters directly from experimental data. The model comprises two multi-layer, fully connected, feed-forward NNs trained with backpropagation. The first network (classification model) predicts the number of distinct trap types. The second network (regression model) then predicts the corresponding trap densities and binding energies. The NN architectures, hyperparameters, and data pre-processing were optimised to minimise the amount of training data. The proposed model demonstrated strong predictive capabilities when applied to three tempered martensitic steels of different compositions. The code developed is freely provided.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Memory Non-Binary LDPC Decoding</title>
<link>https://arxiv.org/abs/2508.03567</link>
<guid>https://arxiv.org/abs/2508.03567</guid>
<content:encoded><![CDATA[
<div> near-memory non-binary LDPC decoders, PiM paradigm, parallel processing, data movement bottleneck, UPMEM system<br />
<br />
Summary:
This paper discusses the challenge of data movement bottleneck in parallel processing systems when using low-density parity-check (LDPC) codes for error correction. The processing in-memory (PiM) paradigm is proposed as a solution, focusing on near-memory non-binary LDPC decoders in the UPMEM system. The study introduces a novel efficient solution for PiM-based non-binary LDPC decoders, benchmarked against low-power GPU parallel solutions. The results show that PiM-based non-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, proving to be competitive even compared to edge GPUs. This research highlights the importance of addressing the data movement bottleneck in parallel processing systems and demonstrates the effectiveness of PiM-based solutions for LDPC decoding. <br /><br />Summary: <div>
arXiv:2508.03567v1 Announce Type: cross 
Abstract: Low-density parity-check (LDPC) codes are an important feature of several communication and storage applications, offering a flexible and effective method for error correction. These codes are computationally complex and require the exploitation of parallel processing to meet real-time constraints. As advancements in arithmetic and logic unit technology allowed for higher performance of computing systems, memory technology has not kept the same pace of development, creating a data movement bottleneck and affecting parallel processing systems more dramatically. To alleviate the severity of this bottleneck, several solutions have been proposed, namely the processing in-memory (PiM) paradigm that involves the design of compute units to where (or near) the data is stored, utilizing thousands of low-complexity processing units to perform out bit-wise and simple arithmetic operations. This paper presents a novel efficient solution for near-memory non-binary LDPC decoders in the UPMEM system, for the best of our knowledge the first real hardware PiM-based non-binary LDPC decoder that is benchmarked against low-power GPU parallel solutions highly optimized for throughput performance. PiM-based non-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, which is even competitive when compared against implementations running in edge GPUs.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA</title>
<link>https://arxiv.org/abs/2508.03590</link>
<guid>https://arxiv.org/abs/2508.03590</guid>
<content:encoded><![CDATA[
<div> Keywords: solar irradiance forecasting, artificial intelligence, high resolution, data assimilation, net-zero energy systems

Summary:
SolarSeer is introduced as an end-to-end artificial intelligence model for accurate solar irradiance forecasting in the Contiguous United States (CONUS). By directly mapping historical satellite observations to future forecasts, SolarSeer eliminates the need for computationally intensive data assimilation and solving complex partial differential equations, making it over 1,500 times faster than traditional numerical weather prediction models. With a resolution of 5-kilometers, SolarSeer outperforms the state-of-the-art High-Resolution Rapid Refresh (HRRR) model by reducing the root mean squared error of solar irradiance forecasting by 27.28% in reanalysis data and 15.35% across 1,800 stations. SolarSeer also excels in capturing solar irradiance fluctuations and enhances first-order irradiance difference forecasting accuracy. The ultrafast and accurate 24-hour solar irradiance forecasts provided by SolarSeer play a crucial role in supporting the shift towards sustainable, net-zero energy systems.<br /><br />Summary: <div>
arXiv:2508.03590v1 Announce Type: cross 
Abstract: Accurate 24-hour solar irradiance forecasting is essential for the safe and economic operation of solar photovoltaic systems. Traditional numerical weather prediction (NWP) models represent the state-of-the-art in forecasting performance but rely on computationally costly data assimilation and solving complicated partial differential equations (PDEs) that simulate atmospheric physics. Here, we introduce SolarSeer, an end-to-end large artificial intelligence (AI) model for solar irradiance forecasting across the Contiguous United States (CONUS). SolarSeer is designed to directly map the historical satellite observations to future forecasts, eliminating the computational overhead of data assimilation and PDEs solving. This efficiency allows SolarSeer to operate over 1,500 times faster than traditional NWP, generating 24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometer resolution in under 3 seconds. Compared with the state-of-the-art NWP in the CONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantly reduces the root mean squared error of solar irradiance forecasting by 27.28% in reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectively captures solar irradiance fluctuations and significantly enhances the first-order irradiance difference forecasting accuracy. SolarSeer's ultrafast, accurate 24-hour solar irradiance forecasts provide strong support for the transition to sustainable, net-zero energy systems.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements</title>
<link>https://arxiv.org/abs/2502.04592</link>
<guid>https://arxiv.org/abs/2502.04592</guid>
<content:encoded><![CDATA[
<div> Dataset, CAMEF, Multi-modal framework, Causal relationships, Counterfactual event augmentation<br />
<br />
Summary:<br />
The article introduces a new framework called CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting) that combines textual and time-series data to forecast the impact of macroeconomic events on financial markets. CAMEF integrates a causal learning mechanism and an event augmentation technique to capture causal relationships between policy texts and historical price data. A new financial dataset with various macroeconomic releases and high-frequency trading data for U.S. financial assets is introduced for analysis. The framework is compared to transformer-based models and ablation studies confirm the effectiveness of the causal learning mechanism and event types in enhancing financial forecasting accuracy. <div>
arXiv:2502.04592v2 Announce Type: replace-cross 
Abstract: Accurately forecasting the impact of macroeconomic events is critical for investors and policymakers. Salient events like monetary policy decisions and employment reports often trigger market movements by shaping expectations of economic growth and risk, thereby establishing causal relationships between events and market behavior. Existing forecasting methods typically focus either on textual analysis or time-series modeling, but fail to capture the multi-modal nature of financial markets and the causal relationship between events and price movements. To address these gaps, we propose CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a multi-modality framework that effectively integrates textual and time-series data with a causal learning mechanism and an LLM-based counterfactual event augmentation technique for causal-enhanced financial forecasting. Our contributions include: (1) a multi-modal framework that captures causal relationships between policy texts and historical price data; (2) a new financial dataset with six types of macroeconomic releases from 2008 to April 2024, and high-frequency real trading data for five key U.S. financial assets; and (3) an LLM-based counterfactual event augmentation strategy. We compare CAMEF to state-of-the-art transformer-based time-series and multi-modal baselines, and perform ablation studies to validate the effectiveness of the causal learning mechanism and event types.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance Agent Benchmark: Benchmarking LLMs on Real-world Financial Research Tasks</title>
<link>https://arxiv.org/abs/2508.00828</link>
<guid>https://arxiv.org/abs/2508.00828</guid>
<content:encoded><![CDATA[
<div> SEC filings, Finance Agent Benchmark, Large Language Models, AI capabilities, Financial analysis <br />
<br />
Summary: 
The article discusses the use of Large Language Models (LLMs) in financial analysis through the creation of the Finance Agent Benchmark. This benchmark consists of real-world finance research problems that require LLMs to analyze SEC filings. Developed with input from industry experts, the benchmark includes 537 questions spanning nine financial task categories. An agent harness provides LLMs with tools like Google Search and EDGAR database access to generate accurate responses. Evaluation results show that current AI capabilities, with the best model achieving only 46.8% accuracy at a cost of $3.79 per query, have limitations that need to be addressed before reliable deployment in high-stakes finance settings. <div>
arXiv:2508.00828v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) technology has emerged as a transformative force in financial analysis and the finance industry, though significant questions remain about the full capabilities of Large Language Model (LLM) agents in this domain. We present the Finance Agent Benchmark, featuring challenging and diverse real-world finance research problems that require LLMs to perform complex analysis using recent SEC filings. We construct the benchmark using a taxonomy of nine financial task categories, developed in consultation with experts from banks, hedge funds, and private equity firms. The dataset includes 537 expert-authored questions covering tasks from information retrieval to complex financial modeling, each validated through a rigorous review process to ensure accuracy and relevance. Moreover, we implement an agentic harness that equips LLMs with tools sufficient to produce accurate responses, including Google Search and EDGAR database access. Overall, the Finance Agent Benchmark provides a comprehensive testbed for measuring the progress of LLM-driven finance agents. Our evaluation reveals significant limitations in current AI capabilities - even the best-performing model (OpenAI o3) achieved only 46.8% accuracy at an average cost of $3.79 per query. This underscores the need for further advancements before reliable deployment in high-stakes finance settings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bike-Bench: A Bicycle Design Benchmark for Generative Models with Objectives and Constraints</title>
<link>https://arxiv.org/abs/2508.00830</link>
<guid>https://arxiv.org/abs/2508.00830</guid>
<content:encoded><![CDATA[
<div> engineering design benchmark, generative models, multi-objective, constraints, Bike-Bench<br />
Summary:<br />
The article introduces Bike-Bench, an engineering design benchmark for assessing generative models in solving real-world problems with multiple objectives and constraints. Bike-Bench evaluates AI models on their ability to generate designs that meet specific performance goals and restrictions across various domains such as aerodynamics, ergonomics, and human usability. The benchmark includes datasets of simulation results, human-rated bicycle assessments, and a large dataset of design variations. Results from experiments show that Language Models (LLMs) and tabular generative models do not perform as well as optimization-based models in terms of validity and optimality scores. This highlights the need for improvement in generative AI approaches for constrained multi-objective engineering design challenges. Bike-Bench aims to drive progress in this area and provides code, data, and resources for further research.<br /> <div>
arXiv:2508.00830v1 Announce Type: new 
Abstract: We introduce Bike-Bench, an engineering design benchmark for evaluating generative models on problems with multiple real-world objectives and constraints. As generative AI's reach continues to grow, evaluating its capability to understand physical laws, human guidelines, and hard constraints grows increasingly important. Engineering product design lies at the intersection of these difficult tasks, providing new challenges for AI capabilities. Bike-Bench evaluates AI models' capability to generate designs that not only resemble the dataset, but meet specific performance objectives and constraints. To do so, Bike-Bench quantifies a variety of human-centered and multiphysics performance characteristics, such as aerodynamics, ergonomics, structural mechanics, human-rated usability, and similarity to subjective text or image prompts. Supporting the benchmark are several datasets of simulation results, a dataset of 10K human-rated bicycle assessments, and a synthetically-generated dataset of 1.4M designs, each with a parametric, CAD/XML, SVG, and PNG representation. Bike-Bench is uniquely configured to evaluate tabular generative models, LLMs, design optimization, and hybrid algorithms side-by-side. Our experiments indicate that LLMs and tabular generative models fall short of optimization and optimization-augmented generative models in both validity and optimality scores, suggesting significant room for improvement. We hope Bike-Bench, a first-of-its-kind benchmark, will help catalyze progress in generative AI for constrained multi-objective engineering design problems. Code, data, and other resources are published at decode.mit.edu/projects/bikebench/.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EngiBench: A Framework for Data-Driven Engineering Design Research</title>
<link>https://arxiv.org/abs/2508.00831</link>
<guid>https://arxiv.org/abs/2508.00831</guid>
<content:encoded><![CDATA[
<div> Keywords: Engineering design optimization, Data-driven, Open-source library, Benchmarking, Machine learning algorithms

Summary: 
EngiBench is an open-source library that provides datasets and benchmarks for data-driven engineering design optimization. It offers a unified API and curated benchmarks in various domains such as aeronautics, heat conduction, and photonics. EngiOpt is another companion library that includes optimization and machine learning algorithms compatible with EngiBench. Both libraries are modular, allowing users to add new algorithms, automate experiment workflows, and use utilities for visualization and performance analysis. Experiments showed that these engineering design problems are challenging for standard machine learning methods due to sensitive and constrained design parameters. This initiative enables fair and reproducible comparisons of algorithms and facilitates faster experimentation in engineering design optimization. 

<br /><br />Summary: <div>
arXiv:2508.00831v1 Announce Type: new 
Abstract: Engineering design optimization seeks to automatically determine the shapes, topologies, or parameters of components that maximize performance under given conditions. This process often depends on physics-based simulations, which are difficult to install, computationally expensive, and require domain-specific expertise. To mitigate these challenges, we introduce EngiBench, the first open-source library and datasets spanning diverse domains for data-driven engineering design. EngiBench provides a unified API and a curated set of benchmarks -- covering aeronautics, heat conduction, photonics, and more -- that enable fair, reproducible comparisons of optimization and machine learning algorithms, such as generative or surrogate models. We also release EngiOpt, a companion library offering a collection of such algorithms compatible with the EngiBench interface. Both libraries are modular, letting users plug in novel algorithms or problems, automate end-to-end experiment workflows, and leverage built-in utilities for visualization, dataset generation, feasibility checks, and performance analysis. We demonstrate their versatility through experiments comparing state-of-the-art techniques across multiple engineering design problems, an undertaking that was previously prohibitively time-consuming to perform. Finally, we show that these problems pose significant challenges for standard machine learning methods due to highly sensitive and constrained design manifolds.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Kernel Bayesian Optimisation for Closed-Loop Electrode Microstructure Design with User-Defined Properties based on GANs</title>
<link>https://arxiv.org/abs/2508.00833</link>
<guid>https://arxiv.org/abs/2508.00833</guid>
<content:encoded><![CDATA[
<div> Keywords: porous electrode microstructures, lithium-ion batteries, deep convolutional Generative Adversarial Network, Gaussian Process Regression, Bayesian optimisation framework

Summary:
In the design of enhanced electrochemical energy storage devices like lithium-ion batteries, generating multiphase porous electrode microstructures with optimized properties is crucial. A closed-loop algorithm has been developed for designing microstructures with tailored properties. This approach involves using a deep convolutional Generative Adversarial Network to create synthetic three-phase three-dimensional images of a battery cathode material. A Gaussian Process Regression model correlates morphological and transport properties, and a deep kernel Bayesian optimization framework optimizes cathode properties based on the generator's latent space. Objective functions are defined for maximizing morphological and transport properties, and the optimized latent space shows correlation with morphological properties. This innovative method allows for efficient generation of visually realistic microstructures with customized properties. <div>
arXiv:2508.00833v1 Announce Type: new 
Abstract: The generation of multiphase porous electrode microstructures with optimum morphological and transport properties is essential in the design of improved electrochemical energy storage devices, such as lithium-ion batteries. Electrode characteristics directly influence battery performance by acting as the main sites where the electrochemical reactions coupled with transport processes occur. This work presents a generation-optimisation closed-loop algorithm for the design of microstructures with tailored properties. A deep convolutional Generative Adversarial Network is used as a deep kernel and employed to generate synthetic three-phase three-dimensional images of a porous lithium-ion battery cathode material. A Gaussian Process Regression uses the latent space of the generator and serves as a surrogate model to correlate the morphological and transport properties of the synthetic microstructures. This surrogate model is integrated into a deep kernel Bayesian optimisation framework, which optimises cathode properties as a function of the latent space of the generator. A set of objective functions were defined to perform the maximisation of morphological properties (e.g., volume fraction, specific surface area) and transport properties (relative diffusivity). We demonstrate the ability to perform simultaneous maximisation of correlated properties (specific surface area and relative diffusivity), as well as constrained optimisation of these properties. This is the maximisation of morphological or transport properties constrained by constant values of the volume fraction of the phase of interest. Visualising the optimised latent space reveals its correlation with morphological properties, enabling the fast generation of visually realistic microstructures with customised properties.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Baseline-free Damage Detection and Localization on Composite Structures with Unsupervised Kolmogorov-Arnold Autoencoder and Guided Waves</title>
<link>https://arxiv.org/abs/2508.01081</link>
<guid>https://arxiv.org/abs/2508.01081</guid>
<content:encoded><![CDATA[
<div> Keywords: Structural health monitoring, damage detection, composite structures, Kolmogorov-Arnold autoencoder, probabilistic elliptical imaging algorithm

Summary:
A novel hybrid baseline-free damage detection and localization framework has been proposed for composite structures. The framework combines an unsupervised Kolmogorov-Arnold autoencoder (KAE) with a modified probabilistic elliptical imaging algorithm (MRAPID). The KAE processes guided wave signals without prior feature extraction, continuously learning and adapting to the baseline model of each structure. The predictions from KAE are then combined with MRAPID to generate a damage probability map. The method was tested on simulated damage data from wind turbine blades and real damage data from composite flat plates, showing effective detection and localization capabilities, including the ability to detect multiple damages. Comparative analysis demonstrated superior performance over classical algorithms and state-of-the-art baseline-free methods, particularly in terms of damage localization accuracy.<br /><br />Summary: <div>
arXiv:2508.01081v1 Announce Type: new 
Abstract: Structural health monitoring (SHM) ensures the safety and longevity of structures such as aerospace equipment and wind power installations. Developing a simple, highly flexible, and scalable SHM method that does not depend on baseline models is significant for ensuring the operational integrity of advanced composite structures. In this regard, a hybrid baseline-free damage detection and localization framework incorporating an unsupervised Kolmogorov-Arnold autoencoder (KAE) and modified probabilistic elliptical imaging algorithm (MRAPID) is proposed for damage detection and localization in composite structures. Specifically, KAE was used to process the guided wave signals (GW) without any prior feature extraction process. The KAE continuously learns and adapts to the baseline model of each structure, learning from the response characteristics of its undamaged state. Then, the predictions from KAE are processed, combined with the MRAPID to generate a damage probability map. The performance of the proposed method for damage detection and localization was verified using the simulated damage data obtained on wind turbine blades and the actual damage data obtained on composite flat plates. The results show that the proposed method can effectively detect and localize damage and can achieve multiple damage localization. In addition, the method outperforms classical damage detection algorithms and state-of-the-art baseline-free damage detection and localization methods in terms of damage localization accuracy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FluidFormer: Transformer with Continuous Convolution for Particle-based Fluid Simulation</title>
<link>https://arxiv.org/abs/2508.01537</link>
<guid>https://arxiv.org/abs/2508.01537</guid>
<content:encoded><![CDATA[
<div> Fluid Attention Block, local-global hierarchy, continuous convolutions, self-attention, Transformer architecture, neural fluid simulation, convolution-based features, attention-based modeling, FluidFormer, stability.

Summary: 
The article introduces a novel approach for fluid simulation using neural networks, emphasizing the importance of global context integration for stabilizing complex simulations. The proposed Fluid Attention Block (FAB) incorporates a local-global hierarchy, combining continuous convolutions for local features with self-attention for capturing global dependencies. This fusion helps suppress error accumulation and model long-range physical phenomena. Additionally, a specialized Transformer architecture is developed for continuous fluid simulation, integrated within a dual-pipeline framework. The method, named FluidFormer, showcases state-of-the-art performance and enhanced stability in various complex fluid scenarios. This innovative approach unifies convolution-based local features with attention-based global context modeling, setting a new standard for neural fluid simulation techniques. <div>
arXiv:2508.01537v1 Announce Type: new 
Abstract: Learning-based fluid simulation networks have been proven as viable alternatives to traditional numerical solvers for the Navier-Stokes equations. Existing neural methods follow Smoothed Particle Hydrodynamics (SPH) frameworks, which inherently rely only on local inter-particle interactions. However, we emphasize that global context integration is also essential for learning-based methods to stabilize complex fluid simulations. We propose the first Fluid Attention Block (FAB) with a local-global hierarchy, where continuous convolutions extract local features while self-attention captures global dependencies. This fusion suppresses the error accumulation and models long-range physical phenomena. Furthermore, we pioneer the first Transformer architecture specifically designed for continuous fluid simulation, seamlessly integrated within a dual-pipeline architecture. Our method establishes a new paradigm for neural fluid simulation by unifying convolution-based local features with attention-based global context modeling. FluidFormer demonstrates state-of-the-art performance, with stronger stability in complex fluid scenarios.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Model Fidelity Evaluation to Support Design Decisions for Complex, Novel Systems of Systems</title>
<link>https://arxiv.org/abs/2508.02456</link>
<guid>https://arxiv.org/abs/2508.02456</guid>
<content:encoded><![CDATA[
<div> trustworthiness, model validation, model fidelity, systems engineering, design process

Summary: 
Model trust is vital in systems design processes where real-world data is scarce, especially for complex systems with emergent behavior. Trustworthy models are crucial for supporting designers in making informed decisions. Model fidelity, defined as the model's adherence to real-world physics, is closely linked to trust and validity, enhancing a designer's ability to rely on physics-based models. The complexity and accuracy of a model's representation of physical phenomena play a significant role in determining its fidelity. Methods for evaluating and selecting models that do not require real-world data are essential challenges in systems engineering. Validating models based on their fidelity to real-world physics helps designers choose the most appropriate model for a given design decision. <div>
arXiv:2508.02456v1 Announce Type: new 
Abstract: Systems design processes are increasingly reliant on simulation models to inform design decisions. A pervasive issue within the systems engineering community is trusting in the models used to make decisions about complex systems. This work presents a method of evaluating the trustworthiness of a model to provide utility to a designer making a decision within a design process. Trusting the results of a model is especially important in design processes where the system is complex, novel, or displays emergent phenomena. Additionally, systems that are in the pre-prototype stages of development often do not have sources of ground truth for validating the models. Developing methods of model validation and trust that do not require real-world data is a key challenge facing systems engineers. Model fidelity in this work refers to the adherence of a model to real-world physics and is closely tied to model trust and model validity. Trust and validity directly support a designer's ability to make decisions using physics-based models. The physics that are captured in a model and the complexity of the mathematical representation of the physics contribute to a model's fidelity, and this work leverages the included physical phenomena to develop a means of selecting the most appropriate for a given design decision.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gearshift Fellowship: A Next-Generation Neurocomputational Game Platform to Model and Train Human-AI Adaptability</title>
<link>https://arxiv.org/abs/2508.00850</link>
<guid>https://arxiv.org/abs/2508.00850</guid>
<content:encoded><![CDATA[
<div> Keywords: Supertask paradigm, adaptive behavior, cognitive neuroscience, computational psychiatry, serious gaming

Summary: 
Gearshift Fellowship (GF) is a new Supertask paradigm that models adaptive behavior in humans and artificial agents by combining cognitive neuroscience, computational psychiatry, economics, and artificial intelligence. It creates a dynamic, multi-mission environment for assessing mechanisms of adaptive behavior across cognitive and social contexts. GF allows for neurocognitive modeling of individual differences in perceptual decisions, learning, and meta-cognitive levels, making it a flexible testbed for understanding cognitive-affective control processes, learning styles, and motivation shifts. Results from an online study show that GF recovers effects from traditional neuropsychological tasks, uncovers novel patterns in learning across contexts, and maps clinical features onto distinct adaptations. This research paves the way for developing in-game interventions that promote self-efficacy and coping skills for real-world stress. GF aims to accelerate science, transform clinical care, and support individual growth by creating an adaptive ecosystem where humans and machines can co-develop greater flexibility and awareness.<br /><br />Summary: <div>
arXiv:2508.00850v1 Announce Type: cross 
Abstract: How do we learn when to persist, when to let go, and when to shift gears? Gearshift Fellowship (GF) is the prototype of a new Supertask paradigm designed to model how humans and artificial agents adapt to shifting environment demands. Grounded in cognitive neuroscience, computational psychiatry, economics, and artificial intelligence, Supertasks combine computational neurocognitive modeling with serious gaming. This creates a dynamic, multi-mission environment engineered to assess mechanisms of adaptive behavior across cognitive and social contexts. Computational parameters explain behavior and probe mechanisms by controlling the game environment. Unlike traditional tasks, GF enables neurocognitive modeling of individual differences across perceptual decisions, learning, and meta-cognitive levels. This positions GF as a flexible testbed for understanding how cognitive-affective control processes, learning styles, strategy use, and motivational shifts adapt across contexts and over time. It serves as an experimental platform for scientists, a phenotype-to-mechanism intervention for clinicians, and a training tool for players aiming to strengthen self-regulated learning, mood, and stress resilience. Online study (n = 60, ongoing) results show that GF recovers effects from traditional neuropsychological tasks (construct validity), uncovers novel patterns in how learning differs across contexts and how clinical features map onto distinct adaptations. These findings pave the way for developing in-game interventions that foster self-efficacy and agency to cope with real-world stress and uncertainty. GF builds a new adaptive ecosystem designed to accelerate science, transform clinical care, and foster individual growth. It offers a mirror and training ground where humans and machines co-develop together deeper flexibility and awareness.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Residual Guided strategy with Generative Adversarial Networks in training Physics-Informed Transformer Networks</title>
<link>https://arxiv.org/abs/2508.00855</link>
<guid>https://arxiv.org/abs/2508.00855</guid>
<content:encoded><![CDATA[
<div> Transformer, Physics-Informed Neural Networks, Generative Adversarial Networks, Residual Guided Training, Partial Differential Equations<br />
<br />
Summary: 
The article introduces a novel Residual Guided Training strategy for Physics-Informed Transformer via Generative Adversarial Networks (GAN) to improve resolving residuals and enforcing temporal causality in modeling nonlinear partial differential equations (PDEs). The proposed framework combines a decoder-only Transformer for capturing temporal correlations with a residual-aware GAN to prioritize high-residual regions. By incorporating a causal penalty term and an adaptive sampling mechanism, the method enhances accuracy in critical spatiotemporal regions. Experimental results on various PDEs like Allen-Cahn, Klein-Gordon, and Navier-Stokes equations demonstrate substantial improvements with up to three orders of magnitude reduction in Mean Squared Error (MSE) compared to conventional methods. This approach effectively bridges the gap between deep learning and physics-driven modeling, offering a robust solution for modeling multiscale and time-dependent PDE systems. <br /><br /> <div>
arXiv:2508.00855v1 Announce Type: cross 
Abstract: Nonlinear partial differential equations (PDEs) are pivotal in modeling complex physical systems, yet traditional Physics-Informed Neural Networks (PINNs) often struggle with unresolved residuals in critical spatiotemporal regions and violations of temporal causality. To address these limitations, we propose a novel Residual Guided Training strategy for Physics-Informed Transformer via Generative Adversarial Networks (GAN). Our framework integrates a decoder-only Transformer to inherently capture temporal correlations through autoregressive processing, coupled with a residual-aware GAN that dynamically identifies and prioritizes high-residual regions. By introducing a causal penalty term and an adaptive sampling mechanism, the method enforces temporal causality while refining accuracy in problematic domains. Extensive numerical experiments on the Allen-Cahn, Klein-Gordon, and Navier-Stokes equations demonstrate significant improvements, achieving relative MSE reductions of up to three orders of magnitude compared to baseline methods. This work bridges the gap between deep learning and physics-driven modeling, offering a robust solution for multiscale and time-dependent PDE systems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Fleet Upgrade Decisions with Machine-Learning Enhanced Optimization</title>
<link>https://arxiv.org/abs/2508.00915</link>
<guid>https://arxiv.org/abs/2508.00915</guid>
<content:encoded><![CDATA[
<div> Keywords: Rental-based business models, fleet management, machine learning, optimization, sustainability

Summary:
Rental-based business models and sustainability requirements are driving the need for efficient strategies to manage large machine and vehicle fleets. Traditional fleet optimization methods based on integer programming are computationally expensive, especially for large fleets. This study proposes two approaches for fleet upgrade optimization: an extended integer programming approach and a machine learning-based method. In a real-world automotive industry case study, the machine learning approach demonstrated near-optimal solutions with improved scalability and computational performance compared to the traditional method. This makes it a practical alternative for large-scale fleet management. By integrating machine learning into fleet upgrade decision-making processes, organizations can achieve optimal solutions that balance utility, cost, and sustainability considerations effectively. 

<br /><br />Summary: <div>
arXiv:2508.00915v1 Announce Type: cross 
Abstract: Rental-based business models and increasing sustainability requirements intensify the need for efficient strategies to manage large machine and vehicle fleet renewal and upgrades. Optimized fleet upgrade strategies maximize overall utility, cost, and sustainability. However, conventional fleet optimization does not account for upgrade options and is based on integer programming with exponential runtime scaling, which leads to substantial computational cost when dealing with large fleets and repeated decision-making processes. This contribution firstly suggests an extended integer programming approach that determines optimal renewal and upgrade decisions. The computational burden is addressed by a second, alternative machine learning-based method that transforms the task to a mixed discrete-continuous optimization problem. Both approaches are evaluated in a real-world automotive industry case study, which shows that the machine learning approach achieves near-optimal solutions with significant improvements in the scalability and overall computational performance, thus making it a practical alternative for large-scale fleet management.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reward-Directed Diffusion Framework for Generative Design Optimization</title>
<link>https://arxiv.org/abs/2508.01509</link>
<guid>https://arxiv.org/abs/2508.01509</guid>
<content:encoded><![CDATA[
<div> Diffusion models, reward-directed sampling, high-performance engineering designs, soft value function, Markov decision process <br />
<br />
Summary: This study introduces a generative optimization framework utilizing a fine-tuned diffusion model and reward-directed sampling to create high-performance engineering designs. The framework uses a parametric design representation to generate new parameter sets with improved performance metrics. It employs a soft value function within a Markov decision process to guide the decoding process, reducing computational costs and achieving high-reward designs. Empirical results show significant enhancements in 3D ship hull design and 2D airfoil design, with samples surpassing the training data distribution. The proposed approach leads to a 25 percent reduction in resistance for ship design and a 10 percent improvement in the lift-to-drag ratio for 2D airfoil design. Integration of this framework in the engineering design cycle can boost designer efficiency and overall design performance. <br /> <div>
arXiv:2508.01509v1 Announce Type: cross 
Abstract: This study presents a generative optimization framework that builds on a fine-tuned diffusion model and reward-directed sampling to generate high-performance engineering designs. The framework adopts a parametric representation of the design geometry and produces new parameter sets corresponding to designs with enhanced performance metrics. A key advantage of the reward-directed approach is its suitability for scenarios in which performance metrics rely on costly engineering simulations or surrogate models (e.g. graph-based, ensemble models, or tree-based) are non-differentiable or prohibitively expensive to differentiate. This work introduces the iterative use of a soft value function within a Markov decision process framework to achieve reward-guided decoding in the diffusion model. By incorporating soft-value guidance during both the training and inference phases, the proposed approach reduces computational and memory costs to achieve high-reward designs, even beyond the training data. Empirical results indicate that this iterative reward-directed method substantially improves the ability of the diffusion models to generate samples with reduced resistance in 3D ship hull design and enhanced hydrodynamic performance in 2D airfoil design tasks. The proposed framework generates samples that extend beyond the training data distribution, resulting in a greater 25 percent reduction in resistance for ship design and over 10 percent improvement in the lift-to-drag ratio for the 2D airfoil design. Successful integration of this model into the engineering design life cycle can enhance both designer productivity and overall design performance.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Policy Iteration for Stochastic Optimal Control: A Physics-Informed Approach</title>
<link>https://arxiv.org/abs/2508.01718</link>
<guid>https://arxiv.org/abs/2508.01718</guid>
<content:encoded><![CDATA[
<div> physics-informed neural network, stochastic optimal control, Hamilton-Jacobi-Bellman equation, policy iteration, value function approximation 
Summary:
The proposed Physics-Informed Neural Network Policy Iteration (PINN-PI) framework tackles stochastic optimal control problems through second-order Hamilton-Jacobi-Bellman equations. A neural network is trained at each iteration to approximate the value function by minimizing the residual of a linear PDE based on a fixed policy, ensuring systematic error control. Explicit Lipschitz-type bounds quantify the propagation of value gradient errors to policy updates, enhancing interpretability during training. Extending deterministic PINN approaches to stochastic environments, the method guarantees global exponential convergence under mild conditions. Demonstrated effectiveness on various benchmark problems like stochastic cartpole, pendulum, and high-dimensional linear quadratic regulation (LQR) challenges up to 10D showcases the versatility and reliability of the approach. <div>
arXiv:2508.01718v1 Announce Type: cross 
Abstract: We propose a physics-informed neural network policy iteration (PINN-PI) framework for solving stochastic optimal control problems governed by second-order Hamilton--Jacobi--Bellman (HJB) equations. At each iteration, a neural network is trained to approximate the value function by minimizing the residual of a linear PDE induced by a fixed policy. This linear structure enables systematic $L^2$ error control at each policy evaluation step, and allows us to derive explicit Lipschitz-type bounds that quantify how value gradient errors propagate to the policy updates. This interpretability provides a theoretical basis for evaluating policy quality during training. Our method extends recent deterministic PINN-based approaches to stochastic settings, inheriting the global exponential convergence guarantees of classical policy iteration under mild conditions. We demonstrate the effectiveness of our method on several benchmark problems, including stochastic cartpole, pendulum problems and high-dimensional linear quadratic regulation (LQR) problems in up to 10D.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization</title>
<link>https://arxiv.org/abs/2508.02002</link>
<guid>https://arxiv.org/abs/2508.02002</guid>
<content:encoded><![CDATA[
<div> transformers, diffusers, trajectory generation, auto-bidding, GRAD
<br />
Summary:
GRAD is a novel approach in the field of auto-bidding systems, combining an Action-Mixture-of-Experts module and the Value Estimator of Causal Transformer. This model addresses challenges faced by generative methods in online advertising, such as distribution shift and limited exploration of the action space, by incorporating constraint-aware optimization techniques. GRAD has been successfully implemented at Meituan, a leading online food delivery platform, resulting in a significant increase in platform revenue, Gross Merchandise Value (GMV), and Return on Investment (ROI). The model demonstrates its effectiveness in enhancing the performance of auto-bidding systems by catering to the dynamic and diverse requirements of modern advertisers. <div>
arXiv:2508.02002v1 Announce Type: cross 
Abstract: Modern auto-bidding systems are required to balance overall performance with diverse advertiser goals and real-world constraints, reflecting the dynamic and evolving needs of the industry. Recent advances in conditional generative models, such as transformers and diffusers, have enabled direct trajectory generation tailored to advertiser preferences, offering a promising alternative to traditional Markov Decision Process-based methods. However, these generative methods face significant challenges, such as the distribution shift between offline and online environments, limited exploration of the action space, and the necessity to meet constraints like marginal Cost-per-Mille (CPM) and Return on Investment (ROI). To tackle these challenges, we propose GRAD (Generative Reward-driven Ad-bidding with Mixture-of-Experts), a scalable foundation model for auto-bidding that combines an Action-Mixture-of-Experts module for diverse bidding action exploration with the Value Estimator of Causal Transformer for constraint-aware optimization. Extensive offline and online experiments demonstrate that GRAD significantly enhances platform revenue, highlighting its effectiveness in addressing the evolving and diverse requirements of modern advertisers. Furthermore, GRAD has been implemented in multiple marketing scenarios at Meituan, one of the world's largest online food delivery platforms, leading to a 2.18% increase in Gross Merchandise Value (GMV) and 10.68% increase in ROI.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval</title>
<link>https://arxiv.org/abs/2508.02222</link>
<guid>https://arxiv.org/abs/2508.02222</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hierarchical queries, passage retrieval, indirect positives mining, Financial Passage Retrieval Generated dataset

Summary: 
The paper introduces a bidirectional generation pipeline for constructing passage retrieval datasets using large language models (LLMs). This pipeline generates 3-level hierarchical queries for intra-doc and cross-doc scenarios by disassembling single-doc text and dividing multi-doc titles into clusters based on industry, topic, and time. It incorporates both bottom-up and top-down query generation methods to enhance query expression and relevance. The pipeline includes a direct mapping annotation process and an indirect positives mining method to enrich relevance labels. The Financial Passage Retrieval Generated dataset (FinCPRG) was created from Chinese financial research reports, containing hierarchical queries and rich relevance labels. Evaluations and experiments demonstrated the quality and effectiveness of FinCPRG as a training and benchmarking dataset for passage retrieval tasks. 

Summary: <div>
arXiv:2508.02222v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have demonstrated significant potential in constructing passage retrieval datasets. However, existing methods still face limitations in expressing cross-doc query needs and controlling annotation quality. To address these issues, this paper proposes a bidirectional generation pipeline, which aims to generate 3-level hierarchical queries for both intra-doc and cross-doc scenarios and mine additional relevance labels on top of direct mapping annotation. The pipeline introduces two query generation methods: bottom-up from single-doc text and top-down from multi-doc titles. The bottom-up method uses LLMs to disassemble and generate structured queries at both sentence-level and passage-level simultaneously from intra-doc passages. The top-down approach incorporates three key financial elements--industry, topic, and time--to divide report titles into clusters and prompts LLMs to generate topic-level queries from each cluster. For relevance annotation, our pipeline not only relies on direct mapping annotation from the generation relationship but also implements an indirect positives mining method to enrich the relevant query-passage pairs. Using this pipeline, we constructed a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k Chinese financial research reports, which includes hierarchical queries and rich relevance labels. Through evaluations of mined relevance labels, benchmarking and training experiments, we assessed the quality of FinCPRG and validated its effectiveness as a passage retrieval dataset for both training and benchmarking.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte Space</title>
<link>https://arxiv.org/abs/2508.02247</link>
<guid>https://arxiv.org/abs/2508.02247</guid>
<content:encoded><![CDATA[
<div> Generative modeling, high-frequency limit order book, finance, ByteGen, raw byte streams<br />
Summary:<br />
The article introduces ByteGen, a novel generative model for high-frequency limit order book dynamics in finance. It works directly on raw byte streams of market events, eliminating the need for feature engineering and tokenization. ByteGen treats the problem as an autoregressive next-byte prediction task, using a compact 32-byte packed binary format for efficient data representation. The H-Net architecture, a hybrid Mamba-Transformer model, is used to discover the structure of market messages without predefined rules. Trained on CME Bitcoin futures data, ByteGen successfully replicates key characteristics of financial markets, such as realistic price distributions and bursty event timing. This approach shows promise for modeling complex financial systems without the biases of tokenization, achieving competitive performance on market quality metrics. <br /> <div>
arXiv:2508.02247v1 Announce Type: cross 
Abstract: Generative modeling of high-frequency limit order book (LOB) dynamics is a critical yet unsolved challenge in quantitative finance, essential for robust market simulation and strategy backtesting. Existing approaches are often constrained by simplifying stochastic assumptions or, in the case of modern deep learning models like Transformers, rely on tokenization schemes that affect the high-precision, numerical nature of financial data through discretization and binning. To address these limitations, we introduce ByteGen, a novel generative model that operates directly on the raw byte streams of LOB events. Our approach treats the problem as an autoregressive next-byte prediction task, for which we design a compact and efficient 32-byte packed binary format to represent market messages without information loss. The core novelty of our work is the complete elimination of feature engineering and tokenization, enabling the model to learn market dynamics from its most fundamental representation. We achieve this by adapting the H-Net architecture, a hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to discover the inherent structure of market messages without predefined rules. Our primary contributions are: 1) the first end-to-end, byte-level framework for LOB modeling; 2) an efficient packed data representation; and 3) a comprehensive evaluation on high-frequency data. Trained on over 34 million events from CME Bitcoin futures, ByteGen successfully reproduces key stylized facts of financial markets, generating realistic price distributions, heavy-tailed returns, and bursty event timing. Our findings demonstrate that learning directly from byte space is a promising and highly flexible paradigm for modeling complex financial systems, achieving competitive performance on standard market quality metrics without the biases of tokenization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Continuous-Time MILP for Integrated Aircraft Hangar Scheduling and Layout</title>
<link>https://arxiv.org/abs/2508.02640</link>
<guid>https://arxiv.org/abs/2508.02640</guid>
<content:encoded><![CDATA[
<div> continuous-time mixed-integer linear programming, aircraft maintenance hangars, aircraft scheduling, spatial allocation, scalability limitations <br />
<br />
Summary: 
This paper presents a novel continuous-time mixed-integer linear programming model for efficient management of aircraft maintenance hangars. The model addresses the complex decisions involved in aircraft scheduling and spatial allocation, overcoming the scalability limitations of traditional approaches by treating time as a continuous variable. Benchmarking against a heuristic shows the model's superior performance, solving instances with up to 25 aircraft quickly and delivering high-quality solutions for cases with up to 40 aircraft. The model's economic benefits and managerial insights are highlighted, with solutions consistently outperforming the heuristic. A custom-built visualization dashboard demonstrates the practical applicability of the model, showcasing its ability to provide optimized solutions quickly and effectively. <div>
arXiv:2508.02640v1 Announce Type: cross 
Abstract: Efficient management of aircraft maintenance hangars is a critical operational challenge, involving complex, interdependent decisions regarding aircraft scheduling and spatial allocation. This paper introduces a novel continuous-time mixed-integer linear programming (MILP) model to solve this integrated spatio-temporal problem. By treating time as a continuous variable, our formulation overcomes the scalability limitations of traditional discrete-time approaches. The performance of the exact model is benchmarked against a constructive heuristic, and its practical applicability is demonstrated through a custom-built visualization dashboard. Computational results are compelling: the model solves instances with up to 25 aircraft to proven optimality, often in mere seconds, and for large-scale cases of up to 40 aircraft, delivers high-quality solutions within known optimality gaps. In all tested scenarios, the resulting solutions consistently and significantly outperform the heuristic, which highlights the framework's substantial economic benefits and provides valuable managerial insights into the trade-off between solution time and optimality.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A locking-free isogeometric thin shell formulation based on higher order accurate diagonalized strain projection via approximate dual splines</title>
<link>https://arxiv.org/abs/2406.16685</link>
<guid>https://arxiv.org/abs/2406.16685</guid>
<content:encoded><![CDATA[
<div> spline basis functions, Kirchhoff-Love shell formulation, membrane locking, Hellinger-Reissner variational principle, isogeometric discretization <br />
Summary:<br />
The article presents a novel approach for isogeometric discretization of the Kirchhoff-Love shell formulation, addressing membrane locking issues. Independent strains are discretized using spline basis functions one degree lower than displacements to enhance accuracy. Variations of strains are discretized using approximate dual splines to obtain a projection matrix that is diagonalized through row-sum lumping for efficient condensation. This diagonalization simplifies static condensation of strain fields without the need for matrix inversion, maintaining higher-order accuracy with optimal convergence rates. Numerical benchmarks, such as a curved Euler-Bernoulli beam and shell obstacle course examples, demonstrate the approach's numerical properties and performance. <br /> 
Summary: <div>
arXiv:2406.16685v4 Announce Type: replace 
Abstract: We present a novel isogeometric discretization approach for the Kirchhoff-Love shell formulation based on the Hellinger-Reissner variational principle. For mitigating membrane locking, we discretize the independent strains with spline basis functions that are one degree lower than those used for the displacements. To enable computationally efficient condensation of the independent strains, we first discretize the variations of the independent strains with approximate dual splines to obtain a projection matrix that is close to a diagonal matrix. We then diagonalize this strain projection matrix via row-sum lumping. Due to this diagonalization, the static condensation of the independent strain fields becomes computationally inexpensive, as no matrix needs to be inverted. At the same time, our approach maintains higher-order accuracy at optimal rates of convergence. We illustrate the numerical properties and the performance of our approach through numerical benchmarks, including a curved Euler-Bernoulli beam and the examples of the shell obstacle course.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADformer: A Multi-Granularity Spatial-Temporal Transformer for EEG-Based Alzheimer Detection</title>
<link>https://arxiv.org/abs/2409.00032</link>
<guid>https://arxiv.org/abs/2409.00032</guid>
<content:encoded><![CDATA[
<div> EEG, Alzheimer's Disease, ADformer, transformer, spatial-temporal<br />
<br />
Summary: <br />
The article introduces ADformer, a novel spatial-temporal transformer for EEG-based Alzheimer's Disease (AD) detection. Existing approaches for AD detection often suffer from information loss and limited generalizability due to manual feature engineering. ADformer addresses these challenges by capturing both temporal and spatial features from raw EEG signals, enabling end-to-end representation learning. It incorporates multi-granularity embedding strategies and a two-stage intra-inter granularity self-attention mechanism to learn local patterns and global dependencies. ADformer is evaluated on 4 large-scale datasets with 1,713 subjects, achieving superior performance in distinguishing AD from healthy control subjects. The model outperforms existing methods with subject-level F1 scores of 92.82%, 89.83%, 67.99%, and 83.98% on the 4 datasets, showcasing its effectiveness for EEG-based AD detection. <div>
arXiv:2409.00032v2 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) has emerged as a cost-effective and efficient tool to support neurologists in the detection of Alzheimer's Disease (AD). However, most existing approaches rely heavily on manual feature engineering or data transformation. While such techniques may provide benefits when working with small-scale datasets, they often lead to information loss and distortion when applied to large-scale data, ultimately limiting model performance. Moreover, the limited subject scale and demographic diversity of datasets used in prior studies hinder comprehensive evaluation of model robustness and generalizability, thus restricting their applicability in real-world clinical settings. To address these challenges, we propose ADformer, a novel multi-granularity spatial-temporal transformer designed to capture both temporal and spatial features from raw EEG signals, enabling effective end-to-end representation learning. Our model introduces multi-granularity embedding strategies across both spatial and temporal dimensions, leveraging a two-stage intra-inter granularity self-attention mechanism to learn both local patterns within each granularity and global dependencies across granularities. We evaluate ADformer on 4 large-scale datasets comprising a total of 1,713 subjects, representing one of the largest corpora for EEG-based AD detection to date, under a cross-validated, subject-independent setting. Experimental results demonstrate that ADformer consistently outperforms existing methods, achieving subject-level F1 scores of 92.82%, 89.83%, 67.99%, and 83.98% on the 4 datasets, respectively, in distinguishing AD from healthy control (HC) subjects.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Tick-Size Too Small: A General Method for Modelling Small Tick Limit Order Books</title>
<link>https://arxiv.org/abs/2410.08744</link>
<guid>https://arxiv.org/abs/2410.08744</guid>
<content:encoded><![CDATA[
<div> tick-sizes, market agents behavior, Limit Order Book, Hawkes Process model, stylized facts<br />
<br />Summary:
This study examines the impact of tick-sizes on the microstructural properties of assets in the market. By analyzing a variety of assets with different tick-sizes, the researchers identify distinct stylized facts that differentiate between large, medium, and small-tick assets. They propose a Hawkes Process model that effectively captures the characteristics of different tick-size assets, including sparsity, multi-tick level price moves, and the shape of the Limit Order Book. Through simulations, they demonstrate the model's versatility and its ability to transition between large and small-tick assets based on key variables. The study also assesses the model's assumptions, highlights challenges, and suggests potential directions for future research in this area. <div>
arXiv:2410.08744v3 Announce Type: replace-cross 
Abstract: Tick-sizes not only influence the granularity of the price formation process but also affect market agents' behavior. We investigate the disparity in the microstructural properties of the Limit Order Book (LOB) across a basket of assets with different relative tick-sizes. A key contribution of this study is the identification of several stylized facts, which are used to differentiate between large, medium, and small-tick assets, along with clear metrics for their measurement. We provide cross-asset visualizations to illustrate how these attributes vary with relative tick-size. Further, we propose a Hawkes Process model that {\color{black}not only fits well for large-tick assets, but also accounts for }sparsity, multi-tick level price moves, and the shape of the LOB in small-tick assets. Through simulation studies, we demonstrate the {\color{black} versatility} of the model and identify key variables that determine whether a simulated LOB resembles a large-tick or small-tick asset. Our tests show that stylized facts like sparsity, shape, and relative returns distribution can be smoothly transitioned from a large-tick to a small-tick asset using our model. We test this model's assumptions, showcase its challenges and propose questions for further directions in this area of research.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Operator Networks for Bayesian Parameter Estimation in PDEs</title>
<link>https://arxiv.org/abs/2501.10684</link>
<guid>https://arxiv.org/abs/2501.10684</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Operator Networks, Physics-Informed Neural Networks, partial differential equations, uncertainty quantification, surrogate modeling

Summary: 

Deep Operator Networks (DeepONets) and Physics-Informed Neural Networks (PINNs) are combined to solve partial differential equations (PDEs) and estimate their parameters. The framework integrates data-driven learning with physical constraints, achieving robust and accurate solutions in various scenarios. Bayesian training via variational inference enables comprehensive uncertainty quantification for aleatoric and epistemic uncertainties, ensuring reliable predictions even in noisy conditions or when governing equations are missing. This approach proves effective in solving forward and inverse problems, including the 1D unsteady heat equation and 2D reaction-diffusion equations, as well as regression tasks with sparse, noisy observations. The framework offers a computationally efficient and generalizable method for addressing uncertainty quantification in PDE surrogate modeling.<br /><br />Summary: <div>
arXiv:2501.10684v2 Announce Type: replace-cross 
Abstract: We present a novel framework combining Deep Operator Networks (DeepONets) with Physics-Informed Neural Networks (PINNs) to solve partial differential equations (PDEs) and estimate their unknown parameters. By integrating data-driven learning with physical constraints, our method achieves robust and accurate solutions across diverse scenarios. Bayesian training is implemented through variational inference, allowing for comprehensive uncertainty quantification for both aleatoric and epistemic uncertainties. This ensures reliable predictions and parameter estimates even in noisy conditions or when some of the physical equations governing the problem are missing. The framework demonstrates its efficacy in solving forward and inverse problems, including the 1D unsteady heat equation and 2D reaction-diffusion equations, as well as regression tasks with sparse, noisy observations. This approach provides a computationally efficient and generalizable method for addressing uncertainty quantification in PDE surrogate modeling.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Finite Element Approach for Simulating Dynamic Crack Growth in Cu/Ultra Low-k Interconnect Structures</title>
<link>https://arxiv.org/abs/2508.00193</link>
<guid>https://arxiv.org/abs/2508.00193</guid>
<content:encoded><![CDATA[
<div> Keywords: finite element modeling, dynamic crack propagation, Crack Element Method, Edge-based Smoothed Finite Element Method, fracture energy release rate <br />
Summary: 
The article introduces the Crack Element Method (CEM) for simulating dynamic crack propagation in 2D structures. CEM utilizes an element-splitting algorithm based on the Edge-based Smoothed Finite Element Method (ES-FEM) to capture crack growth while minimizing poorly shaped elements. A fracture energy release rate formulation is developed using split element topology. Validation on benchmark problems confirms accuracy and robustness. A case study on patterned Cu/Ultra Low-k interconnect structures showcases CEM's applicability.<br /><br />Summary: <div>
arXiv:2508.00193v1 Announce Type: new 
Abstract: This work presents a practical finite element modeling strategy, the Crack Element Method (CEM), for simulating the dynamic crack propagation in two-dimensional structures. The method employs an element-splitting algorithm based on the Edge-based Smoothed Finite Element Method (ES-FEM) to capture the element-wise crack growth while reducing the formation of poorly shaped elements that can compromise numerical accuracy and computational performance. A fracture energy release rate formulation is also developed based on the evolving topology of the split elements. The proposed approach is validated through a series of classical benchmark problems, demonstrating its accuracy and robustness in addressing dynamic fracture scenarios. Finally, the applicability of the CEM is illustrated in a case study involving patterned Cu/Ultra Low-k interconnect structures.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeightFlow: Learning Stochastic Dynamics via Evolving Weight of Neural Network</title>
<link>https://arxiv.org/abs/2508.00451</link>
<guid>https://arxiv.org/abs/2508.00451</guid>
<content:encoded><![CDATA[
<div> Keywords: stochastic dynamics, neural network, weight space, optimal transport, high-dimensional

Summary: 
This article introduces a novel approach, WeightFlow, to model stochastic dynamics directly in the weight space of a neural network. By projecting the evolving probability distribution onto the weight space, WeightFlow connects dynamic optimal transport in measure space to an energy functional in weight space. The neural network weights are constructed into a graph, and their evolution is learned through a graph-controlled differential equation. Experimental results on interdisciplinary datasets show that WeightFlow outperforms state-of-the-art methods by an average of 43.02%. This approach provides an effective and scalable solution for modeling high-dimensional stochastic dynamics. <div>
arXiv:2508.00451v1 Announce Type: new 
Abstract: Modeling stochastic dynamics from discrete observations is a key interdisciplinary challenge. Existing methods often fail to estimate the continuous evolution of probability densities from trajectories or face the curse of dimensionality. To address these limitations, we presents a novel paradigm: modeling dynamics directly in the weight space of a neural network by projecting the evolving probability distribution. We first theoretically establish the connection between dynamic optimal transport in measure space and an equivalent energy functional in weight space. Subsequently, we design WeightFlow, which constructs the neural network weights into a graph and learns its evolution via a graph controlled differential equation. Experiments on interdisciplinary datasets demonstrate that WeightFlow improves performance by an average of 43.02\% over state-of-the-art methods, providing an effective and scalable solution for modeling high-dimensional stochastic dynamics.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources</title>
<link>https://arxiv.org/abs/2508.00654</link>
<guid>https://arxiv.org/abs/2508.00654</guid>
<content:encoded><![CDATA[
<div> management, microscopy, data integration, interoperability, web-based platform
<br />
Summary:<br />
- Managing and integrating large volumes of microscopy data stored across different platforms is a challenge in research. 
- Data types such as bioimages, experimental records, and spectral information are often stored separately, hindering data linkage and alignment with FAIR data management principles. 
- The lack of tools for effectively integrating heterogeneous data sources prompted the development of LEO, a web-based platform. 
- LEO initially linked Electronic Lab Notebooks (ELNs) with OMERO but can now integrate other data sources through a plugin-based architecture. 
- LEO's extensibility makes it a scalable and flexible solution for various microscopy research workflows. 
<br />Summary: <div>
arXiv:2508.00654v1 Announce Type: new 
Abstract: In the interdisciplinary field of microscopy research, managing and integrating large volumes of data stored across disparate platforms remains a major challenge. Data types such as bioimages, experimental records, and spectral information are often maintained in separate repositories, each following different management standards. However, linking these data sources across the research lifecycle is essential to align with the FAIR principles of data management: Findability, Accessibility, Interoperability, and Reusability. Despite this need, there is a notable lack of tools capable of effectively integrating and linking data from heterogeneous sources. To address this gap, we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based platform designed to create and manage links between distributed data systems. LEO was initially developed to link objects between Electronic Lab Notebooks (ELNs) and OMERO, but its functionality has since been extended through a plugin-based architecture, allowing the integration of additional data sources. This extensibility makes LEO a scalable and flexible solution for a wide range of microscopy research workflows.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contact Sensors to Remote Cameras: Quantifying Cardiorespiratory Coupling in High-Altitude Exercise Recovery</title>
<link>https://arxiv.org/abs/2508.00773</link>
<guid>https://arxiv.org/abs/2508.00773</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiorespiratory coupling, high altitude, exercise, remote photoplethysmography, autonomic regulation

Summary: 
Cardiorespiratory coupling (CRC) is the dynamic interaction between the heart and lungs, which is enhanced during physical exercise and associated with improved physiological function. A study examined CRC at high altitudes during rest and post-exercise recovery, revealing significant differences. The analysis showed that recovery involved more frequent yet less stable synchronization between breathing and pulse. The feasibility of non-contact CRC measurement using remote photoplethysmography (rPPG) was explored, showing a strong correlation with oximeter-based metrics. These findings suggest that CRC could serve as a sensitive marker for autonomic regulation and have potential applications in contactless monitoring. Source code for the study is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2508.00773v1 Announce Type: new 
Abstract: Cardiorespiratory coupling (CRC) captures the dynamic interaction between the cardiac and respiratory systems--an interaction strengthened by physical exercise and linked to improved physiological function. We examined CRC at high altitude in two states, rest and post-exercise recovery, and found significant differences (p < 0.05). Quantitative analysis revealed that recovery involved more frequent yet less stable episodes of synchronization between respiration and pulse. Furthermore, we explored the feasibility of non-contact CRC measurement with remote photoplethysmography (rPPG), observing a strong correlation with oximeter-based metrics (Pearson r = 0.96). These findings highlight the potential of CRC as a sensitive marker for autonomic regulation and its future application in contactless monitoring. Source code is available at GitHub: https://github.com/McJackTang/CRC.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>{\tau}-Ring: A Smart Ring Platform for Multimodal Physiological and Behavioral Sensing</title>
<link>https://arxiv.org/abs/2508.00778</link>
<guid>https://arxiv.org/abs/2508.00778</guid>
<content:encoded><![CDATA[
<div> Keyword: Smart rings, wearable, physiological sensing, open-source, reproducible <br />
<br />
Summary: 
The article introduces the {\tau}-Ring platform, designed to address the limitations of proprietary smart rings for continuous physiological and behavioral sensing. The platform offers accessible hardware with multi-channel PPG, IMU, temperature sensing, NFC, and on-board storage. It features adjustable firmware for quick reconfiguration of settings, allowing researchers to customize sampling rates, power modes, and wireless protocols. The platform also includes an open-source Android software suite for real-time streaming and offline logging. These capabilities enable easy acquisition of rich datasets, facilitating research prototyping and standardization. The platform is validated through studies in heart-rate monitoring and ring-based handwriting recognition. Overall, {\tau}-Ring provides a commercial-ready solution that promotes reproducibility in wearable research and accelerates innovation in physiological and behavioral sensing technologies. Source code is available on GitHub for reference and collaboration. <br /><br />Summary: <div>
arXiv:2508.00778v1 Announce Type: new 
Abstract: Smart rings have emerged as uniquely convenient devices for continuous physiological and behavioral sensing, offering unobtrusive, constant access to metrics such as heart rate, motion, and skin temperature. Yet most commercial solutions remain proprietary, hindering reproducibility and slowing innovation in wearable research. We introduce {\tau}-Ring, a commercial-ready platform that bridges this gap through: (i) accessible hardware combining time-synchronized multi-channel PPG, 6-axis IMU, temperature sensing, NFC, and on-board storage; (ii) adjustable firmware that lets researchers rapidly reconfigure sampling rates, power modes, and wireless protocols; and (iii) a fully open-source Android software suite that supports both real-time streaming and 8-hour offline logging. Together, these features enable out-of-the-box, reproducible acquisition of rich physiological and behavioral datasets, accelerating prototyping and standardizing experimentation. We validate the platform with demonstration studies in heart-rate monitoring and ring-based handwriting recognition. Source code is available at GitHub: https://github.com/thuhci/OpenRing.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models</title>
<link>https://arxiv.org/abs/2508.00804</link>
<guid>https://arxiv.org/abs/2508.00804</guid>
<content:encoded><![CDATA[
<div> structured state space models, real-time recurrent learning, online adaptation, linear-recurrent-unit, prediction error 

Summary: 
This paper presents a novel approach for enhancing the predictions of structured state space models (SSMs) using real-time recurrent learning during inference. SSMs are traditionally trained offline and do not adapt to new data during deployment. The proposed method enables online adaptation by continuously updating model parameters based on incoming data. The study evaluated this approach using a small carbon emission dataset from embedded automotive hardware and found that it consistently reduced prediction error during inference. This highlights the potential of the method in dynamic and resource-constrained environments. The use of linear-recurrent-unit SSMs demonstrated the effectiveness of the approach in improving prediction accuracy in real-time scenarios. <div>
arXiv:2508.00804v1 Announce Type: new 
Abstract: This paper introduces a new approach for fine-tuning the predictions of structured state space models (SSMs) at inference time using real-time recurrent learning. While SSMs are known for their efficiency and long-range modeling capabilities, they are typically trained offline and remain static during deployment. Our method enables online adaptation by continuously updating model parameters in response to incoming data. We evaluate our approach for linear-recurrent-unit SSMs using a small carbon emission dataset collected from embedded automotive hardware. Experimental results show that our method consistently reduces prediction error online during inference, demonstrating its potential for dynamic, resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak Values as Geometric Lenses: Deformations of Hilbert Space and the Emergence of superoscillations</title>
<link>https://arxiv.org/abs/2508.00023</link>
<guid>https://arxiv.org/abs/2508.00023</guid>
<content:encoded><![CDATA[
<div> weak measurement, quantum mechanics, signal processing, superoscillations, geometric structure <br />
Summary: 
This paper explores the relationship between weak measurement in quantum mechanics and superoscillations, showing that superoscillations are a natural consequence of the geometric structure underlying weak values. The weak value is described as a ratio of geometric deformation, representing how an observable transforms Hilbert space relative to the standard inner product. This deformation warps quantum states locally, producing oscillations that exceed the global Fourier bandwidth. The weak value is interpreted as a comparison between a deformed sesquilinear form and the standard one, revealing connections to generalized Rayleigh quotients and projective geometry of quantum states. This perspective unifies weak values and superoscillations as manifestations of a single geometric principle. <div>
arXiv:2508.00023v1 Announce Type: cross 
Abstract: The formalism of weak measurement in quantum mechanics has revealed profound connections between measurement theory, quantum foundations, and signal processing. In this paper, we develop a pointer-free derivation of superoscillations, demonstrating that they are a natural and necessary consequence of the geometric structure underlying weak values. We argue that the weak value is best understood as a ratio of geometric deformation, quantifying how an observable transforms the structure of Hilbert space relative to a reference provided by the standard inner product. This deformation acts as a conceptual lens, warping the local structure of quantum states to produce oscillations far exceeding the global Fourier bandwidth. We formalize this by interpreting the weak value as a comparison between a deformed sesquilinear form and the standard one, and explore its deep connections to generalized Rayleigh quotients and the projective geometry of quantum states. This perspective unifies weak values and superoscillations as two facets of a single underlying geometric principle.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis</title>
<link>https://arxiv.org/abs/2508.00381</link>
<guid>https://arxiv.org/abs/2508.00381</guid>
<content:encoded><![CDATA[
<div> marine environment, welding defect detection, neural networks, interpretability, offshore environment
<br />
Summary:<br />
The paper introduces "Adapt-WeldNet", an adaptive framework for welding defect detection in marine and offshore environments. It evaluates pre-trained architectures, transfer learning strategies, and adaptive optimizers to optimize defect detection. A Defect Detection Interpretability Analysis (DDIA) framework enhances system transparency through Explainable AI techniques and domain-specific evaluations by NDE Level II professionals. The Human-in-the-Loop (HITL) approach and Trustworthy AI principles ensure reliability and accountability. By improving performance and interpretability, the system enhances trust, safety, and reliability of welding defect detection, supporting critical operations in challenging environments. <div>
arXiv:2508.00381v1 Announce Type: cross 
Abstract: Weld defect detection is crucial for ensuring the safety and reliability of piping systems in the oil and gas industry, especially in challenging marine and offshore environments. Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment. To address these challenges, this paper introduces ``Adapt-WeldNet", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation. By improving both performance and interpretability, this work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models</title>
<link>https://arxiv.org/abs/2508.00383</link>
<guid>https://arxiv.org/abs/2508.00383</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial transcriptomics, Precision oncology, Vision foundation models, State space models, Colorectal cancer

Summary:
Spatial transcriptomics is a valuable tool for predicting treatment responses in oncology, but its high cost and complexity hinder clinical adoption. Current vision foundation models (VFMs) based on ViT backbones struggle to meet clinical standards. To address this, a hybrid backbone architecture named MVHybrid, combining state space models with ViT, is proposed. Pretrained on colorectal cancer datasets using self-supervised learning, MVHybrid outperforms ViT in predicting gene expression and exhibits superior robustness in leave-one-study-out evaluation. It also performs well in classification, patch retrieval, and survival prediction tasks, showing promise as a next-generation pathology VFM backbone. The code is publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2508.00383v1 Announce Type: cross 
Abstract: Spatial transcriptomics reveals gene expression patterns within tissue context, enabling precision oncology applications such as treatment response prediction, but its high cost and technical complexity limit clinical adoption. Predicting spatial gene expression (biomarkers) from routine histopathology images offers a practical alternative, yet current vision foundation models (VFMs) in pathology based on Vision Transformer (ViT) backbones perform below clinical standards. Given that VFMs are already trained on millions of diverse whole slide images, we hypothesize that architectural innovations beyond ViTs may better capture the low-frequency, subtle morphological patterns correlating with molecular phenotypes. By demonstrating that state space models initialized with negative real eigenvalues exhibit strong low-frequency bias, we introduce $MV_{Hybrid}$, a hybrid backbone architecture combining state space models (SSMs) with ViT. We compare five other different backbone architectures for pathology VFMs, all pretrained on identical colorectal cancer datasets using the DINOv2 self-supervised learning method. We evaluate all pretrained models using both random split and leave-one-study-out (LOSO) settings of the same biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher correlation than the best-performing ViT and shows 43% smaller performance degradation compared to random split in gene expression prediction, demonstrating superior performance and robustness, respectively. Furthermore, $MV_{Hybrid}$ shows equal or better downstream performance in classification, patch retrieval, and survival prediction tasks compared to that of ViT, showing its promise as a next-generation pathology VFM backbone. Our code is publicly available at: https://github.com/deepnoid-ai/MVHybrid.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The time slot allocation problem in liberalised passenger railway markets: a multi-objective approach</title>
<link>https://arxiv.org/abs/2401.12073</link>
<guid>https://arxiv.org/abs/2401.12073</guid>
<content:encoded><![CDATA[
<div> competition, railway market, time slot allocation, multi-objective model, market equilibrium
<br />
Summary:
The article discusses the time slot allocation problem in the European passenger railway market post-liberalization. The Infrastructure Manager assesses bids and allocates resources to Railway Undertakings, influencing market equilibrium. A multi-objective model is proposed for time slot allocation, with the Infrastructure Manager choosing a point from the Pareto front. Two selection criteria are suggested: one based on priorities for time slot allocation to companies, and the other introducing fairness to incentivize competition. The impact of these rules on market equilibrium was evaluated on a high-speed corridor in the Spanish railway network. <div>
arXiv:2401.12073v2 Announce Type: replace 
Abstract: The liberalisation of the European passenger railway markets through the European Directive EU 91/440/EEC states a new scenario where different Railway Undertakings compete with each other in a bidding process for time slots. The infrastructure resources are provided by the Infrastructure Manager, who analyses and assesses the bids received, allocating the resources to each Railway Undertaking. Time slot allocation is a fact that drastically influences the market equilibrium. In this paper, we address the time slot allocation problem within the context of a liberalized passenger railway market as a multi-objective model. The Infrastructure Manager is tasked with selecting a point from the Pareto front as the solution to the time slot allocation problem. We propose two criteria for making this selection: the first one allocates time slots to each company according to a set of priorities, while the second one introduces a criterion of fairness in the treatment of companies to incentive competition. The assessment of the impact of these rules on market equilibrium has been conducted on a liberalized high-speed corridor within the Spanish railway network.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-physics Model of Flow from Coronary Angiography: Insights to Microvascular Function</title>
<link>https://arxiv.org/abs/2412.04798</link>
<guid>https://arxiv.org/abs/2412.04798</guid>
<content:encoded><![CDATA[
<div> coronary microvascular dysfunction, computational fluid dynamics, angiography, contrast intensity profile, lumped parameter model<br />
Summary:<br />
Coronary Microvascular Dysfunction (CMD) causes impaired vasodilation and insufficient blood flow to the myocardium during stress. Invasive wire-based diagnosis techniques like index of microcirculatory resistance (IMR) and coronary flow reserve (CFR) are underutilized due to complexity. A 3D-0D coupled multi-physics computational fluid dynamics (CFD) model was developed to simulate contrast injection during angiography. A contrast intensity profile (CIP) was introduced to describe angiography data dynamics. Sensitivity studies showed that resistance impacts CIP slopes more than capacitance, with higher resistance amplifying the effect. The model offers a tool for interpreting angiographic data, potentially transforming the understanding and utilization of coronary angiography in diagnosing CMD. <br />Summary: <div>
arXiv:2412.04798v2 Announce Type: replace 
Abstract: Coronary Microvascular Dysfunction (CMD) is characterized by impaired vasodilation and can lead to insufficient blood flow to the myocardium during stress or exertion, affecting millions of people globally. Despite their diagnostic value, invasive, wire-based diagnosis techniques of CMD, such as index of microcirculatory resistance (IMR) and coronary flow reserve (CFR), are underutilized due to their complexity and inconsistency. Coronary angiography, one of the most commonly used imaging modalities, offers valuable flow information that assists in diagnosing CMD. However, this information is not fully understood or utilized in current clinical practice. In this study, a 3D-0D coupled multi-physics computational fluid dynamics (CFD) model was developed and calibrated to simulate and study the process of contrast injection and washout during clinical angiography. A contrast intensity profile (CIP) was introduced to describe the dynamics of coronary angiography data. Additionally, sensitivity studies were conducted to evaluate the influence of various coronary lumped parameter model (LPM) parameters on the shapes of CIPs. The results demonstrate that the multi-physics model can be effectively calibrated to produce physiologically meaningful hemodynamic results. Sensitivity studies reveal that resistance has a greater impact on the rising and falling slopes of CIP than capacitance, with higher resistance amplifying this effect. The model and results are presented here. These results are potentially transformative, as they provide a tool for interpreting angiographic data and ultimately extracting information concerning coronary microcirculation.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PATH: A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly Detection Approaches for Multivariate Time Series</title>
<link>https://arxiv.org/abs/2411.13951</link>
<guid>https://arxiv.org/abs/2411.13951</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, multivariate time series, automotive powertrain, semi-supervised learning, threshold selection <br />
Summary: <br />
- Benchmarking anomaly detection approaches for multivariate time series is challenging due to the lack of high-quality datasets.
- A diverse, extensive, and non-trivial dataset generated via simulation tools reflecting realistic behavior of an automotive powertrain is proposed.
- The dataset represents a discrete-sequence problem not addressed by previous literature.
- Different versions of the dataset are provided for unsupervised and semi-supervised anomaly detection settings, time series generation, and forecasting.
- Baseline results from deterministic and variational autoencoders, as well as a non-parametric approach, show the importance of robustness to contaminated training data and the influence of the threshold on detection performance. Further work is needed to improve threshold selection methods without requiring labeled data. <br /> <div>
arXiv:2411.13951v5 Announce Type: replace-cross 
Abstract: Benchmarking anomaly detection approaches for multivariate time series is a challenging task due to a lack of high-quality datasets. Current publicly available datasets are too small, not diverse and feature trivial anomalies, which hinders measurable progress in this research area. We propose a solution: a diverse, extensive, and non-trivial dataset generated via state-of-the-art simulation tools that reflects realistic behaviour of an automotive powertrain, including its multivariate, dynamic and variable-state properties. Additionally, our dataset represents a discrete-sequence problem, which remains unaddressed by previously-proposed solutions in literature. To cater for both unsupervised and semi-supervised anomaly detection settings, as well as time series generation and forecasting, we make different versions of the dataset available, where training and test subsets are offered in contaminated and clean versions, depending on the task. We also provide baseline results from a selection of approaches based on deterministic and variational autoencoders, as well as a non-parametric approach. As expected, the baseline experimentation shows that the approaches trained on the semi-supervised version of the dataset outperform their unsupervised counterparts, highlighting a need for approaches more robust to contaminated training data. Furthermore, results show that the threshold used can have a large influence on detection performance, hence more work needs to be invested in methods to find a suitable threshold without the need for labelled data.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Axioms for Model Fidelity Evaluation</title>
<link>https://arxiv.org/abs/2507.23020</link>
<guid>https://arxiv.org/abs/2507.23020</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital engineering, model fidelity, simulation, evaluation framework, ground vehicle model

Summary:
Digital engineering has revolutionized the design process, with simulations playing a crucial role in providing information consistent with reality. The concept of model fidelity, which refers to the similarity between a simulation and reality, is essential in this context. However, existing definitions of model fidelity lack formal rigor, leading to ambiguity in evaluation processes. This paper introduces seven axioms to guide the development of future fidelity evaluation frameworks. By applying these axioms to a ground vehicle model, the study demonstrates their practicality. The axioms serve as a foundation for potential advancements in evaluating model fidelity and can be used as reference points for future research in this area. This work highlights the importance of ensuring the accuracy and reliability of simulations in digital engineering practices. 

<br /><br />Summary: <div>
arXiv:2507.23020v1 Announce Type: new 
Abstract: Digital engineering has transformed the design and development process. However, the utility of digital engineering is fundamentally dependent on the assumption that a simulation provides information consistent with reality. This relationship is described as model fidelity. Despite the widespread use of the term, existing definitions of model fidelity often lack formal rigor in practical application, which leaves ambiguity in how this similarity should be evaluated. This paper presents seven fundamental axioms to aid the development of future fidelity evaluation frameworks. An example of a ground vehicle model is used under an existing fidelity evaluation framework to observe the applicability of these axioms. In addition, these axioms are used as a reference point for considering future opportunities in future work related to model fidelity.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information Bottleneck Asset Pricing Model</title>
<link>https://arxiv.org/abs/2507.23218</link>
<guid>https://arxiv.org/abs/2507.23218</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep neural networks, financial asset pricing, information bottleneck, mutual information, nonlinear relationships 

Summary: 
Deep neural networks (DNNs) are often used in financial asset pricing due to their ability to model complex nonlinear relationships in financial data. However, these models can over-fit to noise in the data, leading to subpar performance. To combat this issue, a new information bottleneck asset pricing model is proposed. This model compresses data with low signal-to-noise ratios by eliminating redundant information while retaining critical information for asset pricing. By imposing constraints of mutual information during the nonlinear mapping process, the model progressively reduces the mutual information between input data and compressed representation while increasing mutual information between the compressed representation and output prediction. This approach ensures that irrelevant information (noise) is filtered out during the modeling of financial nonlinear relationships, ultimately improving asset pricing accuracy. <div>
arXiv:2507.23218v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) have garnered significant attention in financial asset pricing, due to their strong capacity for modeling complex nonlinear relationships within financial data. However, sophisticated models are prone to over-fitting to the noise information in financial data, resulting in inferior performance. To address this issue, we propose an information bottleneck asset pricing model that compresses data with low signal-to-noise ratios to eliminate redundant information and retain the critical information for asset pricing. Our model imposes constraints of mutual information during the nonlinear mapping process. Specifically, we progressively reduce the mutual information between the input data and the compressed representation while increasing the mutual information between the compressed representation and the output prediction. The design ensures that irrelevant information, which is essentially the noise in the data, is forgotten during the modeling of financial nonlinear relationships without affecting the final asset pricing. By leveraging the constraints of the Information bottleneck, our model not only harnesses the nonlinear modeling capabilities of deep networks to capture the intricate relationships within financial data but also ensures that noise information is filtered out during the information compression process.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adjoint-Based Aerodynamic Shape Optimization with a Manifold Constraint Learned by Diffusion Models</title>
<link>https://arxiv.org/abs/2507.23443</link>
<guid>https://arxiv.org/abs/2507.23443</guid>
<content:encoded><![CDATA[
<div> shape optimization, aerodynamics, adjoint method, diffusion model, automatic differentiation

Summary:
This article presents a novel approach to aerodynamic shape optimization using an adjoint-based framework. The framework incorporates a diffusion model trained on existing designs to learn a smooth manifold of aerodynamically viable shapes, which is enforced as an equality constraint in the optimization problem. By computing adjoint gradients of design objectives with respect to the manifold space, the method eliminates the need for ad hoc parameter tuning and variable scaling. The framework integrates seamlessly into existing adjoint-based design workflows with minimal modification and demonstrates superior aerodynamic performance compared to conventional approaches in transonic RANS airfoil design cases. By combining AI-generated priors with adjoint methods, this approach enables robust, high-fidelity aerodynamic shape optimization through automatic differentiation. <div>
arXiv:2507.23443v1 Announce Type: new 
Abstract: We introduce an adjoint-based aerodynamic shape optimization framework that integrates a diffusion model trained on existing designs to learn a smooth manifold of aerodynamically viable shapes. This manifold is enforced as an equality constraint to the shape optimization problem. Central to our method is the computation of adjoint gradients of the design objectives (e.g., drag and lift) with respect to the manifold space. These gradients are derived by first computing shape derivatives with respect to conventional shape design parameters (e.g., Hicks-Henne parameters) and then backpropagating them through the diffusion model to its latent space via automatic differentiation. Our framework preserves mathematical rigor and can be integrated into existing adjoint-based design workflows with minimal modification. Demonstrated on extensive transonic RANS airfoil design cases using off-the-shelf and general-purpose nonlinear optimizers, our approach eliminates ad hoc parameter tuning and variable scaling, maintains robustness across initialization and optimizer choices, and achieves superior aerodynamic performance compared to conventional approaches. This work establishes how AI generated priors integrates effectively with adjoint methods to enable robust, high-fidelity aerodynamic shape optimization through automatic differentiation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis</title>
<link>https://arxiv.org/abs/2507.22936</link>
<guid>https://arxiv.org/abs/2507.22936</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Financial Natural Language Processing, Comparative Evaluation, 10-K Filings, Model Performance

Summary: 
Large Language Models (LLMs) have shown significant capabilities in various Financial Natural Language Processing (FinNLP) tasks. This study compares five leading LLMs, including GPT, Claude, Perplexity, Gemini, and DeepSeek, using 10-K filings from top technology companies. Evaluation methods include human annotation, automated metrics, and model behavior diagnostics. GPT performs best in coherence, semantic alignment, and contextual relevance, followed by Claude and Perplexity. Gemini and DeepSeek exhibit more variability and disagreement. Model outputs differ based on prompts and source material, varying across companies and over time. The study highlights the importance of careful prompt design and data selection in influencing LLM performance in financial analysis.<br /><br />Summary: <div>
arXiv:2507.22936v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide variety of Financial Natural Language Processing (FinNLP) tasks. However, systematic comparisons among widely used LLMs remain underexplored. Given the rapid advancement and growing influence of LLMs in financial analysis, this study conducts a thorough comparative evaluation of five leading LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the 'Magnificent Seven' technology companies. We create a set of domain-specific prompts and then use three methodologies to evaluate model performance: human annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics (prompt-level variance and across-model similarity). The results show that GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement. Also, the similarity and stability of outputs change from company to company and over time, showing that they are sensitive to how prompts are written and what source material is used.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Machine Learning with Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2507.22959</link>
<guid>https://arxiv.org/abs/2507.22959</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific machine learning, Kolmogorov-Arnold Networks, data encoding, interpretability, nonlinear interactions<br />
Summary:<br />
The article discusses the shift from multilayer perceptrons (MLPs) to Kolmogorov-Arnold Networks (KANs) in scientific machine learning. KANs offer enhanced interpretability, flexibility, and improved capability in capturing complex nonlinear interactions, overcoming the limitations of MLPs. The review categorizes recent progress in KAN-based models from three perspectives: data-driven learning, physics-informed modeling, and deep operator learning. It highlights the advantages of KANs in accuracy, convergence, and spectral representation compared to MLPs. The review also identifies challenges in KAN development such as computational efficiency, theoretical guarantees, hyperparameter tuning, and algorithm complexity. Future research directions focus on improving the robustness, scalability, and physical consistency of KAN-based frameworks.<br /><br /> <div>
arXiv:2507.22959v1 Announce Type: cross 
Abstract: The field of scientific machine learning, which originally utilized multilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold Networks (KANs) for data encoding. This shift is driven by the limitations of MLPs, including poor interpretability, fixed activation functions, and difficulty capturing localized or high-frequency features. KANs address these issues with enhanced interpretability and flexibility, enabling more efficient modeling of complex nonlinear interactions and effectively overcoming the constraints associated with conventional MLP architectures. This review categorizes recent progress in KAN-based models across three distinct perspectives: (i) data-driven learning, (ii) physics-informed modeling, and (iii) deep operator learning. Each perspective is examined through the lens of architectural design, training strategies, application efficacy, and comparative evaluation against MLP-based counterparts. By benchmarking KANs against MLPs, we highlight consistent improvements in accuracy, convergence, and spectral representation, clarifying KANs' advantages in capturing complex dynamics while learning more effectively. Finally, this review identifies critical challenges and open research questions in KAN development, particularly regarding computational efficiency, theoretical guarantees, hyperparameter tuning, and algorithm complexity. We also outline future research directions aimed at improving the robustness, scalability, and physical consistency of KAN-based frameworks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Readiness for Scientific AI at Scale</title>
<link>https://arxiv.org/abs/2507.23018</link>
<guid>https://arxiv.org/abs/2507.23018</guid>
<content:encoded><![CDATA[
<div> Data Readiness for AI, leadership-scale scientific datasets, foundation models, preprocessing patterns, domain-specific constraints <br />
<br />
Summary: This paper explores how Data Readiness for AI (DRAI) principles apply to leadership-scale scientific datasets for training foundation models. By analyzing workflows in climate, nuclear fusion, bio/health, and materials domains, common preprocessing patterns and domain-specific constraints are identified. A two-dimensional readiness framework is introduced, consisting of Data Readiness Levels and Data Processing Stages tailored to high performance computing environments. This framework highlights challenges in transforming scientific data for scalable AI training, particularly focusing on transformer-based generative models. By incorporating these dimensions, a conceptual maturity matrix is formed to characterize scientific data readiness and guide infrastructure development towards standardized, cross-domain support for scalable and reproducible AI applications in science. <br /> <div>
arXiv:2507.23018v1 Announce Type: cross 
Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to leadership-scale scientific datasets used to train foundation models. We analyze archetypal workflows across four representative domains - climate, nuclear fusion, bio/health, and materials - to identify common preprocessing patterns and domain-specific constraints. We introduce a two-dimensional readiness framework composed of Data Readiness Levels (raw to AI-ready) and Data Processing Stages (ingest to shard), both tailored to high performance computing (HPC) environments. This framework outlines key challenges in transforming scientific data for scalable AI training, emphasizing transformer-based generative models. Together, these dimensions form a conceptual maturity matrix that characterizes scientific data readiness and guides infrastructure development toward standardized, cross-domain support for scalable and reproducible AI for science.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution</title>
<link>https://arxiv.org/abs/2507.23600</link>
<guid>https://arxiv.org/abs/2507.23600</guid>
<content:encoded><![CDATA[
<div> matrix factorization, multivariate curve resolution, deep learning, signal unmixing, generative modeling

Summary:
This study introduces a novel energy-based deep learning solver, EB-gMCR, for signal unmixing analysis. By reformulating multivariate curve resolution as a generative process, EB-gMCR automatically determines the optimal component set for faithful data reconstruction. The solver utilizes a differentiable gating network to select active components and estimate their concentrations, achieving high accuracy even in the presence of noise. Additional chemical priors can be easily incorporated into the framework, allowing for adaptation to various instruments and domains without changing the core learning process. With the ability to handle large datasets and unknown component counts, EB-gMCR provides a practical approach to scalable signal unmixing analysis, particularly in chemical library-driven scenarios. The source code for EB-gMCR is available on GitHub for further exploration and use. <br /><br />Summary: <div>
arXiv:2507.23600v1 Announce Type: cross 
Abstract: Signal unmixing analysis decomposes data into basic patterns and is widely applied in chemical and biological research. Multivariate curve resolution (MCR), a branch of signal unmixing, separates mixed chemical signals into base patterns (components) and their concentrations, playing a key role in understanding composition. Classical MCR is typically framed as matrix factorization (MF) and requires a user-specified component count, usually unknown in real data. As dataset size or component count increases, the scalability and reliability of MF-based MCR face significant challenges. This study reformulates MCR as a generative process (gMCR), and introduces an energy-based deep learning solver, EB-gMCR, that automatically discovers the smallest component set able to reconstruct the data faithfully. EB-gMCR starts from a large candidate pool (e.g., 1024 spectra) and employs a differentiable gating network to retain only active components while estimating their concentrations. On noisy synthetic datasets containing up to 256 latent sources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count within 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near exact component estimation. Additional chemical priors, such as non-negativity or nonlinear mixing, enter as simple plug-in functions, enabling adaptation to other instruments or domains without altering the core learning process. By uniting high-capacity generative modeling and hard component selection, EB-gMCR offers a practical route to large-scale signal unmixing analysis, including chemical library-driven scenarios. The source code is available at https://github.com/b05611038/ebgmcr_solver.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An entropy-stable and kinetic energy-preserving macro-element HDG method for compressible flows</title>
<link>https://arxiv.org/abs/2507.22195</link>
<guid>https://arxiv.org/abs/2507.22195</guid>
<content:encoded><![CDATA[
<div> Efficient, robust simulation, compressible flows, high order numerical framework, macro element HDG method, turbulence simulation<br />
Summary:<br />
This paper presents a new high order numerical framework for simulating compressible flows efficiently and robustly. The approach, called macro element HDG method, embeds continuous Galerkin structure within macro-elements to reduce degrees of freedom and enable highly parallel local solves. By using entropy variables and a flux differencing approach, the method extends its robustness, making it suitable for under resolved or turbulent regimes. The formulations ensure entropy stability and kinetic energy preservation while maintaining high order accuracy. The method's performance is demonstrated on benchmark problems, showing optimal accuracy, improved robustness, and significant speedup compared to standard HDG methods. These advancements mark a significant progress in high order methods for direct numerical simulation (DNS) of compressible flows.<br /> <div>
arXiv:2507.22195v1 Announce Type: new 
Abstract: This paper introduces a high order numerical framework for efficient and robust simulation of compressible flows. To address the inefficiencies of standard hybridized discontinuous Galerkin (HDG) methods in large scale settings, we develop a macro element HDG method that reduces global and local degrees of freedom by embedding continuous Galerkin structure within macro-elements. This formulation supports matrix free implementations and enables highly parallel local solves, leading to substantial performance gains and excellent scalability on modern architectures. To enhance robustness in under resolved or turbulent regimes, we extend the method using entropy variables and a flux differencing approach to construct entropy stable and kinetic energy preserving variants. These formulations satisfy a discrete entropy inequality and improve stability without compromising high order accuracy. We demonstrate the performance of the proposed method on benchmark problems including the inviscid isentropic vortex and the Taylor Green vortex in both inviscid and turbulent regimes. Numerical results confirm optimal accuracy, improved robustness, and up to an order of magnitude speedup over standard HDG methods. These developments mark a significant advancement in high order methods for direct numerical simulation (DNS) of compressible flows.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cycles Protocol: A Peer-to-Peer Electronic Clearing System</title>
<link>https://arxiv.org/abs/2507.22309</link>
<guid>https://arxiv.org/abs/2507.22309</guid>
<content:encoded><![CDATA[
<div> Keywords: financial institutions, liquidity challenges, blockchain communities, decentralized settlement systems, Cycles <br />
<br />
Summary: Financial institutions have historically formed closed clearing clubs to address liquidity challenges, excluding many small enterprises and communities. Blockchain communities offer decentralized settlement systems, but they have yet to significantly impact the real economy. To tackle these issues, Cycles introduces an open, decentralized protocol for clearing, settlement, and issuance. It aims to help firms reduce payment inefficiencies, lower working capital costs, and access diverse assets and liquidity sources. Cycles uses a privacy-preserving multilateral settlement platform based on a graph optimization algorithm, recognizing that liquidity can be found within cycles in the payment network structure. By optimizing settlement flows to reduce debt, Cycles seeks to transform the way firms manage liquidity challenges and improve financial access for all actors in the ecosystem. <br /> <div>
arXiv:2507.22309v1 Announce Type: new 
Abstract: For centuries, financial institutions have responded to liquidity challenges by forming closed, centralized clearing clubs with strict rules and membership that allow them to collaborate on using the least money to discharge the most debt. As closed clubs, much of the general public has been excluded from participation. But the vast majority of private sector actors consists of micro or small firms that are vulnerable to late payments and generally ineligible for bank loans. This low liquidity environment often results in gridlock and leads to insolvency, and it disproportionately impacts small enterprises and communities.
  On the other hand, blockchain communities have developed open, decentralized settlement systems, along with a proliferation of store of value assets and new lending protocols, allowing anyone to permissionlessly transact and access credit. However, these protocols remain used primarily for speculative purposes, and so far have fallen short of the large-scale positive impact on the real economy prophesied by their promoters.
  We address these challenges by introducing Cycles, an open, decentralized clearing, settlement, and issuance protocol. Cycles is designed to enable firms to overcome payment inefficiencies, to reduce their working capital costs, and to leverage diverse assets and liquidity sources, including cryptocurrencies, stablecoins, and lending protocols, in service of clearing more debt with less money. Cycles solves real world liquidity challenges through a privacy-preserving multilateral settlement platform based on a graph optimization algorithm. The design is based on a core insight: liquidity resides within cycles in the payment network's structure and can be accessed via settlement flows optimized to reduce debt.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A holomorphic Kolmogorov-Arnold network framework for solving elliptic problems on arbitrary 2D domains</title>
<link>https://arxiv.org/abs/2507.22678</link>
<guid>https://arxiv.org/abs/2507.22678</guid>
<content:encoded><![CDATA[
<div> holomorphic neural networks, Physics-informed, differential equations, Kolmogorov-Arnold representation, Laurent series theory
Summary:
Physics-informed holomorphic neural networks (PIHNNs) have become efficient surrogate models for differential problems. They embed the problem structure into the network, needing training only for boundary conditions. Introduction of a new holomorphic network architecture, PIHKAN, based on Kolmogorov-Arnold representation, improves accuracy with reduced complexity. Mathematical extensions broaden PIHNNs' applicability to a wider class of elliptic partial differential equations like the Helmholtz equation. A new method using Laurent series theory allows holomorphic networks to be applied to multiply-connected plane domains, removing limitations to simply-connected geometries. This advancement enhances accuracy and computational efficiency in solving two-dimensional differential problems. <br /><br />Summary: <div>
arXiv:2507.22678v1 Announce Type: new 
Abstract: Physics-informed holomorphic neural networks (PIHNNs) have recently emerged as efficient surrogate models for solving differential problems. By embedding the underlying problem structure into the network, PIHNNs require training only to satisfy boundary conditions, often resulting in significantly improved accuracy and computational efficiency compared to traditional physics-informed neural networks (PINNs). In this work, we improve and extend the application of PIHNNs to two-dimensional problems. First, we introduce a novel holomorphic network architecture based on the Kolmogorov-Arnold representation (PIHKAN), which achieves higher accuracy with reduced model complexity. Second, we develop mathematical extensions that broaden the applicability of PIHNNs to a wider class of elliptic partial differential equations, including the Helmholtz equation. Finally, we propose a new method based on Laurent series theory that enables the application of holomorphic networks to multiply-connected plane domains, thereby removing the previous limitation to simply-connected geometries.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep reinforcement learning for efficient exploration of combinatorial structural design spaces</title>
<link>https://arxiv.org/abs/2507.22804</link>
<guid>https://arxiv.org/abs/2507.22804</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, structural design, form finding, sequential decision-making, material efficiency

Summary:
The paper introduces a novel reinforcement learning framework for performance-driven structural design, departing from traditional top-down optimization methods. Structures are modeled as compositions of predefined elements to align with practical constraints like constructability. The framework transforms the design task into a sequential decision-making problem and utilizes a training algorithm inspired by human learning. By applying reinforcement learning to structural design, the method efficiently searches large combinatorial design spaces. Through experimentation on steel braced truss frame cantilever structures, the trained policies consistently generate high-performing and structurally efficient designs based on known engineering principles. The analysis reveals that the agent effectively focuses its search on promising regions of the design space, showcasing transferable structural knowledge. <br /><br />Summary: <div>
arXiv:2507.22804v1 Announce Type: new 
Abstract: This paper proposes a reinforcement learning framework for performance-driven structural design that combines bottom-up design generation with learned strategies to efficiently search large combinatorial design spaces. Motivated by the limitations of conventional top-down approaches such as optimization, the framework instead models structures as compositions of predefined elements, aligning form finding with practical constraints like constructability and component reuse. With the formulation of the design task as a sequential decision-making problem and a human learning inspired training algorithm, the method adapts reinforcement learning for structural design. The framework is demonstrated by designing steel braced truss frame cantilever structures, where trained policies consistently generate distinct, high-performing designs that display structural performance and material efficiency with the use of structural strategies that align with known engineering principles. Further analysis shows that the agent efficiently narrows its search to promising regions of the design space, revealing transferable structural knowledge.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling and simulation of electro-mechanically coupled dielectric elastomers and myocardial tissue using smoothed finite element methods</title>
<link>https://arxiv.org/abs/2507.22838</link>
<guid>https://arxiv.org/abs/2507.22838</guid>
<content:encoded><![CDATA[
<div> tetrahedral meshes, cardiac electro-mechanics, finite element method, di-electric elastomer actuators, smoothed finite element methods 

Summary: 
This study explores the use of smoothed finite element methods (S-FEMs) in cardiac electro-mechanics to address issues with stiffness and volume locking that arise from automatically generated tetrahedral meshes. Four approaches, including standard linear FEM, face-based S-FEM (FS-FEM), node-based S-FEM (NS-FEM), and the hybrid FSNS-FEM, were implemented and evaluated in modeling electrically induced contraction in dielectric elastomers and orthotropic myocardial tissue samples. Results show that FSNS-FEM offers the best balance of accuracy and computational efficiency, closely matching reference data. NS-FEM produces softer results, leading to an overestimation of deformation, while FS-FEM and standard FEM exhibit overly stiff behavior and volume locking. These findings suggest that S-FEMs, particularly FSNS-FEM, hold promise for accurately simulating coupled electro-mechanical behavior in complex biomedical applications.<br /><br /> <div>
arXiv:2507.22838v1 Announce Type: new 
Abstract: Computational modelling offers a cost-effective and time-efficient alternative to experimental studies in biomedical engineering. In cardiac electro-mechanics, finite element method (FEM)-based simulations provide valuable insights into diseased tissue behaviour and the development of assistive systems such as di-electric elastomer actuators. However, the use of automatically generated tetrahedral meshes, commonly applied due to geometric complexity, often leads to numerical issues including overly stiff responses and volume locking, particularly in incompressible materials. Smoothed finite element methods (S-FEMs) offer a promising alternative by softening the stiffness matrix through gradient smoothing over defined smoothing domains. This work extends S-FEM formulations to electro-mechanically coupled problems and compares their performance against standard linear FEM. We implement and evaluate four approaches in the Abaqus environment via custom user elements: standard linear FEM, face-based S-FEM (FS-FEM), node-based S-FEM (NS-FEM), and the hybrid face/node-based S-FEM (FSNS-FEM). Two benchmark problems are studied: the electrically induced contraction of a compressible dielectric elastomer and an incompressible, orthotropic myocardial tissue sample. Reference solutions are obtained using a mesh consisting of higher-order elements. Our results demonstrate that FSNS-FEM provides the best balance between accuracy and computational efficiency, closely matching reference data. NS-FEM produces softer results, which leads to an overestimation of the true deformation. FS-FEM and standard FEM consistently exhibit overly stiff behaviour, with pronounced volume locking in the myocardial case. These findings support the potential of S-FEMs, in particular FSNS-FEM, for accurate simulation of coupled electro-mechanical behaviour in complex biomedical applications.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh based segmentation for automated margin line generation on incisors receiving crown treatment</title>
<link>https://arxiv.org/abs/2507.22859</link>
<guid>https://arxiv.org/abs/2507.22859</guid>
<content:encoded><![CDATA[
<div> Keywords: dental crowns, deep learning, segmentation, margin line, dataset 

Summary: 
- Dental crowns are crucial for restoring damaged teeth, with current design methods relying on manual input for margin line definition.
- A new framework utilizing deep learning was developed to automatically and accurately determine margin lines in dental preparations.
- An ensemble model combined with maximum probability showed the highest success rate in predicting margin lines.
- The quality of the preparation directly affected the accuracy of margin line prediction.
- The study provides the community with the datasets used for training and testing the deep learning model. 

<br /><br />Summary: <div>
arXiv:2507.22859v1 Announce Type: new 
Abstract: Dental crowns are essential dental treatments for restoring damaged or missing teeth of patients. Recent design approaches of dental crowns are carried out using commercial dental design software. Once a scan of a preparation is uploaded to the software, a dental technician needs to manually define a precise margin line on the preparation surface, which constitutes a non-repeatable and inconsistent procedure. This work proposes a new framework to determine margin lines automatically and accurately using deep learning. A dataset of incisor teeth was provided by a collaborating dental laboratory to train a deep learning segmentation model. A mesh-based neural network was modified by changing its input channels and used to segment the prepared tooth into two regions such that the margin line is contained within the boundary faces separating the two regions. Next, k-fold cross-validation was used to train 5 models, and a voting classifier technique was used to combine their results to enhance the segmentation. After that, boundary smoothing and optimization using the graph cut method were applied to refine the segmentation results. Then, boundary faces separating the two regions were selected to represent the margin line faces. A spline was approximated to best fit the centers of the boundary faces to predict the margin line. Our results show that an ensemble model combined with maximum probability predicted the highest number of successful test cases (7 out of 13) based on a maximum distance threshold of 200 m (representing human error) between the predicted and ground truth point clouds. It was also demonstrated that the better the quality of the preparation, the smaller the divergence between the predicted and ground truth margin lines (Spearman's rank correlation coefficient of -0.683). We provide the train and test datasets for the community.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach</title>
<link>https://arxiv.org/abs/2507.22070</link>
<guid>https://arxiv.org/abs/2507.22070</guid>
<content:encoded><![CDATA[
<div> metaclass, Protocol Buffers, test data generation, statistical analysis, enterprise systems <br />
<br />
In this paper, a novel framework for generating test data for enterprise systems using Protocol Buffers is proposed. The framework leverages Python's metaclass system for dynamic type enhancement and statistical analysis of production logs to extract realistic value domains. It combines automatic schema introspection, statistical value distribution analysis, and recursive descent algorithms to handle complex nested data structures. Experimental results show significant reduction in test data preparation time and improvement in test coverage compared to existing approaches. The framework is capable of handling protobuf structures with up to 15 levels of nesting and generating over 100,000 test cases within seconds. This approach addresses the challenges posed by large-scale enterprise systems with intricate hierarchical and graph-like structures, making it a valuable tool for performance testing in such environments. <br /><br />Summary: <div>
arXiv:2507.22070v1 Announce Type: cross 
Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present significant challenges for performance testing, particularly when targeting intermediate business interfaces with complex nested data structures. Traditional test data generation approaches are inadequate for handling the intricate hierarchical and graph-like structures inherent in enterprise protobuf schemas. This paper presents a novel test data generation framework that leverages Python's metaclass system for dynamic type enhancement and statistical analysis of production logs for realistic value domain extraction. Our approach combines automatic schema introspection, statistical value distribution analysis, and recursive descent algorithms for handling deeply nested structures. Experimental evaluation on three real-world enterprise systems demonstrates up to 95\% reduction in test data preparation time and 80\% improvement in test coverage compared to existing approaches. The framework successfully handles protobuf structures with up to 15 levels of nesting and generates comprehensive test suites containing over 100,000 test cases within seconds.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Variance Optimization and Algorithm for Finite-Horizon Markov Decision Processes</title>
<link>https://arxiv.org/abs/2507.22327</link>
<guid>https://arxiv.org/abs/2507.22327</guid>
<content:encoded><![CDATA[
<div> Mean-variance optimization, multi-period, Markov decision processes, pseudo mean, pseudo variance

Summary: 
This paper addresses the challenge of multi-period mean-variance optimization in finite-horizon discrete-time Markov decision processes. By introducing pseudo mean and pseudo variance concepts, the problem is transformed into a bilevel MDP. The bilevel MDP consists of an outer optimization for pseudo mean and an inner MDP with augmented state space. The properties of the bilevel MDP are explored, and an iterative algorithm is proposed to efficiently solve it, converging to a local optimum. Conditions for global optimum convergence are also derived. The approach is applied to multi-period portfolio selection and other scenarios such as queueing control and inventory management. The results align with classic financial engineering findings. This innovative approach presents a new method for mean-variance optimization problems in MDP models with broad applicability. <br /><br />Summary: <div>
arXiv:2507.22327v1 Announce Type: cross 
Abstract: Multi-period mean-variance optimization is a long-standing problem, caused by the failure of dynamic programming principle. This paper studies the mean-variance optimization in a setting of finite-horizon discrete-time Markov decision processes (MDPs), where the objective is to maximize the combined metrics of mean and variance of the accumulated rewards at terminal stage. By introducing the concepts of pseudo mean and pseudo variance, we convert the original mean-variance MDP to a bilevel MDP, where the outer is a single parameter optimization of the pseudo mean and the inner is a standard finite-horizon MDP with an augmented state space by adding an auxiliary state of accumulated rewards. We further study the properties of this bilevel MDP, including the optimality of history-dependent deterministic policies and the piecewise quadratic concavity of the inner MDPs' optimal values with respect to the pseudo mean. To efficiently solve this bilevel MDP, we propose an iterative algorithm that alternatingly updates the inner optimal policy and the outer pseudo mean. We prove that this algorithm converges to a local optimum. We also derive a sufficient condition under which our algorithm converges to the global optimum. Furthermore, we apply this approach to study the mean-variance optimization of multi-period portfolio selection problem, which shows that our approach exactly coincides with the classical result by Li and Ng (2000) in financial engineering. Our approach builds a new avenue to solve mean-variance optimization problems and has wide applicability to any problem modeled by MDPs, which is further demonstrated by examples of mean-variance optimization for queueing control and inventory management.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A surrogate model for topology optimisation of elastic structures via parametric autoencoders</title>
<link>https://arxiv.org/abs/2507.22539</link>
<guid>https://arxiv.org/abs/2507.22539</guid>
<content:encoded><![CDATA[
<div> surrogate-based, topology optimisation, linear elastic structures, parametric loads, boundary conditions <br />
Summary: 
A new surrogate-based algorithm for topology optimization of linear elastic structures under parametric loads and boundary conditions is introduced. The approach involves using a surrogate model to predict quasi-optimal topologies based on system parameters, trained through a feed-forward neural network. This predicted topology serves as an initial guess for a computationally efficient optimization algorithm, allowing for correction of errors and refinement of the design. The method demonstrates superior performance compared to high-fidelity optimizers, reducing the average number of iterations by 53% and maintaining discrepancies below 4% in the optimal functional value. Various architectures are proposed and evaluated for their approximation and generalization capabilities. The quasi-optimal topologies generated by the surrogate model enable efficient and accurate topology optimization even when extrapolating beyond the training and validation domain. <br /> <div>
arXiv:2507.22539v1 Announce Type: cross 
Abstract: A surrogate-based topology optimisation algorithm for linear elastic structures under parametric loads and boundary conditions is proposed. Instead of learning the parametric solution of the state (and adjoint) problems or the optimisation trajectory as a function of the iterations, the proposed approach devises a surrogate version of the entire optimisation pipeline. First, the method predicts a quasi-optimal topology for a given problem configuration as a surrogate model of high-fidelity topologies optimised with the homogenisation method. This is achieved by means of a feed-forward net learning the mapping between the input parameters characterising the system setup and a latent space determined by encoder/decoder blocks reducing the dimensionality of the parametric topology optimisation problem and reconstructing a high-dimensional representation of the topology. Then, the predicted topology is used as an educated initial guess for a computationally efficient algorithm penalising the intermediate values of the design variable, while enforcing the governing equations of the system. This step allows the method to correct potential errors introduced by the surrogate model, eliminate artifacts, and refine the design in order to produce topologies consistent with the underlying physics. Different architectures are proposed and the approximation and generalisation capabilities of the resulting models are numerically evaluated. The quasi-optimal topologies allow to outperform the high-fidelity optimiser by reducing the average number of optimisation iterations by $53\%$ while achieving discrepancies below $4\%$ in the optimal value of the objective functional, even in the challenging scenario of testing the model to extrapolate beyond the training and validation domain.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASCA: LLM based-Multi Agents System for Credit Assessment</title>
<link>https://arxiv.org/abs/2507.22758</link>
<guid>https://arxiv.org/abs/2507.22758</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, agent-based systems, credit assessment, MASCA, hierarchical multi-agent systems

Summary: 
MASCA is introduced as a multi-agent system driven by LLMs to improve credit assessment through a layered architecture approach. The system integrates contrastive learning for risk and reward evaluation and incorporates a signaling game theory perspective for theoretical insights. A detailed bias analysis is included to address fairness concerns in credit assessment. Experimental results show that MASCA outperforms baseline methods, showcasing the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring. <div>
arXiv:2507.22758v1 Announce Type: cross 
Abstract: Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic flux surrogate-based partitioned methods for interface problems</title>
<link>https://arxiv.org/abs/2402.03560</link>
<guid>https://arxiv.org/abs/2402.03560</guid>
<content:encoded><![CDATA[
<div> dynamic mode decomposition, partitioned methods, multiphysics problems, surrogate modeling, parametric PDEs  
Summary:  
Loosely coupled partitioned methods for multiphysics problems are beneficial for code reuse, concurrency, and plug-and-play simulations. However, they can compromise accuracy and stability. This study introduces a data-driven partitioned method for coupled parametric PDEs that improves accuracy without sacrificing performance. By replacing field transfers with a surrogate for interface flux dynamics, the approach uses dynamic mode decomposition on a staggered-in-time state. The offline training phase handles the computational load, while applying the surrogate in the online phase involves a single matrix-vector multiplication. Stability analysis of the scheme is provided, along with numerical results showcasing its effectiveness. <div>
arXiv:2402.03560v2 Announce Type: replace 
Abstract: Loosely coupled partitioned methods for multiphysics problems treat each subproblem as a separate entity and advance them independently in time. In so doing these methods enable code reuse, increase concurrency and provide a convenient framework for plug-and-play multiphysics simulations. However, mathematically loosely coupled schemes are equivalent to a single step of an iterative solution method, which can compromise their accuracy and stability. We present a new data-driven partitioned method for coupled parametric PDEs that can improve upon the accuracy of traditional loosely coupled methods without incurring a performance penalty. To that end, we replace conventional field transfers across the interface by a surrogate for the dynamics of the interface flux exchanged between the subdomains. To develop this surrogate we apply dynamic mode decomposition to a non-standard staggered-in-time state, comprising the interface flux and small solution patches near the interface. The new approach shifts the main computational burden to an offline training phase, whereas application of the surrogate in the online phase amounts to a single matrix-vector multiplication. We provide stability analysis of the surrogate-based partitioned scheme and include numerical results that demonstrate its potential.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Neural Network Training using Dynamic Learning Rate Schedule for PINNs and Image Classification</title>
<link>https://arxiv.org/abs/2507.21749</link>
<guid>https://arxiv.org/abs/2507.21749</guid>
<content:encoded><![CDATA[
<div> dynamic learning rate scheduler, neural networks, training, hyperparameters, optimization

Summary:
The paper introduces a dynamic learning rate scheduler (DLRS) algorithm to address the challenges in training neural networks, particularly in complex problems. The conventional static learning rate approach can lead to tedious training processes and suboptimal results. The DLRS adapts the learning rate based on loss values calculated during training, allowing for more efficient navigation of varying gradients and improved optimization. Experiments on physics-informed neural networks (PINNs) and image classification tasks using multilayer perceptrons and convolutional neural networks show that the DLRS accelerates training and enhances stability. This adaptive approach to learning rate optimization proves to be beneficial in improving the training process and achieving better performance in neural network tasks. <div>
arXiv:2507.21749v1 Announce Type: new 
Abstract: Training neural networks can be challenging, especially as the complexity of the problem increases. Despite using wider or deeper networks, training them can be a tedious process, especially if a wrong choice of the hyperparameter is made. The learning rate is one of such crucial hyperparameters, which is usually kept static during the training process. Learning dynamics in complex systems often requires a more adaptive approach to the learning rate. This adaptability becomes crucial to effectively navigate varying gradients and optimize the learning process during the training process. In this paper, a dynamic learning rate scheduler (DLRS) algorithm is presented that adapts the learning rate based on the loss values calculated during the training process. Experiments are conducted on problems related to physics-informed neural networks (PINNs) and image classification using multilayer perceptrons and convolutional neural networks, respectively. The results demonstrate that the proposed DLRS accelerates training and improves stability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge</title>
<link>https://arxiv.org/abs/2507.21990</link>
<guid>https://arxiv.org/abs/2507.21990</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, chemistry, reasoning, interpretable outputs, human-AI collaboration <br />
Summary: 
ChemDFM-R is a Chemical Reasoner LLM developed to improve reasoning capabilities in chemistry. The model is trained on a comprehensive dataset of atomized knowledge points to enhance its understanding of fundamental principles. A mix-sourced distillation strategy is used to integrate expert-curated knowledge and general-domain reasoning skills, followed by domain-specific reinforcement learning for enhanced chemical reasoning. ChemDFM-R achieves state-of-the-art performance on diverse chemical benchmarks and provides interpretable, rationale-driven outputs. The model's explicit reasoning chains improve reliability, transparency, and practical utility in real-world human-AI collaboration scenarios. <div>
arXiv:2507.21990v1 Announce Type: new 
Abstract: While large language models (LLMs) have achieved impressive progress, their application in scientific domains such as chemistry remains hindered by shallow domain understanding and limited reasoning capabilities. In this work, we focus on the specific field of chemistry and develop a Chemical Reasoner LLM, ChemDFM-R. We first construct a comprehensive dataset of atomized knowledge points to enhance the model's understanding of the fundamental principles and logical structure of chemistry. Then, we propose a mix-sourced distillation strategy that integrates expert-curated knowledge with general-domain reasoning skills, followed by domain-specific reinforcement learning to enhance chemical reasoning. Experiments on diverse chemical benchmarks demonstrate that ChemDFM-R achieves state-of-the-art performance while providing interpretable, rationale-driven outputs. Further case studies illustrate how explicit reasoning chains significantly improve the reliability, transparency, and practical utility of the model in real-world human-AI collaboration scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2507.07426</link>
<guid>https://arxiv.org/abs/2507.07426</guid>
<content:encoded><![CDATA[
<div> drug repurposing, large language models, structured reasoning, multi-agent collaboration, Monte Carlo Tree Search

Summary:
DrugMCTS is a novel framework that combines Retrieval-Augmented Generation (RAG), multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing. It utilizes five specialized agents to retrieve and analyze molecular and protein information, enabling structured and iterative reasoning without domain-specific fine-tuning. The framework outperforms Deepseek-R1 by over 20%, achieving higher recall and robustness on DrugBank and KIBA datasets. The results demonstrate the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in enhancing the application of Large Language Models (LLMs) in drug discovery. <div>
arXiv:2507.07426v2 Announce Type: cross 
Abstract: Recent advances in large language models have demonstrated considerable potential in scientific domains such as drug discovery. However, their effectiveness remains constrained when reasoning extends beyond the knowledge acquired during pretraining. Conventional approaches, such as fine-tuning or retrieval-augmented generation, face limitations in either imposing high computational overhead or failing to fully exploit structured scientific data. To overcome these challenges, we propose DrugMCTS, a novel framework that synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing. The framework employs five specialized agents tasked with retrieving and analyzing molecular and protein information, thereby enabling structured and iterative reasoning. Without requiring domain-specific fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially higher recall and robustness compared to both general-purpose LLMs and deep learning baselines. Our results highlight the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in advancing LLM applications for drug discovery.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bubbleformer: Forecasting Boiling with Transformers</title>
<link>https://arxiv.org/abs/2507.21244</link>
<guid>https://arxiv.org/abs/2507.21244</guid>
<content:encoded><![CDATA[
<div> Transformer-based, spatiotemporal model, boiling dynamics, nucleation, interface evolution, heat transfer <br /> 
Summary: Bubbleformer is a transformer-based spatiotemporal model that accurately forecasts boiling dynamics, including nucleation, interface evolution, and heat transfer, without relying on future input during inference. It integrates factorized axial attention, frequency-aware scaling, and conditions on thermophysical parameters to generalize across various fluid types, geometries, and operating conditions. The model is evaluated using physics-based metrics that assess heat-flux consistency, interface geometry, and mass conservation in chaotic systems. The BubbleML 2.0 dataset accompanying the model includes diverse working fluids and boiling configurations. Bubbleformer achieves new benchmark results in both prediction and forecasting of two-phase boiling flows. <br /> <div>
arXiv:2507.21244v1 Announce Type: cross 
Abstract: Modeling boiling (an inherently chaotic, multiphase process central to energy and thermal systems) remains a significant challenge for neural PDE surrogates. Existing models require future input (e.g., bubble positions) during inference because they fail to learn nucleation from past states, limiting their ability to autonomously forecast boiling dynamics. They also fail to model flow boiling velocity fields, where sharp interface-momentum coupling demands long-range and directional inductive biases. We introduce Bubbleformer, a transformer-based spatiotemporal model that forecasts stable and long-range boiling dynamics including nucleation, interface evolution, and heat transfer without dependence on simulation data during inference. Bubbleformer integrates factorized axial attention, frequency-aware scaling, and conditions on thermophysical parameters to generalize across fluids, geometries, and operating conditions. To evaluate physical fidelity in chaotic systems, we propose interpretable physics-based metrics that evaluate heat-flux consistency, interface geometry, and mass conservation. We also release BubbleML 2.0, a high-fidelity dataset that spans diverse working fluids (cryogens, refrigerants, dielectrics), boiling configurations (pool and flow boiling), flow regimes (bubbly, slug, annular), and boundary conditions. Bubbleformer sets new benchmark results in both prediction and forecasting of two-phase boiling flows.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>evoxels: A differentiable physics framework for voxel-based microstructure simulations</title>
<link>https://arxiv.org/abs/2507.21748</link>
<guid>https://arxiv.org/abs/2507.21748</guid>
<content:encoded><![CDATA[
<div> Keywords: materials science, advanced microscopy, predictive simulations, inverse modeling, machine learning

Summary:
Materials science research involves collaboration between experimentalists using advanced microscopy and theorists using computational models to understand the relationship between processing, structure, and properties. Inverse material design, starting from desired performance and working backwards to determine optimal microstructures and manufacturing routes, requires the integration of high-resolution imaging with predictive simulations and data-driven optimization. The evoxels framework, based on a Pythonic voxel-based approach, combines segmented 3D microscopy data with physical simulations, inverse modeling, and machine learning to accelerate discovery and deepen understanding of process-structure-property relationships.<br /><br />Summary: <div>
arXiv:2507.21748v1 Announce Type: cross 
Abstract: Materials science inherently spans disciplines: experimentalists use advanced microscopy to uncover micro- and nanoscale structure, while theorists and computational scientists develop models that link processing, structure, and properties. Bridging these domains is essential for inverse material design where you start from desired performance and work backwards to optimal microstructures and manufacturing routes. Integrating high-resolution imaging with predictive simulations and data-driven optimization accelerates discovery and deepens understanding of process-structure-property relationships. The differentiable physics framework evoxels is based on a fully Pythonic, unified voxel-based approach that integrates segmented 3D microscopy data, physical simulations, inverse modeling, and machine learning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predict Patient Self-reported Race from Skin Histological Images</title>
<link>https://arxiv.org/abs/2507.21912</link>
<guid>https://arxiv.org/abs/2507.21912</guid>
<content:encoded><![CDATA[
<div> Deep learning, Artificial Intelligence, computational pathology, race prediction, bias mitigation  
Summary:   
- The study investigates the use of deep learning models to predict self-reported race from digitized dermatopathology slides.  
- Attention-based mechanisms are applied to uncover race-associated morphological features.  
- Different dataset curation strategies were evaluated to control for confounding factors.  
- White and Black demographic groups showed high prediction performance, while overall performance decreased when considering all groups.  
- Attention analysis revealed the epidermis as a key predictive feature, emphasizing the importance of careful data curation and bias mitigation for equitable AI deployment in pathology.  
<br /><br />Summary: <div>
arXiv:2507.21912v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) has demonstrated success in computational pathology (CPath) for disease detection, biomarker classification, and prognosis prediction. However, its potential to learn unintended demographic biases, particularly those related to social determinants of health, remains understudied. This study investigates whether deep learning models can predict self-reported race from digitized dermatopathology slides and identifies potential morphological shortcuts. Using a multisite dataset with a racially diverse population, we apply an attention-based mechanism to uncover race-associated morphological features. After evaluating three dataset curation strategies to control for confounding factors, the final experiment showed that White and Black demographic groups retained high prediction performance (AUC: 0.799, 0.762), while overall performance dropped to 0.663. Attention analysis revealed the epidermis as a key predictive feature, with significant performance declines when these regions were removed. These findings highlight the need for careful data curation and bias mitigation to ensure equitable AI deployment in pathology. Code available at: https://github.com/sinai-computational-pathology/CPath_SAIF.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falling through the cracks: energy storage along segmented brittle crack fronts</title>
<link>https://arxiv.org/abs/2507.19406</link>
<guid>https://arxiv.org/abs/2507.19406</guid>
<content:encoded><![CDATA[
<div> disjoint crack front, stepped crack, material ligament, 3D measurements, strain energy density
Summary: 
The study focuses on the mechanics of crack propagation in brittle materials, specifically the formation of stepped cracks and material ligaments. Through in-situ 3D measurements using laser scanning, researchers observed the deformation field around stepped cracks and within the ligament feature. They discovered that the ligament concentrates strain energy density, leading to an increase in apparent fracture energy proportional to the strain energy within the ligament. This finding highlights the importance of understanding the role of material ligaments in controlling crack propagation behavior and provides valuable insights into the mechanics of brittle fracture. <div>
arXiv:2507.19406v2 Announce Type: replace 
Abstract: During brittle crack propagation, a smooth crack front curve frequently becomes disjoint, generating a stepped crack and a material ligament that unites the newly formed crack fronts. These universal features fundamentally alter the singular field structure and stability of propagating cracks; however, a quantitative analysis of their mechanics is lacking. Here, we perform in-situ 3D measurements to resolve the deformation field around stepped cracks, and crucially, within the ligament feature. The 3D kinematic data are obtained by scanning a thin laser sheet through the brittle hydrogel samples, while recording the scattered intensity from the embedded tracer particles. We find that the ligament concentrates the strain energy density, and moreover, the apparent fracture energy increases proportionally to the strain energy within the ligament.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IFD: A Large-Scale Benchmark for Insider Filing Violation Detection</title>
<link>https://arxiv.org/abs/2507.20162</link>
<guid>https://arxiv.org/abs/2507.20162</guid>
<content:encoded><![CDATA[
<div> Dataset, Insider trading, Form 4 filings, Regulatory compliance, MaBoost <br />
Summary: 
The article introduces the Insider Filing Delay (IFD) dataset, the largest dataset for insider disclosure behavior, covering over one million Form 4 transactions from 2002 to 2025. It addresses the challenge of insider trading violations and delayed disclosures in financial markets by presenting IFD as a benchmark for detecting strategic disclosure violations. The MaBoost framework, a hybrid model combining Mamba-based state space encoder with XGBoost, achieves high accuracy and interpretability in identifying high-risk behavioral patterns. Experiments show that MaBoost outperforms previous approaches, with an F1-score of up to 99.47% under regulatory settings. IFD serves as a realistic and reproducible benchmark for developing AI models in financial compliance, regulatory forensics, and interpretable time-series classification. The dataset and codes are publicly available for further research and analysis. <br /> <div>
arXiv:2507.20162v1 Announce Type: new 
Abstract: Insider trading violations, particularly delayed disclosures of Form 4 filings, remain a persistent challenge for financial market surveillance. Despite regulatory requirements such as the two-business-day rule of the Securities and Exchange Commission (SEC), enforcement is limited by the lack of large-scale, labeled datasets and task-specific benchmarks. In this paper, we introduce Insider Filing Delay (IFD), the first and largest publicly available dataset for insider disclosure behavior, comprising over one million Form 4 transactions spanning two decades (2002-2025), with structured annotations on delay status, insider roles, governance factors, and firm-level financial indicators. IFD enables the first large-scale formulation of strategic disclosure violation detection as a binary classification task grounded in regulatory compliance. To demonstrate the utility of IFD, we propose MaBoost, a hybrid framework combining a Mamba-based state space encoder with XGBoost, achieving high accuracy and interpretability in identifying high-risk behavioral patterns. Experiments across statistical baselines, deep learning models, and large language models confirm that MaBoost outperforms prior approaches, achieving an F1-score of up to 99.47% under constrained regulatory settings. IFD provides a realistic, reproducible, and behavior-rich benchmark for developing AI models in financial compliance, regulatory forensics, and interpretable time-series classification. All data and codes are available: https://github.com/CH-YellowOrange/MaBoost-and-IFD.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Explainable Stock Predictions with Tweets Using Mixture of Experts</title>
<link>https://arxiv.org/abs/2507.20535</link>
<guid>https://arxiv.org/abs/2507.20535</guid>
<content:encoded><![CDATA[
<div> Keywords: Stock price movements, Textual information, LLMs, FTS-Text-MoE model, Financial time series prediction

Summary: 
The FTS-Text-MoE model proposed in this study aims to improve stock price prediction by integrating numerical data with key summaries from news and social media using point embeddings. This model utilizes a Mixture of Experts Transformer decoder to process both data types, reducing computational costs by activating only a subset of model parameters. Multi-resolution prediction heads allow for flexible forecasting of financial time series at different scales. Experimental results demonstrate that FTS-Text-MoE outperforms baseline methods in terms of investment returns and Sharpe ratio, showcasing its superior accuracy and ability to predict future market trends. This approach addresses limitations in prompt-based methods and enhances financial analysis by leveraging factual textual data alongside historical price data. <div>
arXiv:2507.20535v1 Announce Type: new 
Abstract: Stock price movements are influenced by many factors, and alongside historical price data, tex-tual information is a key source. Public news and social media offer valuable insights into market sentiment and emerging events. These sources are fast-paced, diverse, and significantly impact future stock trends. Recently, LLMs have enhanced financial analysis, but prompt-based methods still have limitations, such as input length restrictions and difficulties in predicting sequences of varying lengths. Additionally, most models rely on dense computational layers, which are resource-intensive. To address these challenges, we propose the FTS- Text-MoE model, which combines numerical data with key summaries from news and tweets using point embeddings, boosting prediction accuracy through the integration of factual textual data. The model uses a Mixture of Experts (MoE) Transformer decoder to process both data types. By activating only a subset of model parameters, it reduces computational costs. Furthermore, the model features multi-resolution prediction heads, enabling flexible forecasting of financial time series at different scales. Experimental results show that FTS-Text-MoE outperforms baseline methods in terms of investment returns and Sharpe ratio, demonstrating its superior accuracy and ability to predict future market trends.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exascale Implicit Kinetic Plasma Simulations on El~Capitan for Solving the Micro-Macro Coupling in Magnetospheric Physics</title>
<link>https://arxiv.org/abs/2507.20719</link>
<guid>https://arxiv.org/abs/2507.20719</guid>
<content:encoded><![CDATA[
<div> Keywords: kinetic simulations, magnetospheres, Particle-in-Cell method, AMD Instinct MI300A Accelerated Processing Units, multi-scale coupling <br />
Summary: 
Our study presents fully kinetic, implicit Particle-in-Cell (PIC) simulations of global magnetospheres using El Capitan's AMD Instinct MI300A Accelerated Processing Units. This computational approach addresses the challenge of resolving the multi-scale coupling between microscopic and macroscopic dynamics in planetary magnetospheres. The implicit scheme of iPIC3D allows for larger time steps and grid spacing compared to explicit methods, without compromising accuracy. This capability enables the simulation of magnetospheres while preserving fine-scale electron physics crucial for processes like magnetic reconnection and plasma turbulence. Our innovations in algorithmic and technological aspects, including GPU-optimized kernels and data compression techniques, support the simulation of global-scale dynamics in small-to-medium planetary magnetospheres like Mercury and Ganymede. This advancement extends the reach of fully kinetic PIC codes to systems previously beyond their capabilities.  <br /><br />Summary: <div>
arXiv:2507.20719v1 Announce Type: new 
Abstract: Our fully kinetic, implicit Particle-in-Cell (PIC) simulations of global magnetospheres on up to 32,768 of El Capitan's AMD Instinct MI300A Accelerated Processing Units (APUs) represent an unprecedented computational capability that addresses a fundamental challenge in space physics: resolving the multi-scale coupling between microscopic (electron-scale) and macroscopic (global-scale) dynamics in planetary magnetospheres. The implicit scheme of iPIC3D supports time steps and grid spacing that are up to 10 times larger than those of explicit methods, without sacrificing physical accuracy. This enables the simulation of magnetospheres while preserving fine-scale electron physics, which is critical for key processes such as magnetic reconnection and plasma turbulence. Our algorithmic and technological innovations include GPU-optimized kernels, particle control, and physics-aware data compression using Gaussian Mixture Models. With simulation domains spanning 100-1,000 ion skin depths, we reach the global scale of small-to-medium planetary magnetospheres, such as those of Mercury and Ganymede, which supports fully kinetic treatment of global-scale dynamics in systems previously out of reach for fully kinetic PIC codes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Virtual Humans Toward Human Physiologically-Based Drug Discovery</title>
<link>https://arxiv.org/abs/2507.19568</link>
<guid>https://arxiv.org/abs/2507.19568</guid>
<content:encoded><![CDATA[
<div> AI, drug discovery, virtual experiments, programmable virtual humans, translational gap
Summary:
Artificial intelligence in drug discovery has primarily focused on digitizing existing experiments without addressing the challenges of predicting drug effects in humans. Biomedical digital twins, while useful in late-phase development, lack the resolution for early-stage discovery. To overcome this disconnect, programmable virtual humans have emerged as a solution, utilizing AI, high-throughput assays, and omics data to simulate drug actions in the human body. By bridging the translational gap, programmable virtual humans offer a new paradigm for drug discovery centered on human physiology. While offering transformative potential, key opportunities and challenges must be addressed for their realization. <div>
arXiv:2507.19568v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has sparked immense interest in drug discovery, but most current approaches only digitize existing high-throughput experiments. They remain constrained by conventional pipelines. As a result, they do not address the fundamental challenges of predicting drug effects in humans. Similarly, biomedical digital twins, largely grounded in real-world data and mechanistic models, are tailored for late-phase drug development and lack the resolution to model molecular interactions or their systemic consequences, limiting their impact in early-stage discovery. This disconnect between early discovery and late development is one of the main drivers of high failure rates in drug discovery. The true promise of AI lies not in augmenting current experiments but in enabling virtual experiments that are impossible in the real world: testing novel compounds directly in silico in the human body. Recent advances in AI, high-throughput perturbation assays, and single-cell and spatial omics across species now make it possible to construct programmable virtual humans: dynamic, multiscale models that simulate drug actions from molecular to phenotypic levels. By bridging the translational gap, programmable virtual humans offer a transformative path to optimize therapeutic efficacy and safety earlier than ever before. This perspective introduces the concept of programmable virtual humans, explores their roles in a new paradigm of drug discovery centered on human physiology, and outlines key opportunities, challenges, and roadmaps for their realization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Domain Shift in Multi-source CT-Scan Classification via Input-Space Standardization</title>
<link>https://arxiv.org/abs/2507.19858</link>
<guid>https://arxiv.org/abs/2507.19858</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-source CT-scan, domain shifts, cross-source generalization, input-space standardization, medical imaging

Summary:
Spatial-Slice Feature Learning (SSFL++) and Kernel-Density-based Slice Sampling (KDS) preprocessing pipelines address domain shifts in multi-source CT-scan classification. This study investigates the mechanisms behind the domain robustness of these pipelines and how they manage the trade-off between local discriminability and cross-source generalization. SSFL++ and KDS preprocess the input data, reducing inter-source variance and aligning inputs into a consistent target space to mitigate domain shift. This alignment simplifies the learning task for network optimization and consistently improves performance across different architectures. Experimental validation confirmed the effectiveness of the preprocessing approach, leading to a first-place finish in a competitive challenge. This study highlights the practicality and robustness of input-space standardization in multi-institutional medical imaging.<br /><br />Summary: Input-space standardization through SSFL++ and KDS preprocessing pipelines effectively addresses domain shifts in multi-source CT-scan classification. By reducing inter-source variance and aligning inputs into a consistent target space, these pipelines improve performance across architectures, simplifying the learning task and supporting cross-source generalization. Experimental validation and a first-place finish in a competitive challenge demonstrate the effectiveness and practicality of this preprocessing approach for multi-institutional medical imaging. <div>
arXiv:2507.19858v1 Announce Type: cross 
Abstract: Multi-source CT-scan classification suffers from domain shifts that impair cross-source generalization. While preprocessing pipelines combining Spatial-Slice Feature Learning (SSFL++) and Kernel-Density-based Slice Sampling (KDS) have shown empirical success, the mechanisms underlying their domain robustness remain underexplored. This study analyzes how this input-space standardization manages the trade-off between local discriminability and cross-source generalization. The SSFL++ and KDS pipeline performs spatial and temporal standardization to reduce inter-source variance, effectively mapping disparate inputs into a consistent target space. This preemptive alignment mitigates domain shift and simplifies the learning task for network optimization. Experimental validation demonstrates consistent improvements across architectures, proving the benefits stem from the preprocessing itself. The approach's effectiveness was validated by securing first place in a competitive challenge, supporting input-space standardization as a robust and practical solution for multi-institutional medical imaging.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQUA: A Large Language Model for Aquaculture &amp; Fisheries</title>
<link>https://arxiv.org/abs/2507.20520</link>
<guid>https://arxiv.org/abs/2507.20520</guid>
<content:encoded><![CDATA[
<div> Keywords: Aquaculture, artificial intelligence, large language model, AQUA, AQUADAPT 

Summary: 
Aquaculture is vital for global food security and economies. Challenges like disease outbreaks, inefficient feeding, and hatchery issues persist. Existing machine learning methods lack domain-specific solutions for aquaculture. AQUA, the first large language model tailored for aquaculture, aims to support farmers and researchers. AQUADAPT, an Agentic Framework, generates high-quality synthetic data combining expert knowledge and automated evaluation techniques. This innovation paves the way for LLM-driven advancements in aquaculture research, advisory systems, and decision-making tools. <div>
arXiv:2507.20520v1 Announce Type: cross 
Abstract: Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery</title>
<link>https://arxiv.org/abs/2507.20954</link>
<guid>https://arxiv.org/abs/2507.20954</guid>
<content:encoded><![CDATA[
<div> Python package, SHRED, deep learning strategy, dynamical systems, spatiotemporal data<br />
<br />
Summary: <br />
PySHRED is a Python package that implements SHRED, a deep learning strategy for modeling high-dimensional dynamical systems and spatiotemporal data. The version 1.0 release of PySHRED includes data preprocessors and cutting-edge SHRED methods designed for handling real-world data that may be noisy, multi-scale, parameterized, high-dimensional, and nonlinear. The package is easy to install, well-documented, and includes extensive code examples. It is modularly-structured to support future additions and is released under the MIT license. PySHRED provides extensions for robust sensing, reduced order modeling, and physics discovery, making it a valuable tool for researchers analyzing complex systems. The codebase is available on GitHub, allowing for collaboration and further development in the field of deep learning for dynamical systems. <div>
arXiv:2507.20954v1 Announce Type: cross 
Abstract: SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for modeling high-dimensional dynamical systems and/or spatiotemporal data from dynamical system snapshot observations. PySHRED is a Python package that implements SHRED and several of its major extensions, including for robust sensing, reduced order modeling and physics discovery. In this paper, we introduce the version 1.0 release of PySHRED, which includes data preprocessors and a number of cutting-edge SHRED methods specifically designed to handle real-world data that may be noisy, multi-scale, parameterized, prohibitively high-dimensional, and strongly nonlinear. The package is easy to install, thoroughly-documented, supplemented with extensive code examples, and modularly-structured to support future additions. The entire codebase is released under the MIT license and is available at https://github.com/pyshred-dev/pyshred.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order transmissibility and its linear approximation for in-service crack identification in train wheelset axles</title>
<link>https://arxiv.org/abs/2507.18636</link>
<guid>https://arxiv.org/abs/2507.18636</guid>
<content:encoded><![CDATA[
<div> Keywords: structural health monitoring, crack detection, train wheelset axles, higher-order harmonics, crack identification

Summary: 
Structural health monitoring is a potential method for early crack detection in train wheelset axles. A new crack detection feature called Higher-Order Transmissibility (HOTr) based on higher-order harmonics of breathing crack is proposed. The sensitivity and efficacy of this feature in crack identification are assessed, and a surrogate model based on linear system theory is developed to speed up the crack identification process. The method accurately reproduces the HOTr feature while eliminating the need for iterative solutions of nonlinear equations, reducing computational burden. The results indicate the potential for adoption in in-service damage identification for wheelset axles in near real-time applications.<br /><br />Summary: <div>
arXiv:2507.18636v1 Announce Type: new 
Abstract: In-service structural health monitoring is a so far rarely exploited, yet potent option for early-stage crack detection and identification in train wheelset axles. This procedure is non-trivial to enforce on the basis of a purely data-driven approach and typically requires the adoption of numerical, e.g. finite element-based, simulation schemes of the dynamic behavior of these axles. Damage in this particular case can be formulated as a breathing crack problem, which further complicates simulation by introducing response-dependent nonlinearities into the picture. In this study, first, a new crack detection feature based on higher-order harmonics of the breathing crack is proposed, termed Higher-Order Transmissibility (HOTr), and, secondly, its sensitivity and efficacy are assessed within the context of crack identification. Next, the mentioned feature is approximated via use of linear system theory, delivering a surrogate model which facilitates the computation and speeds up the crack identification procedure. The accuracy of the proposed method in reproducing the delivered HOTr is compared against the nonlinear simulation model. The obtained results suggest that the approximation of the HOTr can significantly reduce the computational burden by eliminating the need for an iterative solution of the governing nonlinear equation of motion, while maintaining a high level of accuracy when compared against the reference model. This implies great potential for adoption in in-service damage identification for wheelset axles, feasibly within a near real-time context.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning coupled Allen-Cahn and Cahn-Hilliard phase-field equations using Physics-informed neural operator(PINO)</title>
<link>https://arxiv.org/abs/2507.18731</link>
<guid>https://arxiv.org/abs/2507.18731</guid>
<content:encoded><![CDATA[
<div> PINOs, phase-field equations, mesoscale microstructural evolution, Physics informed Neural Operators, Al-Cu alloys<br />
<br />
Summary:<br />
Physics informed Neural Operators (PINOs) offer an alternative approach to predict microstructural evolution in materials subjected to periodic boundary conditions. In this study, PINO successfully predicted the growth of $\theta^{\prime}$ precipitates in Al-Cu alloys by solving three coupled physics equations simultaneously, involving two second-order Allen-Cahn equations and one fourth-order Cahn-Hilliard equation. The use of Fourier derivatives, specifically a pseudo-spectral method and Fourier extension, significantly improved the Cahn-Hilliard equation's accuracy. By leveraging the Fourier domain's properties, computing fourth derivatives of the Cahn-Hilliard equation was made more efficient. This research showcases the potential of PINOs in accurately predicting material microstructural evolution with reduced computational cost. <div>
arXiv:2507.18731v1 Announce Type: new 
Abstract: Phase-field equations, mostly solved numerically, are known for capturing the mesoscale microstructural evolution of a material. However, such numerical solvers are computationally expensive as it needs to generate fine mesh systems to solve the complex Partial Differential Equations(PDEs) with good accuracy. Therefore, we propose an alternative approach of predicting the microstructural evolution subjected to periodic boundary conditions using Physics informed Neural Operators (PINOs).
  In this study, we have demonstrated the capability of PINO to predict the growth of $\theta^{\prime}$ precipitates in Al-Cu alloys by learning the operator as well as by solving three coupled physics equations simultaneously. The coupling is of two second-order Allen-Cahn equation and one fourth-order Cahn-Hilliard equation. We also found that using Fourier derivatives(pseudo-spectral method and Fourier extension) instead of Finite Difference Method improved the Cahn-Hilliard equation loss by twelve orders of magnitude. Moreover, since differentiation is equivalent to multiplication in the Fourier domain, unlike Physics informed Neural Networks(PINNs), we can easily compute the fourth derivative of Cahn-Hilliard equation without converting it to coupled second order derivative.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThermoRL:Structure-Aware Reinforcement Learning for Protein Mutation Design to Enhance Thermostability</title>
<link>https://arxiv.org/abs/2507.18816</link>
<guid>https://arxiv.org/abs/2507.18816</guid>
<content:encoded><![CDATA[
<div> Keywords: protein thermostability, mutation design, reinforcement learning, graph neural networks, computational efficiency

Summary: 
ThermoRL is a novel framework utilizing reinforcement learning and graph neural networks to optimize protein thermostability through mutation design. Traditional methods face challenges in balancing sequence variations, structural dynamics, and thermostability. ThermoRL overcomes these limitations by incorporating a hierarchical Q-learning network and a surrogate model for reward feedback, guiding the agent on optimal mutation positions and amino acids. Experimental results demonstrate ThermoRL's ability to outperform baselines in rewards while efficiently filtering out destabilizing mutations and identifying stabilizing mutations aligned with experimental data. The framework's generalizability is highlighted by its accurate detection of key mutation sites in previously unseen proteins. ThermoRL represents a robust alternative to traditional methods for enhancing protein thermostability. 

<br /><br />Summary: <div>
arXiv:2507.18816v1 Announce Type: new 
Abstract: Designing mutations to optimize protein thermostability remains challenging due to the complex relationship between sequence variations, structural dynamics, and thermostability, often assessed by \delta\delta G
  (the change in free energy of unfolding). Existing methods rely on experimental random mutagenesis or prediction models tested with pre-defined datasets, using sequence-based heuristics and treating enzyme design as a one-step process without iterative refinement, which limits design space exploration and restricts discoveries beyond known variations. We present ThermoRL, a framework based on reinforcement learning (RL) that leverages graph neural networks (GNN) to design mutations with enhanced thermostability. It combines a pre-trained GNN-based encoder with a hierarchical Q-learning network and employs a surrogate model for reward feedback, guiding the RL agent on where (the position) and which (mutant amino acid) to apply for enhanced thermostability. Experimental results show that ThermoRL achieves higher or comparable rewards than baselines while maintaining computational efficiency. It filters out destabilizing mutations and identifies stabilizing mutations aligned with experimental data. Moreover, ThermoRL accurately detects key mutation sites in unseen proteins, highlighting its strong generalizability. This RL-guided approach powered by GNN embeddings offers a robust alternative to traditional protein mutation design.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrinityDNA: A Bio-Inspired Foundational Model for Efficient Long-Sequence DNA Modeling</title>
<link>https://arxiv.org/abs/2507.19229</link>
<guid>https://arxiv.org/abs/2507.19229</guid>
<content:encoded><![CDATA[
<div> TrinityDNA, DNA foundational model, long-range dependencies, structural features, Gated Reverse Complement (GRC), multi-scale attention mechanism. 

Summary:
TrinityDNA is a novel DNA foundational model designed to address the challenges of genomic sequence modeling. It integrates biologically informed components like Groove Fusion and GRC to capture DNA's structural features and symmetry. The model also utilizes a multi-scale attention mechanism to attend to varying levels of sequence dependencies. An evolutionary training strategy adapts the model to prokaryotic and eukaryotic genomes. TrinityDNA offers improvements in gene function prediction and regulatory mechanism discovery in genomics applications. It bridges machine learning techniques with biological insights for more effective genomic data analysis. Additionally, a new DNA long-sequence CDS annotation benchmark is introduced for comprehensive evaluations oriented towards practical applications. 

<br /><br />Summary: 
TrinityDNA, a novel DNA foundational model, integrates biologically informed components like Groove Fusion and GRC, along with a multi-scale attention mechanism and evolutionary training strategy, improving gene function prediction and regulatory mechanism discovery in genomics applications. It bridges machine learning techniques with biological insights for more effective genomic data analysis and introduces a DNA long-sequence CDS annotation benchmark for comprehensive evaluations focused on practical applications. <div>
arXiv:2507.19229v1 Announce Type: new 
Abstract: The modeling of genomic sequences presents unique challenges due to their length and structural complexity. Traditional sequence models struggle to capture long-range dependencies and biological features inherent in DNA. In this work, we propose TrinityDNA, a novel DNA foundational model designed to address these challenges. The model integrates biologically informed components, including Groove Fusion for capturing DNA's structural features and Gated Reverse Complement (GRC) to handle the inherent symmetry of DNA sequences. Additionally, we introduce a multi-scale attention mechanism that allows the model to attend to varying levels of sequence dependencies, and an evolutionary training strategy that progressively adapts the model to both prokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate and efficient approach to genomic sequence modeling, offering significant improvements in gene function prediction, regulatory mechanism discovery, and other genomics applications. Our model bridges the gap between machine learning techniques and biological insights, paving the way for more effective analysis of genomic data. Additionally, we introduced a new DNA long-sequence CDS annotation benchmark to make evaluations more comprehensive and oriented toward practical applications.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Level Monte Carlo sampling with Parallel-in-Time Integration for Uncertainty Quantification in Electric Machine Simulation</title>
<link>https://arxiv.org/abs/2507.19246</link>
<guid>https://arxiv.org/abs/2507.19246</guid>
<content:encoded><![CDATA[
<div> Efficient Uncertainty Quantification, Monte Carlo sampling, Multi-Level Monte Carlo method, Parallel-in-Time integration, computational effort <br />
Summary: <br />
This article introduces a method that combines Multi-Level Monte Carlo sampling with Parallel-in-Time integration to improve the efficiency of Uncertainty Quantification in high-dimensional uncertainty scenarios. While the Multi-Level Monte Carlo method reduces computational effort, it struggles to decrease time to solution in highly parallel computing environments. By leveraging Parallel-in-Time integration for select samples, the proposed method accelerates computation without sacrificing accuracy. Results from numerical examples show a significant speedup of 12-45% compared to Multi-Level Monte Carlo sampling, with a modest increase of 15-18% in total computational effort. The study delves into the tradeoff between time-to-solution and computational effort, presenting theoretical considerations and practical implications for this combined approach. <div>
arXiv:2507.19246v1 Announce Type: new 
Abstract: While generally considered computationally expensive, Uncertainty Quantification using Monte Carlo sampling remains beneficial for applications with uncertainties of high dimension. As an extension of the naive Monte Carlo method, the Multi-Level Monte Carlo method reduces the overall computational effort, but is unable to reduce the time to solution in a sufficiently parallel computing environment. In this work, we propose a Uncertainty Quantification method combining Multi-Level Monte Carlo sampling and Parallel-in-Time integration for select samples, exploiting remaining parallel computing capacity to accelerate the computation. While effective at reducing the time-to-solution, Parallel-in-Time integration methods greatly increase the total computational effort. We investigate the tradeoff between time-to-solution and total computational effort of the combined method, starting from theoretical considerations and comparing our findings to two numerical examples. There, a speedup of 12 - 45% compared to Multi-Level Monte Carlo sampling is observed, with an increase of 15 - 18% in computational effort.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning electromagnetic fields based on finite element basis functions</title>
<link>https://arxiv.org/abs/2507.19255</link>
<guid>https://arxiv.org/abs/2507.19255</guid>
<content:encoded><![CDATA[
<div> Keywords: parametric surrogate models, electric machines, isogeometric analysis, proper orthogonal decomposition, deep learning

Summary: 
Parametric surrogate models are essential for efficient design optimization and operational monitoring of electric machines. This study introduces a novel approach that combines isogeometric analysis, proper orthogonal decomposition, and deep learning to predict spline basis coefficients rapidly and accurately. By directly learning these coefficients, the method enables efficient and physically consistent predictions, particularly for parametric nonlinear magnetostatic models of permanent magnet synchronous machines. The effectiveness of this approach is demonstrated in the study, showcasing its potential for enhancing the design and analysis of electric machines. <br /><br />Summary: <div>
arXiv:2507.19255v1 Announce Type: new 
Abstract: Parametric surrogate models of electric machines are widely used for efficient design optimization and operational monitoring. Addressing geometry variations, spline-based computer-aided design representations play a pivotal role. In this study, we propose a novel approach that combines isogeometric analysis, proper orthogonal decomposition and deep learning to enable rapid and physically consistent predictions by directly learning spline basis coefficients. The effectiveness of this method is demonstrated using a parametric nonlinear magnetostatic model of a permanent magnet synchronous machine.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel multi-thickness topology optimization method for balancing structural performance and manufacturability</title>
<link>https://arxiv.org/abs/2507.19388</link>
<guid>https://arxiv.org/abs/2507.19388</guid>
<content:encoded><![CDATA[
<div> Keywords: Topology optimization, multi-thickness, density-based, manufacturability, compliance

Summary:
This paper introduces a novel multi-thickness, density-based topology optimization method that aims to strike a balance between structural performance and manufacturability. By guiding the design towards a predefined set of discrete allowable thicknesses through a multilevel penalization scheme and smoothed Heaviside projection, the method transitions designs from truss-like structures to high-performance sheet-like structures as the number of allowable thickness levels increases. The approach, validated on standard benchmarks, achieves compliance values within 2% of fully unpenalized optimization while outperforming standard SIMP results. The method eliminates impractically thin regions and features, making designs suitable for both additive manufacturing and conventional fabrication. This approach maximizes both performance and manufacturability, addressing the trade-off commonly encountered in two-dimensional topology optimization.<br /><br />Summary: <div>
arXiv:2507.19388v1 Announce Type: new 
Abstract: Topology optimization (TO) in two dimensions often presents a trade-off between structural performance and manufacturability, with unpenalized (variable-thickness) methods yielding superior but complex designs, and penalized (SIMP) methods producing simpler, truss-like structures with compromised performance. This paper introduces a multi-thickness, density-based topology optimization method designed to bridge this gap. The proposed approach guides the design towards a predefined set of discrete, allowable thicknesses by employing a novel multilevel penalization scheme and a multilevel smoothed Heaviside projection. A continuation strategy for the penalization and projection parameters, combined with an adaptive mesh refinement technique, ensures robust convergence and high-resolution geometric features. The method is validated on standard cantilever and MBB beam benchmarks. Results demonstrate that as the number of allowable thicknesses increases, the designs systematically transition from conventional truss-like structures to high-performance, sheet-like structures. Notably, designs with as few as three discrete thickness levels achieve compliance values within 2\% of those from fully unpenalized, variable-thickness optimization, while significantly outperforming standard SIMP results. The method inherently eliminates impractically thin regions and features, both in the out-of-plane and in-plane directions and produces designs well-suited for both additive manufacturing and conventional fabrication using standard-thickness stock materials, thus maximizing both performance and manufacturability.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance as Extended Biology: Reciprocity as the Cognitive Substrate of Financial Behavior</title>
<link>https://arxiv.org/abs/2506.00099</link>
<guid>https://arxiv.org/abs/2506.00099</guid>
<content:encoded><![CDATA[
<div> reciprocity, financial behaviors, credit, insurance, trade, artificial intelligence
Summary:
- The article argues that financial behaviors such as credit, insurance, trade, and token exchange are not products of institutional design but extensions of reciprocity.
- Reciprocity is considered the foundational logic of early human societies, governing the circulation of goods and maintenance of long-term cooperation.
- Trade is seen as the canonical form of reciprocity, involving simultaneous, symmetric, and partner-contingent interactions.
- The four core financial functions mentioned - credit, insurance, token exchange, and investment - are reconstructed as expressions of the underlying principle of reciprocity under different conditions.
- By focusing on the minimal dynamics of reciprocal interaction, the framework shifts the focus from institutional engineering to behavioral computation, providing a new foundation for modeling decentralized financial behavior in both human and artificial agents. 

<br /><br />Summary: <div>
arXiv:2506.00099v2 Announce Type: cross 
Abstract: A central challenge in economics and artificial intelligence is explaining how financial behaviors-such as credit, insurance, and trade-emerge without formal institutions. We argue that these functions are not products of institutional design, but structured extensions of a single behavioral substrate: reciprocity. Far from being a derived strategy, reciprocity served as the foundational logic of early human societies-governing the circulation of goods, regulation of obligation, and maintenance of long-term cooperation well before markets, money, or formal rules. Trade, commonly regarded as the origin of financial systems, is reframed here as the canonical form of reciprocity: simultaneous, symmetric, and partner-contingent. Building on this logic, we reconstruct four core financial functions-credit, insurance, token exchange, and investment-as expressions of the same underlying principle under varying conditions. By grounding financial behavior in minimal, simulateable dynamics of reciprocal interaction, this framework shifts the focus from institutional engineering to behavioral computation-offering a new foundation for modeling decentralized financial behavior in both human and artificial agents.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable inverse design of optical multilayer thin films based on extended neural adjoint and regression activation mapping</title>
<link>https://arxiv.org/abs/2507.18644</link>
<guid>https://arxiv.org/abs/2507.18644</guid>
<content:encoded><![CDATA[
<div> Neural adjoint, optical multilayer thin films, inverse design, material loss function, interpretability<br />
<br />
Summary:
The article introduces an Extended Neural Adjoint (ENA) framework for the inverse design of optical multilayer thin films (OMTs). It meets six key criteria: accuracy, efficiency, diversity, scalability, flexibility, and interpretability. The ENA framework incorporates a material loss function in the neural adjoint method to explore different material configurations of OMTs. The forward neural network architecture (F-RAM) enhances scalability and interpretability by visualizing feature importance. Ablation studies show that the material loss improves accuracy and diversity of OMT solutions. Compared to a residual network-based method (Res-GLOnet), the ENA achieves higher accuracy and better diversity in inverse design. The interpretability of the ENA method is demonstrated by consistent feature importance distributions across OMT structures with similar optical properties. The flexibility of the ENA method is showcased by imposing constraints on the initial layer of OMTs. <br /><br />Summary: <div>
arXiv:2507.18644v1 Announce Type: cross 
Abstract: We propose an extended neural adjoint (ENA) framework, which meets six key criteria for artificial intelligence-assisted inverse design of optical multilayer thin films (OMTs): accuracy, efficiency, diversity, scalability, flexibility, and interpretability. To enhance the scalability of the existing neural adjoint method, we present a novel forward neural network architecture for OMTs and introduce a material loss function into the existing neural adjoint loss function, facilitating the exploration of material configurations of OMTs. Furthermore, we present the detailed formulation of the regression activation mapping for the presented forward neural network architecture (F-RAM), a feature visualization method aimed at improving interpretability. We validated the efficacy of the material loss by conducting an ablation study, where each component of the loss function is systematically removed and evaluated. The results indicated that the inclusion of the material loss significantly improves accuracy and diversity. To substantiate the performance of the ENA-based inverse design, we compared it against the residual network-based global optimization network (Res-GLOnet). The ENA yielded the OMT solutions of an inverse design with higher accuracy and better diversity compared to the Res-GLOnet. To demonstrate the interpretability, we applied F-RAM to diverse OMT structures with similar optical properties, obtained by the proposed ENA method. We showed that distributions of feature importance for various OMT structures exhibiting analogous optical properties are consistent, despite variations in material configurations, layer number, and thicknesses. Furthermore, we demonstrate the flexibility of the ENA method by restricting the initial layer of OMTs to SiO2 and 100 nm.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report</title>
<link>https://arxiv.org/abs/2507.19402</link>
<guid>https://arxiv.org/abs/2507.19402</guid>
<content:encoded><![CDATA[
<div> Keywords: financial transactions, fraud detection, classical machine learning, quantum machine learning, hybrid models
Summary: 
The report investigates and compares classical, quantum, and hybrid machine learning models for detecting fraudulent financial activities using a comprehensive behavioural feature engineering framework. Classical models such as Random Forest outperformed quantum models on the IBM Anti-Money Laundering dataset, achieving high accuracy and F-measure. The proposed FD4QC architecture offers a classical-first, quantum-enhanced approach for real-world deployment. While classical models showed better performance in this study, the Quantum Support Vector Machine (QSVM) demonstrated promise with high precision and low false-positive rates. The results highlight the current limitations of quantum machine learning in financial fraud detection and suggest avenues for future research. 
<br /><br />Summary: <div>
arXiv:2507.19402v1 Announce Type: cross 
Abstract: The increasing complexity and volume of financial transactions pose significant challenges to traditional fraud detection systems. This technical report investigates and compares the efficacy of classical, quantum, and quantum-hybrid machine learning models for the binary classification of fraudulent financial activities.
  As of our methodology, first, we develop a comprehensive behavioural feature engineering framework to transform raw transactional data into a rich, descriptive feature set. Second, we implement and evaluate a range of models on the IBM Anti-Money Laundering (AML) dataset. The classical baseline models include Logistic Regression, Decision Tree, Random Forest, and XGBoost. These are compared against three hybrid classic quantum algorithms architectures: a Quantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC), and a Hybrid Quantum Neural Network (HQNN).
  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a practical, API-driven system architecture designed for real-world deployment, featuring a classical-first, quantum-enhanced philosophy with robust fallback mechanisms.
  Our results demonstrate that classical tree-based models, particularly \textit{Random Forest}, significantly outperform the quantum counterparts in the current setup, achieving high accuracy (\(97.34\%\)) and F-measure (\(86.95\%\)). Among the quantum models, \textbf{QSVM} shows the most promise, delivering high precision (\(77.15\%\)) and a low false-positive rate (\(1.36\%\)), albeit with lower recall and significant computational overhead.
  This report provides a benchmark for a real-world financial application, highlights the current limitations of quantum machine learning in this domain, and outlines promising directions for future research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Twin Technologies in Predictive Maintenance: Enabling Transferability via Sim-to-Real and Real-to-Sim Transfer</title>
<link>https://arxiv.org/abs/2507.18449</link>
<guid>https://arxiv.org/abs/2507.18449</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, Artificial Intelligence, Digital Twins, Reality Gap Analysis, Simulation Model

Summary:
The article discusses the development of Digital Twins (DTs) in the context of the Internet of Things (IoT) and Artificial Intelligence. It emphasizes the importance of standardized frameworks for transitioning DTs from academia to industry. The focus is on transferability, particularly the transfer of knowledge between simulations and real-world operations, known as sim-to-real and real-to-sim transfer. The concept of the "reality gap," the difference between simulated predictions and actual outcomes, is explored. The authors propose integrating a Reality Gap Analysis (RGA) module into existing DT frameworks to manage sim-to-real and real-to-sim transfers effectively. Data pipelines connect the RGA module with historical repositories and simulation models to facilitate bidirectional knowledge transfer. A case study on a pedestrian bridge at Carnegie Mellon University demonstrates the performance of this approach, showcasing efficient bidirectional knowledge transfer without compromising effectiveness.<br /><br />Summary: The article highlights the importance of standardizing DT frameworks for industry adoption and emphasizes the need for bidirectional knowledge transfer between simulations and real-world operations. The integration of a Reality Gap Analysis module enhances this transferability, addressing the challenge of the "reality gap" discrepancy. The proposed approach, demonstrated through a case study, showcases efficient transfer capabilities without compromising effectiveness. <div>
arXiv:2507.18449v1 Announce Type: new 
Abstract: The advancement of the Internet of Things (IoT) and Artificial Intelligence has catalyzed the evolution of Digital Twins (DTs) from conceptual ideas to more implementable realities. Yet, transitioning from academia to industry is complex due to the absence of standardized frameworks. This paper builds upon the authors' previously established functional and informational requirements supporting standardized DT development, focusing on a crucial aspect: transferability. While existing DT research primarily centers on asset transfer, the significance of "sim-to-real transfer" and "real-to-sim transfer"--transferring knowledge between simulations and real-world operations--is vital for comprehensive lifecycle management in DTs. A key challenge in this process is calibrating the "reality gap," the discrepancy between simulated predictions and actual outcomes. Our research investigates the impact of integrating a single Reality Gap Analysis (RGA) module into an existing DT framework to effectively manage both sim-to-real and real-to-sim transfers. This integration is facilitated by data pipelines that connect the RGA module with the existing components of the DT framework, including the historical repository and the simulation model. A case study on a pedestrian bridge at Carnegie Mellon University showcases the performance of different levels of integration of our approach with an existing framework. With full implementation of an RGA module and a complete data pipeline, our approach is capable of bidirectional knowledge transfer between simulations and real-world operations without compromising efficiency.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents</title>
<link>https://arxiv.org/abs/2507.18067</link>
<guid>https://arxiv.org/abs/2507.18067</guid>
<content:encoded><![CDATA[
<div> supervised deep learning framework, neural operators, PDEs, downscaling models, Copernicus ocean current data <br />
Summary: 
This paper introduces a supervised deep learning framework utilizing neural operators to solve partial differential equations (PDEs) and provide high-resolution solutions for ocean current data. The proposed method aims to address the limitations of available satellite products by downscaling models and predicting solutions at arbitrary resolutions. By applying this approach to Copernicus ocean data, the study demonstrates the ability to enhance spatial granularity for detailed local analyses crucial in oceanography. Additionally, the model's versatility allows it to model surrogate PDEs and predict solutions at varying resolutions, irrespective of the input resolution. Evaluation on real-world Copernicus data and synthetic Navier-Stokes simulation datasets showcases the effectiveness of this approach in generating accurate and detailed current data, with potential applications in coastal management, environmental monitoring, and maritime safety. <br /> <div>
arXiv:2507.18067v1 Announce Type: cross 
Abstract: Accurate modeling of physical systems governed by partial differential equations is a central challenge in scientific computing. In oceanography, high-resolution current data are critical for coastal management, environmental monitoring, and maritime safety. However, available satellite products, such as Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and global ocean models, often lack the spatial granularity required for detailed local analyses. In this work, we (a) introduce a supervised deep learning framework based on neural operators for solving PDEs and providing arbitrary resolution solutions, and (b) propose downscaling models with an application to Copernicus ocean current data. Additionally, our method can model surrogate PDEs and predict solutions at arbitrary resolution, regardless of the input resolution. We evaluated our model on real-world Copernicus ocean current data and synthetic Navier-Stokes simulation datasets.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On zero-order consistency residue and background pressure for the conservative SPH fluid dynamics</title>
<link>https://arxiv.org/abs/2507.18210</link>
<guid>https://arxiv.org/abs/2507.18210</guid>
<content:encoded><![CDATA[
<div> SPH method, smoothed particle hydrodynamics, zero-order consistency issue, numerical dissipation, gravity-driven flow<br />
<br />
Summary: 
The study examines the zero-order consistency issue in conservative smoothed particle hydrodynamics (SPH) and its impact on flow simulation in pressure-driven channels and gravity-driven free-surface flows. It identifies the common root cause of non-physical numerical damping in these scenarios as the zero-order gradient consistency residue. The background pressure exacerbates this issue, leading to excessive numerical dissipation. The study conducts theoretical analysis and numerical experiments to understand and mitigate this residue effect, testing sensitive factors like water depth, input dynamic pressure, channel length, resolution, and outlet pressure. The reverse kernel gradient correction technique is effective but has limitations in reducing the residue effect. The study highlights the necessity of correction schemes, especially in scenarios with high background pressure, as demonstrated in the FDA nozzle engineering benchmark. <div>
arXiv:2507.18210v1 Announce Type: cross 
Abstract: As one of the major challenges for the conservative smoothed particle hydrodynamics (SPH) method, the zero-order consistency issue, although thought to be mitigated by the particle regularization scheme, such as the transport velocity formulation, significantly damps the flow in a long channel for both laminar and turbulent simulations. Building on this finding, this paper not only thoroughly analyzes the damping reason in this pressure-driven channel flow, but also relates this problem with the excessive numerical dissipation in the gravity-driven free-surface flow. The common root cause of the non-physical numerical damping in the two typical flow scenarios, the zero-order gradient consistency residue, is exposed. The adverse influence of the background pressure on the residue for the two scenarios is revealed and discussed. To comprehensively understand the behavior of the residue and mitigate its potential adverse effects, we conduct both theoretical analysis and numerical experiments focusing on the key sensitive factors. For studying the residue-induced non-physical energy dissipation in the gravity-driven free-surface flow, the water depth and input dynamic pressure in the inviscid standing wave case are tested. To investigate the velocity loss in the pressure-driven channel flow, we examine the effects of the channel length, resolution, and outlet pressure. The state-of-the-art reverse kernel gradient correction technique is introduced for the two typical flows, and proved to be effective in reducing the residue effect, but we find its correction capability is fundamentally limited. Finally, the FDA nozzle, an engineering benchmark, is tested to demonstrate the residue influence in a complex geometry, highlighting the necessity of correction schemes in scenarios with unavoidable high background pressure.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A stabilized Two-Step Formulation of Maxwell's Equations in the time-domain</title>
<link>https://arxiv.org/abs/2507.18235</link>
<guid>https://arxiv.org/abs/2507.18235</guid>
<content:encoded><![CDATA[
<div> time-domain, Maxwell's equations, Galerkin discretization, low-frequency instability, numerical stability

Summary:
This study presents a novel approach to simulating electromagnetic fields across broad frequency ranges by extending a stabilized two-step formulation of Maxwell's equations to the time-domain. Utilizing a Galerkin discretization in space, the researchers apply two time-discretization schemes tailored to the first- and second-order partial differential equations. To combat low-frequency instabilities, a generalized tree-cotree gauge is incorporated to eliminate the singularity of the curl-curl operator, ensuring robustness even in the static limit. Numerical experimentation on various 3D problems demonstrates the method's stability, accuracy, and its ability to handle nonlinear, temperature-dependent materials. The results affirm the reliability and applicability of this approach in simulating electromagnetic phenomena with diverse frequency characteristics. 

<br /><br />Summary: <div>
arXiv:2507.18235v1 Announce Type: cross 
Abstract: Simulating electromagnetic fields across broad frequency ranges is challenging due to numerical instabilities at low frequencies. This work extends a stabilized two-step formulation of Maxwell's equations to the time-domain. Using a Galerkin discretization in space, we apply two different time-discretization schemes that are tailored to the first- and second-order in time partial differential equations of the two-step solution procedure used here. To address the low-frequency instability, we incorporate a generalized tree-cotree gauge that removes the singularity of the curl-curl operator, ensuring robustness even in the static limit. Numerical results on academic and application-oriented 3D problems confirm stability, accuracy, and the method's applicability to nonlinear, temperature-dependent materials.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Select2Drive: Pragmatic Communications for Real-Time Collaborative Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.12040</link>
<guid>https://arxiv.org/abs/2501.12040</guid>
<content:encoded><![CDATA[
<div> keyword: Vehicle-to-everything communications, autonomous driving, collaborative perception, decision-making, Select2Drive <br />
Summary: <br />
Vehicle-to-everything communications play a crucial role in advancing autonomous driving, with the emergence of PragComm as a promising paradigm. The Select2Drive framework is proposed to optimize limited computational and communication resources for collaborative driving. It introduces distributed predictive perception to reduce latency and enhance decision-making efficiency. By prioritizing critical regions in communication using area-of-importance-based PragComm, Select2Drive boosts both communication efficiency and decision-making efficacy. Empirical evaluations on V2Xverse and real-world DAIR-V2X demonstrate significant improvements in offline perception tasks and driving performance, especially in dense traffic scenarios. Select2Drive showcases a promising approach to enhance collaborative driving through efficient resource utilization and improved decision-making processes. <br /> <div>
arXiv:2501.12040v2 Announce Type: replace 
Abstract: Vehicle-to-everything communications-assisted autonomous driving has witnessed remarkable advancements in recent years, with pragmatic communications (PragComm) emerging as a promising paradigm for real-time collaboration among vehicles and other agents. Simultaneously, extensive research has explored the interplay between collaborative perception and decision-making in end-to-end driving frameworks. In this work, we revisit the collaborative driving problem and propose the Select2Drive framework to optimize the utilization of limited computational and communication resources. Particularly, to mitigate cumulative latency in perception and decision-making, Select2Drive introduces distributed predictive perception by formulating an active prediction paradigm and simplifying high-dimensional semantic feature prediction into a computation cost-efficient, motion-aware reconstruction. Given the ``less is more" principle that an over-broadened perceptual horizon possibly confuses the decision module rather than contributing to it, Select2Drive utilizes area-of-importance-based PragComm to prioritize the communications of critical regions, thus boosting both communication efficiency and decision-making efficacy. Empirical evaluations on the V2Xverse and real-world DAIR-V2X demonstrate that Select2Drive achieves a $2.60$\% and $1.99$\% improvement in offline perception tasks under limited bandwidth (resp., pose error conditions). Moreover, it delivers at most $8.35$\% and $2.65$\% enhancement in closed-loop driving scores and route completion rates, particularly in scenarios characterized by dense traffic and high-speed dynamics.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoadBench: A Vision-Language Foundation Model and Benchmark for Road Damage Understanding</title>
<link>https://arxiv.org/abs/2507.17353</link>
<guid>https://arxiv.org/abs/2507.17353</guid>
<content:encoded><![CDATA[
<div> dataset, multimodal, road damage, vision language model, infrastructure monitoring <br />
<br /> Summary: RoadBench is introduced as a multimodal benchmark for road damage understanding, combining high-resolution images with textual descriptions for richer context. The RoadCLIP vision language model enhances CLIP with disease-aware positional encoding and road-condition priors for improved road damage recognition. A data generation pipeline using GPT expands the dataset, improving data diversity without manual annotation. Experimental results show RoadCLIP outperforms vision-only models by 19.2%, demonstrating the benefits of combining visual and textual information for enhanced road condition analysis. This work sets new benchmarks for the field and advances infrastructure monitoring through multimodal learning. <div>
arXiv:2507.17353v1 Announce Type: new 
Abstract: Accurate road damage detection is crucial for timely infrastructure maintenance and public safety, but existing vision-only datasets and models lack the rich contextual understanding that textual information can provide. To address this limitation, we introduce RoadBench, the first multimodal benchmark for comprehensive road damage understanding. This dataset pairs high resolution images of road damages with detailed textual descriptions, providing a richer context for model training. We also present RoadCLIP, a novel vision language model that builds upon CLIP by integrating domain specific enhancements. It includes a disease aware positional encoding that captures spatial patterns of road defects and a mechanism for injecting road-condition priors to refine the model's understanding of road damages. We further employ a GPT driven data generation pipeline to expand the image to text pairs in RoadBench, greatly increasing data diversity without exhaustive manual annotation. Experiments demonstrate that RoadCLIP achieves state of the art performance on road damage recognition tasks, significantly outperforming existing vision-only models by 19.2%. These results highlight the advantages of integrating visual and textual information for enhanced road condition analysis, setting new benchmarks for the field and paving the way for more effective infrastructure monitoring through multimodal learning.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning-Driven Retrosynthesis Prediction with Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.17448</link>
<guid>https://arxiv.org/abs/2507.17448</guid>
<content:encoded><![CDATA[
<div> Reversal planning; organic synthesis; drug discovery; AI-driven advancements; RetroDFM-R <br />
Summary: RetroDFM-R is a reasoning-based large language model (LLM) developed for chemical retrosynthesis to address limitations in existing methods. Through reinforcement learning guided by chemically verifiable rewards, RetroDFM-R improves prediction accuracy and explainability. It outperforms state-of-the-art methods with a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Human assessments confirm the model's chemical plausibility and practicality. RetroDFM-R accurately predicts multistep retrosynthetic routes for drug molecules and perovskite materials from the literature. The model's reasoning process provides interpretable insights, enhancing trust and practical value in real-world retrosynthesis applications.<br /><br />Summary: RetroDFM-R, a reasoning-based large language model, enhances prediction accuracy and explainability in chemical retrosynthesis. It outperforms existing methods on benchmarks and accurately predicts multistep routes for various compounds. Human assessments confirm its utility and the practical value of its reasoning process. <div>
arXiv:2507.17448v1 Announce Type: new 
Abstract: Retrosynthesis planning, essential in organic synthesis and drug discovery, has greatly benefited from recent AI-driven advancements. Nevertheless, existing methods frequently face limitations in both applicability and explainability. Traditional graph-based and sequence-to-sequence models often lack generalized chemical knowledge, leading to predictions that are neither consistently accurate nor easily explainable. To address these challenges, we introduce RetroDFM-R, a reasoning-based large language model (LLM) designed specifically for chemical retrosynthesis. Leveraging large-scale reinforcement learning guided by chemically verifiable rewards, RetroDFM-R significantly enhances prediction accuracy and explainability. Comprehensive evaluations demonstrate that RetroDFM-R significantly outperforms state-of-the-art methods, achieving a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Double-blind human assessments further validate the chemical plausibility and practical utility of RetroDFM-R's predictions. RetroDFM-R also accurately predicts multistep retrosynthetic routes reported in the literature for both real-world drug molecules and perovskite materials. Crucially, the model's explicit reasoning process provides human-interpretable insights, thereby enhancing trust and practical value in real-world retrosynthesis applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous Sustainability Assessment via Multimodal AI Agents</title>
<link>https://arxiv.org/abs/2507.17012</link>
<guid>https://arxiv.org/abs/2507.17012</guid>
<content:encoded><![CDATA[
<div> AI agents, life cycle assessment, sustainability, carbon emissions, electronic devices
Summary: 
- AI agents are introduced to streamline the process of calculating cradle-to-gate carbon emissions for electronic devices, reducing expert time while maintaining accuracy.
- A method is developed to estimate environmental impacts by comparing products with similar descriptions, providing quick and accurate results.
- A data-driven approach is implemented to generate emission factors, improving accuracy compared to traditional methods.
- The scalability and implications of this approach for future LCA workflows are analyzed.
- The innovative use of AI and data abstraction tools addresses data availability gaps and enhances the efficiency of sustainability assessments in product manufacturing. <br /><br /> <div>
arXiv:2507.17012v1 Announce Type: cross 
Abstract: Interest in sustainability information has surged in recent years. However, the data required for a life cycle assessment (LCA) that maps the materials and processes from product manufacturing to disposal into environmental impacts (EI) are often unavailable. Here we reimagine conventional LCA by introducing multimodal AI agents that emulate interactions between LCA experts and stakeholders like product managers and engineers to calculate the cradle-to-gate (production) carbon emissions of electronic devices. The AI agents iteratively generate a detailed life-cycle inventory leveraging a custom data abstraction and software tools that extract information from online text and images from repair communities and government certifications. This approach reduces weeks or months of expert time to under one minute and closes data availability gaps while yielding carbon footprint estimates within 19% of expert LCAs with zero proprietary data. Additionally, we develop a method to directly estimate EI by comparing an input to a cluster of products with similar descriptions and known carbon footprints. This runs in 3 ms on a laptop with a MAPE of 12.28% on electronic products. Further, we develop a data-driven method to generate emission factors. We use the properties of an unknown material to represent it as a weighted sum of emission factors for similar materials. Compared to human experts picking the closest LCA database entry, this improves MAPE by 120.26%. We analyze the data and compute scaling of this approach and discuss its implications for future LCA workflows.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image memorability predicts social media virality and externally-associated commenting</title>
<link>https://arxiv.org/abs/2409.14659</link>
<guid>https://arxiv.org/abs/2409.14659</guid>
<content:encoded><![CDATA[
<div> memorability, viral potential, social media, neural network, image categorization  
Summary:  
Memorable images play a crucial role in predicting viral potential on social media platforms. The study utilized neural network ResMem to assess image memorability and correlated it with virality metrics using Reddit image posts. Results showed that memorable images consistently received more comments, even after accounting for image categories. Semantic analysis indicated that memorable images elicited neutral-affect comments, suggesting a unique pathway to virality compared to emotional content. Visual consistency analysis revealed that memorable posts stimulated diverse comments from external sources. Analyses of ResMem's layers highlighted the importance of semantic distinctiveness in both memorability and virality, independent of image category effects. This study sheds light on the link between memorability and social media engagement, emphasizing the role of visual features and human cognitive interactions in online content dissemination.  
<br /><br />Summary: <div>
arXiv:2409.14659v2 Announce Type: replace-cross 
Abstract: Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images are consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional contents. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found that semantic distinctiveness was key to both memorability and virality even after accounting for image category effects. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting CFD Surrogates through Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2507.16069</link>
<guid>https://arxiv.org/abs/2507.16069</guid>
<content:encoded><![CDATA[
<div> Keywords: surrogate models, computational fluid dynamics, interpretability, sparse autoencoders, latent features

Summary: 
Surrogate models are increasingly being used as alternatives to high-fidelity CFD solvers, but their opaque latent representations limit their adoption in safety-critical or regulated environments. This study introduces a posthoc interpretability framework for graph-based surrogate models in CFD, utilizing sparse autoencoders (SAEs). By extracting an overcomplete basis in the node embedding space of a pretrained surrogate, the method generates a dictionary of interpretable latent features. This approach allows for the identification of monosemantic concepts aligned with physical phenomena like vorticity and flow structures. By enhancing explainability and trustworthiness in CFD applications, this model-agnostic pathway provides a valuable tool for improving the transparency of surrogate models. 

<br /><br />Summary: <div>
arXiv:2507.16069v1 Announce Type: new 
Abstract: Learning-based surrogate models have become a practical alternative to high-fidelity CFD solvers, but their latent representations remain opaque and hinder adoption in safety-critical or regulation-bound settings. This work introduces a posthoc interpretability framework for graph-based surrogate models used in computational fluid dynamics (CFD) by leveraging sparse autoencoders (SAEs). By obtaining an overcomplete basis in the node embedding space of a pretrained surrogate, the method extracts a dictionary of interpretable latent features. The approach enables the identification of monosemantic concepts aligned with physical phenomena such as vorticity or flow structures, offering a model-agnostic pathway to enhance explainability and trustworthiness in CFD applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational design of personalized drugs via robust optimization under uncertainty</title>
<link>https://arxiv.org/abs/2507.16470</link>
<guid>https://arxiv.org/abs/2507.16470</guid>
<content:encoded><![CDATA[
<div> Keywords: drug composition, release profile, inverse design, topology optimization, uncertainty-aware drug design

Summary: 
This study introduces a computational inverse design approach for optimizing drug composition to achieve a specific release profile necessary for effective disease treatment. The method, based on topology optimization, considers drug material parameters and shape to determine the desired drug composition. The Noyes-Whitney model is used to govern drug release, with robust topology optimization incorporating random material parameters through the stochastic reduced-order method (SROM). Unlike Monte Carlo methods, SROM reduces computational requirements while accurately predicting release profiles. Application of the method to designing drugs with various target release profiles demonstrates close alignment between designed and target profiles. Moreover, SROM-based drug designs exhibit lower uncertainty in release profiles, indicating the effectiveness of this strategy for uncertainty-aware drug design. <div>
arXiv:2507.16470v1 Announce Type: new 
Abstract: Effective disease treatment often requires precise control of the release of the active pharmaceutical ingredient (API). In this work, we present a computational inverse design approach to determine the optimal drug composition that yields a target release profile. We assume that the drug release is governed by the Noyes-Whitney model, meaning that dissolution occurs at the surface of the drug. Our inverse design method is based on topology optimization. The method optimizes the drug composition based on the target release profile, considering the drug material parameters and the shape of the final drug. Our method is non-parametric and applicable to arbitrary drug shapes. The inverse design method is complemented by robust topology optimization, which accounts for the random drug material parameters. We use the stochastic reduced-order method (SROM) to propagate the uncertainty in the dissolution model. Unlike Monte Carlo methods, SROM requires fewer samples and improves computational performance. We apply our method to designing drugs with several target release profiles. The numerical results indicate that the release profiles of the designed drugs closely resemble the target profiles. The SROM-based drug designs exhibit less uncertainty in their release profiles, suggesting that our method is a convincing approach for uncertainty-aware drug design.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-objective Portfolio Optimization Via Gradient Descent</title>
<link>https://arxiv.org/abs/2507.16717</link>
<guid>https://arxiv.org/abs/2507.16717</guid>
<content:encoded><![CDATA[
<div> Keywords: portfolio optimization, multi-objective, gradient descent, automatic differentiation, constraints<br />
Summary:<br />
The article introduces a new approach to portfolio optimization called multi-objective portfolio optimization (MPO) using gradient descent with automatic differentiation. Traditional methods often struggle with scalability and flexibility in complex scenarios. The MPO framework can handle various optimization objectives, constraints, and scenarios, such as minimizing risk measures or maximizing Sharpe ratio while considering constraints like tracking error limits or asset group restrictions. The authors conducted experiments on six scenarios, showing that their method outperforms standard solvers like CVXPY and SKFOLIO in terms of performance and flexibility. The framework aims to be a practical tool for researchers and practitioners dealing with advanced portfolio optimization problems in real-world conditions.<br /><br />Summary: <div>
arXiv:2507.16717v1 Announce Type: new 
Abstract: Traditional approaches to portfolio optimization, often rooted in Modern Portfolio Theory and solved via quadratic programming or evolutionary algorithms, struggle with scalability or flexibility, especially in scenarios involving complex constraints, large datasets and/or multiple conflicting objectives. To address these challenges, we introduce a benchmark framework for multi-objective portfolio optimization (MPO) using gradient descent with automatic differentiation. Our method supports any optimization objective, such as minimizing risk measures (e.g., CVaR) or maximizing Sharpe ratio, along with realistic constraints, such as tracking error limits, UCITS regulations, or asset group restrictions. We have evaluated our framework across six experimental scenarios, from single-objective setups to complex multi-objective cases, and have compared its performance against standard solvers like CVXPY and SKFOLIO. Our results show that our method achieves competitive performance while offering enhanced flexibility for modeling multiple objectives and constraints. We aim to provide a practical and extensible tool for researchers and practitioners exploring advanced portfolio optimization problems in real-world conditions.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking CO$_2$ Storage Simulations: Results from the 11th Society of Petroleum Engineers Comparative Solution Project</title>
<link>https://arxiv.org/abs/2507.15861</link>
<guid>https://arxiv.org/abs/2507.15861</guid>
<content:encoded><![CDATA[
<div> benchmark, simulation tools, geological carbon dioxide storage, quantitative analysis, grid resolution

Summary:<br />
The 11th Society of Petroleum Engineers Comparative Solution Project (SPE11) benchmarked simulation tools for geological carbon dioxide storage, with 18 valid results included in the study. Qualitative variation in results was related to thermal effects, convective mixing, and facies discontinuities, with grid resolution playing a significant role. The quantitative analysis revealed that unreported variations due to human choices during the simulation process were just as impactful as reported computational choices. The study highlights the need for comprehensive documentation and transparency in reporting simulation results for accurate comparative analysis. <div>
arXiv:2507.15861v1 Announce Type: cross 
Abstract: The 11th Society of Petroleum Engineers Comparative Solution Project (shortened SPE11 herein) benchmarked simulation tools for geological carbon dioxide (CO$_2$) storage. A total of 45 groups from leading research institutions and industry across the globe signed up to participate, with 18 ultimately contributing valid results that were included in the comparative study reported here.
  This paper summarizes the SPE11. A comprehensive introduction and qualitative discussion of the submitted data are provided, together with an overview of online resources for accessing the full depth of data. A global metric for analyzing the relative distance between submissions is proposed and used to conduct a quantitative analysis of the submissions. This analysis attempts to statistically resolve the key aspects influencing the variability between submissions.
  The study shows that the major qualitative variation between the submitted results is related to thermal effects, dissolution-driven convective mixing, and resolution of facies discontinuities. Moreover, a strong dependence on grid resolution is observed across all three versions of the SPE11. However, our quantitative analysis suggests that the observed variations are predominantly influenced by factors not documented in the technical responses provided by the participants. We therefore identify that unreported variations due to human choices within the process of setting up, conducting, and reporting on the simulations underlying each SPE11 submission are at least as impactful as the computational choices reported.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making</title>
<link>https://arxiv.org/abs/2507.14542</link>
<guid>https://arxiv.org/abs/2507.14542</guid>
<content:encoded><![CDATA[
<div> Keywords: High-frequency oscillations, intracranial Electroencephalography, machine learning, variational autoencoder, epilepsy treatment

Summary:<br />
- High-frequency oscillations (HFOs) in intracranial Electroencephalography (iEEG) are important in localizing the epileptogenic zone in epilepsy treatment.
- Traditional rule-based detectors for HFOs have low precision, leading to false positives that require manual review.
- Supervised machine learning approaches for classification depend on labeled datasets, which are hard to acquire and label consistently.
- The Self-Supervised to Label Discovery (SS2LD) framework uses a variational autoencoder (VAE) to refine candidate events from legacy detectors into precise pathological HFOs.
- SS2LD outperforms state-of-the-art methods in identifying pathological HFOs, offering a scalable, label-efficient, and clinically effective strategy using legacy detectors.<br /> 
Summary: <div>
arXiv:2507.14542v1 Announce Type: new 
Abstract: High-frequency oscillations (HFOs) in intracranial Electroencephalography (iEEG) are critical biomarkers for localizing the epileptogenic zone in epilepsy treatment. However, traditional rule-based detectors for HFOs suffer from unsatisfactory precision, producing false positives that require time-consuming manual review. Supervised machine learning approaches have been used to classify the detection results, yet they typically depend on labeled datasets, which are difficult to acquire due to the need for specialized expertise. Moreover, accurate labeling of HFOs is challenging due to low inter-rater reliability and inconsistent annotation practices across institutions. The lack of a clear consensus on what constitutes a pathological HFO further challenges supervised refinement approaches. To address this, we leverage the insight that legacy detectors reliably capture clinically relevant signals despite their relatively high false positive rates. We thus propose the Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of candidate events generated by legacy detectors into a precise set of pathological HFOs. SS2LD employs a variational autoencoder (VAE) for morphological pre-training to learn meaningful latent representation of the detected events. These representations are clustered to derive weak supervision for pathological events. A classifier then uses this supervision to refine detection boundaries, trained on real and VAE-augmented data. Evaluated on large multi-institutional interictal iEEG datasets, SS2LD outperforms state-of-the-art methods. SS2LD offers a scalable, label-efficient, and clinically effective strategy to identify pathological HFOs using legacy detectors.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Generative Models in Condition and Structural Health Monitoring: Opportunities, Limitations and Future Outlook</title>
<link>https://arxiv.org/abs/2507.15026</link>
<guid>https://arxiv.org/abs/2507.15026</guid>
<content:encoded><![CDATA[
<div> Keywords: Condition monitoring, Structural health monitoring, Deep generative models, Fault diagnosis, Anomaly detection

Summary: 
Condition and structural health monitoring (CM/SHM) are crucial for predictive maintenance in various industrial sectors. Conventional deep learning models face challenges such as operational variability, imbalanced datasets, and multimodal sensory data. Deep generative models (DGMs) like autoregressive models and generative adversarial networks offer solutions by generating data samples, reconstructing system states, and handling multimodal data. This review compares DGMs with traditional models in CM/SHM, focusing on tasks like data imbalance, domain adaptation, and fault diagnosis. Limitations of DGMs include explainability issues, computational inefficiencies, and the need for parameter-efficient strategies. Future research can explore zero-shot learning, multimodal generalization, hybrid architectures combining DGMs with physics knowledge, and reinforcement learning with DGMs for industrial scenarios. <div>
arXiv:2507.15026v1 Announce Type: new 
Abstract: Condition and structural health monitoring (CM/SHM) is a pivotal component of predictive maintenance (PdM) strategies across diverse industrial sectors, including mechanical rotating machinery, airplane composite wings, offshore wind turbines, and civil engineering structures. Conventional deep learning models, while effective in fault diagnosis and anomaly detection through supervised feature extraction and rule-based data augmentation, often struggle with operational variability, imbalanced or scarce fault datasets, and multimodal sensory data from complex systems. Deep generative models (DGMs) in this regard, including autoregressive models, variational autoencoders, generative adversarial networks, diffusion-based models, and emerging large language models, offer transformative capabilities by synthesizing high-fidelity data samples, reconstructing latent system states, and modeling complex multimodal data streams. This review systematically examines state-of-the-art DGM applications in CM/SHM systems, emphasizing their role in addressing key challenges: data imbalance and imputation, domain adaptation and generalization, multimodal data fusion, and downstream fault diagnosis and anomaly detection tasks, with rigorous comparison among signal processing, conventional machine learning or deep learning models, and DGMs. We also analyze current limitations of DGMs, including challenges of explainable and trustworthy models, computational inefficiencies for edge deployment, and the need for parameter-efficient fine-tuning strategies. Future research directions can focus on zero-shot and few-shot learning, robust multimodal generalization, hybrid architectures integrating DGMs with physics knowledge, and reinforcement learning with DGMs to enhance robustness and accuracy in industrial scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Configurational-force-driven adaptive refinement and coarsening in topology optimization</title>
<link>https://arxiv.org/abs/2507.15570</link>
<guid>https://arxiv.org/abs/2507.15570</guid>
<content:encoded><![CDATA[
<div> Keywords: topology optimization, configurational forces, mesh adaptivity, Eshelby stress, multi-level refinement

Summary:
Topology optimization often requires solving numerous linear equation systems, leading to high computational costs due to fine mesh requirements. A multi-level adaptive refinement and coarsening strategy based on configurational forces is proposed to address this challenge. Configurational forces, derived from Eshelby stress, predict configurational changes like crack propagation. By utilizing configurational forces for refinement, a high-resolution structure is achieved along design boundaries and stress-critical regions, while multilevel coarsening reduces computational effort. This approach is particularly advantageous in stress-sensitive problems where preventing stress failure is crucial. Ultimately, the use of configurational forces for mesh adaptivity in topology optimization results in geometrically well-defined structures with significantly reduced computational costs. 

<br /><br />Summary: <div>
arXiv:2507.15570v1 Announce Type: new 
Abstract: The iterative nature of topology optimization, especially in combination with nonlinear state problems, often requires the solution of thousands of linear equation systems. Furthermore, due to the pixelated design representation, the use of a fine mesh is essential to obtain geometrically well-defined structures and to accurately compute response quantities such as the von Mises stress. Therefore, the computational cost of solving a fine-mesh topology optimization problem quickly adds up. To address this challenge, we consider a multi-level adaptive refinement and coarsening strategy based on configurational forces. Configurational forces based on the Eshelby stress predict configurational changes such as crack propagation or dislocation motion. Due to a relaxation in the calculation of (Eshelby) stresses with respect to the design variables, discrete configurational forces increase not only in highly stressed regions, but also in grey transition regions (design boundaries). For this reason they are an ideal criterion for mesh adaptivity in topology optimization, especially when avoiding stress failure is a priority. By using configurational forces for refinement, we obtain a high-resolution structure where the refined mesh is present along the design boundaries as well as in stress-critical regions. At the same time, multilevel coarsening using the same criterion drastically minimizes the computational effort.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers</title>
<link>https://arxiv.org/abs/2507.15753</link>
<guid>https://arxiv.org/abs/2507.15753</guid>
<content:encoded><![CDATA[
<div> diffusion transformers, metamaterials, generative framework, 3D geometries, mechanical objectives<br />
<br />
Summary:<br />
A new generative framework called DiffuMeta has been developed to enable the inverse design of three-dimensional metamaterials. It uses diffusion transformers and a novel algebraic language representation to encode complex 3D geometries as mathematical sentences. This approach allows for the precise control of stress-strain responses in shell structures, considering factors like buckling and contact. DiffuMeta can generate diverse solutions to address the one-to-many mapping challenge and simultaneously optimize multiple mechanical objectives, including nonlinear responses. Experimental validation has shown the effectiveness of DiffuMeta in designing metamaterials with tailored properties, demonstrating its potential for accelerating the design process of complex structures. <div>
arXiv:2507.15753v1 Announce Type: new 
Abstract: Generative machine learning models have revolutionized material discovery by capturing complex structure-property relationships, yet extending these approaches to the inverse design of three-dimensional metamaterials remains limited by computational complexity and underexplored design spaces due to the lack of expressive representations. Here, we present DiffuMeta, a generative framework integrating diffusion transformers with a novel algebraic language representation, encoding 3D geometries as mathematical sentences. This compact, unified parameterization spans diverse topologies while enabling direct application of transformers to structural design. DiffuMeta leverages diffusion models to generate novel shell structures with precisely targeted stress-strain responses under large deformations, accounting for buckling and contact while addressing the inherent one-to-many mapping by producing diverse solutions. Uniquely, our approach enables simultaneous control over multiple mechanical objectives, including linear and nonlinear responses beyond training domains. Experimental validation of fabricated structures further confirms the efficacy of our approach for accelerated design of metamaterials and structures with tailored properties.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing Physics Discovery through Fully Differentiable Finite Element-Based Machine Learning</title>
<link>https://arxiv.org/abs/2507.15787</link>
<guid>https://arxiv.org/abs/2507.15787</guid>
<content:encoded><![CDATA[
<div> finite element-based machine learning, surrogate-modelling, PDE solutions, unknown physics, trainable operators <br />
Summary: 
The article introduces a novel approach called fully differentiable finite element-based machine learning (FEBML) to address limitations in existing surrogate-modelling methods for problems involving unknown or incomplete relationships in PDEs. FEBML embeds trainable operators for unknown physics within a state-of-the-art FEM solver, enabling end-to-end differentiation. It represents unknown operators as an encode-process-decode pipeline over finite-element degrees of freedom, ensuring learned physics respects the variational structure. The versatility of FEBML is demonstrated by successfully recovering nonlinear stress-strain laws from laboratory tests, applying the learned model to new mechanical scenarios without retraining, and identifying temperature-dependent conductivity in transient heat flow. This new framework offers promise for improving accuracy and generality in modelling complex scientific and engineering problems. <br /> <div>
arXiv:2507.15787v1 Announce Type: new 
Abstract: Although many problems in science and engineering are modelled by well-established PDEs, they often involve unknown or incomplete relationships, such as material constitutive laws or thermal response, that limit accuracy and generality. Existing surrogate-modelling approaches directly approximate PDE solutions but remain tied to a specific geometry, boundary conditions, and set of physical constraints. To address these limitations, we introduce a fully differentiable finite element-based machine learning (FEBML) framework that embeds trainable operators for unknown physics within a state-of-the-art, general FEM solver, enabling true end-to-end differentiation. At its core, FEBML represents each unknown operator as an encode-process-decode pipeline over finite-element degrees of freedom: field values are projected to nodal coefficients, transformed by a neural network, and then lifted back to a continuous FE function, ensuring the learned physics respects the variational structure. We demonstrate its versatility by recovering nonlinear stress-strain laws from laboratory tests, applying the learned model to a new mechanical scenario without retraining, and identifying temperature-dependent conductivity in transient heat flow.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions</title>
<link>https://arxiv.org/abs/2507.14245</link>
<guid>https://arxiv.org/abs/2507.14245</guid>
<content:encoded><![CDATA[
<div> Keywords: nanomaterials, proteins, AI, NanoPro-3M, multimodal representation learning <br />
Summary: 
NanoPro-3M dataset is introduced, containing over 3.2 million samples and 37,000 unique proteins, aiming to enhance understanding of nanomaterial-protein interactions. NanoProFormer model is proposed to predict these interactions with strong generalization abilities, handling missing features and unseen entities. The model outperforms single-modality approaches by utilizing multimodal representation learning, identifying crucial factors influencing corona formation. It showcases applicability to various tasks via zero-shot inference and fine-tuning. This work lays the groundwork for accurate and generalized predictions of nanomaterial-protein interactions, reducing reliance on experimental data and speeding up in vitro applications. <br /><br />Summary: <div>
arXiv:2507.14245v1 Announce Type: cross 
Abstract: Unlocking the potential of nanomaterials in medicine and environmental science hinges on understanding their interactions with proteins, a complex decision space where AI is poised to make a transformative impact. However, progress has been hindered by limited datasets and the restricted generalizability of existing models. Here, we propose NanoPro-3M, the largest nanomaterial-protein interaction dataset to date, comprising over 3.2 million samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer, a foundational model that predicts nanomaterial-protein affinities through multimodal representation learning, demonstrating strong generalization, handling missing features, and unseen nanomaterials or proteins. We show that multimodal modeling significantly outperforms single-modality approaches and identifies key determinants of corona formation. Furthermore, we demonstrate its applicability to a range of downstream tasks through zero-shot inference and fine-tuning. Together, this work establishes a solid foundation for high-performance and generalized prediction of nanomaterial-protein interaction endpoints, reducing experimental reliance and accelerating various in vitro applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do Large Language Models know about materials?</title>
<link>https://arxiv.org/abs/2507.14586</link>
<guid>https://arxiv.org/abs/2507.14586</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Mechanical Engineering, Materials Science, Periodic Table of Elements, Benchmark 

Summary: 
Large Language Models (LLMs) are being used in the fields of mechanical engineering and materials science to facilitate step-wise reasoning through the Processing-Structure-Property-Performance chain. While current LLMs are trained on a wide range of internet data, much of this data is non-scientific. To ensure LLMs can provide accurate information about materials, it is important to evaluate their intrinsic knowledge. In this study, the researchers focus on the example of the Periodic Table of Elements to assess LLMs' ability to generate factually correct output. By analyzing the vocabulary and tokenization used in different LLM models, the study highlights the uniqueness of material fingerprints and identifies the need for specialized models in various stages of the PSPP chain. This work serves as a benchmark for determining which steps in the material science process LLMs are suitable for and where specialized models may be necessary. 

<br /><br />Summary: <div>
arXiv:2507.14586v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly applied in the fields of mechanical engineering and materials science. As models that establish connections through the interface of language, LLMs can be applied for step-wise reasoning through the Processing-Structure-Property-Performance chain of material science and engineering. Current LLMs are built for adequately representing a dataset, which is the most part of the accessible internet. However, the internet mostly contains non-scientific content. If LLMs should be applied for engineering purposes, it is valuable to investigate models for their intrinsic knowledge -- here: the capacity to generate correct information about materials. In the current work, for the example of the Periodic Table of Elements, we highlight the role of vocabulary and tokenization for the uniqueness of material fingerprints, and the LLMs' capabilities of generating factually correct output of different state-of-the-art open models. This leads to a material knowledge benchmark for an informed choice, for which steps in the PSPP chain LLMs are applicable, and where specialized models are required.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators</title>
<link>https://arxiv.org/abs/2507.14652</link>
<guid>https://arxiv.org/abs/2507.14652</guid>
<content:encoded><![CDATA[
<div> Hamiltonian Monte Carlo, Bayesian neural networks, variational inference, stochastic gradient MCMC, uncertainty estimation <br />
Summary: 
This paper introduces a hybrid approach that combines variational inference and Hamiltonian Monte Carlo (HMC) methods to efficiently estimate uncertainties in neural networks. By initially training with variational inference and identifying non-contributory parameters, the dimension of the parameter space is reduced, allowing for faster and more accurate HMC inference. This approach is demonstrated on deep neural networks and operator networks, showing effectiveness in learning surrogates for complex physical systems. The method enables inference for large networks with tens to hundreds of thousands of parameters, showcasing its efficiency and accuracy in quantifying uncertainties. Additionally, the approach is applied to model an operator mapping in hypersonic flow, illustrating its capability in learning complex relations between input conditions and output data. <br /> <div>
arXiv:2507.14652v1 Announce Type: cross 
Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample from the posterior distribution in Bayesian inference. However, HMC techniques are computationally demanding for Bayesian neural networks due to the high dimensionality of the network's parameter space and the non-convexity of their posterior distributions. Therefore, various approximation techniques, such as variational inference (VI) or stochastic gradient MCMC, are often employed to infer the posterior distribution of the network parameters. Such approximations introduce inaccuracies in the inferred distributions, resulting in unreliable uncertainty estimates. In this work, we propose a hybrid approach that combines inexpensive VI and accurate HMC methods to efficiently and accurately quantify uncertainties in neural networks and neural operators. The proposed approach leverages an initial VI training on the full network. We examine the influence of individual parameters on the prediction uncertainty, which shows that a large proportion of the parameters do not contribute substantially to uncertainty in the network predictions. This information is then used to significantly reduce the dimension of the parameter space, and HMC is performed only for the subset of network parameters that strongly influence prediction uncertainties. This yields a framework for accelerating the full batch HMC for posterior inference in neural networks. We demonstrate the efficiency and accuracy of the proposed framework on deep neural networks and operator networks, showing that inference can be performed for large networks with tens to hundreds of thousands of parameters. We show that this method can effectively learn surrogates for complex physical systems by modeling the operator that maps from upstream conditions to wall-pressure data on a cone in hypersonic flow.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Profiling and Address Role Inference in Tokenized U.S. Treasuries</title>
<link>https://arxiv.org/abs/2507.14808</link>
<guid>https://arxiv.org/abs/2507.14808</guid>
<content:encoded><![CDATA[
<div> Tokenized U.S. Treasuries, real-world assets, blockchain networks, transaction-level behavior, functional dissection<br />
<br />
Summary: Tokenized U.S. Treasuries, a subset of real-world assets, are yield-bearing instruments collateralized by sovereign debt deployed on various blockchain networks. This study delves into the transaction-level behavior of U.S. Treasury-backed RWA tokens (e.g., BUIDL, BENJI, USDY) across multiple chains, identifying core functional primitives like issuance, redemption, transfer, and bridge activity. The analysis reveals a distinction in behavior between institutional and retail users. A curvature-aware representation learning framework utilizing Poincaré embeddings and liquidity-based graph features is introduced for address-level economic role modeling. The method surpasses baseline models in role inference and extends to anomaly detection and wallet classification in broader blockchain transaction networks. These findings offer insights into functional diversity and participant roles in tokenized Treasuries on a transaction-specific level, enhancing understanding of on-chain financialization.<br /><br /> <div>
arXiv:2507.14808v1 Announce Type: cross 
Abstract: Tokenized U.S. Treasuries have emerged as a prominent subclass of real-world assets (RWAs), offering cryptographically enforced, yield-bearing instruments collateralized by sovereign debt and deployed across multiple blockchain networks. While the market has expanded rapidly, empirical analyses of transaction-level behaviour remain limited. This paper conducts a quantitative, function-level dissection of U.S. Treasury-backed RWA tokens including BUIDL, BENJI, and USDY, across multi-chain: mostly Ethereum and Layer-2s. We analyze decoded contract calls to isolate core functional primitives such as issuance, redemption, transfer, and bridge activity, revealing segmentation in behaviour between institutional actors and retail users. To model address-level economic roles, we introduce a curvature-aware representation learning framework using Poincar\'e embeddings and liquidity-based graph features. Our method outperforms baseline models on our RWA Treasury dataset in role inference and generalizes to downstream tasks such as anomaly detection and wallet classification in broader blockchain transaction networks. These findings provide a structured understanding of functional heterogeneity and participant roles in tokenized Treasury in a transaction-level perspective, contributing new empirical evidence to the study of on-chain financialization.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering</title>
<link>https://arxiv.org/abs/2507.15003</link>
<guid>https://arxiv.org/abs/2507.15003</guid>
<content:encoded><![CDATA[
<div> Keywords: AI teammates, autonomous coding agents, software engineering, dataset, collaboration

Summary:<br /><br />
The paper introduces AIDev, a large-scale dataset capturing the operation of autonomous coding agents in software development. It includes data from over 456,000 pull requests by five leading agents across 61,000 repositories and 47,000 developers. AIDev provides rich metadata on pull requests, authorship, review timelines, code changes, and integration outcomes. The dataset allows for research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. While autonomous agents show faster code submission compared to humans, their pull requests are accepted less frequently, indicating a trust and utility gap. The dataset highlights that even though agents accelerate the submission process, the code they generate is structurally simpler. AIDev is intended to be a living resource for the software engineering and AI communities, aiming to support research into AI-native workflows and symbiotic human-AI collaboration. The dataset is publicly available for further analysis and exploration.<br /><br />Summary: <div>
arXiv:2507.15003v1 Announce Type: cross 
Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI teammates: autonomous, goal-driven systems collaborating with human developers. Among these, autonomous coding agents are especially transformative, now actively initiating, reviewing, and evolving code at scale. This paper introduces AIDev, the first large-scale dataset capturing how such agents operate in the wild. Spanning over 456,000 pull requests by five leading agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across 61,000 repositories and 47,000 developers, AIDev provides an unprecedented empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software engineering, AIDev offers structured, open data to support research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. The dataset includes rich metadata on PRs, authorship, review timelines, code changes, and integration outcomes--enabling exploration beyond synthetic benchmarks like SWE-bench. For instance, although agents often outperform humans in speed, their PRs are accepted less frequently, revealing a trust and utility gap. Furthermore, while agents accelerate code submission--one developer submitted as many PRs in three days as they had in three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev enables a new generation of research into AI-native workflows and supports building the next wave of symbiotic human-AI collaboration. The dataset is publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperelastic nature of the Hoek-Brown criterion</title>
<link>https://arxiv.org/abs/2507.15813</link>
<guid>https://arxiv.org/abs/2507.15813</guid>
<content:encoded><![CDATA[
<div> hyperbolic elasticity, elasto-plastic model, yield criterion, plasticity, finite element simulations
Summary:
The article presents a new nonlinear elasto-plastic model that incorporates hyperbolic elasticity resulting from an invariant yield criterion on the plasticity level. This model combines nonlinear elastic behavior with plasticity adhering to the associated flow rule. It highlights the connection between a linear yield criterion on the thermodynamic force of plasticity and a quadratic yield criterion in stress space, indicating a relationship between different yield criteria. Comparisons between linear and hyperbolic elasticity in the context of the Drucker-Prager yield criterion show the nonlinear case exhibiting dilatancy saturation in triaxial compression tests. Structural finite element simulations are conducted to demonstrate the practicality of the proposed model, showcasing its numerical applicability in various scenarios. <div>
arXiv:2507.15813v1 Announce Type: cross 
Abstract: We propose a nonlinear elasto-plastic model, for which a specific class of hyperbolic elasticity arises as a straight consequence of the yield criterion invariance on the plasticity level. We superimpose this nonlinear elastic (or hyperelastic) behavior with plasticity obeying the associated flow rule. Interestingly, we find that a linear yield criterion on the thermodynamical force associated with plasticity results in a quadratic yield criterion in the stress space. This suggests a specific hyperelastic connection between Mohr-Coulomb and Hoek-Brown (or alternatively between Drucker-Prager and Pan-Hudson) yield criteria. We compare the elasto-plastic responses of standard tests for the Drucker-Prager yield criterion using either linear or the suggested hyperbolic elasticity. Notably, the nonlinear case stands out due to dilatancy saturation observed during cyclic loading in the triaxial compression test. We conclude this study with structural finite element simulations that clearly demonstrate the numerical applicability of the proposed model.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection</title>
<link>https://arxiv.org/abs/2507.13459</link>
<guid>https://arxiv.org/abs/2507.13459</guid>
<content:encoded><![CDATA[
<div> Graph neural network, surrogate modeling, contact mechanics, soft deformable bodies, computational cost 
Summary: 
The article introduces a graph neural network architecture for surrogate modeling in nonlinear boundary value problems in mechanics, focusing on contact between soft deformable bodies. This approach incorporates continuous collision detection and sufficient conditions for contact, improving generalization of the network. The framework is tested on soft tissue mechanics problems, such as predicting the closed state of a bioprosthetic aortic valve, demonstrating better generalization with additional contact terms in the loss function. The network can handle complex contact scenarios with varying reference geometries but comes with high computational costs during training. Despite this, the implementation results in significant speedups for inference, showing up to a thousand-fold improvement on benchmark problems. Overall, the graph neural network offers promising advancements in efficiently solving contact mechanics problems involving soft deformable bodies. 
<br /><br />Summary: <div>
arXiv:2507.13459v1 Announce Type: new 
Abstract: Surrogate models for the rapid inference of nonlinear boundary value problems in mechanics are helpful in a broad range of engineering applications. However, effective surrogate modeling of applications involving the contact of deformable bodies, especially in the context of varying geometries, is still an open issue. In particular, existing methods are confined to rigid body contact or, at best, contact between rigid and soft objects with well-defined contact planes. Furthermore, they employ contact or collision detection filters that serve as a rapid test but use only the necessary and not sufficient conditions for detection. In this work, we present a graph neural network architecture that utilizes continuous collision detection and, for the first time, incorporates sufficient conditions designed for contact between soft deformable bodies. We test its performance on two benchmarks, including a problem in soft tissue mechanics of predicting the closed state of a bioprosthetic aortic valve. We find a regularizing effect on adding additional contact terms to the loss function, leading to better generalization of the network. These benefits hold for simple contact at similar planes and element normal angles, and complex contact at differing planes and element normal angles. We also demonstrate that the framework can handle varying reference geometries. However, such benefits come with high computational costs during training, resulting in a trade-off that may not always be favorable. We quantify the training cost and the resulting inference speedups on various hardware architectures. Importantly, our graph neural network implementation results in up to a thousand-fold speedup for our benchmark problems at inference.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems</title>
<link>https://arxiv.org/abs/2507.14043</link>
<guid>https://arxiv.org/abs/2507.14043</guid>
<content:encoded><![CDATA[
<div> Algorithm, Snake Optimizer, Multi-strategy, Levy flight, UAV path planning

Summary:
The study introduces the Multi-strategy Improved Snake Optimizer (MISO) to enhance the Snake Optimizer algorithm by addressing issues such as slow convergence and local optima traps. MISO incorporates adaptive random disturbance and Levy flight strategies to prevent local optima trapping and improve global optimum exploration. A unique position update strategy combining elite leadership and Brownian motion accelerates convergence speed while maintaining precision. Experimental validation against 30 CEC2017 test functions and the CEC2022 test suite showcases MISO's effectiveness compared to 11 popular algorithms. Application of MISO to UAV 3D path planning and engineering design problems demonstrates superior solution quality and stability, highlighting its potential for practical use.

<br /><br />Summary: <div>
arXiv:2507.14043v1 Announce Type: cross 
Abstract: Metaheuristic algorithms have gained widespread application across various fields owing to their ability to generate diverse solutions. One such algorithm is the Snake Optimizer (SO), a progressive optimization approach. However, SO suffers from the issues of slow convergence speed and susceptibility to local optima. In light of these shortcomings, we propose a novel Multi-strategy Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random disturbance strategy based on sine function to alleviate the risk of getting trapped in a local optimum. Secondly, we introduce adaptive Levy flight strategy based on scale factor and leader and endow the male snake leader with flight capability, which makes it easier for the algorithm to leap out of the local optimum and find the global optimum. More importantly, we put forward a position update strategy combining elite leadership and Brownian motion, effectively accelerating the convergence speed while ensuring precision. Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test functions and the CEC2022 test suite, comparing it with 11 popular algorithms across different dimensions to validate its effectiveness. Moreover, Unmanned Aerial Vehicle (UAV) has been widely used in various fields due to its advantages of low cost, high mobility and easy operation. However, the UAV path planning problem is crucial for flight safety and efficiency, and there are still challenges in establishing and optimizing the path model. Therefore, we apply MISO to the UAV 3D path planning problem as well as 6 engineering design problems to assess its feasibility in practical applications. The experimental results demonstrate that MISO exceeds other competitive algorithms in terms of solution quality and stability, establishing its strong potential for application.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Paradigm Shift to Assembly-like Finite Element Model Updating</title>
<link>https://arxiv.org/abs/2502.02592</link>
<guid>https://arxiv.org/abs/2502.02592</guid>
<content:encoded><![CDATA[
<div> Validation, Finite Element Model, Aircraft, Computational Efficiency, Assembly-Like Approach
<br />
Summary: 
The article introduces a new assembly-like approach for updating finite element models in aeronautics, crucial for developing aircraft with modern flexible wings. This method updates the model as parts are assembled, offering significant computational efficiency by requiring 20% fewer iterations and fewer parameters compared to the traditional one-shot approach. The proposed approach maintains fidelity to the global method while reducing computational burden, making it a promising technique for complex structures. <div>
arXiv:2502.02592v2 Announce Type: replace 
Abstract: In general, there is a mismatch between a finite element model of a structure and its real behaviour. In aeronautics, this mismatch must be small because finite element models are a fundamental part of the development of an aircraft and of increasing importance with the trend to more flexible wings in modern designs. Finite element model updating can be computationally expensive for complex structures and surrogate models can be employed to reduce the computational burden. A novel approach for finite element model updating, namely assembly-like, is proposed and validated using real experimental data. The assembly-like model updating framework implies that the model is updated as parts are assembled. Benchmarking against the classical global, or one-shot, approach demonstrates that the proposed method is more computationally efficient since it takes 20% fewer iterations to obtain convergence, also using fewer parameters for the model evaluations. Despite the increase in computational performance, the new approach retains the fidelity of the global approach.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vector-level Feedforward Control of LPBF Melt Pool Area Using a Physics-Based Thermal Model</title>
<link>https://arxiv.org/abs/2507.12557</link>
<guid>https://arxiv.org/abs/2507.12557</guid>
<content:encoded><![CDATA[
<div> Keywords: Laser powder bed fusion, additive manufacturing, feedforward control, melt pool area, part quality <br />
Summary: 
A new feedforward control framework for regulating melt pool area in Laser Powder Bed Fusion (LPBF) additive manufacturing is proposed. The framework combines a thermal model and a melt pool model to efficiently predict and optimize melt pool area, reducing geometric inaccuracies and porosity in metal parts. Calibration of the models using minimal experiments allows for accurate control of laser power scheduling. The framework was successfully validated on complex 3D geometries made of Inconel 718 and 316L stainless steel, showing significant improvements in part quality metrics. By proactively compensating for thermal effects, the approach demonstrates enhanced part quality while remaining computationally efficient and adaptable to different materials and machines. Overall, the vector-level feedforward control framework presents a promising method for improving the quality of LPBF-produced parts. <br /><br /> <div>
arXiv:2507.12557v1 Announce Type: new 
Abstract: Laser powder bed fusion (LPBF) is an additive manufacturing technique that has gained popularity thanks to its ability to produce geometrically complex, fully dense metal parts. However, these parts are prone to internal defects and geometric inaccuracies, stemming in part from variations in the melt pool. This paper proposes a novel vector-level feedforward control framework for regulating melt pool area in LPBF. By decoupling part-scale thermal behavior from small-scale melt pool physics, the controller provides a scale-agnostic prediction of melt pool area and efficient optimization over it. This is done by operating on two coupled lightweight models: a finite-difference thermal model that efficiently captures vector-level temperature fields and a reduced-order, analytical melt pool model. Each model is calibrated separately with minimal single-track and 2D experiments, and the framework is validated on a complex 3D geometry in both Inconel 718 and 316L stainless steel. Results showed that feedforward vector-level laser power scheduling reduced geometric inaccuracy in key dimensions by 62%, overall porosity by 16.5%, and photodiode variation by 6.8% on average. Overall, this modular, data-efficient approach demonstrates that proactively compensating for known thermal effects can significantly improve part quality while remaining computationally efficient and readily extensible to other materials and machines.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDS-Net: A novel framework for few-shot photovoltaic power prediction with interpretable dynamic selection and feature information fusion</title>
<link>https://arxiv.org/abs/2507.12745</link>
<guid>https://arxiv.org/abs/2507.12745</guid>
<content:encoded><![CDATA[
<div> transfer learning, PV power stations, feature selection, interpretable dynamic selection network, prediction

Summary:
The article introduces a novel interpretable dynamic selection network (IDS-Net) for accurate few-shot prediction in PV power stations. The framework includes pre-training on a large dataset, feature selection using the ReliefF algorithm, and outlier correction with the Hampel Identifier. The IDS-Net model incorporates interpretable weights and adaptive selection outcomes for accurate predictions. An end-to-end adaptive transfer learning strategy is designed for final prediction results on the target dataset. The framework's effectiveness and generalization are demonstrated using two PV power datasets from Hebei province, China. <div>
arXiv:2507.12745v1 Announce Type: new 
Abstract: With the growing demand for renewable energy, countries are accelerating the construction of photovoltaic (PV) power stations. However, accurately forecasting power data for newly constructed PV stations is extremely challenging due to limited data availability. To this end, we propose a novel interpretable dynamic selection network (IDS-Net) based on feature information fusion to achieve accurate few-shot prediction. This transfer learning framework primarily consists of two parts. In the first stage, we pre-train on the large dataset, utilizing Maximum Mean Discrepancy (MMD) to select the source domain dataset most similar to the target domain data distribution. Subsequently, the ReliefF algorithm is utilized for feature selection, reducing the influence of feature redundancy. Then, the Hampel Identifier (HI) is used for training dataset outlier correction. In the IDS-Net model, we first obtain the initial extracted features from a pool of predictive models. Following this, two separate weighting channels are utilized to determine the interpretable weights for each sub-model and the adaptive selection outcomes, respectively. Subsequently, the extracted feature results from each sub-model are multiplied by their corresponding weights and then summed to obtain the weighted extracted features. Then, we perform cross-embedding on the additional features and fuse them with the extracted weighted features. This fused information is then passed through the MLP (Multi-Layer Perceptron) layer to obtain predictions. In the second stage, we design an end-to-end adaptive transfer learning strategy to obtain the final prediction results on the target dataset. We validate the transfer learning process using two PV power datasets from Hebei province, China, to demonstrate the effectiveness and generalization of our framework and transfer learning strategy.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Reinforcement Learning with LSTM Forecasting Signals for Optimizing Fintech Trading Decisions</title>
<link>https://arxiv.org/abs/2507.12835</link>
<guid>https://arxiv.org/abs/2507.12835</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, quantum circuits, financial systems, LSTM predictions, performance<br />
<br />
Summary: <br />
Financial trading environments are complex and dynamic, posing challenges for traditional reinforcement learning methods. This study introduces a novel approach by integrating quantum circuits into a reinforcement learning framework customized for financial systems. The comparison between classical A3C and quantum A3C algorithms, alongside incorporating LSTM-based predictions of economic trends, reveals that quantum models with predictive signals outperform traditional methods. The experiments conducted in a Gymnasium-compatible trading environment demonstrate the superior performance and stability of quantum models, even with shallow quantum circuit depth, in noisy financial conditions. This research highlights the potential of quantum reinforcement learning in tackling the complexities of financial markets and improving trading strategies. <div>
arXiv:2507.12835v1 Announce Type: new 
Abstract: Financial trading environments are characterized by high volatility, numerous macroeconomic signals, and dynamically shifting market regimes, where traditional reinforcement learning methods often fail to deliver breakthrough performance. In this study, we design a reinforcement learning framework tailored for financial systems by integrating quantum circuits. We compare (1) the performance of classical A3C versus quantum A3C algorithms, and (2) the impact of incorporating LSTM-based predictions of the following week's economic trends on learning outcomes. The experimental framework adopts a custom Gymnasium-compatible trading environment, simulating discrete trading actions and evaluating rewards based on portfolio feedback. Experimental results show that quantum models - especially when combined with predictive signals - demonstrate superior performance and stability under noisy financial conditions, even with shallow quantum circuit depth.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic Chain-of-Thought Synthesis Optimization</title>
<link>https://arxiv.org/abs/2507.12901</link>
<guid>https://arxiv.org/abs/2507.12901</guid>
<content:encoded><![CDATA[
<div> Dataset, financial reasoning, language models, CoT synthesis, knowledge space

Summary:
Agentar-DeepFinance-300K is a new large-scale financial reasoning dataset created with a focus on optimizing chain-of-thought (CoT) synthesis for robust financial reasoning. The dataset is generated using a Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) pipeline to ensure comprehensive and deep financial reasoning trajectories. The researchers also conducted a systematic investigation called CoT Cube to analyze critical factors affecting the effectiveness of CoT, such as necessity, length, and synthesizer. Models trained on Agentar-DeepFinance-300K show significant improvements on financial benchmarks, highlighting the importance of well-designed CoT construction in financial reasoning models. The dataset is publicly available to advance research in financial reasoning models.
<br /><br />Summary: <div>
arXiv:2507.12901v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have demonstrated remarkable general reasoning capabilities, holding significant potential for applications in the financial domain, a field that requires robust and reliable reasoning. It has been demonstrated that distilling high-quality chain-of-thought (CoT) rationales from advanced general reasoning models offers a promising and efficient path to the financial reasoning model. However, existing CoT synthesis methods suffer from shallow CoT sampling, leaving the question of how to construct a well-designed knowledge space for finance reasoning unexplored. In this paper, we present \textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset characterized by its systematic CoT synthesis optimization. We first introduce a comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and deep financial reasoning trajectories. Furthermore, a systematic investigation, termed CoT Cube, is conducted to analyze critical factors that influence CoT effectiveness, such as necessity, length and synthesizer, yielding valuable insights for high-quality financial CoT construction. Experiments demonstrate that models trained on our Agentar-DeepFinance-300K achieve significant improvements on financial benchmarks. We publicly release Agentar-DeepFinance-300K , hoping to advance the research in financial reasoning models.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To What Extent Can Public Equity Indices Statistically Hedge Real Purchasing Power Loss in Compounded Structural Emerging-Market Crises? An Explainable ML-Based Assessment</title>
<link>https://arxiv.org/abs/2507.13055</link>
<guid>https://arxiv.org/abs/2507.13055</guid>
<content:encoded><![CDATA[
<div> Keywords: local public equity indices, real purchasing power loss, macro-financial collapses, emerging markets, tail dependence copula analysis

Summary: 
This study examines the effectiveness of local public equity indices in hedging real purchasing power loss during macro-financial collapses in emerging markets. Using non-linear real return calculations and advanced statistical analysis techniques, the research focuses on collapse episodes in Turkey (2018), Nigeria (2020), and Pakistan (2021). The findings highlight the limitations of using equity-based protection during simultaneous macroeconomic and monetary dislocations. The study challenges traditional notions of equity pricing theory for inflation and devaluation hedge effectiveness and emphasizes the need for context-sensitive strategies in times of compounded macro-financial distress. The analysis underscores the importance of considering tail risk and the potential breakdown of equity-based protection mechanisms during crises. Contextual factors and crisis triggers play a significant role in shaping the effectiveness of equity indices in preserving purchasing power during economic turmoil. <div>
arXiv:2507.13055v1 Announce Type: new 
Abstract: This study investigates the extent to which local public equity indices can statistically hedge real purchasing power loss during compounded structural macro-financial collapses in emerging markets. We employ a non-linear multiplicative real return calculations consistent with Fisher-parity logics for both domestic and foreign investors with a principled quantile regression, tail dependence copula analysis, and Shapley Additive Explanations (SHAP) to assess the explanatory power of macro variables. The analysis focuses on three recent and data-accessible exemplary collapse episodes: Turkey (2018), Nigeria (2020), and Pakistan (2021). Such cases, selected to align with post-2018 improvements in data standardization and crisis comparability, span varied monetary regimes and crisis triggers. Our tail-focused modeling reveals a systematic breakdown in public-equity-based purchasing power protection precisely during simultaneous macroeconomic and monetary dislocations when such protection is most needed. The findings call into question conventional inflation and devaluation hedge presumptions in equity pricing theory, emphasizing the limitations of equity-based protection and the need for context-sensitive strategies during compounded macro-financial distress.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding</title>
<link>https://arxiv.org/abs/2507.12482</link>
<guid>https://arxiv.org/abs/2507.12482</guid>
<content:encoded><![CDATA[
<div> retrieval, code generation, autonomous code understanding, debugging, software maintenance <br />
Summary: 
The article introduces Kodezi Chronos, a new architecture for autonomous code understanding that can operate across ultra-long contexts without fixed window limits. It leverages a multi-level embedding memory engine to efficiently reason over millions of lines of code, supporting tasks like repository-scale comprehension and real-time self-healing actions. A novel benchmark, the Multi Random Retrieval, evaluates the model’s ability to resolve distant associations across code artifacts, outperforming prior models by 23% in bug detection. Chronos reduces debugging cycles by up to 40% compared to traditional approaches. By integrating with IDEs and CI/CD workflows, Chronos enhances code reliability and productivity, reducing manual effort and advancing towards self-sustaining software ecosystems. <br /><br />Summary: <div>
arXiv:2507.12482v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have advanced code generation and software automation, but are fundamentally constrained by limited inference-time context and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a next-generation architecture for autonomous code understanding, debugging, and maintenance, designed to operate across ultra-long contexts comprising entire codebases, histories, and documentation, all without fixed window limits. Kodezi Chronos leverages a multi-level embedding memory engine, combining vector and graph-based indexing with continuous code-aware retrieval. This enables efficient and accurate reasoning over millions of lines of code, supporting repository-scale comprehension, multi-file refactoring, and real-time self-healing actions. Our evaluation introduces a novel Multi Random Retrieval benchmark, specifically tailored to the software engineering domain. Unlike classical retrieval benchmarks, this method requires the model to resolve arbitrarily distant and obfuscated associations across code artifacts, simulating realistic tasks such as variable tracing, dependency migration, and semantic bug localization. Chronos outperforms prior LLMs and code models, demonstrating a 23% improvement in real-world bug detection and reducing debugging cycles by up to 40% compared to traditional sequence-based approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos enables seamless, autonomous software maintenance, elevating code reliability and productivity while reducing manual effort. These results mark a critical advance toward self-sustaining, continuously optimized software ecosystems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RONOM: Reduced-Order Neural Operator Modeling</title>
<link>https://arxiv.org/abs/2507.12814</link>
<guid>https://arxiv.org/abs/2507.12814</guid>
<content:encoded><![CDATA[
<div> Reduced-order modeling, neural operators, time-dependent partial differential equations, discretization error bound, spatial super-resolution.<br />
Summary:<br />
The article introduces Reduced-Order Neural Operator Modeling (RONOM), combining concepts from reduced-order modeling (ROM) and operator learning. It addresses the computational intensity of time-dependent partial differential equations in many-query scenarios. RONOM bridges the gap between ROM and neural operator approaches, providing insights into discretization convergence and robustness. The framework offers a discretization error bound analogous to ROM for rigorous numerical error estimation. Comparing RONOM to existing neural operators in solving PDEs, results show RONOM's standard vector-to-vector neural networks achieve comparable input generalization and superior performance in spatial super-resolution and discretization robustness. Additionally, RONOM offers novel insights into temporal super-resolution scenarios. <div>
arXiv:2507.12814v1 Announce Type: cross 
Abstract: Time-dependent partial differential equations are ubiquitous in physics-based modeling, but they remain computationally intensive in many-query scenarios, such as real-time forecasting, optimal control, and uncertainty quantification. Reduced-order modeling (ROM) addresses these challenges by constructing a low-dimensional surrogate model but relies on a fixed discretization, which limits flexibility across varying meshes during evaluation. Operator learning approaches, such as neural operators, offer an alternative by parameterizing mappings between infinite-dimensional function spaces, enabling adaptation to data across different resolutions. Whereas ROM provides rigorous numerical error estimates, neural operator learning largely focuses on discretization convergence and invariance without quantifying the error between the infinite-dimensional and the discretized operators. This work introduces the reduced-order neural operator modeling (RONOM) framework, which bridges concepts from ROM and operator learning. We establish a discretization error bound analogous to those in ROM, and get insights into RONOM's discretization convergence and discretization robustness. Moreover, two numerical examples are presented that compare RONOM to existing neural operators for solving partial differential equations. The results demonstrate that RONOM using standard vector-to-vector neural networks achieves comparable performance in input generalization and superior performance in both spatial super-resolution and discretization robustness, while also offering novel insights into temporal super-resolution scenarios.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying data needs in surrogate modeling for flow fields in 2D stirred tanks with physics-informed neural networks (PINNs)</title>
<link>https://arxiv.org/abs/2507.11640</link>
<guid>https://arxiv.org/abs/2507.11640</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Surrogate Models, Stirred Tanks, Computational Fluid Dynamics, Data Requirements<br />
<br />Summary:
Physics-informed neural networks (PINNs) are proposed as a solution to develop efficient surrogate models for flow fields in stirred tanks. This study investigates the data requirements for developing these models and compares them with classical neural networks and boundary-informed neural networks (BINNs). The results show that PINNs can achieve accurate predictions with as few as six data points, demonstrating their effectiveness in reducing data requirements. Surrogate models can achieve prediction errors of around 3% across a range of Reynolds numbers, with an approximation of velocity profile leading to errors of 2.5%. This study highlights the potential of PINNs in efficiently modeling flow fields in stirred tanks, even with limited or approximate datasets. <div>
arXiv:2507.11640v1 Announce Type: new 
Abstract: Stirred tanks are vital in chemical and biotechnological processes, particularly as bioreactors. Although computational fluid dynamics (CFD) is widely used to model the flow in stirred tanks, its high computational cost$-$especially in multi-query scenarios for process design and optimization$-$drives the need for efficient data-driven surrogate models. However, acquiring sufficiently large datasets can be costly. Physics-informed neural networks (PINNs) offer a promising solution to reduce data requirements while maintaining accuracy by embedding underlying physics into neural network (NN) training. This study quantifies the data requirements of vanilla PINNs for developing surrogate models of a flow field in a 2D stirred tank. We compare these requirements with classical supervised neural networks and boundary-informed neural networks (BINNs). Our findings demonstrate that surrogate models can achieve prediction errors around 3% across Reynolds numbers from 50 to 5000 using as few as six datapoints. Moreover, employing an approximation of the velocity profile in place of real data labels leads to prediction errors of around 2.5%. These results indicate that even with limited or approximate datasets, PINNs can be effectively trained to deliver high accuracy comparable to high-fidelity data.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MNO : A Multi-modal Neural Operator for Parametric Nonlinear BVPs</title>
<link>https://arxiv.org/abs/2507.11870</link>
<guid>https://arxiv.org/abs/2507.11870</guid>
<content:encoded><![CDATA[
<div> Multi-parameter, Nonlinear, Boundary value problems, Multimodal Neural Operator, Generalized FMM<br />
Summary:<br />
The article introduces a novel Multimodal Neural Operator (MNO) architecture for learning solution operators for multi-parameter nonlinear boundary value problems (BVPs). Unlike traditional neural operators that map PDE coefficients or source terms independently, the MNO architecture can map multiple parameters, including PDE coefficients, source terms, and boundary conditions, to the solution space in a unified manner. Inspired by the Fast Multipole Method (FMM), the MNO consists of three key components: a Generalized FMM (GFMM) block, a Unimodal Neural Operator (UNO) for single parameter mappings, and a multimodal fusion mechanism. Experiment results demonstrate the MNO's capability to handle variations in PDE coefficients and source or boundary terms simultaneously. <div>
arXiv:2507.11870v1 Announce Type: new 
Abstract: We introduce a novel Multimodal Neural Operator (MNO) architecture designed to learn solution operators for multi-parameter nonlinear boundary value problems (BVPs). Traditional neural operators primarily map either the PDE coefficients or source terms independently to the solution, limiting their flexibility and applicability. In contrast, our proposed MNO architecture generalizes these approaches by mapping multiple parameters including PDE coefficients, source terms, and boundary conditions to the solution space in a unified manner. Our MNO is motivated by the hierarchical nested bases of the Fast Multipole Method (FMM) and is constructed systematically through three key components: a parameter efficient Generalized FMM (GFMM) block, a Unimodal Neural Operator (UNO) built upon GFMM blocks for single parameter mappings, and most importantly, a multimodal fusion mechanism extending these components to learn the joint map. We demonstrate the multimodal generalization capacity of our approach on both linear and nonlinear BVPs. Our experiments show that the network effectively handles simultaneous variations in PDE coefficients and source or boundary terms.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Fourier Neural Operators for Micromechanics</title>
<link>https://arxiv.org/abs/2507.12233</link>
<guid>https://arxiv.org/abs/2507.12233</guid>
<content:encoded><![CDATA[
<div> Fourier Neural Operators, Micromechanics, Homogenization, Deep Learning, Fast Fourier Transform <br />
Summary: <br />
The article discusses the use of Fourier Neural Operators (FNOs) in solving cell problems in micromechanics. Traditional computational frameworks outperform deep-learning frameworks in this area, and the potential of machine-learning approaches for micromechanics is unclear. The study shows that FNOs, empowered by insights from fast Fourier transform (FFT) methods, can accurately predict solutions to cell problems with arbitrary stiffness distribution, subject to a material-contrast constraint. This approach does not require restrictions on material properties, number of phases, or geometry of interfaces between materials. The fidelity provided by FNOs is sharp and uniform, with explicit guarantees of accuracy. Furthermore, an FNO explicitly constructed without training demonstrates the universal approximation property, with memory requirements and runtimes comparable to classical FFT solvers. This work aims to bridge the gap between FFT-based methods and FNOs, potentially facilitating collaboration between the two approaches. <br /> <div>
arXiv:2507.12233v1 Announce Type: new 
Abstract: \noindent Solving cell problems in homogenization is hard, and available deep-learning frameworks fail to match the speed and generality of traditional computational frameworks. More to the point, it is generally unclear what to expect of machine-learning approaches, let alone single out which approaches are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs) for micromechanics, empowering them by insights from computational micromechanics methods based on the fast Fourier transform (FFT). We construct an FNO surrogate mimicking the basic scheme foundational for FFT-based methods and show that the resulting operator predicts solutions to cell problems with \emph{arbitrary} stiffness distribution only subject to a material-contrast constraint up to a desired accuracy. In particular, there are no restrictions on the material symmetry like isotropy, on the number of phases and on the geometry of the interfaces between materials. Also, the provided fidelity is sharp and uniform, providing explicit guarantees leveraging our physical empowerment of FNOs. To show the desired universal approximation property, we construct an FNO explicitly that requires no training to begin with. Still, the obtained neural operator complies with the same memory requirements as the basic scheme and comes with runtimes proportional to classical FFT solvers. In particular, large-scale problems with more than 100 million voxels are readily handled. The goal of this work is to underline the potential of FNOs for solving micromechanical problems, linking FFT-based methods to FNOs. This connection is expected to provide a fruitful exchange between both worlds.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Identification of Nonlinear Dynamics with Conformal Prediction</title>
<link>https://arxiv.org/abs/2507.11739</link>
<guid>https://arxiv.org/abs/2507.11739</guid>
<content:encoded><![CDATA[
<div> Sparse Identification of Nonlinear Dynamics, SINDy, uncertainty quantification, Conformal Prediction, Ensemble-SINDy<br />
<br />
Summary: 
The article investigates the integration of Conformal Prediction with Ensemble-SINDy (E-SINDy) for uncertainty quantification in nonlinear dynamical system models. The study focuses on three key applications: quantifying uncertainty in time series prediction, model selection based on library feature importance, and assessing the uncertainty of identified model coefficients using feature conformal prediction. Results show that the integration of Conformal Prediction with E-SINDy can reliably achieve target coverage for time series forecasting, effectively quantify feature importance in model selection, and produce robust uncertainty intervals for model coefficients, even under non-Gaussian noise conditions. The study demonstrates the versatility and reliability of the approach in various scenarios, including stochastic predator-prey dynamics and chaotic dynamical systems. <div>
arXiv:2507.11739v1 Announce Type: cross 
Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for discovering nonlinear dynamical system models from data. Quantifying uncertainty in SINDy models is essential for assessing their reliability, particularly in safety-critical applications. While various uncertainty quantification methods exist for SINDy, including Bayesian and ensemble approaches, this work explores the integration of Conformal Prediction, a framework that can provide valid prediction intervals with coverage guarantees based on minimal assumptions like data exchangeability. We introduce three applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1) quantifying uncertainty in time series prediction, (2) model selection based on library feature importance, and (3) quantifying the uncertainty of identified model coefficients using feature conformal prediction. We demonstrate the three applications on stochastic predator-prey dynamics and several chaotic dynamical systems. We show that conformal prediction methods integrated with E-SINDy can reliably achieve desired target coverage for time series forecasting, effectively quantify feature importance, and produce more robust uncertainty intervals for model coefficients, even under non-Gaussian noise, compared to standard E-SINDy coefficient estimates.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Purity: Defense Paradigm For Chain-of-Thought Attack</title>
<link>https://arxiv.org/abs/2507.12314</link>
<guid>https://arxiv.org/abs/2507.12314</guid>
<content:encoded><![CDATA[
<div> Large Reasoning Models, Chain-of-Thought Attack, security threats, reinforcement learning, Thought Purity<br />
Summary:<br />
The article discusses the vulnerability of reinforcement learning-trained Large Reasoning Models to Chain-of-Thought Attack (CoTA) due to backdoor prompt attacks. CoTA exploits prompt controllability, compromising both safety and task performance. To address this, the authors propose a defense mechanism called Thought Purity (TP) with three key components: a safety-optimized data processing pipeline, reinforcement learning-enhanced rule constraints, and adaptive monitoring metrics. TP aims to strengthen resistance to malicious content while maintaining operational efficacy. This approach offers the first comprehensive defense against CoTA vulnerabilities in reasoning systems aligned with reinforcement learning, enhancing the security-functionality balance in future AI architectures. <br /><br /> <div>
arXiv:2507.12314v1 Announce Type: cross 
Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g., Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large Language Models (LLMs) domain, their susceptibility to security threats remains a critical vulnerability. This weakness is particularly evident in Chain-of-Thought (CoT) generation processes, where adversarial methods like backdoor prompt attacks can systematically subvert the model's core reasoning mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this vulnerability through exploiting prompt controllability, simultaneously degrading both CoT safety and task performance with low-cost interventions. To address this compounded security-performance vulnerability, we propose Thought Purity (TP): a defense paradigm that systematically strengthens resistance to malicious content while preserving operational efficacy. Our solution achieves this through three synergistic components: (1) a safety-optimized data processing pipeline (2) reinforcement learning-enhanced rule constraints (3) adaptive monitoring metrics. Our approach establishes the first comprehensive defense mechanism against CoTA vulnerabilities in reinforcement learning-aligned reasoning systems, significantly advancing the security-functionality equilibrium for next-generation AI architectures.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data</title>
<link>https://arxiv.org/abs/2507.12425</link>
<guid>https://arxiv.org/abs/2507.12425</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Enterprise Data, Dense Embeddings, Metadata-aware Filtering

Summary: 
The study introduces an advanced Retrieval-Augmented Generation (RAG) framework for enterprise data that combines hybrid retrieval strategies using dense embeddings and BM25, along with metadata-aware filtering and cross-encoder reranking. By applying semantic chunking and retaining tabular data structures, the framework ensures textual coherence and maintains the integrity of data. Experiments on enterprise datasets demonstrate significant improvements in Precision@5, Recall@5, and Mean Reciprocal Rank. Qualitative evaluations also show higher scores in Faithfulness, Completeness, and Relevance. The framework delivers accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work involves extending the framework to handle multimodal data and integrating agent-based retrieval. The source code will be made available on GitHub at the provided link.<br /><br />Summary: <div>
arXiv:2507.12425v1 Announce Type: cross 
Abstract: Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Algorithms and Implementations for Computing the Minimum Distance of Quantum Codes</title>
<link>https://arxiv.org/abs/2408.10743</link>
<guid>https://arxiv.org/abs/2408.10743</guid>
<content:encoded><![CDATA[
<div> stabilizer quantum code, symplectic distance, fast algorithms, computational time, shared-memory parallel architectures  
Summary: The article introduces three new fast algorithms and implementations for computing the symplectic distance of stabilizer quantum codes. The distance of a stabilizer quantum code is crucial for error detection and correction. The new algorithms, based on the Brouwer-Zimmermann algorithm, outperform current state-of-the-art implementations on various processors, showing significant improvements in computational time, especially in highly demanding cases. The study demonstrates superior performance on single-core processors, multicore processors, and shared-memory multiprocessors, with scalability observed on shared-memory parallel architectures. These advancements in computing the symplectic distance offer a substantial leap forward in the efficiency and speed of error detection and correction in stabilizer quantum codes. <br /><br />Summary: <div>
arXiv:2408.10743v2 Announce Type: replace-cross 
Abstract: The distance of a stabilizer quantum code is a very important feature since it determines the number of errors that can be detected and corrected. We present three new fast algorithms and implementations for computing the symplectic distance of the associated classical code. Our new algorithms are based on the Brouwer-Zimmermann algorithm. Our experimental study shows that these new implementations are much faster than current state-of-the-art licensed implementations on single-core processors, multicore processors, and shared-memory multiprocessors. In the most computationally-demanding cases, the performance gain in the computational time can be larger than one order of magnitude. The experimental study also shows a good scalability on shared-memory parallel architectures.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Three-dimensional SPH modeling of brittle fracture under hydrodynamic loading</title>
<link>https://arxiv.org/abs/2507.10553</link>
<guid>https://arxiv.org/abs/2507.10553</guid>
<content:encoded><![CDATA[
<div> Modeling fluid-structure interactions, three-dimensional SPH computational framework, weakly compressible SPH, pseudo-spring-based SPH solver, structural deformation, structural failure<br />
<br />
Summary:<br />
A three-dimensional computational framework using SPH for fluid-structure interactions is introduced. The model integrates weakly compressible SPH with a pseudo-spring-based solver to simulate fluid flow and deformable structures while capturing solid boundaries and fluid-structure interfaces without contact forces. Pressure calculations in the fluid phase are enhanced by the $\delta$-SPH technique, and structural damage is modeled using a pseudo-spring approach with limited particle interactions. The framework accurately simulates detailed fracture patterns without complex crack-tracking algorithms. It has shown effectiveness compared to existing models and experimental data, providing insights into the effects of hydrodynamic events on structural integrity.<br /><br /> <div>
arXiv:2507.10553v1 Announce Type: new 
Abstract: A three-dimensional SPH computational framework is presented for modeling fluid-structure interactions with structural deformation and failure. We combine weakly compressible SPH with a pseudo-spring-based SPH solver to capture the fluid flow and deformable structures. A unified modeling approach captures the solid boundaries and fluid-structure interfaces without penalty-based contact force. The $\delta$-SPH technique improves the pressure calculations in the fluid phase, while structural damage is modeled using a pseudo-spring approach, with particle interactions limited to its neighbors. The present framework can capture the three-dimensional crack surfaces in structures without any computationally intensive crack-tracking algorithm or visibility criteria. The framework has been proven effective against existing models and experimental data, demonstrating high accuracy and robustness in simulating detailed fracture patterns and offering insights into the impact of hydrodynamic events on structural integrity.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Multiple Time-Stepping Method for 3-Body Interactions in High Performance Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2507.11172</link>
<guid>https://arxiv.org/abs/2507.11172</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular dynamics, two-body interactions, three-body interactions, r-RESPA algorithm, High Performance Computing

Summary:
This study focuses on improving the efficiency of molecular dynamics (MD) simulations by incorporating two-body and three-body interactions. Traditional two-body potentials may not fully capture the complexity of molecular systems, necessitating the inclusion of three-body interactions. However, three-body interactions are computationally expensive due to their cubic complexity class. The r-RESPA algorithm is utilized to reduce the number of three-body interaction calculations, enhancing efficiency. The study explores this method in the context of High Performance Computing (HPC) methods for parallelizing calculations. It introduces a communication-reducing distributed-memory parallel method and a novel shared-memory parallel cutoff method implemented in the particle simulation library AutoPas. The results and methods discussed offer insights into potential advancements in MD simulation efficiency. 

<br /><br />Summary: <div>
arXiv:2507.11172v1 Announce Type: new 
Abstract: Understanding the complex behavior of molecular systems is fundamental to fields such as physics, materials science, and biology. Molecular dynamics (MD) simulations are crucial tools for studying atomic-level dynamics. This work focuses on improving the efficiency of MD simulations involving two-body and three-body interactions. Traditional two-body potentials often can not fully capture the complexity of molecular systems, making the inclusion of three-body interactions important. However, these interactions are in a cubic complexity class, compared to a quadratic one for two-body interactions, and therefore are computationally expensive, even when a cutoff distance is applied. One way to improve efficiency is to use the r-RESPA multiple time-stepping algorithm to reduce the number of three-body interaction calculations. In this work, we investigate this method in the context of High Performance Computing (HPC) methods that parallelize the calculations. In particular, we investigate a communication-reducing distributed-memory parallel method from literature and present a novel shared-memory parallel cutoff method, implemented in the particle simulation library AutoPas. The results and methods are discussed, providing insights into potential advancements in MD simulation efficiency.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Differential Evolution in Tire Industry Extrusion: Leveraging Surrogate Models</title>
<link>https://arxiv.org/abs/2507.11191</link>
<guid>https://arxiv.org/abs/2507.11191</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, data-driven optimization, manufacturing systems, machine learning, metaheuristic<br />
<br />Summary: 
This study introduces a data-driven methodology for optimizing complex manufacturing systems using historical process data. By utilizing machine learning models to create surrogate models, the approach, Data-Driven Differential Evolution with Multi-Level Penalty Functions and Surrogate Models, is tailored to the specifics of the industry. The method is applied to an extrusion process in tire manufacturing to optimize initialization parameters and reduce waste and production time. Results show a 65% reduction in setup time and a decrease in material waste compared to historical configurations, demonstrating the superiority of the surrogate-based optimization approach. This research underscores the advantages of combining data-driven modeling with metaheuristic optimization for industrial processes lacking explicit formulations. <br /><br /> <div>
arXiv:2507.11191v1 Announce Type: new 
Abstract: The optimization of industrial processes remains a critical challenge, particularly when no mathematical formulation of objective functions or constraints is available. This study addresses this issue by proposing a surrogate-based, data-driven methodology for optimizing complex real-world manufacturing systems using only historical process data. Machine learning models are employed to approximate system behavior and construct surrogate models, which are integrated into a tailored metaheuristic approach: Data-Driven Differential Evolution with Multi-Level Penalty Functions and Surrogate Models, an adapted version of Differential Evolution suited to the characteristics of the studied process. The methodology is applied to an extrusion process in the tire manufacturing industry, with the goal of optimizing initialization parameters to reduce waste and production time. Results show that the surrogate-based optimization approach outperforms historical best configurations, achieving a 65\% reduction in initialization and setup time, while also significantly minimizing material waste. These findings highlight the potential of combining data-driven modeling and metaheuristic optimization for industrial processes where explicit formulations are unavailable.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tax-Efficient Model Predictive Control Policy for Retirement Funding</title>
<link>https://arxiv.org/abs/2507.10603</link>
<guid>https://arxiv.org/abs/2507.10603</guid>
<content:encoded><![CDATA[
<div> taxes, retirement funding, inflation, investment returns, bequest
<br />
Summary: 
The article presents a retirement funding policy that addresses the challenge of managing a retiree's savings to ensure constant post-tax inflation-adjusted consumption throughout their lifetime. The policy involves two main steps. Firstly, a simplified planning problem is formulated as a convex optimization problem to maximize the bequest while maintaining a constant inflation-adjusted consumption target. This allows for quick and reliable solution of the planning problem. Secondly, a model predictive control (MPC) retirement policy is developed based on the annual update-plan-act cycle. The MPC policy takes into account uncertain factors such as investment returns, inflation, changes in life expectancy, external income, liabilities, and tax rules and rates. The effectiveness of the MPC retirement policy is demonstrated through Monte Carlo simulation, showcasing its ability to adapt to changing circumstances and provide a reliable approach to retirement funding. <div>
arXiv:2507.10603v1 Announce Type: cross 
Abstract: The retirement funding problem addresses the question of how to manage a retiree's savings to provide her with a constant post-tax inflation adjusted consumption throughout her lifetime. This consists of choosing withdrawals and transfers from and between several accounts with different tax treatments, taking into account basic rules such as required minimum distributions and limits on Roth conversions, additional income, liabilities, taxes, and the bequest when the retiree dies. We develop a retirement funding policy in two steps. In the first step, we consider a simplified planning problem in which various future quantities, such as the retiree's remaining lifetime, future investment returns, and future inflation, are known. Using a simplified model of taxes, we pose this planning problem as a convex optimization problem, where we maximize the bequest subject to providing a constant inflation adjusted consumption target. Since this problem is convex, it can be solved quickly and reliably. We leverage this planning method to form a retirement funding policy that determines the actions to take each year, based on information known at that time. Each year the retiree forms a new plan for the future years, using the current account values and life expectancy, and optionally, updated information such as changes in tax rates or rules. The retiree then carries out the actions from the first year of the current plan. This update-plan-act cycle is repeated each year, a general policy called model predictive control (MPC). The MPC retirement policy reacts to the effects of uncertain investment returns and inflation, changes in the retiree's expected lifetime or external income and liabilities, and changes in tax rules and rates. We demonstrate the effectiveness of the MPC retirement policy using Monte Carlo simulation.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong</title>
<link>https://arxiv.org/abs/2507.11502</link>
<guid>https://arxiv.org/abs/2507.11502</guid>
<content:encoded><![CDATA[
<div> foundational sovereign large language model, Hong Kong, multilingual, value-aligned, AI infrastructure<br />
Summary: This paper introduces HKGAI-V1, a large language model tailored for Hong Kong's unique multilingual and socio-legal environment. Developed using the DeepSeek architecture, the model is aligned with regional norms through full parameter fine-tuning. Integrated with a retrieval-augmented generation system, it provides timely and factual information access. The paper showcases two key achievements: the successful development of HKGAI-V1, outperforming general-purpose models in handling culturally sensitive queries, and the creation of the Adversarial HK Value Benchmark for evaluating model alignment with local ethical and legal standards. This work presents a replicable blueprint for developing regionally focused AI systems with a strong emphasis on local identity and values.<br /> <div>
arXiv:2507.11502v1 Announce Type: cross 
Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign large language model (LLM), developed as part of an initiative to establish value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing the region's unique multilingual environment (Cantonese, Mandarin, and English), its distinct socio-legal context under the "one country, two systems" framework, and specific local cultural and value considerations, the model is built upon the DeepSeek architecture and systematically aligned with regional norms through a multifaceted full parameter fine-tuning process. It is further integrated with a retrieval-augmented generation (RAG) system to ensure timely and factually grounded information access. The core contribution lies in the design and implementation of a comprehensive, region-specific AI alignment and safety framework, demonstrated through two key achievements: 1) The successful development of HKGAI-V1 itself - which outper-forms general-purpose models in handling Hong Kong-specific culturally sensitive queries, and embodies a "governance-embedded" approach to digital sovereignty - empowers Hong Kong to exercise control over AI applications in critical sectors including public services, legal systems, and edu-cation. 2) The development of the proprietary Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment with local ethical and legal stand-ards under challenging conditions. By documenting these achievements, the paper provides not only a technological artifact but also a replicable blueprint for developing advanced, regionally focused AI systems deeply rooted in their local identities.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering</title>
<link>https://arxiv.org/abs/2507.11527</link>
<guid>https://arxiv.org/abs/2507.11527</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Model, Civil Engineering, technical drawing revision, automation agents

Summary:
DrafterBench is a comprehensive benchmark designed for evaluating Large Language Model (LLM) agents in the context of technical drawing revision in Civil Engineering. It consists of twelve types of tasks derived from real-world drawing files, encompassing 46 customized functions/tools and a total of 1920 tasks. The benchmark aims to rigorously test AI agents' abilities in interpreting complex instructions, leveraging prior knowledge, and adapting to varying instruction quality. It evaluates capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. Detailed analysis of task accuracy and error statistics is provided to gain deeper insights into agent proficiency and improvement areas for integrating LLMs in engineering applications. The benchmark is open-source and available for access, facilitating the assessment of automation agents from an industrial perspective. 

<br /><br />Summary: <div>
arXiv:2507.11527v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Irrotational Contact Fields</title>
<link>https://arxiv.org/abs/2312.03908</link>
<guid>https://arxiv.org/abs/2312.03908</guid>
<content:encoded><![CDATA[
<div> Framework, convex approximations, complex contact models, Coulomb's law, maximum dissipation

Summary: The article introduces a framework for generating convex approximations of complex contact models, incorporating validated models like Hunt & Crossley and Coulomb's law of friction. The approach is robust across a wide range of stiffness values, suitable for compliant surfaces and rigid approximations. The approximations are evaluated across various test cases, with detailed properties and limitations outlined. The implementation in the open-source robotics toolkit, Drake, provides a fully differentiable solution, allowing for computation of gradients for complex geometric models while reusing contact resolution factorizations. The hybrid approach enables robust simulation of robotic tasks at interactive rates, accurately resolving stiction and contact transitions, thus supporting effective sim-to-real transfer. <div>
arXiv:2312.03908v3 Announce Type: replace-cross 
Abstract: We present a framework for generating convex approximations of complex contact models, incorporating experimentally validated models like Hunt & Crossley coupled with Coulomb's law of friction alongside the principle of maximum dissipation. Our approach is robust across a wide range of stiffness values, making it suitable for both compliant surfaces and rigid approximations. We evaluate these approximations across a wide variety of test cases, detailing properties and limitations. We implement a fully differentiable solution in the open-source robotics toolkit, Drake. Our novel hybrid approach enables computation of gradients for complex geometric models while reusing factorizations from contact resolution. We demonstrate robust simulation of robotic tasks at interactive rates, with accurately resolved stiction and contact transitions, supporting effective sim-to-real transfer.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Parameter Inference and Uncertainty Quantification for a Computational Pulmonary Hemodynamics Model Using Gaussian Processes</title>
<link>https://arxiv.org/abs/2502.14251</link>
<guid>https://arxiv.org/abs/2502.14251</guid>
<content:encoded><![CDATA[
<div> Keywords: subject-specific modeling, cardiovascular research, chronic thromboembolic pulmonary hypertension, microvascular disease, Gaussian process emulators 

Summary: 
Subject-specific modeling in cardiovascular research is crucial for personalized treatment guidance beyond current clinical diagnostics. This study utilized a one-dimensional fluid dynamics model informed by experimental data from a dog model of chronic thromboembolic pulmonary hypertension (CTEPH). The model incorporated measurements from multiple subjects under both baseline and CTEPH conditions, allowing for the assessment of microvascular disease severity. By modeling each lung separately to account for heterogeneity in CTEPH, the study identified distinct parameter shifts reflecting heterogeneous microvascular adaptation. Gaussian process emulators were used to accelerate model calibration, enabling the estimation of microvascular parameters and their uncertainties efficiently. The study demonstrated strong correlations between model parameter changes and disease severity, particularly in the lung with more advanced disease. This framework provides a rapid, uncertainty-aware method for evaluating microvascular dysfunction in CTEPH and may inform targeted treatment strategies with clinical applicability.<br /><br />Summary: <div>
arXiv:2502.14251v2 Announce Type: replace-cross 
Abstract: Subject-specific modeling is a powerful tool in cardiovascular research, providing insights beyond the reach of current clinical diagnostics. Limitations in available clinical data require the incorporation of uncertainty into models to improve guidance for personalized treatments. However, for clinical relevance, such modeling must be computationally efficient. In this study, we used a one-dimensional (1D) fluid dynamics model informed by experimental data from a dog model of chronic thromboembolic pulmonary hypertension (CTEPH), incorporating measurements from multiple subjects under both baseline and CTEPH conditions. Surgical intervention can alleviate CTEPH, yet patients with microvascular disease (e.g., remodeling and narrowing of small vessels) often exhibit persistent pulmonary hypertension, highlighting the importance of assessing microvascular disease severity. Thus, each lung was modeled separately to account for the heterogeneous nature of CTEPH, allowing us to explore lung-specific microvascular narrowing and resistance. We compared inferred parameters between baseline and CTEPH and examined their correlation with clinical markers of disease severity. To accelerate model calibration, we employed Gaussian process (GP) emulators, enabling the estimation of microvascular parameters and their uncertainties within a clinically feasible timeframe. Our results demonstrated that CTEPH leads to heterogeneous microvascular adaptation, reflected in distinct parameter shifts. Notably, the changes in model parameters strongly correlated with disease severity, especially in the lung previously reported to have more advanced disease. This framework provides a rapid, uncertainty-aware method for evaluating microvascular dysfunction in CTEPH and may support more targeted treatment strategies within a timeframe suitable for clinical application.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets</title>
<link>https://arxiv.org/abs/2507.09255</link>
<guid>https://arxiv.org/abs/2507.09255</guid>
<content:encoded><![CDATA[
<div> simulation platform, large language models, financial decision-making, StockSim, open-source
Summary:
StockSim is an open-source simulation platform designed for evaluating large language models (LLMs) in realistic financial decision-making scenarios. It offers a comprehensive system that accurately models market dynamics and supports various simulation modes with different levels of detail. By incorporating real-world factors like latency and order-book microstructure, StockSim allows for more insightful assessment of LLM-based trading agents. Its extensible agent framework supports diverse trading strategies and multi-agent coordination, making it a valuable tool for NLP research on reasoning under uncertainty and sequential decision-making.
<br /><br />Summary: <div>
arXiv:2507.09255v1 Announce Type: new 
Abstract: We present StockSim, an open-source simulation platform for systematic evaluation of large language models (LLMs) in realistic financial decision-making scenarios. Unlike previous toolkits that offer limited scope, StockSim delivers a comprehensive system that fully models market dynamics and supports diverse simulation modes of varying granularity. It incorporates critical real-world factors, such as latency, slippage, and order-book microstructure, that were previously neglected, enabling more faithful and insightful assessment of LLM-based trading agents. An extensible, role-based agent framework supports heterogeneous trading strategies and multi-agent coordination, making StockSim a uniquely capable testbed for NLP research on reasoning under uncertainty and sequential decision-making. We open-source all our code at https: //github.com/harrypapa2002/StockSim.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoWarp: An automatically differentiable and GPU-accelerated implicit MPM framework for geomechanics based on NVIDIA Warp</title>
<link>https://arxiv.org/abs/2507.09435</link>
<guid>https://arxiv.org/abs/2507.09435</guid>
<content:encoded><![CDATA[
<div> Keywords: material point method, implicit formulation, geomechanics, GPU parallelism, automatic differentiation

Summary:
GeoWarp is introduced as an implicit material point method (MPM) framework for geomechanics. It utilizes GPU parallelism and automatic differentiation to compute Jacobian matrices without manual derivation, overcoming the limitations of explicit MPM formulations. The framework includes a sparse Jacobian construction algorithm that takes advantage of localized particle-grid interactions inherent in MPM. GeoWarp is verified through examples in large-deformation elastoplasticity and coupled poromechanics, showcasing its robustness, scalability, and extensibility for differentiable implicit MPM simulation in computational geomechanics. By leveraging GPU parallelism and automatic differentiation, GeoWarp provides a powerful tool for simulating large-deformation and history-dependent behavior in geomechanical systems, making it a valuable asset in the field of computational geomechanics. 

<br /><br />Summary: <div>
arXiv:2507.09435v1 Announce Type: new 
Abstract: The material point method (MPM), a hybrid Lagrangian-Eulerian particle method, is increasingly used to simulate large-deformation and history-dependent behavior of geomaterials. While explicit time integration dominates current MPM implementations due to its algorithmic simplicity, such schemes are unsuitable for quasi-static and long-term processes typical in geomechanics. Implicit MPM formulations are free of these limitations but remain less adopted, largely due to the difficulty of computing the Jacobian matrix required for Newton-type solvers, especially when consistent tangent operators should be derived for complex constitutive models. In this paper, we introduce GeoWarp -- an implicit MPM framework for geomechanics built on NVIDIA Warp -- that exploits GPU parallelism and reverse-mode automatic differentiation to compute Jacobians without manual derivation. To enhance efficiency, we develop a sparse Jacobian construction algorithm that leverages the localized particle-grid interactions intrinsic to MPM. The framework is verified through forward and inverse examples in large-deformation elastoplasticity and coupled poromechanics. Results demonstrate that GeoWarp provides a robust, scalable, and extensible platform for differentiable implicit MPM simulation in computational geomechanics.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EV-STLLM: Electric vehicle charging forecasting based on spatio-temporal large language models with multi-frequency and multi-scale information fusion</title>
<link>https://arxiv.org/abs/2507.09527</link>
<guid>https://arxiv.org/abs/2507.09527</guid>
<content:encoded><![CDATA[
<div> VMD, ICEEMDAN, FIG, ReliefF, EV-STLLM <br />
Summary: <br />
The paper proposes a novel EV spatio-temporal large language model (EV-STLLM) for accurate prediction of electric vehicle (EV) charging demand and station occupancy. The framework consists of two modules: a data processing module using VMD and ICEEMDAN for data denoising and multi-frequency decomposition, FIG for extracting multi-scale information, and ReliefF for feature selection; a forecasting module using EV-STLLM for direct prediction. The model integrates adjacency matrices from regional station networks and spatio-temporal-frequency embedding information to capture data characteristics. The PFGA module maintains sequential feature modeling capabilities and incorporates EV domain knowledge. Experiments on real-world data from Shenzhen show superior accuracy compared to existing methods. <div>
arXiv:2507.09527v1 Announce Type: new 
Abstract: With the proliferation of electric vehicles (EVs), accurate charging demand and station occupancy forecasting are critical for optimizing urban energy and the profit of EVs aggregator. Existing approaches in this field usually struggle to capture the complex spatio-temporal dependencies in EV charging behaviors, and their limited model parameters hinder their ability to learn complex data distribution representations from large datasets. To this end, we propose a novel EV spatio-temporal large language model (EV-STLLM) for accurate prediction. Our proposed framework is divided into two modules. In the data processing module, we utilize variational mode decomposition (VMD) for data denoising, and improved complete ensemble empirical mode decomposition with adaptive noise (ICEEMDAN) for data multi-frequency decomposition. Fuzzy information granulation (FIG) for extracting multi-scale information. Additionally, ReliefF is used for feature selection to mitigate redundancy. In the forecasting module, the EV-STLLM is used to directly achieve EV charging and occupancy forecasting. Firstly, we fully capture the intrinsic spatio-temporal characteristics of the data by integrating adjacency matrices derived from the regional stations network and spatio-temporal-frequency embedding information. Then, the partially frozen graph attention (PFGA) module is utilized to maintain the sequential feature modeling capabilities of the pre-trained large model while incorporating EV domain knowledge. Extensive experiments using real-world data from Shenzhen, China, demonstrate that our proposed framework can achieve superior accuracy and robustness compared to the state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed machine learning surrogate for scalable simulation of thermal histories during wire-arc directed energy deposition</title>
<link>https://arxiv.org/abs/2507.09591</link>
<guid>https://arxiv.org/abs/2507.09591</guid>
<content:encoded><![CDATA[
<div> large-scale structural engineering, wire-arc directed energy deposition, finite element method, physics-informed neural networks, metal additive manufacturing <br /> 
Summary: <br /> 
The study focuses on wire-arc directed energy deposition (DED) for large-scale structural engineering applications. Traditional finite element method (FEM) simulations for thermal history prediction during deposition are computationally intensive. Physics-informed neural networks (PINNs) offer an alternative by combining physical knowledge with machine learning. The study investigates the scalability of PINNs, emphasizing efficient collocation points sampling to reduce computational time. Results show that PINNs can significantly decrease effort by up to 98.6% while maintaining accuracy and providing "super-resolution." The research suggests future enhancements for PINN performance in the context of metal additive manufacturing. <br /> <div>
arXiv:2507.09591v1 Announce Type: new 
Abstract: Wire-arc directed energy deposition (DED) has emerged as a promising additive manufacturing (AM) technology for large-scale structural engineering applications. However, the complex thermal dynamics inherent to the process present challenges in ensuring structural integrity and mechanical properties of fabricated thick walls and plates. While finite element method (FEM) simulations have been conventionally employed to predict thermal history during deposition, their computational demand remains prohibitively high for actual large-scale applications. Given the necessity of multiple repetitive simulations for heat management and the determination of an optimal printing strategy, FEM simulation quickly becomes entirely infeasible. Instead, advancements have been made in using trained neural networks as surrogate models for rapid prediction. However, traditional data-driven approaches necessitate large amounts of relevant and verifiable external data, during the training and validation of the neural network. Regarding large-scale wire-arc DED, none of these data sources are readily available in quantities sufficient for an accurate surrogate. The introduction of physics-informed neural networks (PINNs) has opened up an alternative simulation strategy by leveraging the existing physical knowledge of the phenomena with advanced machine learning methods. Despite their theoretical advantages, PINNs have seen limited application in the context of large-scale wire-arc DED for structural engineering. This study investigates the scalability of PINNs, focusing on efficient collocation points sampling, a critical factor controlling both the training time and model performance. Results show PINNs can reduce computational time and effort by up to 98.6%, while maintaining the desired accuracy and offering "super-resolution". Future directions for enhancing PINN performance in metal AM are discussed.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Matters Most? A Quantitative Meta-Analysis of AI-Based Predictors for Startup Success</title>
<link>https://arxiv.org/abs/2507.09675</link>
<guid>https://arxiv.org/abs/2507.09675</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, startup success, predictor importance, meta-analysis, context

Summary: 
- The study conducts a meta-analysis to synthesize predictor importance in AI-based startup evaluation, analyzing 13 empirical studies and identifying 58 unique predictors.
- The most powerful predictors for startup success are Firm Characteristics, Investor Structure, Digital and Social Traction, and Funding History.
- Predictor importance varies depending on the startup's goals and stage, with context influencing the hierarchy of factors.
- Factors predicting near-term funding milestones focus on immediate deal context, while long-term exits prioritize firm and investor characteristics.
- The study suggests a potential "convenience bias" in the literature, where predictor importance may be linked to data accessibility.
<br /><br />Summary: <div>
arXiv:2507.09675v1 Announce Type: new 
Abstract: Background: Predicting startup success with machine learning is a rapidly growing field, yet findings on key predictors are often fragmented and context-specific. This makes it difficult to discern robust patterns and highlights a need for a systematic synthesis of the evidence.
  Methods: This study conducts a quantitative meta-analysis to synthesize the literature on predictor importance in AI-based startup evaluation. We performed a systematic review to identify a final sample of 13 empirical studies that report rankable feature importance. From these papers, we extracted and categorized 58 unique predictors, synthesizing their importance using a Weighted Importance Score (WIS) that balances a feature's average rank with its frequency of appearance. We also conducted a moderator analysis to investigate how predictor importance changes with context (e.g., success definition).
  Results: Our aggregate analysis reveals that the most consistently powerful predictors are a quartet of foundational attributes: Firm Characteristics (e.g., age, location), Investor Structure (e.g., investor quality), Digital and Social Traction (e.g., online momentum), and Funding History. The moderator analysis further reveals that this hierarchy is highly context-dependent. For instance, predicting near-term funding milestones elevates the importance of the deal's immediate context, while predicting long-term exits prioritizes fundamental firm and investor characteristics.
  Conclusion: The factors that best predict startup success are not universal but are contingent on the startup's goals, stage, and the data used for evaluation. Our findings point to a potential "convenience bias" in the literature, where predictor importance may be tied to data accessibility. We conclude by underscoring the need for standardized reporting practices to enable more robust, cumulative knowledge building in the field.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Legendre Polynomials and Their Use for Karhunen-Lo\`eve Expansion</title>
<link>https://arxiv.org/abs/2507.09825</link>
<guid>https://arxiv.org/abs/2507.09825</guid>
<content:encoded><![CDATA[
<div> Legendre polynomials, recurrence relation, Gaussian random fields, Karhunen-Loève expansions, computational framework<br />
<br />
Summary:<br />
This paper presents a pedagogical review of the derivation of the three-term recurrence relation for Legendre polynomials, aimed at undergraduate students. It also introduces a computational framework for Karhunen-Loève expansions of isotropic Gaussian random fields on hyper-rectangular domains. The framework utilizes Legendre polynomials and associated Gaussian quadrature techniques, ensuring efficiency in higher spatial dimensions. The approach approximates a covariance kernel using a non-negative mixture of squared-exponentials and employs a separable kernel for efficient Legendre-Galerkin discretization. Structural properties like even/odd parity structure in submatrices and a Duffy-type transformation for assembly reduce memory usage and arithmetic cost. The paper includes algorithms and numerical experiments in an open-source repository, reproducing all figures and tables provided in the work. <div>
arXiv:2507.09825v1 Announce Type: new 
Abstract: This paper makes two main contributions. First, we present a pedagogical review of the derivation of the three-term recurrence relation for Legendre polynomials, without relying on the classical Legendre differential equation, Rodrigues' formula, or generating functions. This exposition is designed to be accessible to undergraduate students.
  Second, we develop a computational framework for Karhunen-Lo\`eve expansions of isotropic Gaussian random fields on hyper-rectangular domains. The framework leverages Legendre polynomials and their associated Gaussian quadrature, and it remains efficient even in higher spatial dimensions.
  A covariance kernel is first approximated by a non-negative mixture of squared-exponentials, obtained via a Newton-optimized fit with a theoretically informed initialization. The resulting separable kernel enables a Legendre-Galerkin discretization in the form of a Kronecker product over single dimensions, with submatrices that exhibit even/odd parity structure. For assembly, we introduce a Duffy-type transformation followed by quadrature. These structural properties significantly reduce both memory usage and arithmetic cost compared to naive approaches. All algorithms and numerical experiments are provided in an open-source repository that reproduces every figure and table in this work.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-smooth optimization meets automated material model discovery</title>
<link>https://arxiv.org/abs/2507.10196</link>
<guid>https://arxiv.org/abs/2507.10196</guid>
<content:encoded><![CDATA[
<div> Automated material model discovery, Non-smooth L1-norm regularization, Minimization algorithms, Sparse regression problem, Regularization path computation<br />
<br />
Summary: 
This study explores the minimization of functions with non-smooth L1-norm regularization for automated material model discovery. It investigates the minimization of functions involving a metric quantifying the model-data mismatch and a regularization parameter determining solution sparsity. The study covers cases where the metric function is quadratic or non-quadratic, proposing efficient algorithms for solving the minimization problem and computing the entire regularization path. Algorithms discussed include coordinate descent, LARS for determining critical regularization values, proximal gradient method ISTA for non-quadratic scenarios, and a pathwise extension of ISTA. These algorithms are applied to discover hyperelastic material models from tension and shear data, showcasing their effectiveness in automated material model discovery in mechanics.
<br /> <div>
arXiv:2507.10196v1 Announce Type: new 
Abstract: Automated material model discovery disrupts the tedious and time-consuming cycle of iteratively calibrating and modifying manually designed models. Non-smooth L1-norm regularization is the backbone of automated model discovery; however, the current literature on automated material model discovery offers limited insights into the robust and efficient minimization of non-smooth objective functions. In this work, we examine the minimization of functions of the form f(w) + a ||w||_1, where w are the material model parameters, f is a metric that quantifies the mismatch between the material model and the observed data, and a is a regularization parameter that determines the sparsity of the solution. We investigate both the straightforward case where f is quadratic and the more complex scenario where it is non-quadratic or even non-convex. Importantly, we do not only focus on methods that solve the sparse regression problem for a given value of the regularization parameter a, but propose methods to efficiently compute the entire regularization path, facilitating the selection of a suitable a. Specifically, we present four algorithms and discuss their roles for automated material model discovery in mechanics: First, we recapitulate a well-known coordinate descent algorithm that solves the minimization problem assuming that f is quadratic for a given value of a, also known as the LASSO. Second, we discuss the algorithm LARS, which automatically determines the critical values of a, at which material parameters in w are set to zero. Third, we propose to use the proximal gradient method ISTA for automated material model discovery if f is not quadratic, and fourth, we suggest a pathwise extension of ISTA for computing the regularization path. We demonstrate the applicability of all algorithms for the discovery of hyperelastic material models from uniaxial tension and simple shear data.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios</title>
<link>https://arxiv.org/abs/2507.10448</link>
<guid>https://arxiv.org/abs/2507.10448</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial report generation, LLM models, multi-agent collaborative system, real financial scenarios, human evaluation

Summary:
Financial report generation tasks are complex and require extensive data analysis across various areas of finance. Existing Language Model models are limited in their ability to comprehensively analyze real financial scenarios. To address this, the FinTeam system was developed with a collaborative workflow involving four specialized agents: document analyzer, analyst, accountant, and consultant. Trained on specific financial expertise datasets, these agents work together to produce comprehensive financial reports. Evaluation results show that FinTeam outperformed baseline models like GPT-4o and Xuanyuan, achieving a 62.00% acceptance rate. The system also demonstrated an average improvement of 7.43% on FinCUGE and a 2.06% accuracy boost on FinEval. The project code is available on GitHub for further exploration. 

<br /><br />Summary: 
- Financial report generation tasks are complex and require extensive data analysis.
- The FinTeam system utilizes a collaborative workflow with specialized agents trained on specific financial expertise.
- Evaluation results show that FinTeam outperformed baseline models and achieved a high acceptance rate.
- The system demonstrated improvements on key financial evaluation metrics.
- The project code is available on GitHub for further exploration. <div>
arXiv:2507.10448v1 Announce Type: new 
Abstract: Financial report generation tasks range from macro- to micro-economics analysis, also requiring extensive data analysis. Existing LLM models are usually fine-tuned on simple QA tasks and cannot comprehensively analyze real financial scenarios. Given the complexity, financial companies often distribute tasks among departments. Inspired by this, we propose FinTeam, a financial multi-agent collaborative system, with a workflow with four LLM agents: document analyzer, analyst, accountant, and consultant. We train these agents with specific financial expertise using constructed datasets. We evaluate FinTeam on comprehensive financial tasks constructed from real online investment forums, including macroeconomic, industry, and company analysis. The human evaluation shows that by combining agents, the financial reports generate from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43% average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project is available at https://github.com/FudanDISC/DISC-FinLLM/.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Central Bank Digital Currencies: A Survey</title>
<link>https://arxiv.org/abs/2507.08880</link>
<guid>https://arxiv.org/abs/2507.08880</guid>
<content:encoded><![CDATA[
<div> Keywords: Central Bank Digital Currencies, CBDC design taxonomy, ledger technology, consensus mechanisms, CBDC ecosystem

Summary:
Central banks are exploring the implementation of Central Bank Digital Currencies (CBDCs) due to advancements in digital payment technologies. A review of 135 research papers from 2018 to 2025 examines CBDC design taxonomy and ecosystem frameworks. The study refines key architectural elements, investigates ledger technologies, consensus mechanisms, offline payments, and digital wallet integration. A comparative analysis of 26 existing CBDC systems across system architecture, ledger technology, access model, and application domain reveals trends like a two-tier architecture, distributed ledger technology (DLT), and token-based access model. There is a growing focus on using CBDCs for cross-border payments to improve efficiency. Recommendations for future research are provided. 

<br /><br />Summary: <div>
arXiv:2507.08880v1 Announce Type: cross 
Abstract: With the advancement of digital payment technologies, central banks worldwide have increasingly begun to explore the implementation of Central Bank Digital Currencies (CBDCs). This paper presents a comprehensive review of the latest developments in CBDC system design and implementation. By analyzing 135 research papers published between 2018 and 2025, the study provides an in-depth examination of CBDC design taxonomy and ecosystem frameworks. Grounded in the CBDC Design Pyramid, the paper refines and expands key architectural elements by thoroughly investigating innovations in ledger technologies, the selection of consensus mechanisms, and challenges associated with offline payments and digital wallet integration. Furthermore, it conceptualizes a CBDC ecosystem. A detailed comparative analysis of 26 existing CBDC systems is conducted across four dimensions: system architecture, ledger technology, access model, and application domain. The findings reveal that the most common configuration consists of a two-tier architecture, distributed ledger technology (DLT), and a token-based access model. However, no dominant trend has emerged regarding application domains. Notably, recent research shows a growing focus on leveraging CBDCs for cross-border payments to resolve inefficiencies and structural delays in current systems. Finally, the paper offers several forward-looking recommendations for future research.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components</title>
<link>https://arxiv.org/abs/2507.09443</link>
<guid>https://arxiv.org/abs/2507.09443</guid>
<content:encoded><![CDATA[
<div> Predictive Maintenance, Nuclear Power Plants, Convolutional Neural Network, Computational Thermomechanical Model, Fuel Rod<br />
Summary:<br />
Proactive maintenance strategies, like Predictive Maintenance, are crucial for Nuclear Power Plants as they help in reducing downtime caused by unexpected component failures. This study explores the use of a Convolutional Neural Network combined with a computational thermomechanical model to estimate the temperature, stress, and strain of a Pressurized Water Reactor fuel rod during operation using limited temperature measurements. The datasets for training, validation, and testing were generated through simulations involving a nuclear fuel performance code and a Thermal-Hydraulics Module. The CNN was trained for over 1,000 epochs and showed accurate temperature distribution predictions, further used in a thermomechanical model to determine stress and strain distribution within the fuel rod. This methodology has the potential to aid in the development of Predictive Maintenance tools for real-time monitoring of nuclear reactors. <br /> <div>
arXiv:2507.09443v1 Announce Type: cross 
Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play an important role in the operation of Nuclear Power Plants (NPPs), particularly due to their capacity to reduce offline time by preventing unexpected shutdowns caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN) architecture combined with a computational thermomechanical model to calculate the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel rod during operation. This estimation relies on a limited number of temperature measurements from the cladding's outer surface. This methodology can potentially aid in developing PdM tools for nuclear reactors by enabling real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled simulations involving BISON, a finite element-based nuclear fuel performance code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven simulations, varying the peak linear heat generation rates. Of these, eight were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting, achieving highly accurate temperature distribution predictions. These were then used in a thermomechanical model to determine the stress and strain distribution within the fuel rod.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When the Weak Becomes Strong: Effective Observables via Time-Symmetric Quantum Selection</title>
<link>https://arxiv.org/abs/2507.09716</link>
<guid>https://arxiv.org/abs/2507.09716</guid>
<content:encoded><![CDATA[
<div> weak values, time-symmetric quantum mechanics, sequential composition, state-conditioned observable, interference information

Summary: In this study, the authors explore the sequential composition of weak values within time-symmetric quantum mechanics. They analyze the combination of forward and reverse weak measurements, showing that their product corresponds to the normalized expectation value of a state-conditioned observable. This observable encodes interference information, especially when the initial state is a superposition. The concept extends to mixed states by using a density matrix, connecting to generalized quantum measurements. Practical applications in quantum computing include error detection and the inference of weak value phases through strong measurements in the case of pure states. <div>
arXiv:2507.09716v1 Announce Type: cross 
Abstract: We investigate the sequential composition of weak values in the framework of time-symmetric quantum mechanics. Specifically, we consider a forward'' weak measurement from a preselected state $\ket{\psi}$ to a post-selected state $\ket{\phi}$, followed by a reverse'' weak measurement. We show that the product of these two weak values corresponds to the normalized expectation value of a strong, state-conditioned observable $B = A P_\psi A$, where $P_\psi = \ket{\psi}\bra{\psi}$ is the projector onto the preselected state. Analyzing the structure of $B$, we demonstrate how it encodes interference information, particularly when $\ket{\psi}$ is a superposition rather than an eigenstate of $A$. This formulation extends naturally to mixed states by replacing $P_\psi$ with a generic density matrix $\rho$, linking the construction to the formalism of generalized quantum measurements. We illustrate practical applications in quantum information, including state-specific error witnessing in quantum computing, and show how the phase of a weak value can be inferred via strong measurements in the pure-state case.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coincidence of Wants Mechanism for Swap Trade Execution in Decentralized Exchanges</title>
<link>https://arxiv.org/abs/2507.10149</link>
<guid>https://arxiv.org/abs/2507.10149</guid>
<content:encoded><![CDATA[
<div> cycle, Coincidence of Wants, decentralized exchange, asset matrix, liquidity providing

Summary:
The article presents a novel framework for identifying and completing Coincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators. Unlike existing auction based systems like CoWSwap, this approach uses an asset matrix formulation to verify feasibility, utilize oracle prices, and adhere to formal conservation laws. It also introduces bridging orders to execute slippage-free and capital preserving swap orders, offering a delta-neutral strategy for liquidity providing market makers. By leveraging graph traversal and imbalance correction, the algorithm efficiently discovers CoW cycles in real-world Arbitrum swap data and can seamlessly insert synthetic orders for atomic cycle closure. This structured CoW cycle execution demonstrates the potential for enhanced liquidity management and improved order processing in decentralized exchange ecosystems. <div>
arXiv:2507.10149v1 Announce Type: cross 
Abstract: We propose a mathematically rigorous framework for identifying and completing Coincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators. Unlike existing auction based systems such as CoWSwap, our approach introduces an asset matrix formulation that not only verifies feasibility using oracle prices and formal conservation laws but also completes partial CoW cycles of swap orders that are discovered using graph traversal and are settled using imbalance correction. We define bridging orders and show that the resulting execution is slippage free and capital preserving for LPs. Applied to real world Arbitrum swap data, our algorithm demonstrates efficient discovery of CoW cycles and supports the insertion of synthetic orders for atomic cycle closure. This work can be thought of as the detailing of a potential delta-neutral strategy by liquidity providing market makers: a structured CoW cycle execution.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Model for Composite Microstructures: Reconstruction, Stiffness, and Nonlinear Behavior Prediction</title>
<link>https://arxiv.org/abs/2411.06565</link>
<guid>https://arxiv.org/abs/2411.06565</guid>
<content:encoded><![CDATA[
<div> Keywords: Material Masked Autoencoder, self-supervised learning, Vision Transformer, microstructural features, composite images <br />
Summary: 
The Material Masked Autoencoder (MMAE) is introduced as a self-supervised Vision Transformer pre-trained on a large dataset of short-fiber composite images. The MMAE captures essential microstructural features and has broad applicability across tasks. Through fine-tuning on limited data, the MMAE can predict homogenized stiffness components effectively. The MMAE combined with an interaction-based material network (IMN) allows for inferring physically interpretable parameters and enables the extrapolation of nonlinear stress-strain responses. These results demonstrate the potential of microstructure foundation models in dealing with complex systems like 3D composites and experimental datasets. The MMAE represents a promising approach for advancing research in material science and lays the foundation for future extensions to more intricate systems. <br /><br />Summary: <div>
arXiv:2411.06565v4 Announce Type: replace 
Abstract: We present the Material Masked Autoencoder (MMAE), a self-supervised Vision Transformer pretrained on a large corpus of short-fiber composite images via masked image reconstruction. The pretrained MMAE learns latent representations that capture essential microstructural features and are broadly transferable across tasks. We demonstrate two key applications: (i) predicting homogenized stiffness components through fine-tuning on limited data, and (ii) inferring physically interpretable parameters by coupling MMAE with an interaction-based material network (IMN), thereby enabling extrapolation of nonlinear stress-strain responses. These results highlight the promise of microstructure foundation models and lay the groundwork for future extensions to more complex systems, such as 3D composites and experimental datasets.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EP-GAT: Energy-based Parallel Graph Attention Neural Network for Stock Trend Classification</title>
<link>https://arxiv.org/abs/2507.08184</link>
<guid>https://arxiv.org/abs/2507.08184</guid>
<content:encoded><![CDATA[
arXiv:2507.08184v1 Announce Type: new 
Abstract: Graph neural networks have shown remarkable performance in forecasting stock movements, which arises from learning complex inter-dependencies between stocks and intra-dynamics of stocks. Existing approaches based on graph neural networks typically rely on static or manually defined factors to model changing inter-dependencies between stocks. Furthermore, these works often struggle to preserve hierarchical features within stocks. To bridge these gaps, this work presents the Energy-based Parallel Graph Attention Neural Network, a novel approach for predicting future movements for multiple stocks. First, it generates a dynamic stock graph with the energy difference between stocks and Boltzmann distribution, capturing evolving inter-dependencies between stocks. Then, a parallel graph attention mechanism is proposed to preserve the hierarchical intra-stock dynamics. Extensive experiments on five real-world datasets are conducted to validate the proposed approach, spanning from the US stock markets (NASDAQ, NYSE, SP) and UK stock markets (FTSE, LSE). The experimental results demonstrate that EP-GAT consistently outperforms competitive five baselines on test periods across various metrics. The ablation studies and hyperparameter sensitivity analysis further validate the effectiveness of each module in the proposed method.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models</title>
<link>https://arxiv.org/abs/2507.08030</link>
<guid>https://arxiv.org/abs/2507.08030</guid>
<content:encoded><![CDATA[
arXiv:2507.08030v1 Announce Type: cross 
Abstract: Generative AI models, including large language models (LLMs) and vision-language models (VLMs), are increasingly used to interpret medical images and answer clinical questions. Their responses often include inaccuracies; therefore, safety measures like medical disclaimers are critical to remind users that AI outputs are not professionally vetted or a substitute for medical advice. This study evaluated the presence of disclaimers in LLM and VLM outputs across model generations from 2022 to 2025. Using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs were screened for disclaimer phrases. Medical disclaimer presence in LLM and VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023 to 1.05% in 2025, respectively. By 2025, the majority of models displayed no disclaimers. As public models become more capable and authoritative, disclaimers must be implemented as a safeguard adapting to the clinical context of each output.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk Improves Trading Decisions</title>
<link>https://arxiv.org/abs/2507.08584</link>
<guid>https://arxiv.org/abs/2507.08584</guid>
<content:encoded><![CDATA[
arXiv:2507.08584v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in agentic frameworks, in which prompts trigger complex tool-based analysis in pursuit of a goal. While these frameworks have shown promise across multiple domains including in finance, they typically lack a principled model-building step, relying instead on sentiment- or trend-based analysis. We address this gap by developing an agentic system that uses LLMs to iteratively discover stochastic differential equations for financial time series. These models generate risk metrics which inform daily trading decisions. We evaluate our system in both traditional backtests and using a market simulator, which introduces synthetic but causally plausible price paths and news events. We find that model-informed trading strategies outperform standard LLM-based agents, improving Sharpe ratios across multiple equities. Our results show that combining LLMs with agentic model discovery enhances market risk estimation and enables more profitable trading decisions.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating diversion and treatment policies for opioid use disorder</title>
<link>https://arxiv.org/abs/2311.05076</link>
<guid>https://arxiv.org/abs/2311.05076</guid>
<content:encoded><![CDATA[
arXiv:2311.05076v4 Announce Type: replace 
Abstract: The United States (US) opioid crisis contributed to 81,806 fatalities in 2022. It has strained hospitals, treatment facilities, and law enforcement agencies due to the enormous resources and procedures needed to respond to the crisis. As a result, many individuals who use opioids never receive or finish the treatment they need and instead have many interactions with hospitals or the criminal justice system. This paper introduces a discrete event simulation model that evaluates three opioid use disorder treatment policies: arrest diversion, re-entry case management, and overdose diversion. Publicly available data from 2011 to 2019 in Dane County, Wisconsin, was used to forecast opioid-related outcomes through 2032. Through analyzing a variety of policy-mix implementations, the study offers a versatile framework for evaluating policies at various implementation levels. The results demonstrate that treatment policies that create new pathways and programming by utilizing treatment services and successfully divert at least 20% of eligible individuals can lead to more opioid-resilient communities. The benefits increase when more policies are enacted and/or offered to more individuals, with the largest impact from overdose diversion, followed by re-entry case management, and the smallest impact from arrest diversion. The statistically significant 10-year cumulative total reduction in societal costs from 2023 through 2032 ranges from $39 M (USD) to $584 M (USD), excluding implementation costs of policies. To reverse the opioid crisis within a community, treatment policies may need to be combined with other strategies, such as harm reduction, supply reduction, and use prevention.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing wheel loader performance -- an end-to-end approach</title>
<link>https://arxiv.org/abs/2501.06583</link>
<guid>https://arxiv.org/abs/2501.06583</guid>
<content:encoded><![CDATA[
arXiv:2501.06583v3 Announce Type: replace 
Abstract: Wheel loaders in mines and construction sites repeatedly load soil from a pile to load receivers. Automating this task presents a challenging planning problem since each loading's performance depends on the pile state, which depends on previous loadings. We investigate an end-to-end optimization approach considering future loading outcomes and transportation costs between the pile and load receivers. To predict the evolution of the pile state and the loading performance, we use world models that leverage deep neural networks trained on numerous simulated loading cycles. A look-ahead tree search optimizes the sequence of loading actions by evaluating the performance of thousands of action candidates, which expand into subsequent action candidates under the predicted pile states recursively. Test results demonstrate that, over a horizon of 15 sequential loadings, the look-ahead tree search is 6% more efficient than a greedy strategy, which always selects the action that maximizes the current single loading performance, and 14% more efficient than using a fixed loading controller optimized for the nominal case.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pandora's Box Problem with Sequential Inspections</title>
<link>https://arxiv.org/abs/2507.07508</link>
<guid>https://arxiv.org/abs/2507.07508</guid>
<content:encoded><![CDATA[
<div> optimization, Pandora's box, tradeoff, information acquisition, cost efficiency

Summary:
The study explores a generalization of the Pandora's box problem in economic theory, where an agent can choose to fully open boxes at a certain fee or partially open them at a reduced cost. The research establishes a hardness result and utilizes stochastic optimization techniques to analyze the model comprehensively. Structural properties of the optimal policy are identified, providing insights into decision-making. Problem relaxations and near-optimal solutions are derived, and the optimal policy is characterized in special cases. An extensive numerical study comparing various policies reveals that threshold-based policies extending the Pandora's box optimal solution can effectively guide search decisions. The study contributes to understanding the tradeoff between information acquisition and cost efficiency in decision-making scenarios. <br /><br />Summary: <div>
arXiv:2507.07508v1 Announce Type: new 
Abstract: The Pandora's box problem (Weitzman 1979) is a core model in economic theory that captures an agent's (Pandora's) search for the best alternative (box). We study an important generalization of the problem where the agent can either fully open boxes for a certain fee to reveal their exact values or partially open them at a reduced cost. This introduces a new tradeoff between information acquisition and cost efficiency. We establish a hardness result and employ an array of techniques in stochastic optimization to provide a comprehensive analysis of this model. This includes (1) the identification of structural properties of the optimal policy that provide insights about optimal decisions; (2) the derivation of problem relaxations and provably near-optimal solutions; (3) the characterization of the optimal policy in special yet non-trivial cases; and (4) an extensive numerical study that compares the performance of various policies, and which provides additional insights about the optimal policy. Throughout, we show that intuitive threshold-based policies that extend the Pandora's box optimal solution can effectively guide search decisions.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics</title>
<link>https://arxiv.org/abs/2507.07830</link>
<guid>https://arxiv.org/abs/2507.07830</guid>
<content:encoded><![CDATA[
<div> modal reference spaces, model-order reduction framework, meshless SPH method, proper orthogonal decomposition, Galerkin POD, Adjoint Petrov-Galerkin

Summary:
The paper introduces a model-order reduction framework for meshless weakly compressible smoothed particle hydrodynamics (SPH). It proposes modal reference spaces to handle the challenges of discovering low-dimensional subspaces in SPH simulations. By projecting SPH snapshot data onto a reference space, low dimensionality of field quantities can be identified through modal decomposition techniques like proper orthogonal decomposition (POD). These modal quantities are then mapped back to the meshless SPH space during online prediction using scattered data interpolation. The framework is based on the meshless Galerkin POD and Adjoint Petrov-Galerkin projection model-order reduction formulations. Testing on various numerical experiments demonstrates good agreement in reconstructed and predictive velocity fields. However, the pressure field shows sensitivity to projection error due to weakly-compressible assumptions in SPH, which can be mitigated using nonlinear approximations like the APG approach. Overall, the proposed meshless model-order reduction framework shows promise in reducing computational costs for SPH simulations. 

<br /><br />Summary: <div>
arXiv:2507.07830v1 Announce Type: new 
Abstract: This work proposes a model-order reduction framework for the meshless weakly compressible smoothed particle hydrodynamics (SPH) method. The proposed framework introduces the concept of modal reference spaces to overcome the challenges of discovering low-dimensional subspaces from unstructured, dynamic, and mixing numerical topology that is often seen in SPH simulations. The proposed modal reference spaces enable a low-dimensional representation of the SPH field equations while maintaining their inherent meshless qualities. Modal reference spaces are constructed by projecting SPH snapshot data onto a reference space where low-dimensionality of field quantities can be discovered via traditional modal decomposition techniques (e.g., the proper orthogonal decomposition (POD)). Modal quantities are mapped back to the meshless SPH space via scattered data interpolation during the online predictive stage. The proposed model-order reduction framework is cast into the \emph{meshless} Galerkin POD (GPOD) and the Adjoint Petrov--Galerkin (APG) projection model-order reduction (PMOR) formulation. The PMORs are tested on three numerical experiments: 1) the Taylor--Green vortex; 2) lid-driven cavity; and 3) flow past an open cavity. Results show good agreement in reconstructed and predictive velocity fields, which showcase the ability of the proposed framework to evolve the unstructured, dynamic, and mixing SPH field equations in a low-dimensional subspace. Results also show that the pressure field is sensitive to the projection error due to the stiff weakly-compressible assumption made in the current SPH framework, but can be alleviated through nonlinear approximations, such as the APG approach. Ultimately, the presented meshless model-order reduction framework marks a step toward enabling drastic cost savings of SPH simulations.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Enhanced Multi-Factor Quantitative Trading: A Cross-Sectional Portfolio Optimization Approach with Bias Correction</title>
<link>https://arxiv.org/abs/2507.07107</link>
<guid>https://arxiv.org/abs/2507.07107</guid>
<content:encoded><![CDATA[
<div> alpha discovery, bias correction, factor computation, portfolio optimization, machine learning

Summary:
This paper introduces a machine learning framework for quantitative trading that emphasizes factor engineering, real-time computation optimization, and portfolio construction. The framework combines multi-factor alpha discovery with bias correction techniques using PyTorch-accelerated factor computation. It processes 500-1000 factors from open-source alpha101 extensions and market microstructure signals. Key innovations include tensor-based factor computation acceleration, data augmentation using geometric Brownian motion, and cross-sectional neutralization strategies. Empirical validation on Chinese A-share markets from 2010-2024 showcases annualized returns of 20% with Sharpe ratios exceeding 2.0, surpassing traditional approaches. The study underscores the significance of bias correction in factor construction and the considerable impact of cross-sectional portfolio optimization on strategy performance. Experimental implementations and code are accessible on GitHub at: https://github.com/initial-d/ml-quant-trading

<br /><br />Summary: <div>
arXiv:2507.07107v1 Announce Type: cross 
Abstract: This paper presents a comprehensive machine learning framework for quantitative trading that achieves superior risk-adjusted returns through systematic factor engineering, real-time computation optimization, and cross-sectional portfolio construction. Our approach integrates multi-factor alpha discovery with bias correction techniques, leveraging PyTorch-accelerated factor computation and advanced portfolio optimization. The system processes 500-1000 factors derived from open-source alpha101 extensions and proprietary market microstructure signals. Key innovations include tensor-based factor computation acceleration, geometric Brownian motion data augmentation, and cross-sectional neutralization strategies. Empirical validation on Chinese A-share markets (2010-2024) demonstrates annualized returns of $20\%$ with Sharpe ratios exceeding 2.0, significantly outperforming traditional approaches. Our analysis reveals the critical importance of bias correction in factor construction and the substantial impact of cross-sectional portfolio optimization on strategy performance. Code and experimental implementations are available at: https://github.com/initial-d/ml-quant-trading
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable ADER-DG Transport Method with Polynomial Order Independent CFL Limit</title>
<link>https://arxiv.org/abs/2507.07304</link>
<guid>https://arxiv.org/abs/2507.07304</guid>
<content:encoded><![CDATA[
<div> Discontinuous Galerkin methods, Locally Implicit, Globally Explicit, ADER-DG scheme, Element-width based CFL condition, Transport-dominated problems <br />
Summary:<br />
This paper introduces a novel locally implicit, globally explicit ADER-DG scheme for transport-dominated problems. The method overcomes the restrictive time step constraints seen in high-order DG methods by using an element-width based CFL condition, allowing for a maximum stable time step independent of polynomial degree. By solving element-local implicit problems at each time step, the method effectively captures the domain of dependence and remains stable for CFL numbers up to $1/\sqrt{d}$ in $d$ spatial dimensions. Rigorous stability proofs in one dimension and von Neumann stability analysis in higher dimensions validate the method's accuracy and convergence. Numerical experiments on linear and nonlinear test cases further demonstrate the effectiveness of the proposed approach. <br /> <div>
arXiv:2507.07304v1 Announce Type: cross 
Abstract: Discontinuous Galerkin (DG) methods are known to suffer from increasingly restrictive time step constraints as the polynomial order increases, limiting their efficiency at high orders. In this paper, we introduce a novel locally implicit, but globally explicit ADER-DG scheme designed for transport-dominated problems. The method achieves a maximum stable time step governed by an element-width based CFL condition that is independent of the polynomial degree. By solving a set of element-local implicit problems at each time step, our approach more effectively captures the domain of dependence. As a result, our method remains stable for CFL numbers up to $1/\sqrt{d}$ in $d$ spatial dimensions. We provide a rigorous stability proof in one dimension, and extend the analysis to two and three dimensions using a semi-analytical von Neumann stability analysis. The accuracy and convergence of the method are demonstrated through numerical experiments on both linear and nonlinear test cases.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Plausibility-Validity Gap by Fine-Tuning a Reasoning-Enhanced LLM for Chemical Synthesis and Discovery</title>
<link>https://arxiv.org/abs/2507.07328</link>
<guid>https://arxiv.org/abs/2507.07328</guid>
<content:encoded><![CDATA[
<div> chemistry, language models, plausibility-validity gap, Low-Rank Adaptation, dual-domain dataset

Summary:
The paper addresses the challenge of factually invalid information generated by Large Language Models (LLMs) in specialized domains like chemistry, known as the "plausibility-validity gap." A specialized scientific assistant was developed by fine-tuning the Magistral Small model using Low-Rank Adaptation (LoRA) and a dual-domain dataset curated from various sources. The evaluation showed improved format adherence, chemical validity of generated molecules, and feasibility of proposed synthesis routes. The model exhibited a hierarchical learning pattern, with syntactic correctness learned more easily than chemical possibility and synthesis feasibility. While competitive with human experts in areas like chemical creativity and reasoning, limitations such as errors in stereochemistry, static knowledge cutoff, and occasional reference hallucination were identified. This work establishes a framework for transforming generalist LLMs into specialized tools for chemical research, while also highlighting areas for future enhancement. 

<br /><br />Summary: <div>
arXiv:2507.07328v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often generate scientifically plausible but factually invalid information, a challenge we term the "plausibility-validity gap," particularly in specialized domains like chemistry. This paper presents a systematic methodology to bridge this gap by developing a specialized scientific assistant. We utilized the Magistral Small model, noted for its integrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation (LoRA). A key component of our approach was the creation of a "dual-domain dataset," a comprehensive corpus curated from various sources encompassing both molecular properties and chemical reactions, which was standardized to ensure quality. Our evaluation demonstrates that the fine-tuned model achieves significant improvements over the baseline model in format adherence, chemical validity of generated molecules, and the feasibility of proposed synthesis routes. The results indicate a hierarchical learning pattern, where syntactic correctness is learned more readily than chemical possibility and synthesis feasibility. While a comparative analysis with human experts revealed competitive performance in areas like chemical creativity and reasoning, it also highlighted key limitations, including persistent errors in stereochemistry, a static knowledge cutoff, and occasional reference hallucination. This work establishes a viable framework for adapting generalist LLMs into reliable, specialized tools for chemical research, while also delineating critical areas for future improvement.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computationally Efficient Information-Driven Optical Design with Interchanging Optimization</title>
<link>https://arxiv.org/abs/2507.07789</link>
<guid>https://arxiv.org/abs/2507.07789</guid>
<content:encoded><![CDATA[
<div> IDEAL-IO, imaging systems, information content, optical design, computational decoding<br />
<br />
Summary:<br />
Recent work has shown that evaluating imaging systems based on the information content of their measurements alone can simplify optical design. However, the IDEAL method for automating this process faces challenges such as high memory usage, long runtimes, and a potentially mismatched objective function. To address these issues, IDEAL-IO was introduced, which separates density estimation from optical parameter optimization. By alternating between fitting models to measurements and updating optical parameters using fixed models, IDEAL-IO reduces runtime and memory usage while allowing for more expressive density models. This approach was successfully validated on various imaging applications, demonstrating the practicality and scalability of information-driven optimization for real-world imaging system design. <br /> <div>
arXiv:2507.07789v1 Announce Type: cross 
Abstract: Recent work has demonstrated that imaging systems can be evaluated through the information content of their measurements alone, enabling application-agnostic optical design that avoids computational decoding challenges. Information-Driven Encoder Analysis Learning (IDEAL) was proposed to automate this process through gradient-based. In this work, we study IDEAL across diverse imaging systems and find that it suffers from high memory usage, long runtimes, and a potentially mismatched objective function due to end-to-end differentiability requirements. We introduce IDEAL with Interchanging Optimization (IDEAL-IO), a method that decouples density estimation from optical parameter optimization by alternating between fitting models to current measurements and updating optical parameters using fixed models for information estimation. This approach reduces runtime and memory usage by up to 6x while enabling more expressive density models that guide optimization toward superior designs. We validate our method on diffractive optics, lensless imaging, and snapshot 3D microscopy applications, establishing information-theoretic optimization as a practical, scalable strategy for real-world imaging system design.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development and Real-World Application of Commercial Motor Vehicle Safety Enforcement Dashboards</title>
<link>https://arxiv.org/abs/2507.06351</link>
<guid>https://arxiv.org/abs/2507.06351</guid>
<content:encoded><![CDATA[
<div> Keywords: Commercial Motor Vehicle Safety, Dashboards, Performance Measures, Enforcement Initiatives, Maryland State Police

Summary: 
This study introduces CMV safety dashboards developed with input from CMV enforcement professionals. The dashboards enhance analysis of CMV safety performance measures based on probe vehicle speeds, inspection/citation data, and enforcement activities. Collaboration with Maryland State Police identified a section of I-81 for targeted CMV enforcement, with a post-enforcement evaluation revealing mixed results. The dashboards aim to facilitate efficient monitoring of CMV safety and enforcement initiatives, with a focus on improving highway safety. Further refinement of the dashboards and citation data is needed to enhance the effectiveness of targeted enforcement efforts.<br /><br /> <div>
arXiv:2507.06351v1 Announce Type: new 
Abstract: Commercial Motor Vehicle (CMV) safety is crucial in traffic management and public safety. CMVs account for numerous traffic incidents, so monitoring CMV safety and safety inspections is essential for ensuring safe and efficient highway movement. This paper presents the development and real-world application of CMV dashboards designed under the guidance of CMV safety enforcement professionals from the Maryland State Police (MSP), the Maryland Department of Transportation - State Highway Administration (MDOT - SHA), and the Federal Motor Carrier Safety Administration (FMCSA) to enable intuitive and efficient analysis of CMV safety performance measures. First, three CMV safety dashboards enable CMV safety professionals to identify sites with a history of safety performance issues. A supplemental dashboard automates the analysis of CMV enforcement initiatives using the same performance measures. These performance measures are based on CMV probe vehicle speeds, inspection/citation data from Truck Weigh and Inspection Stations (TWIS), patrolling enforcement, and Virtual Weigh Stations (VWS). The authors collaborated with MSP to identify a portion of I-81 in Maryland, susceptible to improvement from targeted CMV enforcement. The supplemental enforcement assessment dashboard was employed to evaluate the impact of enforcement, including the post-enforcement halo effect. The results of the post-enforcement evaluation were mixed, indicating a need for more fine-grained citation data.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eyes on the Road, Mind Beyond Vision: Context-Aware Multi-modal Enhanced Risk Anticipation</title>
<link>https://arxiv.org/abs/2507.06444</link>
<guid>https://arxiv.org/abs/2507.06444</guid>
<content:encoded><![CDATA[
<div> Keywords: accident anticipation, multi-modal framework, driver attention maps, adaptive mechanism, spatio-temporal dependencies <br />
Summary:<br />
The paper introduces CAMERA, a multi-modal framework for accident anticipation that integrates dashcam video, textual annotations, and driver attention maps. This model utilizes an adaptive mechanism based on scene complexity and gaze entropy to reduce false alarms while maintaining high recall in dynamic traffic scenarios. By employing a hierarchical fusion pipeline with Bi-GRU and a Geo-Context Vision-Language module, CAMERA captures spatio-temporal dependencies and translates spatial relationships into human-centric alerts. Evaluations on the DADA-2000 dataset and benchmarks show that CAMERA outperforms existing methods in accuracy and lead time, demonstrating the effectiveness of incorporating driver cognition and contextual information in accident anticipation models. <div>
arXiv:2507.06444v1 Announce Type: new 
Abstract: Accurate accident anticipation remains challenging when driver cognition and dynamic road conditions are underrepresented in predictive models. In this paper, we propose CAMERA (Context-Aware Multi-modal Enhanced Risk Anticipation), a multi-modal framework integrating dashcam video, textual annotations, and driver attention maps for robust accident anticipation. Unlike existing methods that rely on static or environment-centric thresholds, CAMERA employs an adaptive mechanism guided by scene complexity and gaze entropy, reducing false alarms while maintaining high recall in dynamic, multi-agent traffic scenarios. A hierarchical fusion pipeline with Bi-GRU (Bidirectional GRU) captures spatio-temporal dependencies, while a Geo-Context Vision-Language module translates 3D spatial relationships into interpretable, human-centric alerts. Evaluations on the DADA-2000 and benchmarks show that CAMERA achieves state-of-the-art performance, improving accuracy and lead time. These results demonstrate the effectiveness of modeling driver attention, contextual description, and adaptive risk thresholds to enable more reliable accident anticipation.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forex Trading Robot Using Fuzzy Logic</title>
<link>https://arxiv.org/abs/2507.06383</link>
<guid>https://arxiv.org/abs/2507.06383</guid>
<content:encoded><![CDATA[
<div> Proposed fuzzy system, short-term transactions, forex market, fuzzy logic, improved accuracy<br />Summary:<br /> This study introduces a fuzzy system for short-term forex transactions, enhancing traditional strategies by utilizing fuzzy logic. Unlike conventional approaches using predefined ranges for technical indicators like RSI and CCI, this system employs fuzzy Mamdani systems for each indicator, with results combined via voting to create a trading robot. Compared to other methods, the proposed approach shows a significant increase in profitability factor, as demonstrated by calculations of net profit, gross profit, and maximum capital reduction. <div>
arXiv:2507.06383v1 Announce Type: cross 
Abstract: In this study, we propose a fuzzy system for conducting short-term transactions in the forex market. The system is designed to enhance common strategies in the forex market using fuzzy logic, thereby improving the accuracy of transactions. Traditionally, technical strategies based on oscillator indicators have relied on predefined ranges for indicators such as Relative Strength Index (RSI), Commodity Channel Indicator (CCI), and Stochastic to determine entry points for trades. However, the use of these classic indicators has yielded suboptimal results due to the changing nature of the market over time. In our proposed approach, instead of employing classical indicators, we introduce a fuzzy Mamdani system for each indicator. The results obtained from these systems are then combined through voting to design a trading robot. Our findings demonstrate a considerable increase in the profitability factor compared to three other methods. Additionally, net profit, gross profit, and maximum capital reduction are calculated and compared across all approaches.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rugsafe: A multichain protocol for recovering from and defending against Rug Pulls</title>
<link>https://arxiv.org/abs/2507.06423</link>
<guid>https://arxiv.org/abs/2507.06423</guid>
<content:encoded><![CDATA[
<div> Keywords: Rugsafe, cryptocurrency, protocol, vaults, anticoins

Summary: 
Rugsafe introduces a protocol to address rug pulls in the cryptocurrency ecosystem by using cryptographic security and economic incentives. The protocol includes specialized vaults where rugged tokens can be securely deposited, and anticoins are issued as receipts. These anticoins are pegged to the price of rugged tokens and can be used within the ecosystem or burned for additional rewards. The supply of Rugsafe tokens is adjusted based on activity, ensuring stability. Users deposit rugged tokens into vaults on multiple chains and burn anticoins to receive incentives on the RugSafe chain. The protocol's vaults work across different blockchains, offering a practical solution to cryptocurrency market challenges.<br /><br />Summary: <div>
arXiv:2507.06423v1 Announce Type: cross 
Abstract: Rugsafe introduces a comprehensive protocol aimed at mitigating the risks of rug pulls in the cryptocurrency ecosystem. By utilizing cryptographic security measures and economic incentives, the protocol provides a secure multichain system for recovering assets and transforming rugged tokens into opportunities and rewards. Foundational to Rugsafe are specialized vaults where rugged tokens can be securely deposited, and anticoin tokens are issued as receipts. These anticoins are designed to be inversely pegged to the price movement of the underlying rugged token. Users can utilize these anticoins within the ecosystem or choose to burn them, further securing the protocol and earning additional rewards. The supply of the native Rugsafe token is dynamically adjusted based on the volume, value, and activity of rugged tokens, ensuring stability and resilience. By depositing rugged tokens into a vault on several chains, and by burning anticoins, users receive incentives on the RugSafe chain. This protocol's vaults are designed to work in heterogenous blockchain ecosystems, offering a practical and effective solution to one of the most significant challenges in the cryptocurrency market.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams</title>
<link>https://arxiv.org/abs/2507.06803</link>
<guid>https://arxiv.org/abs/2507.06803</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamical systems, domain knowledge, expert knowledge, SysML diagrams, natural language processing

Summary:
This paper presents a strategy for automating the generation of computational models for engineering dynamical systems. The approach leverages domain and expert knowledge to extract information from relevant documents using System Modeling Language (SysML) diagrams. Natural Language Processing (NLP) techniques and Large Language Models (LLMs) are used to enhance the accuracy of the generated diagrams. The process involves several steps, including extracting key nouns and relationships, generating block attributes and relationships, and creating Block Definition Diagrams (BDDs). Case studies demonstrate the application of automated SysML diagram generation. The computational models are then derived from the SysML diagrams via code generation and computational model generation steps. NLP aids in summarization during code generation, while LLMs are used for validation. The proposed approach is flexible across systems, domains, and software, as shown through an example with a simple pendulum model. Improved performance is achieved compared to using LLMs alone.<br /><br />Summary: <div>
arXiv:2507.06803v1 Announce Type: cross 
Abstract: This paper contributes to speeding up the design and deployment of engineering dynamical systems by proposing a strategy for exploiting domain and expert knowledge for the automated generation of dynamical system computational model starting from a corpus of document relevant to the dynamical system of interest and an input document describing the specific system. This strategy is implemented in five steps and, crucially, it uses system modeling language diagrams (SysML) to extract accurate information about the dependencies, attributes, and operations of components. Natural Language Processing (NLP) strategies and Large Language Models (LLMs) are employed in specific tasks to improve intermediate outputs of the SySML diagrams automated generation, such as: list of key nouns; list of extracted relationships; list of key phrases and key relationships; block attribute values; block relationships; and BDD diagram generation. The applicability of automated SysML diagram generation is illustrated with different case studies. The computational models of complex dynamical systems from SysML diagrams are then obtained via code generation and computational model generation steps. In the code generation step, NLP strategies are used for summarization, while LLMs are used for validation only. The proposed approach is not limited to a specific system, domain, or computational software. The applicability of the proposed approach is shown via an end-to-end example from text to model of a simple pendulum, showing improved performance compared to results yielded by LLMs only.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models</title>
<link>https://arxiv.org/abs/2507.06853</link>
<guid>https://arxiv.org/abs/2507.06853</guid>
<content:encoded><![CDATA[
<div> diffusion models, structure elucidation, generative modeling, molecular spectra, machine learning

Summary: 
DiffSpectra is a novel generative framework for molecular structure elucidation from spectral data. It combines diffusion models with SE(3)-equivariant architectures to infer both 2D and 3D molecular structures. The model integrates topological and geometric information to accurately predict molecular structures. SpecFormer, a transformer-based spectral encoder, captures spectral dependencies for conditioning the generation process. DiffSpectra achieves high accuracy in structure elucidation, with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model's effectiveness is attributed to its 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. This work advances the field by unifying multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation. <div>
arXiv:2507.06853v1 Announce Type: cross 
Abstract: Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Load and Information Processing in Financial Markets: Theory and Evidence from Disclosure Complexity</title>
<link>https://arxiv.org/abs/2507.07037</link>
<guid>https://arxiv.org/abs/2507.07037</guid>
<content:encoded><![CDATA[
<div> complexity, cognitive load, financial markets, price discovery, information processing 

Summary: 
The article presents a theoretical framework for understanding how cognitive load affects information processing in financial markets. It distinguishes between attention allocation and cognitive processing capacity, demonstrating that complex information has varying effects on different types of investors. Using a dataset of corporate disclosures and regulatory changes, the study finds that cognitive load significantly impairs price discovery, particularly among less sophisticated investors. A one-standard-deviation increase in cognitive complexity leads to an 18% reduction in information incorporation speed and a 23% increase in mispricing duration. The research supports three theoretical mechanisms: selective attention, processing errors, and strategic complexity, indicating that cognitive constraints create inefficiencies in financial markets. These findings have implications for disclosure regulation and market design. 

Summary: <div>
arXiv:2507.07037v1 Announce Type: cross 
Abstract: We develop a theoretical framework for understanding how cognitive load affects information processing in financial markets and test it using exogenous variation in disclosure complexity. Our model distinguishes between attention allocation and cognitive processing capacity, showing that complex information creates differential effects across investor types. Using a comprehensive dataset of corporate disclosures and a novel identification strategy based on regulatory changes, we find that cognitive load significantly impairs price discovery, with effects concentrated among less sophisticated investors. A one-standard-deviation increase in cognitive complexity reduces information incorporation speed by 18\% and increases mispricing duration by 23\%. We provide evidence for three theoretical mechanisms: selective attention, processing errors, and strategic complexity. Our findings suggest that cognitive constraints create systematic inefficiencies in financial markets, with important implications for disclosure regulation and market design.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Bounded Rationality: Formal Verification of Simon's Satisficing Through Flexible Stochastic Dominance</title>
<link>https://arxiv.org/abs/2507.07052</link>
<guid>https://arxiv.org/abs/2507.07052</guid>
<content:encoded><![CDATA[
<div> formalization, bounded rationality, Lean 4, machine-verified proofs, decision-making

Summary: 
The paper introduces Flexible First-Order Stochastic Dominance (FFSD), a formal framework that bridges classical expected utility theory with Herbert Simon's theory of bounded rationality. Machine-verified proofs in Lean 4 demonstrate how FFSD incorporates parameterized tolerance thresholds to capture satisficing behavior. The paper identifies a critical threshold for unique reference points, establishes an equivalence theorem between FFSD and expected utility maximization for approximate indicator functions, and extends the framework to multi-dimensional decision settings. By encoding these concepts in Lean 4's dependent type theory, the paper presents the first machine-checked formalization of bounded rationality, enabling mechanized reasoning about economic decision-making under uncertainty and cognitive limitations. This work showcases the synergy between formal mathematics and economic theory, illustrating how interactive theorem proving can enhance the understanding of behavioral economics concepts traditionally expressed qualitatively.<br /><br />Summary: <div>
arXiv:2507.07052v1 Announce Type: cross 
Abstract: This paper introduces Flexible First-Order Stochastic Dominance (FFSD), a mathematically rigorous framework that formalizes Herbert Simon's concept of bounded rationality using the Lean 4 theorem prover. We develop machine-verified proofs demonstrating that FFSD bridges classical expected utility theory with Simon's satisficing behavior through parameterized tolerance thresholds. Our approach yields several key results: (1) a critical threshold $\varepsilon < 1/2$ that guarantees uniqueness of reference points, (2) an equivalence theorem linking FFSD to expected utility maximization for approximate indicator functions, and (3) extensions to multi-dimensional decision settings. By encoding these concepts in Lean 4's dependent type theory, we provide the first machine-checked formalization of Simon's bounded rationality, creating a foundation for mechanized reasoning about economic decision-making under uncertainty with cognitive limitations. This work contributes to the growing intersection between formal mathematics and economic theory, demonstrating how interactive theorem proving can advance our understanding of behavioral economics concepts that have traditionally been expressed only qualitatively.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolFORM: Multi-modal Flow Matching for Structure-Based Drug Design</title>
<link>https://arxiv.org/abs/2507.05503</link>
<guid>https://arxiv.org/abs/2507.05503</guid>
<content:encoded><![CDATA[
<div> drug design, structure-based, generative models, molecular modalities, multi-modal flow

Summary:
This article introduces MolFORM, a novel generative framework for structure-based drug design that incorporates both discrete (atom types) and continuous (3D coordinates) molecular modalities using multi-flow matching. The framework also includes a preference-guided fine-tuning stage based on Direct Preference Optimization (DPO) using the Vina score as a reward signal. The proposed multi-modal flow DPO co-modeling strategy aligns discrete and continuous modalities, leading to consistent improvements in generation quality as measured by various evaluation metrics. This approach provides a promising alternative to diffusion-based generative models in structure-based drug design, offering a new direction for improving the effectiveness of drug discovery efforts. <div>
arXiv:2507.05503v1 Announce Type: new 
Abstract: Structure-based drug design (SBDD) seeks to generate molecules that bind effectively to protein targets by leveraging their 3D structural information. While diffusion-based generative models have become the predominant approach for SBDD, alternative non-autoregressive frameworks remain relatively underexplored. In this work, we introduce MolFORM, a novel generative framework that jointly models discrete (atom types) and continuous (3D coordinates) molecular modalities using multi-flow matching. To further enhance generation quality, we incorporate a preference-guided fine-tuning stage based on \textit{Direct Preference Optimization} (DPO), using Vina score as a reward signal. We propose a multi-modal flow DPO co-modeling strategy that simultaneously aligns discrete and continuous modalities, leading to consistent improvements across multiple evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCNP-GO: A python package for assembling MCNP input files with a systems engineering approach</title>
<link>https://arxiv.org/abs/2507.05659</link>
<guid>https://arxiv.org/abs/2507.05659</guid>
<content:encoded><![CDATA[
<div> MCNP-GO, Python package, MCNP input files, assembly, precise modeling<br />
<br />
Summary: 
MCNP-GO is a Python package designed to manipulate and assemble MCNP input files, making it easier for users to assemble multiple independent objects into a cohesive file. It addresses challenges in managing large databases of input files by providing various functionalities such as renumbering, extracting subsets, transforming, and assembling files while handling collisions and materials. The tool ensures reliability and traceability through configuration management systems, keeping track of operations performed on files for easy modification. It is especially useful for applications where precise modeling and positioning of equipment are crucial. The package is user-friendly and efficient, showcased through a practical example of assembling an MCNP input file for a tomographic experiment. MCNP-GO is suitable for users with minimal Python knowledge. <br /><br /> <div>
arXiv:2507.05659v1 Announce Type: new 
Abstract: This article introduces MCNP-GO (https://github.com/afriou/mcnpgo), a Python package designed to manipulate and assemble MCNP input files, allowing users to assemble a set of independent objects, each described by a valid MCNP file, into a single cohesive file. This tool is particularly useful for applications where precise modeling and positioning of equipment are crucial. The package addresses the challenges of managing large databases of MCNP input files, ensuring reliability and traceability through configuration management systems. MCNP-GO provides functionalities such as renumbering, extracting subsets of files, transforming files, and assembling files while managing collisions and materials. It also keeps track of the operations performed on files, enhancing traceability and ease of modification. The article demonstrates the package's capabilities through a practical example of assembling an MCNP input file for a tomographic experiment, highlighting its efficiency and user-friendliness. MCNP-GO is designed for users with minimal Python knowledge.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions</title>
<link>https://arxiv.org/abs/2507.06133</link>
<guid>https://arxiv.org/abs/2507.06133</guid>
<content:encoded><![CDATA[
<div> video diffusion, physics surrogate, partial differential equations, Sequential Deep Operator Network, conditional video diffusion

Summary:
- The article introduces a physics surrogate using conditional video diffusion models.
- A two-stage surrogate is proposed, consisting of an S-DeepONet for producing a physics-consistent prior and a conditional video diffusion model for learning the residual.
- By focusing on the residual space, the model can sharpen high-frequency structures while maintaining global coherence.
- The hybrid surrogate outperforms single-stage counterparts in vortex-dominated flow and plastic deformation benchmarks.
- The approach allows for faithful reconstruction of localized features, accelerates convergence, and transfers seamlessly between different types of nonlinear, time-dependent continua. 

<br /><br />Summary: <div>
arXiv:2507.06133v1 Announce Type: new 
Abstract: Video-diffusion models have recently set the standard in video generation, inpainting, and domain translation thanks to their training stability and high perceptual fidelity. Building on these strengths, we repurpose conditional video diffusion as a physics surrogate for spatio-temporal fields governed by partial differential equations (PDEs). Our two-stage surrogate first applies a Sequential Deep Operator Network (S-DeepONet) to produce a coarse, physics-consistent prior from the prescribed boundary or loading conditions. The prior is then passed to a conditional video diffusion model that learns only the residual: the point-wise difference between the ground truth and the S-DeepONet prediction. By shifting the learning burden from the full solution to its much smaller residual space, diffusion can focus on sharpening high-frequency structures without sacrificing global coherence. The framework is assessed on two disparate benchmarks: (i) vortex-dominated lid-driven cavity flow and (ii) tensile plastic deformation of dogbone specimens. Across these data sets the hybrid surrogate consistently outperforms its single-stage counterpart, cutting the mean relative L2 error from 4.57% to 0.83% for the flow problem and from 4.42% to 2.94% for plasticity, a relative improvements of 81.8% and 33.5% respectively. The hybrid approach not only lowers quantitative errors but also improves visual quality, visibly recovering fine spatial details. These results show that (i) conditioning diffusion on a physics-aware prior enables faithful reconstruction of localized features, (ii) residual learning reduces the problem, accelerating convergence and enhancing accuracy, and (iii) the same architecture transfers seamlessly from incompressible flow to nonlinear elasto-plasticity without problem-specific architectural modifications, highlighting its broad applicability to nonlinear, time-dependent continua.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new engineering theory describing oblique free surface impact by flexible plates</title>
<link>https://arxiv.org/abs/2103.08012</link>
<guid>https://arxiv.org/abs/2103.08012</guid>
<content:encoded><![CDATA[
<div> fluid-structural interaction, slamming loads, water entry, flexible plate, impact force <br />
Summary: 
This paper addresses the challenge of incorporating slamming loads into the structural design of planning hulls by proposing a new engineering theory using a specialized fluid-structural interaction simulation approach. The researchers validated their simulation approach through water entry experiments with flexible plates. They then conducted numerical analyses to understand the impact force and plate deformations based on different parameters. Using their simulation as a "microscope," the study observed the evolution of fluid flows and plate deformations during slamming events. From these observations, a new engineering theory was proposed for flexible plates obliquely impacting the water surface, such as high-speed water craft reentry characterized by porpoising. The research contributes to advancing the understanding of slamming phenomena and offers insight into designing structures to withstand slamming loads.<br /><br />Summary: <div>
arXiv:2103.08012v3 Announce Type: cross 
Abstract: Consideration of slamming loads within the structural design of planning hulls is of critical importance in ensuring adequate structural performance in order to avoid potential catastrophic consequences. However, because of the intricacy in the interplay between complex fluid flows and nonlinear structural deformations that accompany the phenomenology of slamming, a general engineering theory in slamming has yet to be uncovered, and so design relies on specialized theories. In this paper, we propose one such theory for a design case that has, until now, eluded a proper description. In pursuit of this theory, we employ a specialized implicit, partitioned fluid-structural interaction (FSI) simulation approach, in order to study the underlying physical mechanisms accompanying the oblique impact of a flexible plate during water entry. In the present work, we first present validation results from flexible plate water entry experiments, to confirm the veracity of the developed FSI solver. Subsequent to validation, we carry out a series of numerical analyses, in an effort to characterize the regimes in impact force and plate out-of-plane deformations, as a function of impact velocities and plate flexural rigidity. Finally, we use our FSI solver, as a kind of "microscope", to study the mechanistic evolution of fluid flows and elastic plate deformations that occur during slamming. Based on these observations, we propose a novel, but simple engineering theory for flexible plates obliquely impacting the water free surface (e.g. high speed porpoising water craft reentry).
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying heterogeneous micromechanical properties of biological tissues via physics-informed neural networks</title>
<link>https://arxiv.org/abs/2402.10741</link>
<guid>https://arxiv.org/abs/2402.10741</guid>
<content:encoded><![CDATA[
<div> machine learning, elastic properties, soft materials, hyperelastic materials, physics-informed

Summary:
- The study focuses on identifying heterogeneous elastic properties in soft materials using a physics-informed machine learning approach.
- Traditional methods struggle with estimating local stress fields making it challenging to determine full-field mechanical responses.
- The proposed method utilizes physics-informed neural networks (PINNs) to infer elasticity maps in large deformation hyperelastic materials.
- The accuracy and computational efficiency of the approach were evaluated across various materials with structural complexity resembling real tissue patterns.
- The network architecture consistently produced highly accurate estimations of heterogeneous elasticity maps, even in the presence of up to 10% noise in the training data. <div>
arXiv:2402.10741v3 Announce Type: cross 
Abstract: The heterogeneous micromechanical properties of biological tissues have profound implications across diverse medical and engineering domains. However, identifying full-field heterogeneous elastic properties of soft materials using traditional engineering approaches is fundamentally challenging due to difficulties in estimating local stress fields. Recently, there has been a growing interest in using data-driven models to learn full-field mechanical responses such as displacement and strain from experimental or synthetic data. However, research studies on inferring full-field elastic properties of materials, a more challenging problem, are scarce, particularly for large deformation, hyperelastic materials. Here, we propose a physics-informed machine learning approach to identify the elasticity map in nonlinear, large deformation hyperelastic materials. We evaluate the prediction accuracies and computational efficiency of physics-informed neural networks (PINNs) by inferring the heterogeneous elasticity maps across three materials with structural complexity that closely resemble real tissue patterns, such as brain tissue and tricuspid valve tissue. We further applied our improved architecture to three additional examples of breast cancer tissue and extended our analysis to three hyperelastic constitutive models: Neo-Hookean, Mooney Rivlin, and Gent. Our selected network architecture consistently produced highly accurate estimations of heterogeneous elasticity maps, even when there was up to 10% noise present in the training data.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADEPT: A Noninvasive Method for Determining Elastic Parameters of Valve Tissue</title>
<link>https://arxiv.org/abs/2409.19081</link>
<guid>https://arxiv.org/abs/2409.19081</guid>
<content:encoded><![CDATA[
<div> Keywords: valve repair, computer simulation, noninvasive method, mechanical parameters, tricuspid valve

Summary: 
The study introduces a novel noninvasive method, ADEPT, for determining elastic parameters of valve tissue, focusing on the tricuspid valve in a child. By tracking valve displacements in 3D echocardiogram sequences and employing physics-informed neural networks, patient-specific mechanical properties were estimated and applied to a simulated model. The method significantly improved accuracy compared to generic parameters from literature, with the simulated model closely aligning with reference image segmentation. This approach enhances the feasibility of computer simulations for predicting optimal valve repair outcomes before intervention, addressing the current limitation of insufficient noninvasive methods to assess in vivo mechanical parameters of valves. The study demonstrates the potential of ADEPT in personalized medicine for valve interventions, paving the way for more precise and tailored treatment strategies in clinical practice.<br /><br />Summary: <div>
arXiv:2409.19081v2 Announce Type: cross 
Abstract: Computer simulation of "virtual interventions" may inform optimal valve repair for a given patient prior to intervention. However, the paucity of noninvasive methods to determine in vivo mechanical parameters of valves limits the accuracy of computer prediction and their clinical application. To address this, we propose ADEPT: A noninvasive method for Determining Elastic Parameters of valve Tissue. In this work, we demonstrated its application to the tricuspid valve of a child. We first tracked valve displacements from open to closed frames within a 3D echocardiogram time sequence using image registration. Physics-informed neural networks were subsequently applied to estimate the nonlinear mechanical properties from first principles and reference displacements. The simulated model using these patient-specific parameters closely aligned with the reference image segmentation, achieving a mean symmetric distance of less than 1 mm. Our approach doubled the accuracy of the simulated model compared to the generic parameters reported in the literature.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity</title>
<link>https://arxiv.org/abs/2507.05816</link>
<guid>https://arxiv.org/abs/2507.05816</guid>
<content:encoded><![CDATA[
<div> Keywords: ROP risk prediction, large language models, affective biases, Chinese benchmark dataset, Affective-ROPTester <br />
<br />
Summary: 
The study introduces a new Chinese benchmark dataset, CROP, for predicting retinopathy of prematurity (ROP) risk using large language models (LLMs). The Affective-ROPTester framework is proposed to evaluate LLMs' predictive capabilities and affective biases in ROP risk stratification. Results show that LLMs perform better with structured external inputs than with intrinsic knowledge alone in predicting ROP risk. Affective biases are observed in model outputs, with a tendency to overestimate medium- and high-risk cases. Positive emotional framing helps mitigate predictive bias compared to negative emotions. The study underscores the importance of affect-sensitive prompt engineering in enhancing diagnostic reliability and proposes Affective-ROPTester as a tool for evaluating and mitigating affective bias in clinical language modeling systems. <div>
arXiv:2507.05816v1 Announce Type: cross 
Abstract: Despite the remarkable progress of large language models (LLMs) across various domains, their capacity to predict retinopathy of prematurity (ROP) risk remains largely unexplored. To address this gap, we introduce a novel Chinese benchmark dataset, termed CROP, comprising 993 admission records annotated with low, medium, and high-risk labels. To systematically examine the predictive capabilities and affective biases of LLMs in ROP risk stratification, we propose Affective-ROPTester, an automated evaluation framework incorporating three prompting strategies: Instruction-based, Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and ICL schemes leverage external medical knowledge to enhance predictive accuracy. Crucially, we integrate emotional elements at the prompt level to investigate how different affective framings influence the model's ability to predict ROP and its bias patterns. Empirical results derived from the CROP dataset yield two principal observations. First, LLMs demonstrate limited efficacy in ROP risk prediction when operating solely on intrinsic knowledge, yet exhibit marked performance gains when augmented with structured external inputs. Second, affective biases are evident in the model outputs, with a consistent inclination toward overestimating medium- and high-risk cases. Third, compared to negative emotions, positive emotional framing contributes to mitigating predictive bias in model outputs. These findings highlight the critical role of affect-sensitive prompt engineering in enhancing diagnostic reliability and emphasize the utility of Affective-ROPTester as a framework for evaluating and mitigating affective bias in clinical language modeling systems.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topic Modeling and Link-Prediction for Material Property Discovery</title>
<link>https://arxiv.org/abs/2507.06139</link>
<guid>https://arxiv.org/abs/2507.06139</guid>
<content:encoded><![CDATA[
<div> Hierarchical Link Prediction, Matrix Factorization, Material Discovery, Topic Tree, Transition-Metal Dichalcogenides <br />
Summary:<br />
The study introduces a hierarchical link prediction framework utilizing matrix factorization to uncover hidden connections within scientific literature networks and knowledge graphs. By combining Hierarchical Nonnegative Matrix Factorization (HNMFk), Boolean matrix factorization (BNMFk), and Logistic matrix factorization (LMF), a three-level topic tree is constructed from a vast document corpus focusing on transition-metal dichalcogenides (TMDs). Through an ensemble approach of BNMFk + LMF, the method provides both interpretable clusters and probabilistic scoring, revealing coherent topics related to TMDs such as superconductivity and energy storage. Missing or weakly connected links between topics and materials are highlighted, sparking new hypotheses for cross-disciplinary exploration. Validation shows the model accurately predicts associations within TMD clusters, showcasing its ability to uncover hidden connections in a diverse scientific document corpus. Interactive tools like the Streamlit dashboard facilitate human-in-the-loop scientific discovery by presenting the inferred links and enabling further exploration. <br /> <div>
arXiv:2507.06139v1 Announce Type: cross 
Abstract: Link prediction infers missing or future relations between graph nodes, based on connection patterns. Scientific literature networks and knowledge graphs are typically large, sparse, and noisy, and often contain missing links between entities. We present an AI-driven hierarchical link prediction framework that integrates matrix factorization to infer hidden associations and steer discovery in complex material domains. Our method combines Hierarchical Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization (BNMFk) with automatic model selection, as well as Logistic matrix factorization (LMF), we use to construct a three-level topic tree from a 46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs). These materials are studied in a variety of physics fields with many current and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with probabilistic scoring. The resulting HNMFk clusters map each material onto coherent topics like superconductivity, energy storage, and tribology. Also, missing or weakly connected links are highlight between topics and materials, suggesting novel hypotheses for cross-disciplinary exploration. We validate our method by removing publications about superconductivity in well-known superconductors, and show the model predicts associations with the superconducting TMD clusters. This shows the method finds hidden connections in a graph of material to latent topic associations built from scientific literature, especially useful when examining a diverse corpus of scientific documents covering the same class of phenomena or materials but originating from distinct communities and perspectives. The inferred links generating new hypotheses, produced by our method, are exposed through an interactive Streamlit dashboard, designed for human-in-the-loop scientific discovery.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Membrane Degradation in PEM Electrolyzers with Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2507.02887</link>
<guid>https://arxiv.org/abs/2507.02887</guid>
<content:encoded><![CDATA[
<div> Keywords: Proton exchange membrane electrolyzers, degradation modeling, Physics-Informed Neural Networks, membrane thinning, predictive tools 

Summary: 
Proton exchange membrane (PEM) electrolyzers play a crucial role in sustainable hydrogen production but face challenges due to membrane degradation, impacting long-term performance. Traditional physics-based models require numerous parameters, while data-driven approaches like machine learning may lack physical consistency. This study introduces Physics-Informed Neural Networks (PINNs) to model membrane degradation in PEM electrolyzers accurately. By coupling two differential equations, one modeling membrane thinning and another governing cell voltage evolution, the PINN captures system degradation dynamics with limited noisy data. The hybrid approach offers a balance between interpretability and flexibility, providing a foundation for more robust predictive tools in electrochemical system diagnostics.<br /><br />Summary: <div>
arXiv:2507.02887v1 Announce Type: new 
Abstract: Proton exchange membrane (PEM) electrolyzers are pivotal for sustainable hydrogen production, yet their long-term performance is hindered by membrane degradation, which poses reliability and safety challenges. Therefore, accurate modeling of this degradation is essential for optimizing durability and performance. To address these concerns, traditional physics-based models have been developed, offering interpretability but requiring numerous parameters that are often difficult to measure and calibrate. Conversely, data-driven approaches, such as machine learning, offer flexibility but may lack physical consistency and generalizability. To address these limitations, this study presents the first application of Physics-Informed Neural Networks (PINNs) to model membrane degradation in PEM electrolyzers. The proposed PINN framework couples two ordinary differential equations, one modeling membrane thinning via a first-order degradation law and another governing the time evolution of the cell voltage under membrane degradation. Results demonstrate that the PINN accurately captures the long-term system's degradation dynamics while preserving physical interpretability with limited noisy data. Consequently, this work introduces a novel hybrid modeling approach for estimating and understanding membrane degradation mechanisms in PEM electrolyzers, offering a foundation for more robust predictive tools in electrochemical system diagnostics.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanics Simulation with Implicit Neural Representations of Complex Geometries</title>
<link>https://arxiv.org/abs/2507.03087</link>
<guid>https://arxiv.org/abs/2507.03087</guid>
<content:encoded><![CDATA[
<div> Neural Implicit Representations, Shifted Boundary Method, Linear Elasticity Simulations, Computational Framework, Meshless Simulations <br />
Summary: <br />
This study introduces a novel computational framework that seamlessly integrates Implicit Neural Representations (INRs) with the Shifted Boundary Method (SBM) for linear elasticity simulations, eliminating the need for explicit geometry transformations and meshing. By directly accessing neural implicit geometry, the method acquires essential boundary information and distance vectors for SBM, enhancing efficiency and accuracy. The approach is tested on complex geometries (Stanford Bunny, Eiffel Tower, gyroids) sourced from triangle soups and point clouds, demonstrating significant computational advantages. The method's robustness and accuracy make it suitable for various applications, including biomedical, geophysical, and advanced manufacturing fields. <div>
arXiv:2507.03087v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs), characterized by neural network-encoded signed distance fields, provide a powerful means to represent complex geometries continuously and efficiently. While successful in computer vision and generative modeling, integrating INRs into computational analysis workflows, such as finite element simulations, remains underdeveloped. In this work, we propose a computational framework that seamlessly combines INRs with the Shifted Boundary Method (SBM) for high-fidelity linear elasticity simulations without explicit geometry transformations. By directly querying the neural implicit geometry, we obtain the surrogate boundaries and distance vectors essential for SBM, effectively eliminating the meshing step. We demonstrate the efficacy and robustness of our approach through elasticity simulations on complex geometries (Stanford Bunny, Eiffel Tower, gyroids) sourced from triangle soups and point clouds. Our method showcases significant computational advantages and accuracy, underscoring its potential in biomedical, geophysical, and advanced manufacturing applications.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Cross-Attention Interaction in Transformers for Interpreting TCR-pMHC Binding</title>
<link>https://arxiv.org/abs/2507.03197</link>
<guid>https://arxiv.org/abs/2507.03197</guid>
<content:encoded><![CDATA[
<div> Keywords: CD8+ killer T cells, CD4+ helper T cells, T cell receptors, transformer-based models, post-hoc explainable AI

Summary:
CD8+ "killer" T cells and CD4+ "helper" T cells are crucial in the immune system, recognizing antigens presented by pMHC via T cell receptors. Transformer models like TULIP have shown great performance in this area but lack interpretability. The new QCAI method aims to interpret cross-attention mechanisms in transformer decoders for TCR-pMHC modeling. A benchmark, TCR-XAI, with 274 TCR-pMHC structures is used to evaluate the method's performance. By computing physical distances between amino acid residues and assessing residue importance, QCAI demonstrates state-of-the-art interpretability and prediction accuracy. The study highlights the significance of understanding T cell response mechanisms and the potential of QCAI in enhancing comprehensibility in TCR-pMHC modeling. 

<br /><br />Summary: <div>
arXiv:2507.03197v1 Announce Type: new 
Abstract: CD8+ "killer" T cells and CD4+ "helper" T cells play a central role in the adaptive immune system by recognizing antigens presented by Major Histocompatibility Complex (pMHC) molecules via T Cell Receptors (TCRs). Modeling binding between T cells and the pMHC complex is fundamental to understanding basic mechanisms of human immune response as well as in developing therapies. While transformer-based models such as TULIP have achieved impressive performance in this domain, their black-box nature precludes interpretability and thus limits a deeper mechanistic understanding of T cell response. Most existing post-hoc explainable AI (XAI) methods are confined to encoder-only, co-attention, or model-specific architectures and cannot handle encoder-decoder transformers used in TCR-pMHC modeling. To address this gap, we propose Quantifying Cross-Attention Interaction (QCAI), a new post-hoc method designed to interpret the cross-attention mechanisms in transformer decoders. Quantitative evaluation is a challenge for XAI methods; we have compiled TCR-XAI, a benchmark consisting of 274 experimentally determined TCR-pMHC structures to serve as ground truth for binding. Using these structures we compute physical distances between relevant amino acid residues in the TCR-pMHC interaction region and evaluate how well our method and others estimate the importance of residues in this region across the dataset. We show that QCAI achieves state-of-the-art performance on both interpretability and prediction accuracy under the TCR-XAI benchmark.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElliottAgents: A Natural Language-Driven Multi-Agent System for Stock Market Analysis and Prediction</title>
<link>https://arxiv.org/abs/2507.03435</link>
<guid>https://arxiv.org/abs/2507.03435</guid>
<content:encoded><![CDATA[
<div> Keywords: ElliottAgents, multi-agent system, natural language processing, stock market data, AI-driven analysis

Summary:
ElliottAgents is a multi-agent system that utilizes natural language processing and large language models to analyze complex stock market data. The system integrates AI-driven analysis with the Elliott Wave Principle to create predictions and explanations that are easily understandable by humans. One of its key features is the natural language dialogue between agents, allowing for collaborative analysis refinement. The architecture, enhanced by large language models, enables advanced language understanding, reasoning, and autonomous decision-making. Through experiments, it has been demonstrated that ElliottAgents is effective in pattern recognition and generating natural language descriptions of market trends. This research contributes to the field of natural language processing applications in specialized domains, showcasing how AI-driven dialogue systems can enhance collaborative analysis in data-intensive fields. By bridging the gap between complex financial data and human comprehension, ElliottAgents addresses the need for interpretable and adaptive prediction systems in finance. 

<br /><br />Summary: <div>
arXiv:2507.03435v1 Announce Type: new 
Abstract: This paper presents ElliottAgents, a multi-agent system leveraging natural language processing (NLP) and large language models (LLMs) to analyze complex stock market data. The system combines AI-driven analysis with the Elliott Wave Principle to generate human-comprehensible predictions and explanations. A key feature is the natural language dialogue between agents, enabling collaborative analysis refinement. The LLM-enhanced architecture facilitates advanced language understanding, reasoning, and autonomous decision-making. Experiments demonstrate the system's effectiveness in pattern recognition and generating natural language descriptions of market trends. ElliottAgents contributes to NLP applications in specialized domains, showcasing how AI-driven dialogue systems can enhance collaborative analysis in data-intensive fields. This research bridges the gap between complex financial data and human understanding, addressing the need for interpretable and adaptive prediction systems in finance.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Concept for Autonomous Problem-Solving in Intralogistics Scenarios</title>
<link>https://arxiv.org/abs/2507.03534</link>
<guid>https://arxiv.org/abs/2507.03534</guid>
<content:encoded><![CDATA[
<div> autonomy, automation systems, enabling technologies, Large Language Models, problem-solving capabilities  
Summary:  
- The paper discusses the importance of achieving greater autonomy in automation systems to effectively handle unforeseen situations in complex real-world environments.  
- It outlines a structured concept consisting of context enrichment, situation analysis, and solution strategy generation as key steps towards increasing autonomy in automation systems.  
- The proposed approach aims to reduce the need for human intervention by enabling automation systems to make more independent decisions.  
- Possible realizations of the concept, including the use of Large Language Models, are discussed as ways to enhance autonomy in automation systems.  
- While some tasks may still require human assistance, the approach significantly improves the adaptive and intelligent problem-solving capabilities of automation systems.  
<br /><br />Summary: <div>
arXiv:2507.03534v1 Announce Type: new 
Abstract: Achieving greater autonomy in automation systems is crucial for handling unforeseen situations effectively. However, this remains challenging due to technological limitations and the complexity of real-world environments. This paper examines the need for increased autonomy, defines the problem, and outlines key enabling technologies. A structured concept is proposed, consisting of three main steps: context enrichment, situation analysis, and generation of solution strategies. By following this approach, automation systems can make more independent decisions, reducing the need for human intervention. Additionally, possible realizations of the concept are discussed, especially the use of Large Language Models. While certain tasks may still require human assistance, the proposed approach significantly enhances the autonomy of automation systems, enabling more adaptive and intelligent problem-solving capabilities.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Based Control for Power-to-X Platforms: Knowledge Integration for Digital Twins</title>
<link>https://arxiv.org/abs/2507.03553</link>
<guid>https://arxiv.org/abs/2507.03553</guid>
<content:encoded><![CDATA[
<div> Keywords: Offshore Power-to-X platforms, Digital Twins, adaptive process control, semantic technologies, Neo4j <br />
Summary: <br />
Offshore Power-to-X platforms face challenges in adaptive process control due to volatile operating conditions. To address this issue, utilizing Digital Twins in these platforms is seen as a promising solution. The integration of heterogeneous models and structured representation of model information is crucial for comprehensive knowledge integration in Digital Twins. The proposed approach utilizes a standardized description of behavior models, semantic technologies, and a graph-based model understanding for automatic adaptation and selection of suitable models. This approach is implemented using a graph-based knowledge representation with Neo4j, automatic data extraction from Asset Administration Shells, and port matching to ensure compatible model configurations. By combining these elements, the approach aims to enhance the flexibility and efficiency of Power-to-X platforms. <div>
arXiv:2507.03553v1 Announce Type: new 
Abstract: Offshore Power-to-X platforms enable flexible conversion of renewable energy, but place high demands on adaptive process control due to volatile operating conditions. To face this challenge, using Digital Twins in Power-to-X platforms is a promising approach. Comprehensive knowledge integration in Digital Twins requires the combination of heterogeneous models and a structured representation of model information. The proposed approach uses a standardized description of behavior models, semantic technologies and a graph-based model understanding to enable automatic adaption and selection of suitable models. It is implemented using a graph-based knowledge representation with Neo4j, automatic data extraction from Asset Administration Shells and port matching to ensure compatible model configurations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operator-based machine learning framework for generalizable prediction of unsteady treatment dynamics in stormwater infrastructure</title>
<link>https://arxiv.org/abs/2507.04682</link>
<guid>https://arxiv.org/abs/2507.04682</guid>
<content:encoded><![CDATA[
<div> neural network, stormwater treatment, urban water management, computational fluid dynamics, pollutant transport

Summary: 
The study introduces a composite operator-based neural network (CPNN) framework for predicting hydraulics and pollutant dynamics in stormwater treatment systems. Traditional models lack accuracy due to oversimplified processes, while computational fluid dynamics is too computationally expensive. The CPNN framework achieves high accuracy in predicting hydraulic behavior and particulate matter concentration in stormwater treatment devices. Challenges are found in capturing dynamics under extreme low-flow conditions, as they contribute less to the training process. Sensitivity analyses using the CPNN highlight the impact of storm event loading on pollutant transport. The CPNN framework shows promise for continuous, long-term evaluation of stormwater infrastructure performance, aiding in climate-aware planning and implementation. <div>
arXiv:2507.04682v1 Announce Type: new 
Abstract: Stormwater infrastructures are decentralized urban water-management systems that face highly unsteady hydraulic and pollutant loadings from episodic rainfall-runoff events. Accurately evaluating their in-situ treatment performance is essential for cost-effective design and planning. Traditional lumped dynamic models (e.g., continuously stirred tank reactor, CSTR) are computationally efficient but oversimplify transport and reaction processes, limiting predictive accuracy and insight. Computational fluid dynamics (CFD) resolves detailed turbulent transport and pollutant fate physics but incurs prohibitive computational cost for unsteady and long-term simulations. To address these limitations, this study develops a composite operator-based neural network (CPNN) framework that leverages state-of-the-art operator learning to predict the spatial and temporal dynamics of hydraulics and particulate matter (PM) in stormwater treatment. The framework is demonstrated on a hydrodynamic separator (HS), a common urban treatment device. Results indicate that the CPNN achieves R2 > 0.8 for hydraulic predictions in 95.2% of test cases; for PM concentration predictions, R2 > 0.8 in 72.6% of cases and 0.4 < R2 < 0.8 in 22.6%. The analysis identifies challenges in capturing dynamics under extreme low-flow conditions, owing to their lower contribution to the training loss. Exploiting the automatic-differentiation capability of the CPNN, sensitivity analyses quantify the influence of storm event loading on PM transport. Finally, the potential of the CPNN framework for continuous, long-term evaluation of stormwater infrastructure performance is discussed, marking a step toward robust, climate-aware planning and implementation.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Gradient Low-Rank Projection Fine-Tuning for LLMs</title>
<link>https://arxiv.org/abs/2507.02503</link>
<guid>https://arxiv.org/abs/2507.02503</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Continual Learning, Low-Rank Adaptation, GORP, Gradient Projection

Summary:
Continual fine-tuning of Large Language Models (LLMs) often faces a trade-off between efficiency and expressiveness. The Low-Rank Adaptation (LoRA) method, while efficient, restricts the model's learning capabilities and knowledge transfer due to its low-rank nature and reliance on explicit parameter constraints. In response, GORP (Gradient LOw Rank Projection) for Continual Learning introduces a novel training strategy that combines full and low-rank parameters to update within a unified low-rank gradient subspace. This approach expands the optimization space while maintaining efficiency and reducing catastrophic forgetting. Through extensive experiments on continual learning benchmarks, GORP outperforms existing state-of-the-art methods. The code for GORP is publicly available on GitHub, providing a framework for implementing and testing this innovative training strategy.<br /><br />Summary: <div>
arXiv:2507.02503v1 Announce Type: cross 
Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable diffusion-based generation for multi-channel biological data</title>
<link>https://arxiv.org/abs/2507.02902</link>
<guid>https://arxiv.org/abs/2507.02902</guid>
<content:encoded><![CDATA[
<div> diffusion-based models; spatial profiling technologies; generative modeling; high-dimensional data; spatial alignment <br />
Summary: This article introduces a unified diffusion framework for generating structured and spatial biological data, such as imaging mass cytometry and spatial transcriptomics. The model incorporates a hierarchical feature injection mechanism for multi-resolution conditioning and a combination of latent-space and output-space channel-wise attention to capture inter-channel relationships. It is trained using a random masking strategy to support flexible conditioning and generalization to arbitrary subsets of observed channels. The model demonstrates superior performance in tasks such as protein imputation in IMC and gene-to-protein prediction in single-cell datasets, as well as strong generalization to unseen conditional configurations. <div>
arXiv:2507.02902v1 Announce Type: cross 
Abstract: Spatial profiling technologies in biology, such as imaging mass cytometry (IMC) and spatial transcriptomics (ST), generate high-dimensional, multi-channel data with strong spatial alignment and complex inter-channel relationships. Generative modeling of such data requires jointly capturing intra- and inter-channel structure, while also generalizing across arbitrary combinations of observed and missing channels for practical application. Existing diffusion-based models generally assume low-dimensional inputs (e.g., RGB images) and rely on simple conditioning mechanisms that break spatial correspondence and ignore inter-channel dependencies. This work proposes a unified diffusion framework for controllable generation over structured and spatial biological data. Our model contains two key innovations: (1) a hierarchical feature injection mechanism that enables multi-resolution conditioning on spatially aligned channels, and (2) a combination of latent-space and output-space channel-wise attention to capture inter-channel relationships. To support flexible conditioning and generalization to arbitrary subsets of observed channels, we train the model using a random masking strategy, enabling it to reconstruct missing channels from any combination of inputs. We demonstrate state-of-the-art performance across both spatial and non-spatial prediction tasks, including protein imputation in IMC and gene-to-protein prediction in single-cell datasets, and show strong generalization to unseen conditional configurations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolProphecy: Bridging Medicinal Chemists' Knowledge and Molecular Pre-Trained Models via a Multi-Modal Framework</title>
<link>https://arxiv.org/abs/2507.02932</link>
<guid>https://arxiv.org/abs/2507.02932</guid>
<content:encoded><![CDATA[
<div> framework, molecular property prediction, human-in-the-loop, ChatGPT, chemist knowledge <br />
Summary: 
The MolProphecy framework integrates chemists' domain knowledge into molecular property prediction models using a human-in-the-loop approach. By leveraging ChatGPT as a virtual chemist, it simulates expert reasoning and decision-making to enhance model accuracy. MolProphecy outperforms state-of-the-art models on benchmark datasets, showing improvements in RMSE and AUROC metrics. The framework combines chemist knowledge with structural features through a gated cross-attention mechanism, improving both accuracy and interpretability. It offers a collaborative approach to drug discovery, with the flexibility to incorporate real chemist input without requiring retraining. The implementation of MolProphecy is publicly available for use. <br /> <div>
arXiv:2507.02932v1 Announce Type: cross 
Abstract: MolProphecy is a human-in-the-loop (HITL) multi-modal framework designed to integrate chemists' domain knowledge into molecular property prediction models. While molecular pre-trained models have enabled significant gains in predictive accuracy, they often fail to capture the tacit, interpretive reasoning central to expert-driven molecular design. To address this, MolProphecy employs ChatGPT as a virtual chemist to simulate expert-level reasoning and decision-making. The generated chemist knowledge is embedded by the large language model (LLM) as a dedicated knowledge representation and then fused with graph-based molecular features through a gated cross-attention mechanism, enabling joint reasoning over human-derived and structural features. Evaluated on four benchmark datasets (FreeSolv, BACE, SIDER, and ClinTox), MolProphecy outperforms state-of-the-art (SOTA) models, achieving a 15.0 percent reduction in RMSE on FreeSolv and a 5.39 percent improvement in AUROC on BACE. Analysis reveals that chemist knowledge and structural features provide complementary contributions, improving both accuracy and interpretability. MolProphecy offers a practical and generalizable approach for collaborative drug discovery, with the flexibility to incorporate real chemist input in place of the current simulated proxy--without the need for model retraining. The implementation is publicly available at https://github.com/zhangruochi/MolProphecy.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-Through Tensors: A Unified Computational Graph Architecture for Multi-Layer Transportation Network Optimization</title>
<link>https://arxiv.org/abs/2507.02961</link>
<guid>https://arxiv.org/abs/2507.02961</guid>
<content:encoded><![CDATA[
<div> Keywords: Flow Through Tensors, transportation network modeling, tensor decomposition, gradient-based optimization, integrated mobility systems<br />
Summary:<br />
Flow Through Tensors (FTT) is introduced as a unified computational graph architecture for transportation network modeling. It connects origin destination flows, path probabilities, and link travel times as interconnected tensors, allowing for multidimensional analysis of traffic patterns. The framework enables gradient-based optimization across different modeling elements, supports efficient system efficiency quantification over time, space, and user groups, and implements tensor decomposition techniques for large-scale applications. These innovations enable real-time control strategies, coordination between transportation modes/operators, and enforcement of network constraints. FTT bridges the gap between theoretical models and practical deployment needs, providing a foundation for next-generation integrated mobility systems. <div>
arXiv:2507.02961v1 Announce Type: cross 
Abstract: Modern transportation network modeling increasingly involves the integration of diverse methodologies including sensor-based forecasting, reinforcement learning, classical flow optimization, and demand modeling that have traditionally been developed in isolation. This paper introduces Flow Through Tensors (FTT), a unified computational graph architecture that connects origin destination flows, path probabilities, and link travel times as interconnected tensors. Our framework makes three key contributions: first, it establishes a consistent mathematical structure that enables gradient-based optimization across previously separate modeling elements; second, it supports multidimensional analysis of traffic patterns over time, space, and user groups with precise quantification of system efficiency; third, it implements tensor decomposition techniques that maintain computational tractability for large scale applications. These innovations collectively enable real time control strategies, efficient coordination between multiple transportation modes and operators, and rigorous enforcement of physical network constraints. The FTT framework bridges the gap between theoretical transportation models and practical deployment needs, providing a foundation for next generation integrated mobility systems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LANTERN: A Machine Learning Framework for Lipid Nanoparticle Transfection Efficiency Prediction</title>
<link>https://arxiv.org/abs/2507.03209</link>
<guid>https://arxiv.org/abs/2507.03209</guid>
<content:encoded><![CDATA[
<div> ionizable lipids, lipid nanoparticle, RNA delivery, machine learning, transfection efficiency

Summary:
LANTERN is a machine learning framework designed to predict transfection efficiency for lipid nanoparticles used in RNA delivery. By utilizing chemically informative features like Morgan fingerprints and Expert descriptors, LANTERN outperformed previous models, including AGILE, in predicting transfection efficiency. The model achieved a high performance with an R-squared value of 0.8161 and a correlation coefficient of 0.9053. LANTERN showed consistent strong performance across multiple evaluation metrics, making it a valuable tool for accelerating the design of lipid-based RNA delivery systems. The discovery of new ionizable lipids for efficient RNA delivery is crucial for the development of RNA-based therapeutics. The use of machine learning in predicting transfection efficiency from molecular structure has the potential to streamline the identification of lead compounds and advance the field of RNA-based therapeutics. <div>
arXiv:2507.03209v1 Announce Type: cross 
Abstract: The discovery of new ionizable lipids for efficient lipid nanoparticle (LNP)-mediated RNA delivery remains a critical bottleneck for RNA-based therapeutics development. Recent advances have highlighted the potential of machine learning (ML) to predict transfection efficiency from molecular structure, enabling high-throughput virtual screening and accelerating lead identification. However, existing approaches are hindered by inadequate data quality, ineffective feature representations, low predictive accuracy, and poor generalizability. Here, we present LANTERN (Lipid nANoparticle Transfection Efficiency pRedictioN), a robust ML framework for predicting transfection efficiency based on ionizable lipid representation. We benchmarked a diverse set of ML models against AGILE, a previously published model developed for transfection prediction. Our results show that combining simpler models with chemically informative features, particularly count-based Morgan fingerprints, outperforms more complex models that rely on internally learned embeddings, such as AGILE. We also show that a multi-layer perceptron trained on a combination of Morgan fingerprints and Expert descriptors achieved the highest performance ($\text{R}^2$ = 0.8161, r = 0.9053), significantly exceeding AGILE ($\text{R}^2$ = 0.2655, r = 0.5488). We show that the models in LANTERN consistently have strong performance across multiple evaluation metrics. Thus, LANTERN offers a robust benchmarking framework for LNP transfection prediction and serves as a valuable tool for accelerating lipid-based RNA delivery systems design.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time prediction of plasma instabilities with sparse-grid-accelerated optimized dynamic mode decomposition</title>
<link>https://arxiv.org/abs/2507.03245</link>
<guid>https://arxiv.org/abs/2507.03245</guid>
<content:encoded><![CDATA[
<div> sparse grid interpolation, parametric reduced-order models, gyrokinetic simulations, fusion experiments, dynamic mode decomposition <br />
Summary: 
This paper explores the efficient training of parametric reduced-order models (ROMs) using sparse grid interpolation with (L)-Leja points for scenarios with high-dimensional input spaces. By focusing on gyrokinetic simulations of plasma micro-instabilities in fusion experiments, the study constructs parametric ROMs for the full 5D gyrokinetic distribution function using optimized dynamic mode decomposition and sparse grids based on (L)-Leja points. The research assesses the ROM prediction capabilities in different scenarios, including the Cyclone Base Case benchmark and a real-world micro-instability simulation with six input parameters. The results demonstrate that accurate parametric ROMs can be achieved at a low cost of high-fidelity simulations using sparse grids, showing the potential of these models in enabling complex many-query tasks in fusion research. <br /><br />Summary: <div>
arXiv:2507.03245v1 Announce Type: cross 
Abstract: Parametric data-driven reduced-order models (ROMs) that embed dependencies in a large number of input parameters are crucial for enabling many-query tasks in large-scale problems. These tasks, including design optimization, control, and uncertainty quantification, are essential for developing digital twins in real-world applications. However, standard training data generation methods are computationally prohibitive due to the curse of dimensionality, as their cost scales exponentially with the number of inputs.This paper investigates efficient training of parametric data-driven ROMs using sparse grid interpolation with (L)-Leja points, specifically targeting scenarios with higher-dimensional input parameter spaces. (L)-Leja points are nested and exhibit slow growth, resulting in sparse grids with low cardinality in low-to-medium dimensional settings, making them ideal for large-scale, computationally expensive problems. Focusing on gyrokinetic simulations of plasma micro-instabilities in fusion experiments as a representative real-world application, we construct parametric ROMs for the full 5D gyrokinetic distribution function via optimized dynamic mode decomposition (optDMD) and sparse grids based on (L)-Leja points. We perform detailed experiments in two scenarios: First, the Cyclone Base Case benchmark assesses optDMD ROM prediction capabilities beyond training time horizons and across variations in the binormal wave number. Second, for a real-world electron temperature gradient driven micro-instability simulation featuring six input parameters, we demonstrate that an accurate parametric optDMD ROM can be constructed at a cost of only $28$ high-fidelity gyrokinetic simulations thanks to sparse grids. In the broader context of fusion research, these results demonstrate the potential of sparse grid-based parametric ROMs to enable otherwise intractable many-query tasks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-robust multi-fidelity surrogate modelling for parametric partial differential equations</title>
<link>https://arxiv.org/abs/2507.03691</link>
<guid>https://arxiv.org/abs/2507.03691</guid>
<content:encoded><![CDATA[
<div> surrogate models, multi-fidelity, stochastic collocation, numerical noise, PDEs <br />
Summary:
The article addresses the challenge of constructing noise-robust surrogate models for quantities of interest (QoIs) from parametric partial differential equations (PDEs) using multi-fidelity collocation techniques, specifically the Multi-Index Stochastic Collocation (MISC). In scenarios where PDE evaluations are corrupted by numerical noise, an improved version of MISC is proposed to automatically detect and ignore noisy fidelities, mitigating overfitting and degradation of surrogate quality. The approach monitors the spectral decay of the surrogate at each iteration to identify noise onset and selectively halt the use of noisy fidelities. Numerical validation on parabolic advection-diffusion PDE and parametric turbulent incompressible Navier-Stokes problem demonstrates the accuracy and robustness of the resulting multi-fidelity surrogate even on under-resolved meshes unsuitable for single-fidelity computations. This method enhances the utility of multi-fidelity surrogates for downstream tasks like uncertainty quantification, optimization, and control. <br /><br /> <div>
arXiv:2507.03691v1 Announce Type: cross 
Abstract: We address the challenge of constructing noise-robust surrogate models for quantities of interest (QoIs) arising from parametric partial differential equations (PDEs), using multi-fidelity collocation techniques; specifically, the Multi-Index Stochastic Collocation (MISC). In practical scenarios, the PDE evaluations used to build a response surface are often corrupted by numerical noise, especially for the low-fidelity models. This noise, which may originate from loose solver tolerances, coarse discretisations, or transient effects, can lead to overfitting in MISC, degrading surrogate quality through nonphysical oscillations and loss of convergence, thereby limiting its utility in downstream tasks like uncertainty quantification, optimisation, and control. To correct this behaviour, we propose an improved version of MISC that can automatically detect the presence of solver noise during the surrogate model construction and then ignore the exhausted fidelities. Our approach monitors the spectral decay of the surrogate at each iteration, identifying stagnation in the coefficient spectrum that signals the onset of noise. Once detected, the algorithm selectively halts the use of noisy fidelities, focusing computational resources on those fidelities that still provide meaningful information. The effectiveness of this approach is numerically validated on two challenging test cases: a parabolic advection--diffusion PDE with uncertain coefficients, and a parametric turbulent incompressible Navier--Stokes problem. The results showcase the accuracy and robustness of the resulting multi-fidelity surrogate and its capability to extract relevant information, even from under-resolved meshes not suitable for reliable single-fidelity computations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital Wills</title>
<link>https://arxiv.org/abs/2507.03694</link>
<guid>https://arxiv.org/abs/2507.03694</guid>
<content:encoded><![CDATA[
<div> Decentralized protocol, digital estate planning, cryptography, distributed computing, blockchain technology <br />
<br />
Summary: This work introduces a decentralized protocol for digital estate planning that leverages advanced distributed computing and cryptography. The protocol is implemented as a layer-1 solution using modern interchain communication to bridge different types of blockchains. It incorporates modern cryptographic primitives to support various forms of claims for information validation, ensuring enhanced privacy for digital inheritance. The protocol features heterogeneous smart contracts on multiple chains, facilitating secure asset distribution according to the decedent's wishes without the need to transfer funds. A user interaction model with a check-in system and account abstraction process improves flexibility and security. By utilizing a permissionless blockchain secured by validators and interchain relayers, the protocol revolutionizes digital estate planning, demonstrating the potential of blockchain technology in legal and personal spheres. The incorporation of a cryptoeconomic network enables unique incentive-compatible economic mechanisms in inheritance planning. <br /><br /> <div>
arXiv:2507.03694v1 Announce Type: cross 
Abstract: This work presents a novel decentralized protocol for digital estate planning that integrates advances distributed computing, and cryptography. The original proof-of-concept was constructed using purely solidity contracts. Since then, we have enhanced the implementation into a layer-1 protocol that uses modern interchain communication to connect several heterogeneous chain types. A key contribution of this research is the implementation of several modern cryptographic primitives to support various forms of claims for information validation. These primitives introduce an unmatched level of privacy to the process of digital inheritance. We also demonstrate on a set of heterogeneous smart contracts, following the same spec, on each chain to serve as entry points, gateways, or bridge contracts that are invoked via a path from the will module on our protocol, to the contract. This ensures a fair and secure distribution of digital assets in accordance with the wishes of the decedent without the requirement of moving their funds. This research further extends its innovations with a user interaction model, featuring a check-in system and account abstraction process, which enhances flexibility and user-friendliness without compromising on security. By developing a dedicated permissionless blockchain that is secured by a network of validators, and interchain relayers, the proposed protocol signifies a transformation in the digital estate planning industry and illustrates the potential of blockchain technology in revolutionizing traditional legal and personal spheres. Implementing a cryptoeconomic network at the core of inheritance planning allows for unique incentive compatible economic mechanisms to be constructed.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM</title>
<link>https://arxiv.org/abs/2507.03868</link>
<guid>https://arxiv.org/abs/2507.03868</guid>
<content:encoded><![CDATA[
<div> Query-style prototypes, Prompt Bank, Mixture-of-Expert Low-Rank Adaptation, Uni-Retrieval, Uni-RAG <br />
Summary: 
The article introduces Uni-Retrieval, a multi-modal retrieval module that dynamically matches query-style prototypes with tokens from a Prompt Bank, enhancing its capability to handle diverse educational scenarios. By incorporating a language model, Uni-Retrieval becomes Uni-RAG, which retrieves educational materials and generates human-readable content aligned with learning objectives. Experimental results indicate Uni-RAG's superior performance in retrieval accuracy and generation quality compared to baseline systems, all while being computationally efficient. The framework offers a scalable and pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to provide personalized, explainable, and efficient learning assistance across STEM scenarios. <br /> <br />Summary: <div>
arXiv:2507.03868v1 Announce Type: cross 
Abstract: In AI-facilitated teaching, leveraging various query styles to interpret abstract educational content is crucial for delivering effective and accessible learning experiences. However, existing retrieval systems predominantly focus on natural text-image matching and lack the capacity to address the diversity and ambiguity inherent in real-world educational scenarios. To address this limitation, we develop a lightweight and efficient multi-modal retrieval module, named Uni-Retrieval, which extracts query-style prototypes and dynamically matches them with tokens from a continually updated Prompt Bank. This Prompt Bank encodes and stores domain-specific knowledge by leveraging a Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to enhance Uni-Retrieval's capability to accommodate unseen query types at test time. To enable natural language educational content generation, we integrate the original Uni-Retrieval with a compact instruction-tuned language model, forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given a style-conditioned query, Uni-RAG first retrieves relevant educational materials and then generates human-readable explanations, feedback, or instructional content aligned with the learning objective. Experimental results on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline retrieval and RAG systems in both retrieval accuracy and generation quality, while maintaining low computational cost. Our framework provides a scalable, pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to support personalized, explainable, and efficient learning assistance across diverse STEM scenarios.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency through Evolution, A Darwinian Approach to Agent-Based Economic Forecast Modeling</title>
<link>https://arxiv.org/abs/2507.04074</link>
<guid>https://arxiv.org/abs/2507.04074</guid>
<content:encoded><![CDATA[
<div> methodology, macroeconomic forecasting, agent-based modeling, evolutionary principles, computational efficiency

Summary:
- The paper introduces a novel Darwinian Agent-Based Modeling (ABM) methodology for macroeconomic forecasting.
- It leverages evolutionary principles to achieve computational efficiency and emergent realism.
- The approach uses simple "common sense" rules representative of small firms serving final consumers.
- The methodology treats households as primary drivers of economic dynamics, with firms adapting through market-based natural selection.
- The model, constrained by Input-Output table structures, generates realistic economic patterns without extensive parameter calibration.
- Using FIGARO Input-Output tables for 46 countries, the model reproduces empirical regularities for Austria with minimal country-specific parameter calibration.
- Key findings include realistic firm and employment distributions, reproduction of initial Social Accounting Matrix values, successful calibration with a few parameters, and computational efficiency on standard laptops.
- Evolutionary ABM approaches offer robust policy insights by capturing decentralized market adaptations while avoiding the complexity of traditional DSGE and comprehensive ABM models.

<br /><br />Summary: <div>
arXiv:2507.04074v1 Announce Type: cross 
Abstract: This paper presents a novel Darwinian Agent-Based Modeling (ABM) methodology formacroeconomic forecasting that leverages evolutionary principles to achieve remarkablecomputational efficiency and emergent realism. Unlike conventional DSGE and ABM approachesthat rely on complex behavioral rules derived from large firm analysis, our framework employssimple "common sense" rules representative of small firms directly serving final consumers. Themethodology treats households as the primary drivers of economic dynamics, with firms adaptingthrough market-based natural selection within limited interaction neighborhoods. We demonstrate that this approach, when constrained by Input-Output table structures,generates realistic economic patterns including wealth distributions, firm size distributions, andsectoral employment patterns without extensive parameter calibration. Using FIGARO Input-Output tables for 46 countries and focusing on Austria as a case study, we show that the modelreproduces empirical regularities while maintaining computational efficiency on standard laptopsrather than requiring supercomputing clusters. Key findings include: (1) emergence of realistic firm and employment distributions fromminimal behavioral assumptions, (2) accurate reproduction of the initial Social Accounting Matrixvalues through evolutionary dynamics, (3) successful calibration using only 5-6 country-specificparameters to complement the FIGARO data, and (4) computational performance enabling fullsimulations on consumer hardware. These results suggest that evolutionary ABM approaches canprovide robust policy insights by capturing decentralized market adaptations while avoiding thecomputational complexity of traditional DSGE and comprehensive ABM models.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems</title>
<link>https://arxiv.org/abs/2507.04996</link>
<guid>https://arxiv.org/abs/2507.04996</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomy, autonomous vehicles, agentic AI, human-machine interaction, mobility ecosystems

Summary: 
This paper introduces the concept of agentic vehicles (AgVs) as vehicles integrated with agentic AI to interact and reason in complex environments. It highlights the gap between technical autonomy and the cognitive and social capabilities required for future mobility systems. The paper presents a framework to characterize AgVs, focusing on their cognitive and communicative layers, differentiating them from conventional autonomous vehicles (AuVs). It discusses the integration of agentic AI, robotics, and human-machine interaction in AgVs, emphasizing their role as interactive agents within mobility ecosystems. Key challenges in developing and governing AgVs are identified, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.<br /><br />Summary: <div>
arXiv:2507.04996v1 Announce Type: cross 
Abstract: Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are defined as systems capable of perceiving their environment and executing preprogrammed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 1 to 6), such as interaction with humans and machines, goal adaptation, contextual reasoning, external tool use, and long-term planning, particularly with the integration of large language models (LLMs) and agentic AI systems. These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this, we introduce the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and interact within complex environments. This paper presents a systems-level framework to characterize AgVs, focusing on their cognitive and communicative layers and differentiating them from conventional AuVs. It synthesizes relevant advances in agentic AI, robotics, multi-agent systems, and human-machine interaction, and highlights how agentic AI, through high-level reasoning and tool use, can function not merely as computational tools but as interactive agents embedded in mobility ecosystems. The paper concludes by identifying key challenges in the development and governance of AgVs, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Tweet Posting Behavior on Citizen Security: A Hawkes Point Process Analysis</title>
<link>https://arxiv.org/abs/2402.03378</link>
<guid>https://arxiv.org/abs/2402.03378</guid>
<content:encoded><![CDATA[
<div> Keywords: Perception of Security, social network data, predictive insights, external factors, proactive security planning

Summary: 
This article presents a novel approach to measuring the Perception of Security (PoS) using social network data to provide real-time monitoring and predictive insights. By analyzing social network content related to security perceptions, the model incorporates external factors that influence the publication and reposting of such content. The results show that the proposed model achieves competitive predictive performance and offers a high level of interpretability regarding the factors influencing security perceptions. The research highlights the importance of understanding temporal patterns and external factors in anticipating security perceptions, providing valuable insights for proactive security planning. <div>
arXiv:2402.03378v2 Announce Type: replace-cross 
Abstract: The Perception of Security (PoS) refers to people's opinions about security or insecurity in a place or situation. While surveys have traditionally been the primary means to capture such perceptions, they need to be improved in their ability to offer real-time monitoring or predictive insights into future security perceptions. Recent evidence suggests that social network content can provide complementary insights into quantifying these perceptions. However, the challenge of accurately predicting these perceptions, with the capacity to anticipate them, still needs to be explored. This article introduces an innovative approach to PoS within short time frames using social network data. Our model incorporates external factors that influence the publication and reposting of content related to security perceptions. Our results demonstrate that this proposed model achieves competitive predictive performance and maintains a high degree of interpretability regarding the factors influencing security perceptions. This research contributes to understanding how temporal patterns and external factors impact the anticipation of security perceptions, providing valuable insights for proactive security planning.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative and parametric insurance on the Ethereum blockchain</title>
<link>https://arxiv.org/abs/2412.05321</link>
<guid>https://arxiv.org/abs/2412.05321</guid>
<content:encoded><![CDATA[
<div> Blockchain, insurance, parametric, collaborative, smart contract
Summary:
This paper presents a novel blockchain-based insurance scheme that combines parametric and collaborative elements. It involves a group of investors, called surplus providers, who lock funds in a smart contract to allow blockchain users to underwrite parametric insurance contracts. These contracts automatically trigger compensation upon meeting specific conditions. The collaborative aspect is reflected in the generation of tokens distributed to surplus providers, representing their share of surplus and granting voting rights for management decisions. The smart contract is coded in Solidity for the Ethereum blockchain and deployed on the Sepolia testnet. Data processing and analysis are done using Python. Open-source code is provided for transparency, and key research challenges are outlined for potential improvements. Overall, this paper sets the foundation for future research and development in blockchain-based insurance mechanisms.'<br /><br />Summary: <div>
arXiv:2412.05321v2 Announce Type: replace-cross 
Abstract: This paper introduces a blockchain-based insurance scheme that integrates parametric and collaborative elements. A pool of investors, referred to as surplus providers, locks funds in a smart contract, enabling blockchain users to underwrite parametric insurance contracts. These contracts automatically trigger compensation when predefined conditions are met. The collaborative aspect is embodied in the generation of tokens, which are distributed to surplus providers. These tokens represent each participant's share of the surplus and grant voting rights for management decisions. The smart contract is developed in Solidity, a high-level programming language for the Ethereum blockchain, and deployed on the Sepolia testnet, with data processing and analysis conducted using Python. In addition, open-source code is provided and main research challenges are identified, so that further research can be carried out to overcome limitations of this first proof of concept.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener</title>
<link>https://arxiv.org/abs/2507.02005</link>
<guid>https://arxiv.org/abs/2507.02005</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Explainable AI, Fatigue Strength, Welded Steel Structures, Feature Engineering<br />
Summary:<br />
This research presents a unified approach that combines Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. By incorporating expert-driven feature engineering with algorithmic feature creation, the accuracy and explainability of the models are enhanced. The study utilizes regression models such as gradient boosting, random forests, and neural networks trained using AutoML under three feature schemes. Ensemble methods, including CatBoost and LightGBM, demonstrated top performance. The domain-informed model achieved the best balance of test RMSE and $R^2 values. XAI methods identified key predictors such as stress ratio, stress range, yield strength, and post-weld treatment. Secondary geometric factors also significantly influenced fatigue life. The integration of AutoML with XAI leads to accurate, interpretable, and robust fatigue strength models for welded steel structures, facilitating AI-assisted design and assessment. Future research will focus on probabilistic fatigue life modeling and integration into digital twin environments. <br />Summary: <div>
arXiv:2507.02005v1 Announce Type: new 
Abstract: This research introduces a unified approach combining Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. It integrates expert-driven feature engineering with algorithmic feature creation to enhance accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient boosting, random forests, and neural networks - were trained using AutoML under three feature schemes: domain-informed, algorithmic, and combined. This allowed a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE $\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta \sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527% within the engineering-relevant 0 - 150 MPa domain. The denser-feature model ($\mathcal M_3$) showed minor gains during training but poorer generalization, while the simpler base-feature model ($\mathcal M_1$) performed comparably, confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG dressing vs. as-welded) as dominant predictors. Secondary geometric factors - plate width, throat thickness, stiffener height - also significantly affected fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate, interpretable, and robust fatigue strength models for welded steel structures. It bridges data-driven modeling with engineering validation, enabling AI-assisted design and assessment. Future work will explore probabilistic fatigue life modeling and integration into digital twin environments.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Resolution Independent Operator Learning</title>
<link>https://arxiv.org/abs/2507.02524</link>
<guid>https://arxiv.org/abs/2507.02524</guid>
<content:encoded><![CDATA[
<div> Keywords: time-dependent partial differential equations, recurrent DeepONet, Neural Controlled Differential Equation, transient mechanics, operator learning

Summary:
Accurately learning solution operators for time-dependent partial differential equations (PDEs) from sparse and irregular data is a challenging task. Existing methods like recurrent DeepONet extensions and neural-ODE surrogates have limitations in handling discrete-time and new inputs after initialization. To address these issues, NCDE-DeepONet is introduced, which combines a Neural Controlled Differential Equation (NCDE) with explicit space-time coordinates to create a continuous-time operator network. The NCDE encodes the entire load history as a solution of a controlled ODE driven by a spline-interpolated input path, allowing for input-resolution-independent representation. The trunk of the network can probe this latent path at arbitrary spatial locations and times, enabling output-resolution independence. Benchmarks on various transient mechanics problems demonstrate the robustness and accuracy of the framework, showcasing almost instant solution prediction. The approach of using controlled dynamics proves to be a principled and efficient method for high-fidelity operator learning in transient mechanics. 

<br /><br />Summary: <div>
arXiv:2507.02524v1 Announce Type: new 
Abstract: Accurately learning solution operators for time-dependent partial differential equations (PDEs) from sparse and irregular data remains a challenging task. Recurrent DeepONet extensions inherit the discrete-time limitations of sequence-to-sequence (seq2seq) RNN architectures, while neural-ODE surrogates cannot incorporate new inputs after initialization. We introduce NCDE-DeepONet, a continuous-time operator network that embeds a Neural Controlled Differential Equation (NCDE) in the branch and augments the trunk with explicit space-time coordinates. The NCDE encodes an entire load history as the solution of a controlled ODE driven by a spline-interpolated input path, making the representation input-resolution-independent: it encodes different input signal discretizations of the observed samples. The trunk then probes this latent path at arbitrary spatial locations and times, rendering the overall map output-resolution independent: predictions can be queried on meshes and time steps unseen during training without retraining or interpolation. Benchmarks on transient Poisson, elastodynamic, and thermoelastic problems confirm the robustness and accuracy of the framework, achieving almost instant solution prediction. These findings suggest that controlled dynamics provide a principled and efficient foundation for high-fidelity operator learning in transient mechanics.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation and Heterogeneity Shape the Resilience of Community Currency Networks</title>
<link>https://arxiv.org/abs/2507.02678</link>
<guid>https://arxiv.org/abs/2507.02678</guid>
<content:encoded><![CDATA[
<div> Keywords: community currency, mutual credit systems, graph theory, behavioral connectivity, network evolution

Summary:
This paper examines community currency networks, focusing on the case study of Sardex in Sardinia, Italy. The analysis is done through a graph theoretic framework, studying strongly connected components, condensed representations, and behavioral connectivity patterns. The evolution of the network over three years is analyzed, revealing temporal contraction, flow imbalances, and structural fragmentation based on user types. The findings show deviations from degree-based models, indicating behavioral imitation among users and a preference for more active peers. The impact of heterogeneous connections between user types is also evaluated, highlighting their role in strengthening the network topology and enhancing resilience.<br /><br />Summary: <div>
arXiv:2507.02678v1 Announce Type: new 
Abstract: Community currency networks are made up of individuals and or companies that share some physical or social characteristics and engage in economic transactions using a virtual currency. This paper investigates the structural and dynamic properties of such mutual credit systems through a case study of Sardex, a community currency initiated and mainly operating in Sardinia, Italy. The transaction network is modeled as a directed weighted graph and analyzed through a graph theoretic framework focused on the analysis of strongly connected components, condensed representations, and behavioral connectivity patterns. Emphasis is placed on understanding the evolution of the network's core and peripheral structures over a three year period, with attention to temporal contraction, flow asymmetries, and structural fragmentation depending on different user types. Our findings reveal persistent deviations from degree based null models and suggest the presence of behavioral imitation, specifically, a user preference for more active peers. We further assess the impact of heterogeneous connections between different type of users, which strengthen the network topology and enhance its resilience.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery</title>
<link>https://arxiv.org/abs/2507.02730</link>
<guid>https://arxiv.org/abs/2507.02730</guid>
<content:encoded><![CDATA[
<div> Physics-Informed Automated Discovery of Kinetics, catalytic processes, kinetic models, symbolic regression, Metropolis-Hastings algorithm <br />
<br />
Summary: The article introduces the Physics-Informed Automated Discovery of Kinetics (PI-ADoK) framework for industrial catalytic processes. Traditional mechanistic models for kinetics demand expertise, while data-driven approaches lack interpretability. PI-ADoK integrates physical constraints into symbolic regression to reduce the search space and experiments needed for model convergence. It includes a Metropolis-Hastings algorithm for uncertainty quantification, providing credible prediction intervals. Benchmarking against conventional methods in catalytic case studies shows PI-ADoK enhances model fidelity and decreases the experimental burden, offering efficient and reliable kinetic model discovery for chemical reaction engineering. <div>
arXiv:2507.02730v1 Announce Type: new 
Abstract: The industrialization of catalytic processes hinges on the availability of reliable kinetic models for design, optimization, and control. Traditional mechanistic models demand extensive domain expertise, while many data-driven approaches often lack interpretability and fail to enforce physical consistency. To overcome these limitations, we propose the Physics-Informed Automated Discovery of Kinetics (PI-ADoK) framework. By integrating physical constraints directly into a symbolic regression approach, PI-ADoK narrows the search space and substantially reduces the number of experiments required for model convergence. Additionally, the framework incorporates a robust uncertainty quantification strategy via the Metropolis-Hastings algorithm, which propagates parameter uncertainty to yield credible prediction intervals. Benchmarking our method against conventional approaches across several catalytic case studies demonstrates that PI-ADoK not only enhances model fidelity but also lowers the experimental burden, highlighting its potential for efficient and reliable kinetic model discovery in chemical reaction engineering.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification</title>
<link>https://arxiv.org/abs/2507.01971</link>
<guid>https://arxiv.org/abs/2507.01971</guid>
<content:encoded><![CDATA[
<div> Keywords: support and resistance levels, deep learning, financial analysis, market microstructure, attention-based architecture

Summary:
DeepSupp is a new deep learning approach designed to detect financial support levels using multi-head attention mechanisms and advanced feature engineering. It leverages dynamic correlation matrices to capture evolving market relationships and uses an attention-based autoencoder for robust representation learning. The final support levels are identified through unsupervised clustering with DBSCAN, resulting in state-of-the-art performance across six financial metrics on S&amp;P 500 tickers. DeepSupp outperforms six baseline methods in essential support accuracy and market regime sensitivity, demonstrating consistent results across diverse market conditions. This approach fills critical gaps in support and resistance level detection, providing a scalable and reliable solution for modern financial analysis. The use of attention-based architectures in DeepSupp uncovers nuanced market patterns and enhances technical trading strategies. <br /><br />Summary: <div>
arXiv:2507.01971v1 Announce Type: cross 
Abstract: Support and resistance (SR) levels are central to technical analysis, guiding traders in entry, exit, and risk management. Despite widespread use, traditional SR identification methods often fail to adapt to the complexities of modern, volatile markets. Recent research has introduced machine learning techniques to address the following challenges, yet most focus on price prediction rather than structural level identification. This paper presents DeepSupp, a new deep learning approach for detecting financial support levels using multi-head attention mechanisms to analyze spatial correlations and market microstructure relationships. DeepSupp integrates advanced feature engineering, constructing dynamic correlation matrices that capture evolving market relationships, and employs an attention-based autoencoder for robust representation learning. The final support levels are extracted through unsupervised clustering, leveraging DBSCAN to identify significant price thresholds. Comprehensive evaluations on S&amp;P 500 tickers demonstrate that DeepSupp outperforms six baseline methods, achieving state-of-the-art performance across six financial metrics, including essential support accuracy and market regime sensitivity. With consistent results across diverse market conditions, DeepSupp addresses critical gaps in SR level detection, offering a scalable and reliable solution for modern financial analysis. Our approach highlights the potential of attention-based architectures to uncover nuanced market patterns and improve technical trading strategies.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design of Corrugated Boards: A New FEM Modeling and Experimental Validation</title>
<link>https://arxiv.org/abs/2507.02189</link>
<guid>https://arxiv.org/abs/2507.02189</guid>
<content:encoded><![CDATA[
<div> FEM modeling, corrugated boards, homogenization method, Weibull distributions, packaging design<br />
Summary:<br />
The study presents a simplified Finite Element Method (FEM) modeling approach for large structures made of corrugated boards, focusing on customized packages. It utilizes a homogenization method to transform flute geometries into equivalent elastic models, reducing computational time. Correction factors are introduced for internal mechanisms, adjusting the effective elastic modulus and thickness in the presence of large deformations and contact. Two statistical Weibull distributions representing contact and buckling mechanisms in corrugated boards are derived experimentally and validated for computational efficiency. The statistical parameters obtained ($\beta_1 = 0.14$, $\beta_2 = 1.31) can be effectively used for simplistic representation. The research contributes to optimizing corrugated packaging design by simplifying FEM models for faster yet accurate simulations. <br /><br />Summary: <div>
arXiv:2507.02189v1 Announce Type: cross 
Abstract: This study presents a simplified FEM modeling approach suitable for large structures made of corrugated boards, such as customized packages, based on a homogenization method, which is combined with correction factors for internal mechanisms. The homogenization process reduces computational time by transforming flute geometries into equivalent elastic models. In large deformations and in the presence of contact for a given geometry, the effective elastic modulus in the thickness direction, as well as the effective thickness of the structure, are corrected by two statistical Weibull distributions representing the contact and buckling mechanisms in a corrugated board. The Weibull parameters are obtained via experimental analysis, and such a process is then validated. The results demonstrate that the statistical parameters ($\beta_1 = 0.14$, $\beta_2 = 1.31$) can be used for the simplistic representation of corrugated boards, being computationally efficient. This research contributes to the optimization of corrugated packaging design, specifically by simplifying FEM models for faster yet equally accurate simulations.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Scale Finite Element Method for Investigating Fiber Remodeling in Hypertrophic Cardiomyopathy</title>
<link>https://arxiv.org/abs/2507.02193</link>
<guid>https://arxiv.org/abs/2507.02193</guid>
<content:encoded><![CDATA[
<div> fiber disarray, hypertrophic cardiomyopathy, cellular abnormalities, cardiac pumping function, myocardium mechanics <br />
Summary: <br />
- Fiber disarray is a significant hallmark of hypertrophic cardiomyopathy (HCM) and is associated with various cardiac events such as heart failure.
- Heterogeneous distributions of hypercontractility, hypocontractility, and fibrosis contribute to the development of fiber disarray in the myocardium.
- The pattern of fiber disarray varies depending on the specific perturbation, providing insights into the progression of HCM.
- Higher fiber disarray near the epicardium compared to the endocardium suggests the role of regional myocardial mechanics in HCM development.
- Remodeled left ventricles (LVs) with fibrosis and hypocontractility exhibit declined cardiac performance, highlighting the structural and functional consequences of HCM. <br /> <div>
arXiv:2507.02193v1 Announce Type: cross 
Abstract: A significant hallmark of hypertrophic cardiomyopathy (HCM) is fiber disarray, which is associated with various cardiac events such as heart failure. Quantifying fiber disarray remains critical for understanding the disease s complex pathophysiology. This study investigates the role of heterogeneous HCM-induced cellular abnormalities in the development of fiber disarray and their subsequent impact on cardiac pumping function. Fiber disarray is predicted using a stress-based law to reorient myofibers and collagen within a multiscale finite element cardiac modeling framework, MyoFE. Specifically, the model is used to quantify the distinct impacts of heterogeneous distributions of hypercontractility, hypocontractility, and fibrosis on fiber disarray development and examines their effect on functional characteristics of the heart. Our results show that heterogenous cell level abnormalities highly disrupt the normal mechanics of myocardium and lead to significant fiber disarray. The pattern of disarray varies depending on the specific perturbation, offering valuable insights into the progression of HCM. Despite the random distribution of perturbed regions within the cardiac muscle, significantly higher fiber disarray is observed near the epicardium compared to the endocardium across all perturbed left ventricle (LV) models. This regional difference in fiber disarray, irrespective of perturbation severity, aligns with previous DT-MRI studies, highlighting the role of regional myocardial mechanics in the development of fiber disarray. Furthermore, cardiac performance declined in the remodeled LVs, particularly in those with fibrosis and hypocontractility. These findings provide important insights into the structural and functional consequences of HCM and offer a framework for future investigations into therapeutic interventions targeting cardiac remodeling.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Effective Elastic Modulus and Thickness of Corrugated Boards Using Gaussian Process Regression and Expected Hypervolume Improvement</title>
<link>https://arxiv.org/abs/2507.02208</link>
<guid>https://arxiv.org/abs/2507.02208</guid>
<content:encoded><![CDATA[
<div> Keywords: hypersurface modeling, effective elastic modulus, thickness, corrugated boards, Gaussian Process Regression 

Summary: 
Latin Hypercube Sampling (LHS) combined with Gaussian Process Regression (GP) and Enhanced Expected Hypervolume Improvement (EHVI) were used to model the hypersurface of the effective elastic modulus and thickness in corrugated boards. Accurate modeling of these properties is crucial for optimizing the mechanical properties of corrugated materials. LHS efficiently samples the input space, while GP adapts to the complexity of response surfaces by incorporating prediction and uncertainty. Points are generated and evaluated based on the complexity of hypersurfaces, with emphasis on points with higher variance. The performance evaluation showed that GP with EHVI had improved accuracy, indicated by lower Mean Squared Error (MSE) values for the effective elastic modulus and thickness predictions. This approach demonstrates potential for future applications in structural optimization. 

Summary: <div>
arXiv:2507.02208v1 Announce Type: cross 
Abstract: This work aims to model the hypersurface of the effective elastic modulus, \( E_{z, \text{eff}} \), and thickness, \( th_{\text{eff}} \), in corrugated boards. A Latin Hypercube Sampling (LHS) is followed by Gaussian Process Regression (GP), enhanced by EHVI as a multi-objective acquisition function. Accurate modeling of \( E_{z, \text{eff}} \) and \( th_{\text{eff}} \) is critical for optimizing the mechanical properties of corrugated materials in engineering applications. LHS provides an efficient and straightforward approach for an initial sampling of the input space; GP is expected to be able to adapt to the complexity of the response surfaces by incorporating both prediction and uncertainty. Therefore, the next points being generated and evaluated are based on the complexity of the hypersurfaces, and some points, especially those with higher variance, are more exploited and carry more importance. The performance of GP with EHVI is measured by Mean Squared Error (MSE). Prediction of GP resulted in \( \text{MSE}(E_{z, \text{eff}}) = 5.24 \, \text{kPa}^2 \) and \( \text{MSE}(th_{\text{eff}}) = 1 \, \text{mm}^2 \). GP possesses then improved accuracy and adaptability for future applications in structural optimization.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in Particle Flow Filters with Taylor Expansion Series</title>
<link>https://arxiv.org/abs/2505.01597</link>
<guid>https://arxiv.org/abs/2505.01597</guid>
<content:encoded><![CDATA[
<div> Particle Flow Filters, Measurement Update, Drift Term, Diffusion Term, High-Order Polynomial Expansions<br />
<br />
Summary:<br />
Particle Flow Filters update measurements by moving particles rather than adjusting weights based on likelihood. This study introduces a new derivation method using high-order polynomial expansions, improving upon linearization techniques. The technique utilizes differential algebra to derive high-order particle flows directly onto polynomial representations of distributions. Two new particle flow filters are proposed, differing in the selection of the expansion center for Taylor polynomial evaluations. Numerical experiments demonstrate enhanced performance, particularly compared to Gromov flow and "exact" flow. <div>
arXiv:2505.01597v2 Announce Type: replace 
Abstract: Particle Flow Filters perform the measurement update by moving particles to a different location rather than modifying the particles' weight based on the likelihood. Their movement (flow) is dictated by a drift term, which continuously pushes the particle toward the posterior distribution, and a diffusion term, which guarantees the spread of particles. This work presents a novel derivation of these terms based on high-order polynomial expansions, where the common techniques based on linearization reduce to a simpler version of the new methodology. Thanks to differential algebra, the high-order particle flow is derived directly onto the polynomials representation of the distribution, embedded with differentiation and evaluation. The resulting technique proposes two new particle flow filters, whose difference relies on the selection of the expansion center for the Taylor polynomial evaluation. Numerical applications show the improvement gained by the inclusion of high-order terms, especially when comparing performance with the Gromov flow and the "exact" flow.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPC-AI Coupling Methodology for Scientific Applications</title>
<link>https://arxiv.org/abs/2507.01025</link>
<guid>https://arxiv.org/abs/2507.01025</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, High-performance computing, Coupling, Materials science, Scientific discovery

Summary: 
This study explores the integration of high-performance computing (HPC) and artificial intelligence (AI) in scientific applications, focusing on three coupling patterns: surrogate, directive, and coordinate. Through case studies in materials science, the effectiveness of these patterns is demonstrated, highlighting technical challenges, performance improvements, and implementation details. The proposed coupling patterns offer valuable guidance for future HPC-AI ensembles in various scientific domains, not limited to materials science. The study emphasizes the transformation of numerical-based HPC applications with data-driven AI approaches to address computational intensity challenges. This research provides insight into promising perspectives for HPC-AI coupling and its potential impact on scientific discovery. <div>
arXiv:2507.01025v1 Announce Type: new 
Abstract: Artificial intelligence (AI) technologies have fundamentally transformed numerical-based high-performance computing (HPC) applications with data-driven approaches and endeavored to address existing challenges, e.g. high computational intensity, in various scientific domains. In this study, we explore the scenarios of coupling HPC and AI (HPC-AI) in the context of emerging scientific applications, presenting a novel methodology that incorporates three patterns of coupling: surrogate, directive, and coordinate. Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite, and typical HPC-AI ensembles. Through case studies in materials science, we demonstrate the application and effectiveness of these patterns. The study highlights technical challenges, performance improvements, and implementation details, providing insight into promising perspectives of HPC-AI coupling. The proposed coupling patterns are applicable not only to materials science but also to other scientific domains, offering valuable guidance for future HPC-AI ensembles in scientific discovery.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI in Product Management: A Co-Evolutionary Model</title>
<link>https://arxiv.org/abs/2507.01069</link>
<guid>https://arxiv.org/abs/2507.01069</guid>
<content:encoded><![CDATA[
<div> agentic AI, product management, co-evolutionary framework, systems theory, human-AI interaction theory

Summary: 
This study delves into the transformative impact of agentic AI on product management, proposing a conceptual framework for its integration throughout the product lifecycle. Agentic AI, known for its autonomy and goal-driven behavior, reshapes the role of product managers (PMs) as orchestrators within socio-technical ecosystems. The framework, drawing on systems theory, co-evolutionary theory, and human-AI interaction theory, outlines the capabilities of agentic AI in various stages of product development. Through an integrative review of 70+ sources, the study showcases the evolving roles of PMs in overseeing AI operations, aligning strategies, and ensuring effective integration. A key finding emphasizes the need for PMs to enhance their skills in AI literacy, governance, and systems thinking to facilitate mutual adaptation between PMs and AI. This study lays the groundwork for future research and practical implementation to promote responsible and efficient integration of agentic AI in software organizations. 

<br /><br />Summary: <div>
arXiv:2507.01069v1 Announce Type: new 
Abstract: This study explores agentic AI's transformative role in product management, proposing a conceptual co-evolutionary framework to guide its integration across the product lifecycle. Agentic AI, characterized by autonomy, goal-driven behavior, and multi-agent collaboration, redefines product managers (PMs) as orchestrators of socio-technical ecosystems. Using systems theory, co-evolutionary theory, and human-AI interaction theory, the framework maps agentic AI capabilities in discovery, scoping, business case development, development, testing, and launch. An integrative review of 70+ sources, including case studies from leading tech firms, highlights PMs' evolving roles in AI orchestration, supervision, and strategic alignment. Findings emphasize mutual adaptation between PMs and AI, requiring skills in AI literacy, governance, and systems thinking. Addressing gaps in traditional frameworks, this study provides a foundation for future research and practical implementation to ensure responsible, effective agentic AI integration in software organizations.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatially Distributed Wettability Characterization in Porous Media</title>
<link>https://arxiv.org/abs/2507.01617</link>
<guid>https://arxiv.org/abs/2507.01617</guid>
<content:encoded><![CDATA[
<div> contact angle measurement, micro-CT images, wettability heterogeneity, spatially distributed, open-source tools<br />
Summary:
An enhanced geometric algorithm for automated contact angle measurement from micro-CT images is introduced, offering improved accuracy through robust interface extrapolation. The method generates contact angle maps revealing wettability heterogeneity in mixed-wet systems. Analysis shows that averaged metrics can mask significant variability within samples; a seemingly uniformly weakly water-wet sample may exhibit substantial intermediate-wetting regions. This variation impacts pore-filling mechanisms and interface structure. The study's open-source tools enable precise spatial wettability characterization, enhancing predictions of multiphase flow behavior in porous materials. Such insights are crucial for optimizing subsurface energy processes. <div>
arXiv:2507.01617v1 Announce Type: new 
Abstract: An enhanced geometric algorithm for automated pore-by-pore contact angle measurement from micro-CT images, is presented that achieves superior accuracy compared to existing methods through robust fluid-fluid and solid-fluid interface extrapolation. Using this high resolution data, we generate spatially distributed contact angle maps that reveal previously hidden wettability heterogeneity. Our analysis of mixed-wet systems demonstrates the severe limitations of averaged metrics: a sample with a mean contact angle of 64.7 degrees, conventionally classified as uniformly weakly water-wet, exhibits 40% of its pore space in the intermediate-wetting regime (70-110 degrees). This heterogeneity explains the presence of minimal surface interfaces and fundamentally different pore-filling mechanisms operating within the same sample. By providing open-source tools for spatially-resolved wettability characterization, this work enables more accurate predictions of multiphase flow behavior in heterogeneous porous materials, essential for optimizing subsurface energy storage and recovery processes.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified Levenberg-Marquardt method for estimating the elastic material parameters of polymer waveguides using residuals between autocorrelated frequency responses</title>
<link>https://arxiv.org/abs/2507.01706</link>
<guid>https://arxiv.org/abs/2507.01706</guid>
<content:encoded><![CDATA[
<div> optimization, ultrasound, polymers, elasticity, material parameters
Summary:
This article addresses the estimation of frequency-dependent elastic parameters of polymers in the ultrasound range as an inverse problem. The approach involves fitting simulation signals to measurement signals of displacement responses in hollow cylindrical waveguides for efficiency. Two novel methods are proposed to accelerate optimization: an adapted Levenberg-Marquardt method and an improved objective function based on autocorrelated envelopes of signals. Realistic material parameter ranges are considered for reproducibility. The study focuses on isotropic materials, showing that the proposed methods reduce the total number of model evaluations, speeding up parameter identification. <div>
arXiv:2507.01706v1 Announce Type: new 
Abstract: In this contribution, we address the estimation of the frequency-dependent elastic parameters of polymers in the ultrasound range, which is formulated as an inverse problem. This inverse problem is implemented as a nonlinear regression-type optimization problem, in which the simulation signals are fitted to the measurement signals. These signals consist of displacement responses in waveguides, focusing on hollow cylindrical geometries to enhance the simulation efficiency. To accelerate the optimization and reduce the number of model evaluations and wait times, we propose two novel methods. First, we introduce an adaptation of the Levenberg-Marquardt method derived from a geometrical interpretation of the least-squares optimization problem. Second, we introduce an improved objective function based on the autocorrelated envelopes of the measurement and simulation signals. Given that this study primarily relies on simulation data to quantify optimization convergence, we aggregate the expected ranges of realistic material parameters and derive their distributions to ensure the reproducibility of optimizations with proper measurements. We demonstrate the effectiveness of our objective function modification and step adaptation for various materials with isotropic material symmetry by comparing them with a state-of-the-art optimization method. In all cases, our method reduces the total number of model evaluations, thereby shortening the time to identify the material parameters.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relevance of the Basset history term for Lagrangian particle dynamics</title>
<link>https://arxiv.org/abs/2407.01041</link>
<guid>https://arxiv.org/abs/2407.01041</guid>
<content:encoded><![CDATA[
<div> Keywords: Maxey-Riley equation, fluid dynamics, Lagrangian dynamics, clustering patterns, turbulent flow <br />
Summary: <br />
The study focuses on the impact of the integral "history term" in the Maxey-Riley equation (MRE) on the movement of finite spherical particles in fluid dynamics. Numerical computations were carried out to compare trajectories with and without the history term for a large number of particles in various flow fields. The findings reveal that neglecting the history term significantly affects clustering patterns, especially for moderate to large Stokes numbers. Additionally, the computation of finite-time Lyapunov exponents demonstrates that even for small particles, disregarding the history term leads to notable differences in the resulting scalar field, particularly in turbulent flows. This highlights the importance of considering the history term in the MRE for accurately predicting the Lagrangian dynamics of particles in fluid systems. <br /> <div>
arXiv:2407.01041v3 Announce Type: replace-cross 
Abstract: The movement of small but finite spherical particles in a fluid can be described by the Maxey-Riley equation (MRE) if they are too large to be considered passive tracers. The MRE contains an integral "history term" modeling wake effects, which causes the force acting on a particle at some given time to depend on its full past trajectory. The history term causes complications in the numerical solution of the MRE and is therefore often neglected, despite both numerical and experimental evidence that its effects are generally not negligible. By numerically computing trajectories with and without the history term of a large number of particles in different flow fields, we investigate its impact on the large-scale Lagrangian dynamics of simulated particles. We show that for moderate to large Stokes numbers, ignoring the history term leads to significant differences in clustering patterns. Furthermore, we compute finite-time Lyapunov exponents and show that, even for small particles, the differences in the resulting scalar field from ignoring the BHT can be significant, in particular if the underlying flow is turbulent.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-design of magnetic soft robots with large deformation and contacts via material point method and topology optimization</title>
<link>https://arxiv.org/abs/2503.22767</link>
<guid>https://arxiv.org/abs/2503.22767</guid>
<content:encoded><![CDATA[
<div> soft robots, magnetic particles, topology optimization, magneto-elastic dynamics, autonomous design

Summary:
The article introduces a topology optimization framework for magnetic soft robots embedded with hard magnetic particles. This framework simultaneously designs structures, material magnetization, and time-varying magnetic stimuli to achieve target behaviors such as shape morphing and locomotion. It integrates generalized topology optimization with the magneto-elastic material point method, allowing for dynamic motion and solid contacts. The framework is computationally efficient, completing all design cases within minutes. It enables the autonomous co-design of active soft materials for various tasks, including metasurfaces, drug delivery, and minimally invasive procedures. <div>
arXiv:2503.22767v2 Announce Type: replace-cross 
Abstract: Magnetic soft robots embedded with hard magnetic particles enable untethered actuation via external magnetic fields, offering remote, rapid, and precise control, which is highly promising for biomedical applications. However, designing such systems is challenging due to the complex interplay of magneto-elastic dynamics, large deformation, solid contacts, time-varying stimuli, and posture-dependent loading. As a result, most existing research relies on heuristics and trial-and-error methods or focuses on the independent design of stimuli or structures under static conditions. We propose a topology optimization framework for magnetic soft robots that simultaneously designs structures, location-specific material magnetization and time-varying magnetic stimuli, accounting for large deformations, dynamic motion, and solid contacts. This is achieved by integrating generalized topology optimization with the magneto-elastic material point method, which supports GPU-accelerated parallel simulations and auto-differentiation for sensitivity analysis. We applied this framework to design magnetic robots for various tasks, including multi-task shape morphing and locomotion, in both 2D and 3D. The method autonomously generates optimized robotic systems to achieve target behaviors without requiring human intervention. Despite the nonlinear physics and large design space, it demonstrates high computational efficiency, completing all cases within minutes. The framework provides a computational foundation for the autonomous co-design of active soft materials in applications such as metasurfaces, drug delivery, and minimally invasive procedures.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Kalman Filter for Data Assimilation coupled with low-resolution computations techniques applied in Fluid Dynamics</title>
<link>https://arxiv.org/abs/2507.00539</link>
<guid>https://arxiv.org/abs/2507.00539</guid>
<content:encoded><![CDATA[
<div> Ensemble Kalman Filter, Reduced-Order Model, Data Assimilation, Fluid Dynamics, Computational Efficiency <br />
<br />
Summary: 
This paper introduces an innovative Reduced-Order Model (ROM) that merges experimental and simulation data using Data Assimilation (DA) to estimate the "True" state of fluid dynamics systems. The methodology incorporates the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework to improve prediction accuracy. To address the computational demands, the ROM employs low-resolution techniques, such as downsampling datasets and utilizing low-cost Singular Value Decomposition (lcSVD) for advanced reconstruction. Results show significant reductions in computation time and RAM usage without sacrificing accuracy. The EnKF is effective in estimating and predicting fluid flow systems based on limited observations and low-fidelity data. The proposed DA method shows promise in improving computational efficiency in CFD and related fields, making it suitable for large-scale and real-time applications like environmental monitoring and aerospace. <div>
arXiv:2507.00539v1 Announce Type: new 
Abstract: This paper presents an innovative Reduced-Order Model (ROM) for merging experimental and simulation data using Data Assimilation (DA) to estimate the "True" state of a fluid dynamics system, leading to more accurate predictions. Our methodology introduces a novel approach implementing the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework, grounded in a robust theoretical foundation and applied to fluid dynamics. To address the substantial computational demands of DA, the proposed ROM employs low-resolution (LR) techniques to drastically reduce computational costs. This approach involves downsampling datasets for DA computations, followed by an advanced reconstruction technique based on low-cost Singular Value Decomposition (lcSVD). The lcSVD method, a key innovation in this paper, has never been applied to DA before and offers a highly efficient way to enhance resolution with minimal computational resources. Our results demonstrate significant reductions in both computation time and RAM usage through the LR techniques without compromising the accuracy of the estimations. For instance, in a turbulent test case, the LR approach with a compression rate of 15.9 can achieve a speed-up of 13.7 and a RAM compression of 90.9% while maintaining a low Relative Root Mean Square Error (RRMSE) of 2.6%, compared to 0.8% in the high-resolution (HR) reference. Furthermore, we highlight the effectiveness of the EnKF in estimating and predicting the state of fluid flow systems based on limited observations and low-fidelity numerical data. This paper highlights the potential of the proposed DA method in fluid dynamics applications, particularly for improving computational efficiency in CFD and related fields. Its ability to balance accuracy with low computational and memory costs makes it suitable for large-scale and real-time applications, such as environmental monitoring or aerospace.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark</title>
<link>https://arxiv.org/abs/2507.00034</link>
<guid>https://arxiv.org/abs/2507.00034</guid>
<content:encoded><![CDATA[
<div> neural network, critical heat flux, non-uniform heating, spatial power profiles, transfer-learning

Summary:
- The study compiles a comprehensive dataset on critical heat flux (CHF) under both uniform and non-uniform axial heating conditions to support Phase II of the OECD/NEA AI/ML CHF benchmark.
- Classical CHF correlations show errors under uniform heating and struggle with non-uniform profiles, while modern tabular methods offer improved but imperfect predictions.
- A neural network trained on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, emphasizing the need for models that consider axial power distributions.
- The curated datasets and modeling results provided in this study set the stage for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark. <br /><br />Summary: <div>
arXiv:2507.00034v1 Announce Type: cross 
Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water reactors, defining safe thermal-hydraulic operating limits. To support Phase II of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power profiles, this work compiles and digitizes a broad CHF dataset covering both uniform and non-uniform axial heating conditions. Heating profiles were extracted from technical reports, interpolated onto a consistent axial mesh, validated via energy-balance checks, and encoded in machine-readable formats for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating and degrade markedly when applied to non-uniform profiles, while modern tabular methods offer improved but still imperfect predictions. A neural network trained solely on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, underscoring the need for models that explicitly incorporate axial power distributions. By providing these curated datasets and baseline modeling results, this study lays the groundwork for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process</title>
<link>https://arxiv.org/abs/2507.00046</link>
<guid>https://arxiv.org/abs/2507.00046</guid>
<content:encoded><![CDATA[
<div> evolutionary computing, image segmentation, Additive Friction Stir Deposition, Particle Swarm Optimization, defect detection

Summary:
Evolutionary computing-based image segmentation using Particle Swarm Optimization (PSO) was proposed for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. The methodology integrates gradient magnitude analysis with distance transforms to create attention-weighted visualizations highlighting critical interface regions. Multiple visualization techniques, including self-attention maps and multi-channel visualization, were applied to five AFSD samples to detect material transitions and potential defects. PSO algorithm determined optimal threshold values for precise segmentation. The multi-channel visualization technique combined boundary information, spatial relationships, and material density data for cohesive representations. Attention-based analysis successfully identified incomplete bonding regions and inhomogeneities in AFSD joints, offering quantitative metrics for process optimization and quality assessment of additively manufactured components. <br /><br />Summary: <div>
arXiv:2507.00046v1 Announce Type: cross 
Abstract: This work proposes an evolutionary computing-based image segmentation approach for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. Particle Swarm Optimization (PSO) was employed to determine optimal segmentation thresholds for detecting defects and features in multilayer AFSD builds. The methodology integrates gradient magnitude analysis with distance transforms to create novel attention-weighted visualizations that highlight critical interface regions. Five AFSD samples processed under different conditions were analyzed using multiple visualization techniques i.e. self-attention maps, and multi-channel visualization. These complementary approaches reveal subtle material transition zones and potential defect regions which were not readily observable through conventional imaging. The PSO algorithm automatically identified optimal threshold values (ranging from 156-173) for each sample, enabling precise segmentation of material interfaces. The multi-channel visualization technique effectively combines boundary information (red channel), spatial relationships (green channel), and material density data (blue channel) into cohesive representations that quantify interface quality. The results demonstrate that attention-based analysis successfully identifies regions of incomplete bonding and inhomogeneities in AFSD joints, providing quantitative metrics for process optimization and quality assessment of additively manufactured components.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A collaborative digital twin built on FAIR data and compute infrastructure</title>
<link>https://arxiv.org/abs/2507.00048</link>
<guid>https://arxiv.org/abs/2507.00048</guid>
<content:encoded><![CDATA[
<div> machine learning, automated experimentation, self-driving laboratories, FAIR data infrastructure, optimization

Summary:
The article discusses the integration of machine learning and automated experimentation in self-driving laboratories (SDL) to accelerate discovery and optimization tasks in science and engineering. By utilizing findable, accessible, interoperable, and reusable (FAIR) data infrastructure, geographically dispersed researchers can collaborate effectively within a distributed SDL implementation on nanoHUB services. The framework allows for the sharing of raw experimental data in a central database, enabling researchers to benefit from analysis tools and machine learning models that update automatically with new data. A separate workflow on nanoHUB facilitates sequential optimization through active learning, where researchers define the optimization objective and machine learning models guide future experiment selection. The article presents the application of these concepts in an optimization task involving food dyes, showing how researchers and students can conduct experiments, share data, and explore the combination of FAIR data, predictive ML models, and sequential optimization in a cost-effective manner. <div>
arXiv:2507.00048v1 Announce Type: cross 
Abstract: The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively. This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management. In this framework, geographically dispersed collaborators conducting independent optimization tasks contribute raw experimental data to a shared central database. These researchers can then benefit from analysis tools and machine learning models that automatically update as additional data become available. New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments. Inspired by the concept of ``frugal twin", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color. With easily accessible and inexpensive materials, researchers and students can set up their own experiments, share data with collaborators, and explore the combination of FAIR data, predictive ML models, and sequential optimization. The tools introduced are generally applicable and can easily be extended to other optimization problems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>