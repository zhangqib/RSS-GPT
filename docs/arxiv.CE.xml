<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CE updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CE</link>


<item>
<title>Connectomics Informed by Large Language Models</title>
<link>https://arxiv.org/abs/2511.05383</link>
<guid>https://arxiv.org/abs/2511.05383</guid>
<content:encoded><![CDATA[
<div> Tractography, white matter connections, prior knowledge, large language models, connectomics <br />
<br />Summary:
Tractography, a method for mapping white matter connections in the brain, faces accuracy limitations due to a trade-off between sensitivity and specificity. This study develops a pipeline using large language models (LLMs) to generate quantitative priors for connectomics based on neuroanatomy knowledge. By benchmarking against a gold-standard tractography atlas, accurate connectivity information is obtained from the LLMs using specific prompting techniques. Incorporating external knowledge sources enhances accuracy by grounding the LLM. The LLM-derived priors augment existing tractography filtering approaches by identifying true-positive connections for retention, improving the accuracy of a connectome-based model of pathology spread. This study demonstrates that the connections retained by the LLM are valid, providing valuable insights for improving tractography algorithms and connectomics research. <br /> <div>
arXiv:2511.05383v1 Announce Type: new 
Abstract: Tractography is a unique method for mapping white matter connections in the brain, but tractography algorithms suffer from an inherent trade-off between sensitivity and specificity that limits accuracy. Incorporating prior knowledge of white matter anatomy is an effective strategy for improving accuracy and has been successful for reducing false positives and false negatives in bundle-mapping protocols. However, it is challenging to scale this approach for connectomics due to the difficulty in synthesising information relating to many thousands of possible connections. In this work, we develop and evaluate a pipeline using large language models (LLMs) to generate quantitative priors for connectomics, based on their knowledge of neuroanatomy. We benchmark our approach against an evaluation set derived from a gold-standard tractography atlas, identifying prompting techniques to elicit accurate connectivity information from the LLMs. We further identify strategies for incorporating external knowledge sources into the pipeline, which can provide grounding for the LLM and improve accuracy. Finally, we demonstrate how the LLM-derived priors can augment existing tractography filtering approaches by identifying true-positive connections to retain during the filtering process. We show that these additional connections can improve the accuracy of a connectome-based model of pathology spread, which provides supporting evidence that the connections preserved by the LLM are valid.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block-structured Operator Inference for coupled multiphysics model reduction</title>
<link>https://arxiv.org/abs/2511.05389</link>
<guid>https://arxiv.org/abs/2511.05389</guid>
<content:encoded><![CDATA[
<div> Operator Inference, Structured Reduced-Order Models, Multiphysics Systems, Block-Structured Formulation, Aeroelastic Analysis<br />
<br />Summary: This paper introduces a block-structured approach to Operator Inference for learning structured reduced-order models in multiphysics systems. By specifying the governing equation and coupling term structures for each physics component, it preserves the physical system structure while reducing the overall dimensionality of the learning problem. The method allows tailored regularization for each physics component and maintains system properties like stability and second-order structure. In aeroelastic analysis, the block-structured formulation outperforms monolithic Operator Inference by providing a 20% online prediction speedup for the AGARD 445.6 wing case study. It retains accuracy while improving computational efficiency, making it a promising approach for modeling complex multiphysics systems. <br /><br /> <div>
arXiv:2511.05389v1 Announce Type: new 
Abstract: This paper presents a block-structured formulation of Operator Inference as a way to learn structured reduced-order models for multiphysics systems. The approach specifies the governing equation structure for each physics component and the structure of the coupling terms. Once the multiphysics structure is specified, the reduced-order model is learned from snapshot data following the nonintrusive Operator Inference methodology. In addition to preserving physical system structure, which in turn permits preservation of system properties such as stability and second-order structure, the block-structured approach has the advantages of reducing the overall dimensionality of the learning problem and admitting tailored regularization for each physics component. The numerical advantages of the block-structured formulation over a monolithic Operator Inference formulation are demonstrated for aeroelastic analysis, which couples aerodynamic and structural models. For the benchmark test case of the AGARD 445.6 wing, block-structured Operator Inference provides an average 20% online prediction speedup over monolithic Operator Inference across subsonic and supersonic flow conditions in both the stable and fluttering parameter regimes while preserving the accuracy achieved with monolithic Operator Inference.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Alignment between Visual Stimuli and Neural Responses in the Visual Cortex</title>
<link>https://arxiv.org/abs/2511.04096</link>
<guid>https://arxiv.org/abs/2511.04096</guid>
<content:encoded><![CDATA[
<div> Keywords: visual stimuli, neural responses, visual cortex, discriminative encoding, discriminative decoding

Summary: 
This paper explores the mapping between visual stimuli and neural responses in the visual cortex through discriminative encoding and decoding tasks, introducing a novel approach called Visual-Neural Alignment (VNA). The study shifts from traditional direct encoding and decoding methods to address challenges related to neural response variability and recording limitations. Experiments conducted on invasive visual cortex datasets involving mice and macaques demonstrate that VNA outperforms direct encoding and decoding approaches. The results indicate that VNA offers a more precise characterization of the visual-neural mapping, highlighting its potential for advancing understanding of biological visual processing mechanisms. The proposed approach provides a valuable contribution to the field of visual neuroscience research by offering a more robust and generalized methodology for studying the relationship between visual stimuli and neural responses in the visual cortex. 

<br /><br />Summary: <div>
arXiv:2511.04096v1 Announce Type: new 
Abstract: Investigating the mapping between visual stimuli and neural responses in the visual cortex contributes to a deeper understanding of biological visual processing mechanisms. Most existing studies characterize this mapping by training models to directly encode visual stimuli into neural responses or decode neural responses into visual stimuli. However, due to neural response variability and limited neural recording techniques, these studies suffer from overfitting and lack generalizability. Motivated by this challenge, in this paper we shift the tasks from conventional direct encoding and decoding to discriminative encoding and decoding, which are more reasonable. And on top of this we propose a cross-modal alignment approach, named Visual-Neural Alignment (VNA). To thoroughly test the performance of the three methods (direct encoding, direct decoding, and our proposed VNA) on discriminative encoding and decoding tasks, we conduct extensive experiments on three invasive visual cortex datasets, involving two types of subject mammals (mice and macaques). The results demonstrate that our VNA generally outperforms direct encoding and direct decoding, indicating our VNA can most precisely characterize the above visual-neural mapping among the three methods.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fitting Reinforcement Learning Model to Behavioral Data under Bandits</title>
<link>https://arxiv.org/abs/2511.04454</link>
<guid>https://arxiv.org/abs/2511.04454</guid>
<content:encoded><![CDATA[
<div> optimization, reinforcement learning, multi-armed bandit, convexity, Python package

Summary:
The paper addresses the problem of fitting reinforcement learning (RL) models to behavioral data in multi-armed bandit environments. It introduces a mathematical optimization formulation for a wide range of RL models and analyzes their convexity properties. A novel solution method based on convex relaxation and optimization is proposed, showing comparable performance to benchmark methods with reduced computation time. The method is evaluated in simulated bandit environments and an open-source Python package is provided for easy implementation by researchers. <div>
arXiv:2511.04454v1 Announce Type: new 
Abstract: We consider the problem of fitting a reinforcement learning (RL) model to some given behavioral data under a multi-armed bandit environment. These models have received much attention in recent years for characterizing human and animal decision making behavior. We provide a generic mathematical optimization problem formulation for the fitting problem of a wide range of RL models that appear frequently in scientific research applications, followed by a detailed theoretical analysis of its convexity properties. Based on the theoretical results, we introduce a novel solution method for the fitting problem of RL models based on convex relaxation and optimization. Our method is then evaluated in several simulated bandit environments to compare with some benchmark methods that appear in the literature. Numerical results indicate that our method achieves comparable performance to the state-of-the-art, while significantly reducing computation time. We also provide an open-source Python package for our proposed method to empower researchers to apply it in the analysis of their datasets directly, without prior knowledge of convex optimization.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvSim algorithm for pre-computing airplane flight controls in limited-range autonomous missions, and demonstration via double-roll maneuver of Mirage III fighters</title>
<link>https://arxiv.org/abs/2511.03745</link>
<guid>https://arxiv.org/abs/2511.03745</guid>
<content:encoded><![CDATA[
<div> mathematical framework, flight mechanics, inverse simulation, numerical procedure, fixed-wing aircraft
Summary:
- The article presents a mathematical framework for the equations of motion (EOM) in flight mechanics with six degrees of freedom for a fixed-wing aircraft.
- The framework incorporates body, inertial, and wind axes, spherical flight path and flight angles.
- A customized version for inverse simulation flight mechanics is derived, predicting necessary flight controls to achieve a target trajectory.
- A numerical procedure for integrating the inverse simulation system in time is presented, utilizing symbolic mathematics and numerical integration techniques.
- The calculated control values enable the aircraft to achieve the desired flight trajectory specified by inertial Cartesian coordinates and Euler's roll angle. 
Summary: <div>
arXiv:2511.03745v1 Announce Type: cross 
Abstract: In this work, we start with a generic mathematical framework for the equations of motion (EOM) in flight mechanics with six degrees of freedom (6-DOF) for a general (not necessarily symmetric) fixed-wing aircraft. This mathematical framework incorporates (1) body axes (fixed in the airplane at its center of gravity), (2) inertial axes (fixed in the earth/ground at the take-off point), wind axes (aligned with the flight path/course), (3) spherical flight path angles (azimuth angle measured clockwise from the geographic north, and elevation angle measured above the horizon plane), and (4) spherical flight angles (angle of attack and sideslip angle). We then manipulate these equations of motion to derive a customized version suitable for inverse simulation flight mechanics, where a target flight trajectory is specified while a set of corresponding necessary flight controls to achieve that maneuver are predicted. We then present a numerical procedure for integrating the developed inverse simulation (InvSim) system in time; utilizing (1) symbolic mathematics, (2) explicit fourth-order Runge-Kutta (RK4) numerical integration technique, and (3) expressions based on the finite difference method (FDM); such that the four necessary control variables (engine thrust force, ailerons' deflection angle, elevators' deflection angle, and rudder's deflection angle) are computed as discrete values over the entire maneuver time, and these calculated control values enable the airplane to achieve the desired flight trajectory, which is specified by three inertial Cartesian coordinates of the airplane, in addition to the Euler's roll angle. We finally demonstrate the proposed numerical procedure of flight mechanics inverse simulation (InvSim).
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices</title>
<link>https://arxiv.org/abs/2511.03753</link>
<guid>https://arxiv.org/abs/2511.03753</guid>
<content:encoded><![CDATA[
<div> framework, privacy-preserving, electrocardiogram, federated learning, IoT
Summary:<br />
This study introduces a federated learning framework for ECG classification in IoT healthcare settings, using 2D GAF images for feature extraction with CNNs while keeping sensitive data local. The experimental validation shows high accuracy of 95.18% across diverse IoT devices. The deployment on server, laptop, and Raspberry Pi 4 demonstrates edge-cloud integration in IoT. The FL-GAF model outperforms single-client setups in accuracy and training time. Despite the added complexity, the framework maintains efficiency in resource utilization and communication overhead. This research showcases the potential of lightweight, secure AI for monitoring in IoT-based healthcare, supporting scalable and secure edge deployments in smart health systems.<br /> <div>
arXiv:2511.03753v1 Announce Type: cross 
Abstract: This study presents a federated learning (FL) framework for privacy-preserving electrocardiogram (ECG) classification in Internet of Things (IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian Angular Field (GAF) images, the proposed approach enables efficient feature extraction through Convolutional Neural Networks (CNNs) while ensuring that sensitive medical data remain local to each device. This work is among the first to experimentally validate GAF-based federated ECG classification across heterogeneous IoT devices, quantifying both performance and communication efficiency. To evaluate feasibility in realistic IoT settings, we deployed the framework across a server, a laptop, and a resource-constrained Raspberry Pi 4, reflecting edge-cloud integration in IoT ecosystems. Experimental results demonstrate that the FL-GAF model achieves a high classification accuracy of 95.18% in a multi-client setup, significantly outperforming a single-client baseline in both accuracy and training time. Despite the added computational complexity of GAF transformations, the framework maintains efficient resource utilization and communication overhead. These findings highlight the potential of lightweight, privacy-preserving AI for IoT-based healthcare monitoring, supporting scalable and secure edge deployments in smart health systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure Code Generation at Scale with Reflexion</title>
<link>https://arxiv.org/abs/2511.03898</link>
<guid>https://arxiv.org/abs/2511.03898</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, secure code generation, zero-shot baseline, reflexion prompting, security metrics

Summary: 
Large language models (LLMs) are utilized for code drafting and refactoring, but ensuring secure code is crucial. This study assesses the generation of secure code using Instruct Prime, evaluating five instruction-tuned code LLMs through a zero-shot baseline and reflexion prompting approach. Insecurity is prevalent initially, with around 25-33% of programs being insecure at baseline (t0). Weak cryptography/config-dependent bugs pose challenges, while templated vulnerabilities such as XSS and code injection are handled more effectively. Python exhibits the highest secure rates, while C and C# show the lowest. Reflexion prompting enhances security across all models, with significant improvements in the first round. Applying one to two rounds yields the most benefits in terms of Repair, Regression, and NetGain metrics. The study's findings highlight the importance of prompt design in improving code security. <div>
arXiv:2511.03898v1 Announce Type: cross 
Abstract: Large language models (LLMs) are now widely used to draft and refactor code, but code that works is not necessarily secure. We evaluate secure code generation using the Instruct Prime, which eliminated compliance-required prompts and cue contamination, and evaluate five instruction-tuned code LLMs using a zero-shot baseline and a three-round reflexion prompting approach. Security is measured using the Insecure Code Detector (ICD), and results are reported by measuring Repair, Regression, and NetGain metrics, considering the programming language and CWE family. Our findings show that insecurity remains common at the first round: roughly 25-33% of programs are insecure at a zero-shot baseline (t0 ). Weak cryptography/config-dependent bugs are the hardest to avoid while templated ones like XSS, code injection, and hard-coded secrets are handled more reliably. Python yields the highest secure rates; C and C# are the lowest, with Java, JS, PHP, and C++ in the middle. Reflexion prompting improves security for all models, improving average accuracy from 70.74% at t0 to 79.43% at t3 , with the largest gains in the first round followed by diminishing returns. The trends with Repair, Regression, and NetGain metrics show that applying one to two rounds produces most of the benefits. A replication package is available at https://doi.org/10.5281/zenodo.17065846.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating scientific discovery with the common task framework</title>
<link>https://arxiv.org/abs/2511.04001</link>
<guid>https://arxiv.org/abs/2511.04001</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, artificial intelligence, dynamic systems, common task framework, objective metrics

Summary:
Machine learning and artificial intelligence algorithms are revolutionizing the characterization and control of dynamic systems in various scientific disciplines. The need for comparative metrics to evaluate these algorithms in tasks such as forecasting, state reconstruction, generalization, and control, considering limited data scenarios and noisy measurements, is crucial. The introduction of a common task framework (CTF) for science and engineering aims to provide a standardized platform with challenge datasets that address practical objectives. This framework facilitates the comparison of diverse algorithms rapidly being developed and deployed across science and engineering domains. By leveraging the CTF, researchers and practitioners can objectively assess the performance of ML/AI algorithms and drive advancements in traditional applications and emerging areas of research. <div>
arXiv:2511.04001v1 Announce Type: cross 
Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are transforming and empowering the characterization and control of dynamic systems in the engineering, physical, and biological sciences. These emerging modeling paradigms require comparative metrics to evaluate a diverse set of scientific objectives, including forecasting, state reconstruction, generalization, and control, while also considering limited data scenarios and noisy measurements. We introduce a common task framework (CTF) for science and engineering, which features a growing collection of challenge data sets with a diverse set of practical and common objectives. The CTF is a critically enabling technology that has contributed to the rapid advance of ML/AI algorithms in traditional applications such as speech recognition, language processing, and computer vision. There is a critical need for the objective metrics of a CTF to compare the diverse algorithms being rapidly developed and deployed in practice today across science and engineering.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared Spatial Memory Through Predictive Coding</title>
<link>https://arxiv.org/abs/2511.04235</link>
<guid>https://arxiv.org/abs/2511.04235</guid>
<content:encoded><![CDATA[
<div> Framework, Multi-agent, Coordination, Spatial memory, Social representations

Summary:
The article introduces a multi-agent predictive coding framework for enhancing coordination in multi-agent systems. The framework minimizes mutual uncertainty among agents by formulating coordination as an information bottleneck objective. Agents learn when, what, and who to communicate, with a focus on self-localization using an internal spatial coding similar to grid cells. Through a hierarchical reinforcement learning policy, agents develop a bandwidth-efficient communication mechanism and artificial social place cells that encode partners' locations. The approach demonstrates remarkable resilience to bandwidth constraints on the Memory-Maze benchmark, outperforming a full-broadcast baseline. The findings offer insights into how complex social representations can emerge from a unified predictive drive, leading to improved social collective intelligence. 

<br /><br />Summary: <div>
arXiv:2511.04235v1 Announce Type: cross 
Abstract: Sharing and reconstructing a consistent spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulate coordination as the minimization of mutual uncertainty among agents. Instantiated as an information bottleneck objective, it prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners' locations: an artificial analogue of hippocampal social place cells (SPCs). These social representations are further enacted by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to social collective intelligence.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the relationship between MESP and 0/1 D-Opt and their upper bounds</title>
<link>https://arxiv.org/abs/2511.04350</link>
<guid>https://arxiv.org/abs/2511.04350</guid>
<content:encoded><![CDATA[
<div> maximum entropy sampling, 0/1 optimization, experimental design, branch-and-bound, domination results

Summary: 
This article explores the relationship between two key nonlinear 0/1 optimization problems in experimental design: maximum entropy sampling (MESP) and 0/1 D-optimality. By establishing mappings between instances of these problems, the study investigates how upper-bounding methods can be transferred between the two, leading to the discovery of new domination results and inequalities. The research also delves into the comparison of different branch-and-bound schemes based on these mappings. Surprisingly, numerical experiments reveal that bounding methods previously deemed ineffective for real-data MESP instances can prove useful when applied to MESP instances derived from 0/1 D-Optimality. This study sheds light on the interconnected nature of these optimization problems and demonstrates the potential for cross-fertilization of techniques in the field of experimental design. 

<br /><br />Summary: <div>
arXiv:2511.04350v1 Announce Type: cross 
Abstract: We establish strong connections between two fundamental nonlinear 0/1 optimization problems coming from the area of experimental design, namely maximum entropy sampling and 0/1 D-Optimality. The connections are based on maps between instances, and we analyze the behavior of these maps. Using these maps, we transport basic upper-bounding methods between these two problems, and we are able to establish new domination results and other inequalities relating various basic upper bounds. Further, we establish results relating how different branch-and-bound schemes based on these maps compare. Additionally, we observe some surprising numerical results, where bounding methods that did not seem promising in their direct application to real-data MESP instances, are now useful for MESP instances that come from 0/1 D-Optimality.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition</title>
<link>https://arxiv.org/abs/2511.04461</link>
<guid>https://arxiv.org/abs/2511.04461</guid>
<content:encoded><![CDATA[
<div> ensemble, Hankel Dynamic Mode Decomposition, uncertainty-aware, seakeeping predictions, high-speed catamaran

Summary:
The study introduces Hankel Dynamic Mode Decomposition with control (HDMDc) for seakeeping predictions of a high-speed catamaran by creating a linear reduced-order model. Experimental wave basin test data from the Delft 372 model were used to train and test the HDMDc algorithm. Two ensembling strategies, Bayesian HDMDc (BHDMDc) and Frequentist HDMDc (FHDMDc), were compared for seakeeping prediction and uncertainty quantification. FHDMDc showed improved prediction accuracy and robust uncertainty estimation compared to the deterministic model. However, BHDMDc did not provide significant benefits in this case. FHDMDc-derived probability density functions closely matched experimental and computational results, demonstrating reliable seakeeping prediction for design and operational support. The study showcases the effectiveness of HDMDc in capturing nonlinear and memory effects for uncertainty-aware seakeeping predictions. 

<br /><br />Summary: <div>
arXiv:2511.04461v1 Announce Type: cross 
Abstract: In this study, we present and validate an ensemble-based Hankel Dynamic Mode Decomposition with control (HDMDc) for uncertainty-aware seakeeping predictions of a high-speed catamaran, namely the Delft 372 model. Experimental measurements (time histories) of wave elevation at the longitudinal center of gravity, heave, pitch, notional flight-deck velocity, notional bridge acceleration, and total resistance were collected from irregular wave basin tests on a 1:33.3 scale replica of the Delft 372 model under sea state 5 conditions at Fr = 0.425, and organized into training, validation, and test sets. The HDMDc algorithm constructs an equation-free linear reduced-order model of the seakeeping vessel by augmenting states and inputs with their time-lagged copies to capture nonlinear and memory effects. Two ensembling strategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparameters considered stochastic variables with prior distribution to produce posterior mean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), which aggregates multiple model obtained over data subsets, are compared in providing seakeeping prediction and uncertainty quantification. The FHDMDc approach is found to improve the accuracy of the predictions compared to the deterministic counterpart, also providing robust uncertainty estimation; whereas the application of BHDMDc to the present test case is not found beneficial in comparison to the deterministic model. FHDMDc-derived probability density functions for the motions closely match both experimental data and URANS results, demonstrating reliable and computationally efficient seakeeping prediction for design and operational support.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach</title>
<link>https://arxiv.org/abs/2511.04556</link>
<guid>https://arxiv.org/abs/2511.04556</guid>
<content:encoded><![CDATA[
<div> optimization, urban surface water flooding, sensor placement, peak flowrates, data-driven

Summary:
The study presents a data-driven sparse sensing (DSS) framework integrated with EPA-SWMM to optimize sensor placement and reconstruct peak flowrates in urban drainage networks. Using the Woodland Avenue catchment in Duluth, Minnesota, as a case study, the framework leverages singular value decomposition and QR factorization to identify optimal monitoring nodes based on a SWMM-generated training dataset. Three optimally placed sensors among 77 nodes achieved satisfactory peak flowrate reconstruction performance. The model exhibited robustness to uncertainty in measurements and sensor failures, improving with an increased number of sensors. Balancing computational efficiency and physical interpretability, the DSS framework enables high-accuracy flow reconstruction with minimal sensors. Integration of this framework with predictive models can facilitate flood early warning and real-time control in urban areas with limited sensing and monitoring resources.<br /><br />Summary: <div>
arXiv:2511.04556v1 Announce Type: cross 
Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming drainage systems, is increasingly frequent and widespread. While flood prediction and monitoring in high spatial-temporal resolution are desired, practical constraints in time, budget, and technology hinder its full implementation. How to monitor urban drainage networks and predict flow conditions under constrained resource is a major challenge. This study presents a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to optimize sensor placement and reconstruct peak flowrates in a stormwater system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case study. We utilized a SWMM model to generate a training dataset of peak flowrate profiles across the stormwater network. Furthermore, we applied DSS - leveraging singular value decomposition for dimensionality reduction and QR factorization for sensor allocation - to identify the optimal monitoring nodes based on the simulated training dataset. We then validated the representativeness of these identified monitoring nodes by comparing the DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three optimally placed sensors among 77 nodes achieved satisfactory reconstruction performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to 75th percentiles). In addition, the model showed good robustness to uncertainty in measurements. Its robustness to sensor failures is location-dependent and improves with the number of sensors deployed. The framework balances computational efficiency and physical interpretability, enabling high-accuracy flow reconstruction with minimal sensors. This DSS framework can be further integrated with predictive models to realize flood early warning and real-time control under limited sensing and monitoring resource.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Electron-Scale Turbulence Modeling in W7-X</title>
<link>https://arxiv.org/abs/2511.04567</link>
<guid>https://arxiv.org/abs/2511.04567</guid>
<content:encoded><![CDATA[
<div> machine-learning, reduced models, turbulent transport, ETG turbulence, W7-X stellarator

Summary:
- This paper presents machine-learning-driven reduced models for Electron Temperature Gradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator.
- The models predict ETG heat flux based on three plasma parameters: $\omega_{T_e}$, $\eta_e$, and $\tau.
- The models are constructed using regression and an active machine-learning-based procedure across seven radial locations.
- Evaluation using out-of-sample datasets shows robust performance with prediction intervals estimated via bootstrapping.
- Generalized reduced models are also developed and demonstrate accurate heat flux predictions even beyond the training domain.<br /><br />Summary: <div>
arXiv:2511.04567v1 Announce Type: cross 
Abstract: Constructing reduced models for turbulent transport is essential for accelerating profile predictions and enabling many-query tasks such as uncertainty quantification, parameter scans, and design optimization. This paper presents machine-learning-driven reduced models for Electron Temperature Gradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator. Each model predicts the ETG heat flux as a function of three plasma parameters: the normalized electron temperature radial gradient ($\omega_{T_e}$), the ratio of normalized electron temperature and density radial gradients ($\eta_e$), and the electron-to-ion temperature ratio ($\tau$). We first construct models across seven radial locations using regression and an active machine-learning-based procedure. This process initializes models using low-cardinality sparse-grid training data and then iteratively refines their training sets by selecting the most informative points from a pre-existing simulation database. We evaluate the prediction capabilities of our models using out-of-sample datasets with over $393$ points per location, and $95\%$ prediction intervals are estimated via bootstrapping to assess prediction uncertainty. We then investigate the construction of generalized reduced models, including a generic, position-independent model, and assess their heat flux prediction capabilities at three additional locations. Our models demonstrate robust performance and predictive accuracy comparable to the original reference simulations, even when applied beyond the training domain.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic causal discovery in Alzheimer's disease through latent pseudotime modelling</title>
<link>https://arxiv.org/abs/2511.04619</link>
<guid>https://arxiv.org/abs/2511.04619</guid>
<content:encoded><![CDATA[
<div> Keywords: causal discovery, Alzheimer's disease, latent variable model, pseudotime, dynamic interactions

Summary:
Causal discovery methods face limitations when applied to diseases like Alzheimer's due to static graph assumptions. A proposed solution involves using a latent variable model on real-world AD data to infer pseudotime, ordering patients along a disease trajectory independent of age. Pseudotime outperformed age in predicting diagnosis, demonstrating the model's efficacy. Incorporating minimal background knowledge improved graph accuracy and orientation. The framework unveiled dynamic interactions between novel and established AD markers, enabling practical causal discovery despite assumptions being violated. This study sheds light on the evolving pathophysiology of AD and provides valuable insights for future research and potential clinical applications. 

<br /><br />Summary: <div>
arXiv:2511.04619v1 Announce Type: cross 
Abstract: The application of causal discovery to diseases like Alzheimer's (AD) is limited by the static graph assumptions of most methods; such models cannot account for an evolving pathophysiology, modulated by a latent disease pseudotime. We propose to apply an existing latent variable model to real-world AD data, inferring a pseudotime that orders patients along a data-driven disease trajectory independent of chronological age, then learning how causal relationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC 0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge substantially improved graph accuracy and orientation. Our framework reveals dynamic interactions between novel (NfL, GFAP) and established AD markers, enabling practical causal discovery despite violated assumptions.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaboration Dynamics and Reliability Challenges of Multi-Agent LLM Systems in Finite Element Analysis</title>
<link>https://arxiv.org/abs/2408.13406</link>
<guid>https://arxiv.org/abs/2408.13406</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, multi-agent systems, Finite Element Analysis, collaboration effectiveness, verification reliability<br />
Summary:<br />
- The study evaluates the impact of inter-agent dynamics on reasoning quality and verification reliability in Large Language Model-based multi-agent systems for Finite Element Analysis.<br />
- Collaboration effectiveness is found to be more dependent on functional complementarity than team size.<br />
- Three-agent configurations like Coder-Executor-Critic showed the best performance in producing physically and visually correct solutions, while redundant reviewers reduced success rates.<br />
- Systematic failure modes like affirmation bias, premature consensus, and a verification-validation gap were identified in the study.<br />
- Design principles proposed include assigning complementary agent roles, enforcing multi-level validation, and preventing early consensus through interaction controls. <br /> <div>
arXiv:2408.13406v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based multi-agent systems are increasingly applied to automate computational workflows in science and engineering. However, how inter-agent dynamics influence reasoning quality and verification reliability remains unclear. We study these mechanisms using an AutoGen-based multi-agent framework for linear-elastic Finite Element Analysis (FEA), evaluating seven role configurations across four tasks under a fixed 12-turn conversation limit. From 1,120 controlled trials, we find that collaboration effectiveness depends more on functional complementarity than team size: the three-agent Coder-Executor-Critic configuration uniquely produced physically and visually correct solutions, while adding redundant reviewers reduced success rates. Yet three systematic failure modes persist: (1) affirmation bias, where the Rebuttal agent endorsed rather than challenged outputs (85-92% agreement, including errors); (2) premature consensus caused by redundant reviewers; and (3) a verification-validation gap where executable but physically incorrect code passed undetected. No agent combination successfully validated constitutive relations in complex tasks. Building on theories of functional diversity, role differentiation, and computational validation, we propose actionable design principles: (i) assign complementary agent roles, (ii) enforce multi-level validation (execution, specification, physics), and (iii) prevent early consensus through adversarial or trigger-based interaction control. These findings establish a principled foundation for designing trustworthy LLM collaborations in engineering workflows.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Workday's Approach to Secure and Compliant Cloud ERP Systems</title>
<link>https://arxiv.org/abs/2511.02856</link>
<guid>https://arxiv.org/abs/2511.02856</guid>
<content:encoded><![CDATA[
<div> GDPR, SOC 2, HIPAA, ISO 27001, FedRAMP <br />
Summary:<br />
The paper discusses Workday's compliance with global standards such as GDPR, SOC 2, HIPAA, ISO 27001, and FedRAMP, highlighting its ability to protect critical financial, healthcare, and government data. Automated compliance attributes like audit trails and behavioral analytics enhance risk management and operational flexibility, while reducing manual effort. The paper also explores the use of AI, ML, and blockchain technologies for enhanced attack detection and data integrity. Overall, Workday emerges as a secure, compliant, and future-ready ERP solution. The integration of emerging technologies like AI, machine learning, and blockchain further strengthens threat detection capabilities, positioning Workday as a reliable choice for secure enterprise cloud management. <br /> <div>
arXiv:2511.02856v1 Announce Type: new 
Abstract: Workday's compliance with global standards -- such as GDPR, SOC 2, HIPAA, ISO 27001, and FedRAMP -- shows its ability to best protect critical financial, healthcare, and government data.Automated compliance attributes like audit trails, behavioral analytics, and continuous reporting improve automation of the process and cut down on the manual effort to audit. A comparative review demonstrates enhanced risk management, operational flexibility, and breach mitigation. The paper also discusses potential future solutions with AI, ML and blockchain, to enhance attackdetection and data integrity. Overall, Workday turns out to be a secure, compliant and future-ready ERP solution. The paper also explores emerging trends, including the integration of AI, machine learning, and blockchain technologies to enhance next-generation threat detection and data integrity. The findings position Workday as a reliable, compliant, and future-ready ERP solution, setting a new benchmark for secure enterprise cloud management.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Conditional Diffusion Model for Building Energy Modeling Workflows</title>
<link>https://arxiv.org/abs/2511.02930</link>
<guid>https://arxiv.org/abs/2511.02930</guid>
<content:encoded><![CDATA[
<div> generate modeling, energy consumption behavior, urban energy models, building attributes, conditional diffusion<br />
<br />
Summary: 
This study focuses on understanding energy consumption behavior in communities by using generative modeling to fill in missing building characteristics for urban energy models. The researchers develop a tabular diffusion-based framework to handle heterogeneous building data and create a conditional diffusion model for imputing missing attributes. They validate the model by comparing generated distributions with real data and conducting a case study in a Baltimore residential region. The results demonstrate the potential of generative modeling to enhance building energy modeling workflows. <div>
arXiv:2511.02930v1 Announce Type: new 
Abstract: Understanding current energy consumption behavior in communities is critical for informing future energy use decisions and enabling efficient energy management. Urban energy models, which are used to simulate these energy use patterns, require large datasets with detailed building characteristics for accurate outcomes. However, such detailed characteristics at the individual building level are often unknown and costly to acquire, or unavailable. Through this work, we propose using a generative modeling approach to generate realistic building attributes to fill in the data gaps and finally provide complete characteristics as inputs to energy models. Our model learns complex, building-level patterns from training on a large-scale residential building stock model containing 2.2 million buildings. We employ a tabular diffusion-based framework that is designed to handle heterogeneous (discrete and continuous) features in tabular building data, such as occupancy, floor area, heating, cooling, and other equipment details. We develop a capability for conditional diffusion, enabling the imputation of missing building characteristics conditioned on known attributes. We conduct a comprehensive validation of our conditional diffusion model, firstly by comparing the generated conditional distributions against the underlying data distribution, and secondly, by performing a case study for a Baltimore residential region, showing the practical utility of our approach. Our work is one of the first to demonstrate the potential of generative modeling to accelerate building energy modeling workflows.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A physics-augmented neural network framework for finite strain incompressible viscoelasticity</title>
<link>https://arxiv.org/abs/2511.02959</link>
<guid>https://arxiv.org/abs/2511.02959</guid>
<content:encoded><![CDATA[
<div> Keywords: physics-augmented neural network, finite strain incompressible viscoelasticity, generalized standard materials theory, thermodynamic consistency, internal variables

Summary: 
A new framework, Physics-Augmented Neural Network (PANN), is proposed for modeling finite strain incompressible viscoelasticity within the Generalized Standard Materials Theory. The formulation of PANN involves the decomposition of the deformation gradient and ensures unimodularity of the inelastic deformation part. The neural networks representing the free energy and dual dissipation potential are constructed to be thermodynamically consistent, objective, and material symmetric. The evolution of internal variables is handled using an implicit exponential time integrator during training. A trainable gate layer with lp regularization automatically determines the number of internal variables needed. The model is calibrated using synthetic and experimental data, showing excellent agreement across various deformation rates and load paths, achieving both interpolation and extrapolation accuracy. Furthermore, the PANN demonstrates consistency with linear viscoelasticity through linearization of the full model. <br /><br />Summary: <div>
arXiv:2511.02959v1 Announce Type: new 
Abstract: We propose a physics-augmented neural network (PANN) framework for finite strain incompressible viscoelasticity within the generalized standard materials theory. The formulation is based on the multiplicative decomposition of the deformation gradient and enforces unimodularity of the inelastic deformation part throughout the evolution. Invariant-based representations of the free energy and the dual dissipation potential by monotonic and fully input-convex neural networks ensure thermodynamic consistency, objectivity, and material symmetry by construction. The evolution of the internal variables during training is handled by solving the evolution equations using an implicit exponential time integrator. In addition, a trainable gate layer combined with lp regularization automatically identifies the required number of internal variables during training. The PANN is calibrated with synthetic and experimental data, showing excellent agreement for a wide range of deformation rates and different load paths. We also show that the proposed model achieves excellent interpolation as well as plausible and accurate extrapolation behaviors. In addition, we demonstrate consistency of the PANN with linear viscoelasticity by linearization of the full model.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid DeepONet Surrogates for Multiphase Flow in Porous Media</title>
<link>https://arxiv.org/abs/2511.02962</link>
<guid>https://arxiv.org/abs/2511.02962</guid>
<content:encoded><![CDATA[
<div> surrogate models, deep learning, porous media, multiphase flow, PDE solutions<br />
<br />
Summary: <br />
The article discusses the development of hybrid neural operator surrogates for solving partial differential equations (PDEs) in complex porous media flows. These surrogates combine Fourier Neural Operators, Multi-Layer Perceptrons (MLPs), and Kolmogorov-Arnold Networks (KANs) to address challenges such as high memory requirements and handling the time dimension. The framework splits spatial and temporal learning tasks into branch and trunk networks, respectively, leading to accurate surrogate modeling with fewer parameters. The hybrid models are evaluated on various multiphase flow problems, showing strong predictive performance on large-scale reservoir simulations. This approach offers a promising solution for efficiently solving complex PDEs in real-world applications involving multiphase flow in porous media. <br /> <div>
arXiv:2511.02962v1 Announce Type: new 
Abstract: The solution of partial differential equations (PDEs) plays a central role in numerous applications in science and engineering, particularly those involving multiphase flow in porous media. Complex, nonlinear systems govern these problems and are notoriously computationally intensive, especially in real-world applications and reservoirs. Recent advances in deep learning have spurred the development of data-driven surrogate models that approximate PDE solutions with reduced computational cost. Among these, Neural Operators such as Fourier Neural Operator (FNO) and Deep Operator Networks (DeepONet) have shown strong potential for learning parameter-to-solution mappings, enabling the generalization across families of PDEs. However, both methods face challenges when applied independently to complex porous media flows, including high memory requirements and difficulty handling the time dimension. To address these limitations, this work introduces hybrid neural operator surrogates based on DeepONet models that integrate Fourier Neural Operators, Multi-Layer Perceptrons (MLPs), and Kolmogorov-Arnold Networks (KANs) within their branch and trunk networks. The proposed framework decouples spatial and temporal learning tasks by splitting these structures into the branch and trunk networks, respectively. We evaluate these hybrid models on multiphase flow in porous media problems ranging in complexity from the steady 2D Darcy flow to the 2D and 3D problems belonging to the $10$th Comparative Solution Project from the Society of Petroleum Engineers. Results demonstrate that hybrid schemes achieve accurate surrogate modeling with significantly fewer parameters while maintaining strong predictive performance on large-scale reservoir simulations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes</title>
<link>https://arxiv.org/abs/2511.03170</link>
<guid>https://arxiv.org/abs/2511.03170</guid>
<content:encoded><![CDATA[
<div> Keywords: quantitative structure-activity relationship, activity cliffs, machine learning models, molecular graph structures, GraphCliff <br />
Summary: 
Quantitative structure-activity relationship (QSAR) is based on the assumption of a smooth relationship between molecular structure and biological activity, but activity cliffs, where structurally similar compounds have large potency differences, challenge this notion. Recent benchmarks have shown that traditional machine learning models with extended connectivity fingerprints outperform graph neural networks in addressing activity cliffs. Graph embeddings fail to effectively differentiate between structurally similar molecules, limiting their utility. To address this, a new model called GraphCliff is proposed, which integrates short- and long-range information through a gating mechanism. Experimental results demonstrate GraphCliff's ability to improve performance on both cliff and non-cliff compounds. Layer-wise node embedding analyses show reduced over-smoothing and enhanced discriminative power compared to baseline models. This approach preserves the expressive power of molecular graph structures while addressing the limitations of existing methods. <br /> <br />Summary: <div>
arXiv:2511.03170v1 Announce Type: new 
Abstract: Quantitative structure-activity relationship assumes a smooth relationship between molecular structure and biological activity. However, activity cliffs defined as pairs of structurally similar compounds with large potency differences break this continuity. Recent benchmarks targeting activity cliffs have revealed that classical machine learning models with extended connectivity fingerprints outperform graph neural networks. Our analysis shows that graph embeddings fail to adequately separate structurally similar molecules in the embedding space, making it difficult to distinguish between structurally similar but functionally different molecules. Despite this limitation, molecular graph structures are inherently expressive and attractive, as they preserve molecular topology. To preserve the structural representation of molecules as graphs, we propose a new model, GraphCliff, which integrates short- and long-range information through a gating mechanism. Experimental results demonstrate that GraphCliff consistently improves performance on both non-cliff and cliff compounds. Furthermore, layer-wise node embedding analyses reveal reduced over-smoothing and enhanced discriminative power relative to strong baseline graph models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Framework for Environmental Similarity and Vessel Mobility as Coupled Predictors of Marine Invasive Species Pathways</title>
<link>https://arxiv.org/abs/2511.03499</link>
<guid>https://arxiv.org/abs/2511.03499</guid>
<content:encoded><![CDATA[
<div> Keywords: marine invasive species, global shipping, risk assessment, maritime mobility, climate-based feature representations

Summary: 
This study introduces a novel theoretical framework for assessing invasion risk posed by marine invasive species spread through global shipping. By combining environmental similarity across ports with observed and forecasted maritime mobility, the framework offers a comprehensive approach to estimate exposure levels at both the port and voyage levels. Climate-based feature representations capture marine conditions at each port, while mobility networks derived from Automatic Identification System data track vessel flows and potential transfer pathways. Clustering and metric learning techniques identify climate analogues and assess species survival likelihood along shipping routes. A temporal link prediction model anticipates changes in traffic patterns under evolving environmental conditions. This integrated approach enables targeted monitoring, routing adjustments, and management interventions to mitigate the ecological and economic impacts of marine invasive species. 

<br /><br />Summary: <div>
arXiv:2511.03499v1 Announce Type: new 
Abstract: Marine invasive species spread through global shipping and generate substantial ecological and economic impacts. Traditional risk assessments require detailed records of ballast water and traffic patterns, which are often incomplete, limiting global coverage. This work advances a theoretical framework that quantifies invasion risk by combining environmental similarity across ports with observed and forecasted maritime mobility. Climate-based feature representations characterize each port's marine conditions, while mobility networks derived from Automatic Identification System data capture vessel flows and potential transfer pathways. Clustering and metric learning reveal climate analogues and enable the estimation of species survival likelihood along shipping routes. A temporal link prediction model captures how traffic patterns may change under shifting environmental conditions. The resulting fusion of environmental similarity and predicted mobility provides exposure estimates at the port and voyage levels, supporting targeted monitoring, routing adjustments, and management interventions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation-Based Validation of an Integrated 4D/5D Digital-Twin Framework for Predictive Construction Control</title>
<link>https://arxiv.org/abs/2511.03684</link>
<guid>https://arxiv.org/abs/2511.03684</guid>
<content:encoded><![CDATA[
<div> Keywords: construction industry, 4D/5D digital twin framework, AI-based analytics, probabilistic CPM, deep reinforcement learning

Summary:
This study introduces an integrated 4D/5D digital twin framework that combines Building Information Modeling (BIM) with natural-language processing (NLP), computer-vision progress measurement, Bayesian probabilistic CPM updating, and deep-reinforcement learning resource-leveling. The framework was implemented on a mid-rise project in Dallas-Fort Worth, resulting in significant improvements in accuracy and efficiency. The project saw reductions in estimating labor and overtime, as well as improved project-buffer utilization, while maintaining an on-time finish. The digital twin sandbox allowed for real-time "what-if" forecasting and traceable cost-schedule alignment through a 5D knowledge graph. The findings confirm that integrating AI-based analytics with probabilistic CPM and deep reinforcement learning enhances forecasting precision, transparency, and control resilience. This validated workflow paves the way for predictive, adaptive, and auditable construction management. 

<br /><br />Summary: <div>
arXiv:2511.03684v1 Announce Type: new 
Abstract: Persistent cost and schedule deviations remain a major challenge in the U.S. construction industry, revealing the limitations of deterministic CPM and static document-based estimating. This study presents an integrated 4D/5D digital-twin framework that couples Building Information Modeling (BIM) with natural-language processing (NLP)-based cost mapping, computer-vision (CV)-driven progress measurement, Bayesian probabilistic CPM updating, and deep-reinforcement-learning (DRL) resource-leveling. A nine-month case implementation on a Dallas-Fort Worth mid-rise project demonstrated measurable gains in accuracy and efficiency: 43% reduction in estimating labor, 6% reduction in overtime, and 30% project-buffer utilization, while maintaining an on-time finish at 128 days within P50-P80 confidence bounds. The digital-twin sandbox also enabled real-time "what-if" forecasting and traceable cost-schedule alignment through a 5D knowledge graph. Findings confirm that integrating AI-based analytics with probabilistic CPM and DRL enhances forecasting precision, transparency, and control resilience. The validated workflow establishes a practical pathway toward predictive, adaptive, and auditable construction management.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Region Matrix Interpolation for Dynamic Analysis of Aperiodic Structures under Large Model Parameter Perturbations</title>
<link>https://arxiv.org/abs/2511.03711</link>
<guid>https://arxiv.org/abs/2511.03711</guid>
<content:encoded><![CDATA[
<div> surrogate-based model, dynamic mechanical metamaterials, frequency response, parametric perturbations, interpolation <br />
<br />Summary: This work introduces a surrogate-based model for estimating the frequency response of dynamic mechanical metamaterials efficiently. It addresses limitations of existing methods by providing insight into common modal projection restrictions, proposing a sampling-based procedure for identifying usable parameter space boundaries, and enhancing the surrogate model with a multi-region interpolation strategy. The research demonstrates the effectiveness of the framework through two examples: validating the approach for a single well-conditioned projection region with a unit cell structure and showcasing the advantage of the multi-region approach with a beam-like structure. The proposed method maintains high accuracy across different perturbation levels, unlike traditional Lagrange interpolation, which deviates significantly with increasing perturbations. <div>
arXiv:2511.03711v1 Announce Type: new 
Abstract: This work introduces a surrogate-based model for efficiently estimating the frequency response of dynamic mechanical metamaterials, particularly when dealing with large parametric perturbations and aperiodic substructures. The research builds upon a previous matrix interpolation method applied on top of a Craig-Bampton modal reduction, allowing the variations of geometrical features without the need to remesh and recompute Finite Element matrices. This existing procedure has significant limitations since it requires a common modal projection, which inherently restricts the allowable perturbation size of the model parameters, thereby limiting the model parameter space where matrices can be effectively interpolated. The present work offers three contributions: (1) It provides structural dynamic insight into the restrictions imposed by the common modal projection, demonstrating that ill-conditioning can be controlled, (2) it proposes an efficient, sampling-based procedure to identify the non-regular boundaries of the usable region in the model parameter space, and (3) it enhances the surrogate model to accommodate larger model parameter perturbations by proposing a multi-region interpolation strategy. The efficacy of this proposed framework is verified through two illustrative examples. The first example, involving a unit cell with a square plate and circular core, validates the approach for a single well-conditioned projection region. The second example, using a beam-like structure with vibration attenuation bands, demonstrates the true advantage of the multi-region approach, where predictions from traditional Lagrange interpolation deviated significantly with increasing perturbations, while the proposed method maintained high accuracy across different perturbation levels.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.02957</link>
<guid>https://arxiv.org/abs/2511.02957</guid>
<content:encoded><![CDATA[
<div> Keywords: Pavement infrastructure monitoring, Digital Twin, Graph Neural Network, predictive maintenance, proactive interventions

Summary:<br />
The article proposes a unified framework that combines Digital Twin and Graph Neural Network for monitoring pavement health and predictive maintenance. It models pavement segments and spatial relations as nodes and edges on a graph, incorporating real-time data from UAVs, sensors, and LiDAR. The Graph Neural Network learns deterioration patterns from this data, enabling proactive interventions to prevent failures. The model, trained on a realistic dataset, achieves a high R2 value, outperforming baseline regressors and capturing non-linear degradation effectively. An interactive dashboard and reinforcement learning module are developed for simulation, visualization, and adaptive maintenance planning. This integration enhances forecasting precision, establishing a closed feedback loop for continuous improvement. The approach sets the foundation for intelligent and sustainable pavement management, with potential extensions for real-world deployment, multi-agent coordination, and smart-city integration.<br /> <div>
arXiv:2511.02957v1 Announce Type: cross 
Abstract: Pavement infrastructure monitoring is challenged by complex spatial dependencies, changing environmental conditions, and non-linear deterioration across road networks. Traditional Pavement Management Systems (PMS) remain largely reactive, lacking real-time intelligence for failure prevention and optimal maintenance planning. To address this, we propose a unified Digital Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven pavement health monitoring and predictive maintenance. Pavement segments and spatial relations are modeled as graph nodes and edges, while real-time UAV, sensor, and LiDAR data stream into the DT. The inductive GNN learns deterioration patterns from graph-structured inputs to forecast distress and enable proactive interventions. Trained on a real-world-inspired dataset with segment attributes and dynamic connectivity, our model achieves an R2 of 0.3798, outperforming baseline regressors and effectively capturing non-linear degradation. We also develop an interactive dashboard and reinforcement learning module for simulation, visualization, and adaptive maintenance planning. This DT-GNN integration enhances forecasting precision and establishes a closed feedback loop for continuous improvement, positioning the approach as a foundation for proactive, intelligent, and sustainable pavement management, with future extensions toward real-world deployment, multi-agent coordination, and smart-city integration.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive-Sensorless Monitoring of Shipping Containers</title>
<link>https://arxiv.org/abs/2511.03022</link>
<guid>https://arxiv.org/abs/2511.03022</guid>
<content:encoded><![CDATA[
<div> sensorless monitoring, machine learning, residual correction, adaptive-sensorless models, cargo transportation <br />
<br />
Summary: 
The paper introduces the concept of adaptive-sensorless monitoring for shipping containers, which utilizes machine learning models to predict internal conditions while incorporating live telemetry data to correct for systematic errors. By training and evaluating these adaptive-sensorless models on a large dataset of 3.48 million sensor readings, the study demonstrates consistent improvements over traditional sensorless models. When tested on a holdout set, the adaptive-sensorless models achieve lower mean absolute errors and root mean-squared errors for temperature and relative humidity monitoring. These models offer more accurate cargo monitoring, early risk detection, and reduce the reliance on full connectivity in global shipping, making them a valuable tool for enhancing the quality and efficiency of cargo transportation. <br /> <div>
arXiv:2511.03022v1 Announce Type: cross 
Abstract: Monitoring the internal temperature and humidity of shipping containers is essential to preventing quality degradation during cargo transportation. Sensorless monitoring -- machine learning models that predict the internal conditions of the containers using exogenous factors -- shows promise as an alternative to monitoring using sensors. However, it does not incorporate telemetry information and correct for systematic errors, causing the predictions to differ significantly from the live data and confusing the users. In this paper, we introduce the residual correction method, a general framework for correcting for systematic biases in sensorless models after observing live telemetry data. We call this class of models ``adaptive-sensorless'' monitoring. We train and evaluate adaptive-sensorless models on the 3.48 million data points -- the largest dataset of container sensor readings ever used in academic research -- and show that they produce consistent improvements over the baseline sensorless models. When evaluated on the holdout set of the simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$ 2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$ 7.09% for relative humidity (vs 7.99% by sensorless) and average root mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs 3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs 10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and less dependence on full connectivity in global shipping.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tsallis-Entropy Lens on Genetic Variation</title>
<link>https://arxiv.org/abs/2511.03063</link>
<guid>https://arxiv.org/abs/2511.03063</guid>
<content:encoded><![CDATA[
<div> Tsallis-order q F-statistic, fixation statistic, variance-based fixation index, mutual information, allele-frequency spectra<br />
<br />
Summary: <br />
The article introduces a new statistic, the Tsallis-order q F-statistic, Fq, which measures the fraction of Tsallis q-entropy lost within subpopulations. This statistic generalizes the fixation statistic and provides a more fine-grained view of population differentiation compared to traditional methods. By varying q, Fq can emphasize rare or common variants, offering a spectral differentiation of population structure. The study, conducted on Oceanian genomes and simulated data from diverse populations, demonstrates that Fq in One-vs-Rest and Leave-One-Out modes can pinpoint influential subpopulations and reveal isolation-migration events and founder effects. Fq serves as a valuable tool for simulation audits and summarizing population structure with higher resolution. <div>
arXiv:2511.03063v1 Announce Type: cross 
Abstract: We introduce an information-theoretic generalization of the fixation statistic, the Tsallis-order $q$ F-statistic, $F_q$, which measures the fraction of Tsallis $q$-entropy lost within subpopulations relative to the pooled population. The family nests the classical variance-based fixation index $F_{\textbf{ST}}$ at $q{=}2$ and a Shannon-entropy analogue at $q{=}1$, whose absolute form equals the mutual information between alleles and population labels. By varying $q$, $F_q$ acts as a spectral differentiator that up-weights rare variants at low $q$, while $q{>}1$ increasingly emphasizes common variants, providing a more fine-grained view of differentiation than $F_{\textbf{ST}}$ when allele-frequency spectra are skewed. On real data (865 Oceanian genomes with 1,823,000 sites) and controlled genealogical simulations (seeded from 1,432 founders from HGDP and 1000 Genomes panels, with 322,216 sites), we show that $F_q$ in One-vs-Rest (OVR) and Leave-One-Out (LOO) modes provides clear attribution of which subpopulations drive regional structure, and sensitively timestamps isolation-migration events and founder effects. $F_q$ serves as finer-resolution complement for simulation audits and population-structure summaries.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>System Identification of a Moored ASV with Recessed Moon Pool via Deterministic and Bayesian Hankel-DMDc</title>
<link>https://arxiv.org/abs/2511.03482</link>
<guid>https://arxiv.org/abs/2511.03482</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous surface vehicle, system identification, dynamic mode decomposition, Bayesian extension, mooring loads

Summary:
This study focuses on system identification of a small autonomous surface vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were conducted on a Codevintec CK-14e ASV in varying wave conditions, including irregular and regular head-sea waves. The ASV has a recessed moon pool, leading to nonlinear responses due to sloshing. Data-driven reduced-order models were constructed based on measurements of vessel motions and mooring loads. The HDMDc framework provided accurate deterministic predictions of the vessel's dynamics, while the Bayesian formulation accounted for uncertainty in model response. Validation against experimental data demonstrated the models' capability to predict the vessel's response to both regular and irregular wave excitations. Overall, HDMDc-based ROMs offer a reliable data-driven approach for system identification, showcasing their generalization capability and accuracy in reproducing vessel dynamics in different sea conditions. 

<br /><br />Summary: <div>
arXiv:2511.03482v1 Announce Type: cross 
Abstract: This study addresses the system identification of a small autonomous surface vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were carried out on a Codevintec CK-14e ASV in the towing tank of CNR-INM, under both irregular and regular head-sea wave conditions. The ASV under investigation features a recessed moon pool, which induces nonlinear responses due to sloshing, thereby increasing the modelling challenge. Data-driven reduced-order models were built from measurements of vessel motions and mooring loads. The HDMDc framework provided accurate deterministic predictions of vessel dynamics, while the Bayesian formulation enabled uncertainty-aware characterization of the model response by accounting for variability in hyperparameter selection. Validation against experimental data demonstrated that both HDMDc and BHDMDc can predict the vessel's response to unseen regular and irregular wave excitations. In conclusion, the study shows that HDMDc-based ROMs are a viable data-driven alternative for system identification, demonstrating for the first time their generalization capability for a sea condition different from the training set, achieving high accuracy in reproducing vessel dynamics.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveTradeBench: Seeking Real-World Alpha with Large Language Models</title>
<link>https://arxiv.org/abs/2511.03628</link>
<guid>https://arxiv.org/abs/2511.03628</guid>
<content:encoded><![CDATA[
<div> LiveTradeBench, large language models, evaluation, trading, real-time uncertainty <br />
Summary:<br />
1. The study introduces LiveTradeBench, a live trading environment for evaluating Large Language Models (LLMs) in real and evolving markets, focusing on decision-making under uncertainty.
2. LiveTradeBench follows design principles including live data streaming of market prices and news, multi-asset portfolio management, and multi-market evaluation across different environments.
3. Results from 50-day live evaluations of 21 LLMs show that high LMArena scores do not guarantee superior trading outcomes; models exhibit distinct portfolio styles based on risk appetite and decision-making dynamics.
4. Some LLMs effectively adapt decisions using live signals, highlighting the need for benchmarks that assess sequential decision-making and consistency under live uncertainty.
5. The findings emphasize the disparity between static evaluations and real-world trading competence, underscoring the importance of evaluating models in dynamic and uncertain environments. <br /> <div>
arXiv:2511.03628v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Gene Trees without more data</title>
<link>https://arxiv.org/abs/2511.03692</link>
<guid>https://arxiv.org/abs/2511.03692</guid>
<content:encoded><![CDATA[
<div> Keywords: gene tree estimation, species tree estimation, incomplete lineage sorting, Statistical Binning, BestML

Summary: 
The study addresses the challenges of estimating species and gene trees from sequence data. Gene tree estimation is often inaccurate due to low phylogenetic signal in alignments, while species tree estimation is complicated by incomplete lineage sorting (ILS). Existing methods like MP-EST, ASTRAL2, and ASTRID suffer from low gene tree accuracy. The study proposes a novel pipeline, WSB+WQMC, which improves gene tree estimation and species tree accuracy under the GTR+MSC model. Evaluations using BestML analysis on simulated datasets showed that WSB+WQMC significantly enhances gene tree and species tree accuracy, particularly in the presence of low to high ILS levels. While WSB+WQMC may perform slightly less accurately than WSB+CAML under certain conditions, it excels in estimating gene trees and species trees in datasets with moderately high and high ILS levels. Overall, WSB+WQMC shows promise as a reliable alternative for phylogenetic estimation, especially when dealing with low phylogenetic signal. 

<br /><br />Summary: <div>
arXiv:2511.03692v1 Announce Type: cross 
Abstract: Estimating species and gene trees from sequence data is challenging. Gene tree estimation is often hampered by low phylogenetic signal in alignments, leading to inaccurate trees. Species tree estimation is complicated by incomplete lineage sorting (ILS), where gene histories differ from the species' history. Summary methods like MP-EST, ASTRAL2, and ASTRID infer species trees from gene trees but suffer when gene tree accuracy is low. To address this, the Statistical Binning (SB) and Weighted Statistical Binning (WSB) pipelines were developed to improve gene tree estimation. However, previous studies only tested these pipelines using multi-locus bootstrapping (MLBS), not the BestML approach.
  This thesis proposes a novel pipeline, WSB+WQMC, which shares design features with the existing WSB+CAML pipeline but has other desirable properties and is statistically consistent under the GTR+MSC model. This study evaluated WSB+WQMC against WSB+CAML using BestML analysis on various simulated datasets. The results confirmed many trends seen in prior MLBS analyses. WSB+WQMC substantially improved gene tree and species tree accuracy (using ASTRAL2 and ASTRID) on most datasets with low, medium, and moderately high ILS levels. In a direct comparison, WSB+WQMC computed less accurate trees than WSB+CAML under certain low and medium ILS conditions. However, WSB+WQMC performed better or at least as accurately as WSB+CAML on all datasets with moderately high and high ILS. It also proved better for estimating gene trees on some medium and low ILS datasets. Thus, WSB+WQMC is a promising alternative to WSB+CAML for phylogenetic estimation, especially in the presence of low phylogenetic signal.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChemFM as a Scaling Law Guided Foundation Model Pre-trained on Informative Chemicals</title>
<link>https://arxiv.org/abs/2410.21422</link>
<guid>https://arxiv.org/abs/2410.21422</guid>
<content:encoded><![CDATA[
<div> ChemFM, foundation model, chemicals, scalability, generalization <br />
Summary:<br />
ChemFM is introduced as a large foundation model for chemicals, pre-trained on 178 million molecules using self-supervised causal language modeling. It outperforms task-specific AI models in various chemical tasks, showing up to 67.48% performance improvement in property prediction and 3.7% top-1 accuracy improvement in reaction prediction. ChemFM also excels in predicting antibiotic activity and cytotoxicity, showcasing its potential in discovering novel antibiotics. Additionally, it exhibits strong data efficiency, requiring fewer labeled training samples to achieve state-of-the-art performance. The model's scalability and generalizability make it a valuable tool for advancing chemistry research across a wide range of tasks with minimal additional training. <br />Summary: <div>
arXiv:2410.21422v3 Announce Type: replace 
Abstract: Traditional AI methods often rely on task-specific model designs and training, which constrain both the scalability of model size and generalization across different tasks. Here, we introduce ChemFM, a large foundation model specifically developed for chemicals. By conducting a series of scaling experiments, we identify UniChem as the informative molecular database for pre-training the foundation model. ChemFM comprises 3 billion parameters and is pre-trained on 178 million molecules using self-supervised causal language modeling to extract generalizable molecular representations. This model can be adapted to diverse downstream chemical applications using either full-parameter or parameter-efficient fine-tuning methods. ChemFM consistently outperforms state-of-the-art task-specific AI models across all tested tasks. Notably, it achieves up to 67.48% performance improvement across 34 property prediction benchmarks, up to 33.80% reduction in mean average deviation between conditioned and actual properties of generated molecules in conditional molecular generation tasks, and up to 3.7% top-1 accuracy improvement across 4 reaction prediction datasets. Moreover, ChemFM demonstrates its superior performance in predicting antibiotic activity and cytotoxicity, highlighting its potential to advance the discovery of novel antibiotics. Furthermore, we demonstrate that, as a foundation model, ChemFM exhibits strong data efficiency, requiring significantly fewer labeled training samples to achieve state-of-the-art performance. We anticipate that ChemFM will significantly advance chemistry research by providing a foundation model capable of effectively generalizing across a broad range of tasks with minimal additional training.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-driven framework for team selection in Fantasy Premier League</title>
<link>https://arxiv.org/abs/2505.02170</link>
<guid>https://arxiv.org/abs/2505.02170</guid>
<content:encoded><![CDATA[
<div> Fantasy football, optimization, linear programming, data-driven, Premier League
<br />
Summary: This study presents a data-driven approach to optimize fantasy football lineup selection in the Fantasy Premier League (FPL). Using deterministic and robust mixed-integer linear programs, the study formulates the selection of starting eleven, bench players, and captain under various constraints. An objective function combining realized FPL points with predictions from a regression model is used, with benchmarks including ARIMA, weighted averages, and Monte Carlo simulation. Experiments on the 2023/24 Premier League season show that ARIMA with a constrained budget performs consistently well. The framework also extends to other FPL strategies like multi-week transfer planning and dynamic captaincy decisions. <div>
arXiv:2505.02170v2 Announce Type: replace 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Under a fixed budget, managers select squads to maximize future Fantasy Premier League (FPL) points. This study formulates lineup selection as data-driven optimization and develops deterministic and robust mixed-integer linear programs that choose the starting eleven, bench, and captain under budget, formation, and club-quota constraints (maximum three players per club). The objective is parameterized by a hybrid scoring metric that combines realized FPL points with predictions from a linear regression model trained on match-performance features identified using exploratory data analysis techniques. The study benchmarks alternative objectives and cost estimators, including simple and recency-weighted averages, exponential smoothing, autoregressive integrated moving average (ARIMA), and Monte Carlo simulation. Experiments on the 2023/24 Premier League season show that ARIMA with a constrained budget and a rolling window yields the most consistent out-of-sample performance; weighted averages and Monte Carlo are also competitive. Robust variants improve some objectives but are not uniformly superior. The framework provides transparent decision support for fantasy roster construction and extends to FPL chips, multi-week rolling-horizon transfer planning, and week-by-week dynamic captaincy.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Fidelity Global Search Framework for Hotspot Prevention in 3D Thermal Design Space</title>
<link>https://arxiv.org/abs/2511.02211</link>
<guid>https://arxiv.org/abs/2511.02211</guid>
<content:encoded><![CDATA[
<div> Bezier-based, Multi-Fidelity, Thermal Optimization, Framework, Heat Sinks <br />
<br />
Summary: 
The article presents a Bezier-based Multi-Fidelity Thermal Optimization Framework for global optimization of 3D heat sinks. The framework utilizes flexible Bezier-parameterized fin geometries and a multi-fidelity pseudo-3D thermal modeling strategy to balance accuracy and computational cost efficiently. By representing fins using smooth Bezier curves, the design space can generate diverse topologies with minimal variables. The Covariance Matrix Adaptation Evolution Strategy is employed as a global optimizer to minimize pressure drop while respecting surface-average temperature constraints. The pseudo-3D model couples thermally interacting 2D layers representing fluid flow through fins and conductive base plates. Validation against full 3D simulations shows good agreement in temperature distribution and pressure drops but at significantly reduced computational costs. Optimization results demonstrate up to a 50% reduction in pressure loss compared to conventional fin configurations, indicating a trade-off between thermal performance and hydraulic efficiency. The framework enables fast, geometry-flexible, and optimized heat sink design, facilitating efficient exploration of complex geometries. <br /> <div>
arXiv:2511.02211v1 Announce Type: new 
Abstract: We present a B\'ezier-based Multi-Fidelity Thermal Optimization Framework, which is a computationally efficient methodology for the global optimization of 3D heat sinks. The flexible B\'ezier-parameterized fin geometries and the adopted multi-fidelity pseudo-3D thermal modeling strategy meet at a balance between accuracy and computational cost. In this method, the smooth and compact B\'ezier representation of fins defines the design space from which diverse topologies can be generated with minimal design variables. A global optimizer, the Covariance Matrix Adaptation Evolution Strategy, minimizes the pressure drop with respect to a given surface-average temperature constraint to achieve improvement in the pressure loss. In the framework, the pseudo-3D model couples two thermally interacting 2D layers: a thermofluid layer representing the fluid domain passing through the fins, and a conductive base plate representing the surface where excessive average temperature is to be avoided. Both layers are coupled with calibrated heat transfer coefficients obtained from high-fidelity 3D simulations. For several fin geometries, the proposed framework has been validated by comparing the pseudo-3D results with those of full 3D simulations, which yielded good agreement in terms of temperature distribution and pressure drops when the computational cost was reduced by several orders of magnitude. Optimization results show that it attains up to 50\% pressure loss reduction compared to conventional straight-fin configurations, and it reveals a clear trade-off between thermal performance and hydraulic efficiency. Thus, the proposed method forms a new basis for fast, geometry-flexible, and optimized heat sink design, enabling efficient exploration of complex geometries.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wavelet-Optimized Motion Artifact Correction in 3D MRI Using Pre-trained 2D Score Priors</title>
<link>https://arxiv.org/abs/2511.02256</link>
<guid>https://arxiv.org/abs/2511.02256</guid>
<content:encoded><![CDATA[
<div> Keywords: MRI, motion artifacts, generative models, wavelet-optimized, image quality<br />
Summary:<br />
Motion artifacts in MRI are a significant challenge, affecting image quality and diagnosis. Existing 3D generative models struggle with known operators and slow speed. To address this, a new 3D-WMoCo framework combines 2D priors and a mean-reverting SDE for motion artifact correction. Wavelet diffusion and convolution techniques enhance speed and feature extraction. The method is validated using simulated and real clinical data, showing improved performance. The approach is available on GitHub for implementation. <div>
arXiv:2511.02256v1 Announce Type: new 
Abstract: Motion artifacts in magnetic resonance imaging (MRI) remain a major challenge, as they degrade image quality and compromise diagnostic reliability. Score-based generative models (SGMs) have recently shown promise for artifact removal. However, existing 3D SGM-based approaches are limited in two key aspects: (1) their strong dependence on known forward operators makes them ineffective for correcting MRI motion artifacts, and (2) their slow inference speed hinders clinical translation. To overcome these challenges, we propose a wavelet-optimized end-to-end framework for 3D MRI motion correct using pre-trained 2D score priors (3D-WMoCo). Specifically, two orthogonal 2D score priors are leveraged to guide the 3D distribution prior, while a mean-reverting stochastic differential equation (SDE) is employed to model the restoration process of motion-corrupted 3D volumes to motion-free 3D distribution. Furthermore, wavelet diffusion is introduced to accelerate inference, and wavelet convolution is applied to enhance feature extraction. We validate the effectiveness of our approach through both simulated motion artifact experiments and real-world clinical motion artifact correction tests. The proposed method achieves robust performance improvements over existing techniques. Implementation details and source code are available at: https://github.com/ZG-yuan/3D-WMoCo.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas</title>
<link>https://arxiv.org/abs/2511.02458</link>
<guid>https://arxiv.org/abs/2511.02458</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, macroeconomic forecasting, persona-based prompting, GPT-4o, ECB Survey

Summary:
The study evaluates the impact of persona-based prompting on the performance of the Large Language Model (LLM) in macroeconomic forecasting tasks. Using economics-related personas, GPT-4o is prompted to replicate the ECB Survey of Professional Forecasters across multiple rounds. The comparison between persona-prompted forecasts and human experts reveals similar accuracy levels, with GPT-4o maintaining competitiveness in out-of-sample forecasting for 2024-2025. Ablation experiments show that persona descriptions do not provide a measurable advantage in forecasting accuracy, suggesting they can be omitted to reduce computational costs. The results indicate that GPT-4o can achieve competitive forecasting accuracy even on unseen macroeconomic events with relevant context data, but diverse prompts lead to homogeneous forecasts compared to human panels.<br /><br />Summary: <div>
arXiv:2511.02458v1 Announce Type: cross 
Abstract: We evaluate whether persona-based prompting improves Large Language Model (LLM) performance on macroeconomic forecasting tasks. Using 2,368 economics-related personas from the PersonaHub corpus, we prompt GPT-4o to replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds (2013-2025). We compare the persona-prompted forecasts against the human experts panel, across four target variables (HICP, core HICP, GDP growth, unemployment) and four forecast horizons. We also compare the results against 100 baseline forecasts without persona descriptions to isolate its effect. We report two main findings. Firstly, GPT-4o and human forecasters achieve remarkably similar accuracy levels, with differences that are statistically significant yet practically modest. Our out-of-sample evaluation on 2024-2025 data demonstrates that GPT-4o can maintain competitive forecasting performance on unseen events, though with notable differences compared to the in-sample period. Secondly, our ablation experiment reveals no measurable forecasting advantage from persona descriptions, suggesting these prompt components can be omitted to reduce computational costs without sacrificing accuracy. Our results provide evidence that GPT-4o can achieve competitive forecasting accuracy even on out-of-sample macroeconomic events, if provided with relevant context data, while revealing that diverse prompts produce remarkably homogeneous forecasts compared to human panels.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural-gas storage modelling by deep reinforcement learning</title>
<link>https://arxiv.org/abs/2511.02646</link>
<guid>https://arxiv.org/abs/2511.02646</guid>
<content:encoded><![CDATA[
<div> simulator, GasRL, deep reinforcement learning, natural gas market, equilibrium prices <br />
Summary:
GasRL is introduced as a simulator integrating a representation of the natural gas market with storage-operator policies trained using deep reinforcement learning (RL). Soft Actor Critic (SAC) emerges as the most effective RL algorithm in achieving various storage operator objectives, including profitability and price stabilization. The equilibrium price dynamics produced by SAC-derived policies closely mirror real-world price characteristics. The simulator demonstrates the impact of EU-mandated minimum storage thresholds, highlighting their positive effect on market resilience against supply shocks. Specifically, the presence of thresholds reduces market disruptions in response to unexpectedly large shocks. Overall, GasRL provides insights into the effects of optimal stockpile management on market dynamics and the benefits of policy interventions in ensuring market stability. <br /><br />Summary: <div>
arXiv:2511.02646v1 Announce Type: cross 
Abstract: We introduce GasRL, a simulator that couples a calibrated representation of the natural gas market with a model of storage-operator policies trained with deep reinforcement learning (RL). We use it to analyse how optimal stockpile management affects equilibrium prices and the dynamics of demand and supply. We test various RL algorithms and find that Soft Actor Critic (SAC) exhibits superior performance in the GasRL environment: multiple objectives of storage operators - including profitability, robust market clearing and price stabilisation - are successfully achieved. Moreover, the equilibrium price dynamics induced by SAC-derived optimal policies have characteristics, such as volatility and seasonality, that closely match those of real-world prices. Remarkably, this adherence to the historical distribution of prices is obtained without explicitly calibrating the model to price data. We show how the simulator can be used to assess the effects of EU-mandated minimum storage thresholds. We find that such thresholds have a positive effect on market resilience against unanticipated shifts in the distribution of supply shocks. For example, with unusually large shocks, market disruptions are averted more often if a threshold is in place.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization</title>
<link>https://arxiv.org/abs/2511.02659</link>
<guid>https://arxiv.org/abs/2511.02659</guid>
<content:encoded><![CDATA[
<div> implicit neural representations, in situ training protocol, sketching, continual learning, implicit neural compression<br />
<br />
Summary: 
The article introduces a novel in situ training protocol for implicit neural representations, utilizing limited memory buffers of full and sketched data samples to prevent catastrophic forgetting. The use of sketching as a regularizer is theoretically supported by a Johnson-Lindenstrauss-informed result. The focus is on in situ neural compression with implicit neural representation-based hypernetworks. The method is evaluated on complex simulation data in two and three dimensions, showcasing strong reconstruction performance at high compression rates. The study demonstrates that sketching allows the in situ scheme to closely approximate the performance of the offline method on various tasks, including long time horizons and unstructured grids. <div>
arXiv:2511.02659v1 Announce Type: cross 
Abstract: Focusing on implicit neural representations, we present a novel in situ training protocol that employs limited memory buffers of full and sketched data samples, where the sketched data are leveraged to prevent catastrophic forgetting. The theoretical motivation for our use of sketching as a regularizer is presented via a simple Johnson-Lindenstrauss-informed result. While our methods may be of wider interest in the field of continual learning, we specifically target in situ neural compression using implicit neural representation-based hypernetworks. We evaluate our method on a variety of complex simulation data in two and three dimensions, over long time horizons, and across unstructured grids and non-Cartesian geometries. On these tasks, we show strong reconstruction performance at high compression rates. Most importantly, we demonstrate that sketching enables the presented in situ scheme to approximately match the performance of the equivalent offline method.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient GA: Gradient Genetic Algorithm for Drug Molecular Design</title>
<link>https://arxiv.org/abs/2502.09860</link>
<guid>https://arxiv.org/abs/2502.09860</guid>
<content:encoded><![CDATA[
<div> Keywords: Molecular discovery, molecule design, genetic algorithms, gradient information, neural network

Summary:
The article introduces the Gradient Genetic Algorithm (Gradient GA) as a novel approach to enhance molecular design techniques. Traditional optimization methods like genetic algorithms have limitations in random exploration, impacting solution quality and convergence speed. The Gradient GA incorporates gradient information from the objective function by utilizing a differentiable objective function parameterized with a neural network. This allows for iterative progress towards optimal solutions by following the gradient direction, instead of random exploration. The Discrete Langevin Proposal is used to enable gradient guidance in discrete molecular spaces. Experimental results show significant improvements in both convergence speed and solution quality, surpassing state-of-the-art techniques with up to a 25% enhancement in the top-10 score over the vanilla genetic algorithm. The code for the Gradient GA is openly accessible on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2502.09860v2 Announce Type: replace-cross 
Abstract: Molecular discovery has brought great benefits to the chemical industry. Various molecule design techniques are developed to identify molecules with desirable properties. Traditional optimization methods, such as genetic algorithms, continue to achieve state-of-the-art results across multiple molecular design benchmarks. However, these techniques rely solely on random walk exploration, which hinders both the quality of the final solution and the convergence speed. To address this limitation, we propose a novel approach called Gradient Genetic Algorithm (Gradient GA), which incorporates gradient information from the objective function into genetic algorithms. Instead of random exploration, each proposed sample iteratively progresses toward an optimal solution by following the gradient direction. We achieve this by designing a differentiable objective function parameterized by a neural network and utilizing the Discrete Langevin Proposal to enable gradient guidance in discrete molecular spaces. Experimental results demonstrate that our method significantly improves both convergence speed and solution quality, outperforming cutting-edge techniques. For example, it achieves up to a 25% improvement in the top-10 score over the vanilla genetic algorithm. The code is publicly available at https://github.com/debadyuti23/GradientGA.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEDICorrect: A Scalable Python Tool for Orbit-, Beam-, and Footprint-Level GEDI Geolocation Correction</title>
<link>https://arxiv.org/abs/2511.00319</link>
<guid>https://arxiv.org/abs/2511.00319</guid>
<content:encoded><![CDATA[
<div> geolocation, GEDI data, LiDAR, accuracy, framework

Summary:
- The study introduces a framework called GEDICorrect for geolocation correction of GEDI LiDAR data at orbit, beam, and footprint levels.
- GEDICorrect integrates existing GEDI Simulator modules and enhances functionality with flexible correction logic, multiple similarity metrics, adaptive clustering, and optimized I/O handling.
- By using the Kullback--Leibler divergence as a waveform similarity metric, GEDICorrect significantly improves canopy height and terrain elevation accuracy.
- Canopy height accuracy is enhanced from $R^2 = 0.61$ to 0.78 with footprint-level correction, reducing RMSE and rRMSE.
- Terrain elevation accuracy also sees improvements, with GEDICorrect reducing RMSE compared to uncorrected data and the GEDI Simulator baseline.
- The framework demonstrates computational efficiency, achieving a speedup over the GEDI Simulator and scaling efficiently to 24 cores for a significant improvement in runtime.
<br /><br />Summary: <div>
arXiv:2511.00319v1 Announce Type: new 
Abstract: Accurate geolocation is essential for the reliable use of GEDI LiDAR data in footprint-scale applications such as aboveground biomass modeling, data fusion, and ecosystem monitoring. However, residual geolocation errors arising from both systematic biases and random ISS-induced jitter can significantly affect the accuracy of derived vegetation and terrain metrics. The main goal of this study is to develop and evaluate a flexible, computationally efficient framework (GEDICorrect) that enables geolocation correction of GEDI data at the orbit, beam, and footprint levels. The framework integrates existing GEDI Simulator modules (gediRat and gediMetrics) and extends their functionality with flexible correction logic, multiple similarity metrics, adaptive footprint clustering, and optimized I/O handling. Using the Kullback--Leibler divergence as the waveform similarity metric, GEDICorrect improved canopy height (RH95) accuracy from $R^2 = 0.61$ (uncorrected) to 0.74 with the orbit-level correction, and up to $R^2 = 0.78$ with the footprint-level correction, reducing RMSE from 2.62~m ($rRMSE = 43.13\%$) to 2.12~m ($rRMSE = 34.97\%$) at the orbit level, and 2.01~m ($rRMSE = 33.05\%$) at the footprint level. Terrain elevation accuracy also improved, decreasing RMSE by 0.34~m relative to uncorrected data and by 0.37~m compared to the GEDI Simulator baseline. In terms of computational efficiency, GEDICorrect achieved a $\sim2.4\times$ speedup over the GEDI Simulator in single-process mode (reducing runtime from $\sim84$~h to $\sim35$~h) and scaled efficiently to 24 cores, completing the same task in $\sim4.3$~h -- an overall $\sim19.5\times$ improvement. GEDICorrect offers a robust and scalable solution for improving GEDI geolocation accuracy while maintaining full compatibility with standard GEDI data products.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STARC-9: A Large-scale Dataset for Multi-Class Tissue Classification for CRC Histopathology</title>
<link>https://arxiv.org/abs/2511.00383</link>
<guid>https://arxiv.org/abs/2511.00383</guid>
<content:encoded><![CDATA[
<div> Dataset, tissue classification, colorectal cancer, histopathologic images, DeepCluster++

Summary:
The article introduces STARC-9, a large-scale dataset for multi-class tissue classification in colorectal cancer (CRC) histopathologic images. The dataset consists of 630,000 image tiles across nine tissue classes from 200 CRC patients. STARC-9 was created using the DeepCluster++ framework, which ensures intra-class diversity and reduces manual curation. The process involves extracting feature vectors, clustering similar tiles, and sampling diverse morphologic patterns within each class. Expert pathologists verify the selected tiles, resulting in high-quality data. Models trained on STARC-9 show superior generalizability compared to existing datasets, demonstrating its utility in CRC tissue classification and segmentation tasks. DeepCluster++ is highlighted as a flexible framework that can be applied to construct high-quality datasets from large WSI repositories for various cancer and non-cancer applications. 

<br /><br />Summary: <div>
arXiv:2511.00383v1 Announce Type: new 
Abstract: Multi-class tissue-type classification of colorectal cancer (CRC) histopathologic images is a significant step in the development of downstream machine learning models for diagnosis and treatment planning. However, existing public CRC datasets often lack morphologic diversity, suffer from class imbalance, and contain low-quality image tiles, limiting model performance and generalizability. To address these issues, we introduce STARC-9 (STAnford coloRectal Cancer), a large-scale dataset for multi-class tissue classification. STARC-9 contains 630,000 hematoxylin and eosin-stained image tiles uniformly sampled across nine clinically relevant tissue classes (70,000 tiles per class) from 200 CRC patients at the Stanford University School of Medicine. The dataset was built using a novel framework, DeepCluster++, designed to ensure intra-class diversity and reduce manual curation. First, an encoder from a histopathology-specific autoencoder extracts feature vectors from tiles within each whole-slide image. Then, K-means clustering groups morphologically similar tiles, followed by equal-frequency binning to sample diverse morphologic patterns within each class. The selected tiles are subsequently verified by expert gastrointestinal pathologists to ensure accuracy. This semi-automated process significantly reduces manual effort while producing high-quality, diverse tiles. To evaluate STARC-9, we benchmarked convolutional neural networks, transformers, and pathology-specific foundation models on multi-class CRC tissue classification and segmentation tasks, showing superior generalizability compared to models trained on existing datasets. Although we demonstrate the utility of DeepCluster++ on CRC as a pilot use-case, it is a flexible framework that can be used for constructing high-quality datasets from large WSI repositories across a wide range of cancer and non-cancer applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeltaLag: Learning Dynamic Lead-Lag Patterns in Financial Markets</title>
<link>https://arxiv.org/abs/2511.00390</link>
<guid>https://arxiv.org/abs/2511.00390</guid>
<content:encoded><![CDATA[
<div> deep learning, lead-lag effect, financial markets, portfolio construction, predictive signals
<br />
Summary:
DeltaLag is introduced as an innovative deep learning approach for detecting and exploiting dynamic lead-lag structures in financial markets. It utilizes a sparsified cross-attention mechanism to identify relevant lead-lag pairs and extract lag-aligned raw features for predicting future returns of lagger stocks. The method outperforms traditional baselines and precomputed lead-lag graphs, demonstrating superior adaptability in dynamic market conditions. DeltaLag surpasses temporal and spatio-temporal deep learning models in stock prediction accuracy and interpretability, offering enhanced trading performance. <div>
arXiv:2511.00390v1 Announce Type: new 
Abstract: The lead-lag effect, where the price movement of one asset systematically precedes that of another, has been widely observed in financial markets and conveys valuable predictive signals for trading. However, traditional lead-lag detection methods are limited by their reliance on statistical analysis methods and by the assumption of persistent lead-lag patterns, which are often invalid in dynamic market conditions. In this paper, we propose \textbf{DeltaLag}, the first end-to-end deep learning method that discovers and exploits dynamic lead-lag structures with pair-specific lag values in financial markets for portfolio construction. Specifically, DeltaLag employs a sparsified cross-attention mechanism to identify relevant lead-lag pairs. These lead-lag signals are then leveraged to extract lag-aligned raw features from the leading stocks for predicting the lagger stock's future return. Empirical evaluations show that DeltaLag substantially outperforms both fixed-lag and self-lead-lag baselines. In addition, its adaptive mechanism for identifying lead-lag relationships consistently surpasses precomputed lead-lag graphs based on statistical methods. Furthermore, DeltaLag outperforms a wide range of temporal and spatio-temporal deep learning models designed for stock prediction or time series forecasting, offering both better trading performance and enhanced interpretability.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Meta-Cognitive Swarm Intelligence Framework for Resilient UAV Navigation in GPS-Denied and Cluttered Environments</title>
<link>https://arxiv.org/abs/2511.00884</link>
<guid>https://arxiv.org/abs/2511.00884</guid>
<content:encoded><![CDATA[
<div> framework, UAV swarms, autonomous navigation, meta-cognition, Self-Learning Slime Mould Algorithm

Summary:
The paper introduces a new framework for autonomous navigation of UAV swarms in challenging environments. The framework, called Self-Learning Slime Mould Algorithm (SLSMA), incorporates meta-cognitive principles to enable real-time adaptation and recovery from planning failures. It includes a situation-aware search strategy, a collective memory mechanism, and an adaptive recovery behavior. The multi-UAV trajectory problem is formulated as a resilient planning challenge, considering path length, collisions, navigational uncertainty, and proximity to failure states. Extensive simulations demonstrate that the SLSMA outperforms existing metaheuristics in terms of mission success rate, recovery speed, and solution reliability. This work marks a crucial advancement towards the development of truly autonomous UAV swarms capable of operating persistently in complex and dynamic environments. 

Summary:<br /><br />framework: new framework for autonomous UAV swarm navigation introduced<br />UAV swarms: multiple UAVs working together autonomously<br />autonomous navigation: navigating without human intervention<br />meta-cognition: incorporating cognitive principles in algorithms<br />Self-Learning Slime Mould Algorithm: algorithm integrating meta-cognition for real-time adaptation and recovery of UAV swarms <div>
arXiv:2511.00884v1 Announce Type: new 
Abstract: Autonomous navigation of UAV swarms in perceptually-degraded environments, where GPS is unavailable and terrain is densely cluttered, presents a critical bottleneck for real-world deployment. Existing optimization-based planners lack the resilience to avoid catastrophic convergence to local optima under such uncertainty. Inspired by principles of computational meta-cognition, this paper introduces a novel swarm intelligence framework that enables a fleet of UAVs to autonomously sense, adapt, and recover from planning failures in real-time. At its core is the Self-Learning Slime Mould Algorithm (SLSMA), which integrates three meta-cognitive layers: a situation-aware search strategy that dynamically selects between exploration and exploitation based on perceived search stagnation; a collective memory mechanism that allows the swarm to learn from and avoid previously failed trajectories; and an adaptive recovery behavior that triggers global re-exploration upon entrapment. We formulate the multi-UAV trajectory problem as a resilient planning challenge, with a cost function that penalizes not only path length and collisions but also navigational uncertainty and proximity to failure states. Extensive simulations in synthetically complex 3D worlds and against the CEC 2017 benchmark suite demonstrate the framework's superior performance. The SLSMA does not merely optimize paths; it generates resilient trajectories, demonstrating a 99.5% mission success rate and significantly outperforming state-of-the-art metaheuristics in recovery speed and solution reliability. This work provides a foundational step towards truly autonomous swarms capable of persistent operation in denied and dynamic environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigation of Performance and Scalability of a Quantum-Inspired Evolutionary Optimizer (QIEO) on NVIDIA GPU</title>
<link>https://arxiv.org/abs/2511.01298</link>
<guid>https://arxiv.org/abs/2511.01298</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum Inspired Evolutionary Optimization, GPU acceleration, 01 Knapsack problems, CUDA, parallel processing

Summary:
This study explores the performance and scalability of GPU-accelerated Quantum Inspired Evolutionary Optimization (QIEO) for solving large-scale 01 Knapsack problems. By leveraging CUDA's parallel processing capabilities on NVIDIA Tesla V100 GPUs, the researchers systematically analyze various problem sizes, kernel configurations, and memory models. They find that optimizing memory strategies and kernel configurations is crucial for maximizing efficiency, with constant memory outperforming other options up to hardware limits. Beyond these limits, global memory and tiling strategies offer trade-offs in performance. The study highlights both the potential and practical constraints of implementing QIEO on GPUs for complex combinatorial optimization problems, providing valuable insights for future large-scale metaheuristic implementations. 

<br /><br />Summary: <div>
arXiv:2511.01298v1 Announce Type: new 
Abstract: Quantum inspired evolutionary optimization leverages quantum computing principles like superposition, interference, and probabilistic representation to enhance classical evolutionary algorithms with improved exploration and exploitation capabilities. Implemented on NVIDIA Tesla V100 SXM2 GPUs, this study systematically investigates the performance and scalability of a GPU-accelerated Quantum Inspired Evolutionary Optimizer applied to large scale 01 Knapsack problems. By exploiting CUDA`s parallel processing capabilities, particularly through optimized memory management and thread configuration, significant speedups and efficient utilization of GPU resources is demonstrated. The analysis covers various problem sizes, kernel launch configurations, and memory models including constant, shared, global, and pinned memory, alongside extensive scaling studies. The results reveal that careful tuning of memory strategies and kernel configurations is essential for maximizing throughput and efficiency, with constant memory providing superior performance up to hardware limits. Beyond these limits, global memory and strategic tiling become necessary, albeit with some performance trade offs. The findings highlight both the promise and the practical constraints of applying QIEO on GPUs for complex combinatorial optimization, offering actionable insights for future large scale metaheuristic implementations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSMD: Curated Multimodal Dataset for Chinese Stock Analysis</title>
<link>https://arxiv.org/abs/2511.01318</link>
<guid>https://arxiv.org/abs/2511.01318</guid>
<content:encoded><![CDATA[
<div> Keywords: stock market analysis, Chinese stock market, multimodal dataset, LightQuant framework, financial domains<br />
Summary:<br />
The article introduces a new multimodal dataset, CSMD, specifically curated for analyzing the Chinese stock market. The dataset is meticulously processed to ensure validated quality and addresses the limitations of existing resources mainly focused on the U.S. stock market in English. The authors also present a lightweight and user-friendly framework called LightQuant for researchers and practitioners with expertise in financial domains. Experimental results using various backbone models demonstrate the effectiveness of the dataset and framework compared to existing datasets. The datasets and code are publicly available on GitHub, providing a valuable resource for researchers and practitioners interested in stock market analysis in the Chinese market. <br /><br />Summary: <div>
arXiv:2511.01318v1 Announce Type: new 
Abstract: The stock market is a complex and dynamic system, where it is non-trivial for researchers and practitioners to uncover underlying patterns and forecast stock movements. The existing studies for stock market analysis rely on leveraging various types of information to extract useful factors, which are highly conditional on the quality of the data used. However, the currently available resources are mainly based on the U.S. stock market in English, which is inapplicable to adapt to other countries. To address these issues, we propose CSMD, a multimodal dataset curated specifically for analyzing the Chinese stock market with meticulous processing for validated quality. In addition, we develop a lightweight and user-friendly framework LightQuant for researchers and practitioners with expertise in financial domains. Experimental results on top of our datasets and framework with various backbone models demonstrate their effectiveness compared with using existing datasets. The datasets and code are publicly available at the link: https://github.com/ECNU-CILAB/LightQuant.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solution Space Topology Guides CMTS Search</title>
<link>https://arxiv.org/abs/2511.01701</link>
<guid>https://arxiv.org/abs/2511.01701</guid>
<content:encoded><![CDATA[
<div> Keywords: Monte Carlo Tree Search, puzzle solving, solution space topology, compatibility graphs, algebraic connectivity <br />
Summary: 
- The study focuses on determining the most effective topology to guide Monte Carlo Tree Search (MCTS) in puzzle solving tasks.
- The research identifies that grid topology, commonly used in prior work, is not suitable for guiding MCTS due to its constant nature across all instances.
- The proposed approach involves measuring solution space topology, which determines the structure of valid color assignments based on detected pattern rules.
- The automatic detection of pattern rules is achieved with 100% accuracy for five types, and compatibility graphs are constructed to encode solution space structure.
- Topological features such as algebraic connectivity play a crucial role in task difficulty and are integrated into MCTS node selection to improve search efficiency. <br /><br />Summary: <div>
arXiv:2511.01701v1 Announce Type: new 
Abstract: A fundamental question in search-guided AI: what topology should guide Monte Carlo Tree Search (MCTS) in puzzle solving? Prior work applied topological features to guide MCTS in ARC-style tasks using grid topology -- the Laplacian spectral properties of cell connectivity -- and found no benefit. We identify the root cause: grid topology is constant across all instances. We propose measuring \emph{solution space topology} instead: the structure of valid color assignments constrained by detected pattern rules. We build this via compatibility graphs where nodes are $(cell, color)$ pairs and edges represent compatible assignments under pattern constraints.
  Our method: (1) detect pattern rules automatically with 100\% accuracy on 5 types, (2) construct compatibility graphs encoding solution space structure, (3) extract topological features (algebraic connectivity, rigidity, color structure) that vary with task difficulty, (4) integrate these features into MCTS node selection via sibling-normalized scores.
  We provide formal definitions, a rigorous selection formula, and comprehensive ablations showing that algebraic connectivity is the dominant signal. The work demonstrates that topology matters for search -- but only the \emph{right} topology. For puzzle solving, this is solution space structure, not problem space structure.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Synthesizability-Guided Pipeline for Materials Discovery</title>
<link>https://arxiv.org/abs/2511.01790</link>
<guid>https://arxiv.org/abs/2511.01790</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational materials discovery, crystal structures, density functional theory, synthesizability prediction, experimental synthesis <br />
Summary: 
Computational materials discovery involves generating crystal structures judged for plausibility with density functional theory, but these structures may not be experimentally accessible. A new combined compositional and structural synthesizability score has been developed to predict which compounds can be synthesized in a laboratory. Using this score, non-synthesized structures from various databases were evaluated, leading to the identification of several highly synthesizable candidates. Predicted synthesis pathways were tested experimentally across 16 targets, resulting in successful synthesis of 7 out of 16 compounds within just three days. This process not only filled gaps in known synthesized structures but also provided valuable insights into the practical usability of current materials databases. The study underscores the importance of synthesizability prediction in accelerating materials discovery. <br /><br />Summary: <div>
arXiv:2511.01790v1 Announce Type: new 
Abstract: Computational materials discovery relies on the generation of plausible crystal structures. The plausibility is typically judged through density functional theory methods which, while typically accurate at zero Kelvin, often favor low-energy structures that are not experimentally accessible. We develop a combined compositional and structural synthesizability score which provides an accurate way of predicting which compounds can actually be synthesized in a laboratory. We use it to evaluate non-synthesized structures from the Materials Project, GNoME, and Alexandria, and identified several hundred highly synthesizable candidates. We then predict synthesis pathways, conduct corresponding experiments, and characterize the products across 16 targets, successfully synthesizing 7 of 16. The entire experimental process was completed in only three days. Our results highlight omissions in lists of known synthesized structures, deliver insights into the practical utility of current materials databases, and showcase the central role synthesizability prediction can play in materials discovery.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection</title>
<link>https://arxiv.org/abs/2511.00047</link>
<guid>https://arxiv.org/abs/2511.00047</guid>
<content:encoded><![CDATA[
<div> Graph-BERT, financial fraud detection, dynamic financial transaction analysis, DynBERG, Gated Recurrent Unit (GRU) <br />
<br />
Summary: 
The study introduces DynBERG, a novel model that combines Graph-BERT with a GRU layer to analyze dynamic financial transaction networks. The model is designed to capture temporal evolution over multiple time steps and support directed edges, making it suitable for analyzing evolving structures in financial data. Tested on the Elliptic dataset containing Bitcoin transactions, including those during the Dark Market Shutdown event, DynBERG shows superior performance compared to other dynamic graph classification models like EvolveGCN and GCN. Its ability to adapt to significant market shifts is evident, with GRU playing a crucial role in modeling the temporal dynamics of financial transactions. DynBERG outperforms EvolveGCN before the market shutdown and surpasses GCN post-event, highlighting its effectiveness in detecting financial fraud and adapting to changing market conditions. <div>
arXiv:2511.00047v1 Announce Type: cross 
Abstract: Financial fraud detection is critical for maintaining the integrity of financial systems, particularly in decentralised environments such as cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are widely used for financial fraud detection, graph Transformer models such as Graph-BERT are gaining prominence due to their Transformer-based architecture, which mitigates issues such as over-smoothing. Graph-BERT is designed for static graphs and primarily evaluated on citation networks with undirected edges. However, financial transaction networks are inherently dynamic, with evolving structures and directed edges representing the flow of money. To address these challenges, we introduce DynBERG, a novel architecture that integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture temporal evolution over multiple time steps. Additionally, we modify the underlying algorithm to support directed edges, making DynBERG well-suited for dynamic financial transaction analysis. We evaluate our model on the Elliptic dataset, which includes Bitcoin transactions, including all transactions during a major cryptocurrency market event, the Dark Market Shutdown. By assessing DynBERG's resilience before and after this event, we analyse its ability to adapt to significant market shifts that impact transaction behaviours. Our model is benchmarked against state-of-the-art dynamic graph classification approaches, such as EvolveGCN and GCN, demonstrating superior performance, outperforming EvolveGCN before the market shutdown and surpassing GCN after the event. Additionally, an ablation study highlights the critical role of incorporating a time-series deep learning component, showcasing the effectiveness of GRU in modelling the temporal dynamics of financial transactions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications</title>
<link>https://arxiv.org/abs/2511.00366</link>
<guid>https://arxiv.org/abs/2511.00366</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital twins, Gaussian process models, Derivative data, Dynamic updating, Sparse GP approximation

Summary: 
Digital twins are crucial for modeling the behavior of physical assets, utilizing high-fidelity models or surrogates for accurate real-time forecasting. This study extends Gaussian process models by incorporating derivative data for improved accuracy, enabling dynamic updating with in-service data from the physical twin. Though derivative data enhances accuracy, it increases the covariance matrix dimension significantly. To address this issue, a sparse GP approximation is used, with developed extensions for derivative inclusion. Numerical experiments show that the derivative-enhanced sparse GP method produces more accurate predictions with dynamic data additions. The algorithm is applied in a digital twin framework to model fatigue crack growth in an aerospace vehicle, demonstrating practical utility in real-world applications. <div>
arXiv:2511.00366v1 Announce Type: cross 
Abstract: Digital twins are developed to model the behavior of a specific physical asset (or twin), and they can consist of high-fidelity physics-based models or surrogates. A highly accurate surrogate is often preferred over multi-physics models as they enable forecasting the physical twin future state in real-time. To adapt to a specific physical twin, the digital twin model must be updated using in-service data from that physical twin. Here, we extend Gaussian process (GP) models to include derivative data, for improved accuracy, with dynamic updating to ingest physical twin data during service. Including derivative data, however, comes at a prohibitive cost of increased covariance matrix dimension. We circumvent this issue by using a sparse GP approximation, for which we develop extensions to incorporate derivatives. Numerical experiments demonstrate that the prediction accuracy of the derivative-enhanced sparse GP method produces improved models upon dynamic data additions. Lastly, we apply the developed algorithm within a DT framework to model fatigue crack growth in an aerospace vehicle.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights</title>
<link>https://arxiv.org/abs/2511.01019</link>
<guid>https://arxiv.org/abs/2511.01019</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, oceanography, data integration, conversational platform, NOAA.

Summary:<br /><br />Artificial intelligence is revolutionizing scientific research, but conventional conversational AI systems often produce unreliable information, posing a threat to scientific integrity. In response to this challenge, the OceanAI platform has been developed to combine the linguistic proficiency of large language models with direct access to real-time oceanographic data from NOAA. By leveraging APIs to access and synthesize authoritative data, OceanAI can provide accurate and reproducible answers to queries, such as historical water levels in specific locations. In a comparison with other AI chat interfaces, OceanAI was the only platform to consistently produce accurate values supported by original data sources. The platform's extensibility allows it to connect with various NOAA datasets, enabling applications in marine hazard prediction, ecosystem evaluation, and water quality monitoring. By prioritizing transparency and verifiability, OceanAI enhances the trustworthiness and credibility of AI-driven decision-making processes in ocean-related domains. Visit https://oceanai.ai4ocean.xyz for a live demonstration of OceanAI. <div>
arXiv:2511.01019v1 Announce Type: cross 
Abstract: Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified "hallucinations" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as "What was Boston Harbor's highest water level in 2024?" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at https://oceanai.ai4ocean.xyz.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novelty and Impact of Economics Papers</title>
<link>https://arxiv.org/abs/2511.01211</link>
<guid>https://arxiv.org/abs/2511.01211</guid>
<content:encoded><![CDATA[
<div> framework, scientific novelty, spatial novelty, temporal novelty, Large Language Models

Summary:
The article introduces a framework for understanding scientific novelty as a reflection of a paper's position within the evolving intellectual landscape. It decomposes novelty into spatial and temporal dimensions, using semantic isolation metrics derived from Large Language Models to quantify a paper's location relative to the literature. The study on a corpus of economics articles reveals a trade-off between spatial and temporal novelty: temporal novelty predicts citation counts, while spatial novelty predicts disruptive impact. This distinction helps identify four archetypes of semantic neighborhoods with distinct impact profiles. The research highlights that novelty is a multidimensional construct with different forms that have measurable and distinct consequences for scientific progress. <div>
arXiv:2511.01211v1 Announce Type: cross 
Abstract: We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper's intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Modeling of Precipitation in Electrolyte Systems</title>
<link>https://arxiv.org/abs/2511.01519</link>
<guid>https://arxiv.org/abs/2511.01519</guid>
<content:encoded><![CDATA[
<div> modeling, precipitation, electrolyte systems, continuous processes, crystallization 

Summary: 
The study proposes a dynamic modeling approach for precipitation in electrolyte systems, specifically focusing on the crystallization of an aromatic amine in continuous processes. The novel model integrates equilibrium and crystallization kinetics, assuming rapid equilibrium establishment and formulating as a set of differential algebraic equations. Key features include a population balance equation model for particle size distribution and modeling dynamically changing equilibria. The dynamic model's predictions align well with experimental measurements, aiming to assist the transition from batch to continuous processes by providing a foundation for numerical optimization and advanced control. <div>
arXiv:2511.01519v1 Announce Type: cross 
Abstract: This study presents a dynamic modeling approach for precipitation in electrolyte systems, focusing on the crystallization of an aromatic amine through continuous processes. A novel model, integrating equilibrium and crystallization kinetics, is formulated and applied to a continuous oscillatory baffled reactor. The approach assumes rapid equilibrium establishment and is formulated as a set of differential algebraic equations. Key features include a population balance equation model to describe the particle size distribution and the modeling of dynamically changing equilibria. The predictions of the dynamic model show good agreement with the available experimental measurements. The model is aimed at aiding the transition from a batch process to continuous process by forming the basis for numerical optimization and advanced control.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disciplined Biconvex Programming</title>
<link>https://arxiv.org/abs/2511.01813</link>
<guid>https://arxiv.org/abs/2511.01813</guid>
<content:encoded><![CDATA[
<div> Keywords: disciplined biconvex programming, biconvex optimization, alternate convex search, convex optimization, Python package<br />
Summary:<br />
Disciplined biconvex programming (DBCP) is proposed as a framework for solving biconvex optimization problems in various fields such as machine learning and signal processing. Typically, these problems are tackled using heuristics like alternate convex search (ACS) methods. DBCP simplifies the process by extending disciplined convex programming principles to biconvex problems, enabling users to specify problems with minimal syntax rules. The framework automatically splits and transforms problems into convex subproblems, generating customized ACS solvers for efficient solutions. Implemented in the Python package dbcp, DBCP integrates seamlessly with the popular CVXPY language for convex optimization. This enhancement allows users to experiment with diverse biconvex formulations without requiring specialized knowledge in convex optimization. <div>
arXiv:2511.01813v1 Announce Type: cross 
Abstract: We introduce disciplined biconvex programming (DBCP), a modeling framework for specifying and solving biconvex optimization problems. Biconvex optimization problems arise in various applications, including machine learning, signal processing, computational science, and control. Solving a biconvex optimization problem in practice usually resolves to heuristic methods based on alternate convex search (ACS), which iteratively optimizes over one block of variables while keeping the other fixed, so that the resulting subproblems are convex and can be efficiently solved. However, designing and implementing an ACS solver for a specific biconvex optimization problem usually requires significant effort from the user, which can be tedious and error-prone. DBCP extends the principles of disciplined convex programming to biconvex problems, allowing users to specify biconvex optimization problems in a natural way based on a small number of syntax rules. The resulting problem can then be automatically split and transformed into convex subproblems, for which a customized ACS solver is then generated and applied. DBCP allows users to quickly experiment with different biconvex problem formulations, without expertise in convex optimization. We implement DBCP into the open source Python package dbcp, as an extension to the famous domain specific language CVXPY for convex optimization.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning</title>
<link>https://arxiv.org/abs/2505.17050</link>
<guid>https://arxiv.org/abs/2505.17050</guid>
<content:encoded><![CDATA[
<div> multimodal data, Project-Based Learning, large language models, educational tasks, benchmark

Summary:
The article introduces PBLBench, a benchmark designed to evaluate complex reasoning in Project-Based Learning using large language models. It addresses the limitations of existing benchmarks by providing a free-form output structure and rigorous human expert validation process. Automated pipelines are lacking due to model hallucination and instability. PBLBench challenges models with tasks that resemble those handled by human experts, grounded in domain-specific knowledge and long-context understanding. The benchmark utilizes the Analytic Hierarchy Process for reliable ground truth, resulting in structured and weighted evaluation criteria. Performance of 15 MLLMs/LLMs on PBLBench shows that even advanced models achieve only 59% rank accuracy, highlighting the challenges it presents. PBLBench aims to facilitate the development of more capable AI agents to assist teachers and enhance educational productivity.<br /><br />Summary: <div>
arXiv:2505.17050v2 Announce Type: replace-cross 
Abstract: Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMint-SDE: A Multimodal Foundation Model for Accelerating Numerical Simulation of SDEs via Error Correction</title>
<link>https://arxiv.org/abs/2510.27173</link>
<guid>https://arxiv.org/abs/2510.27173</guid>
<content:encoded><![CDATA[
<div> decoder-only transformer, in-context learning, error-correction scheme, universal model, simulation tool

Summary:
FMint-SDE is a new multi-modal foundation model for large-scale simulations of differential equations, combining numerical and textual modalities to improve accuracy and efficiency. Unlike traditional numerical integrators, FMint-SDE uses a decoder-only transformer with in-context learning to learn a universal error-correction scheme. By training the model on prompted sequences of coarse solutions, it achieves superior accuracy and efficiency tradeoff compared to classical solvers. The model shows promise in various applications such as molecular dynamics, mechanical systems, finance, and biology, highlighting its potential as a general-purpose simulation tool for dynamical systems. <div>
arXiv:2510.27173v1 Announce Type: new 
Abstract: Fast and accurate simulation of dynamical systems is a fundamental challenge across scientific and engineering domains. Traditional numerical integrators often face a trade-off between accuracy and computational efficiency, while existing neural network-based approaches typically require training a separate model for each case. To overcome these limitations, we introduce a novel multi-modal foundation model for large-scale simulations of differential equations: FMint-SDE (Foundation Model based on Initialization for stochastic differential equations). Based on a decoder-only transformer with in-context learning, FMint-SDE leverages numerical and textual modalities to learn a universal error-correction scheme. It is trained using prompted sequences of coarse solutions generated by conventional solvers, enabling broad generalization across diverse systems. We evaluate our models on a suite of challenging SDE benchmarks spanning applications in molecular dynamics, mechanical systems, finance, and biology. Experimental results show that our approach achieves a superior accuracy-efficiency tradeoff compared to classical solvers, underscoring the potential of FMint-SDE as a general-purpose simulation tool for dynamical systems.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Control for a Physics-Informed Model of a Thermal Energy Distribution System: Qualitative Analysis</title>
<link>https://arxiv.org/abs/2510.26959</link>
<guid>https://arxiv.org/abs/2510.26959</guid>
<content:encoded><![CDATA[
<div> optimal control, adaptive control, integrated energy systems, uncertainties, heat exchangers

Summary:
- Integrated energy systems (IES) are complex architectures that require operating control strategy optimization but are hindered by uncertainties.
- Adaptive control (AC) is proposed as a methodology to accommodate uncertainties in real-time for linear systems where all states are observable.
- A specific application of AC to a glycol heat exchanger (GHX) in an IES showed significant reduction in error metrics with minimal computing overhead.
- The study found that AC can reduce mean absolute error and integral time absolute error by 30%-75% compared to a nominal control approach.
- The control effort induced by AC is significant, requiring further study to estimate its impact on physical systems. 
- The formulation of AC is being enhanced to address challenges such as partially observable and non-linear dynamics. 

<br /><br />Summary: <div>
arXiv:2510.26959v1 Announce Type: cross 
Abstract: Integrated energy systems (IES) are complex heterogeneous architectures that typically encompass power sources, hydrogen electrolyzers, energy storage, and heat exchangers. This integration is achieved through operating control strategy optimization. However, the lack of physical understanding as to how these systems evolve over time introduces uncertainties that hinder reliable application thereof. Techniques that can accommodate such uncertainties are fundamental for ensuring proper operation of these systems. Unfortunately, no unifying methodology exists for accommodating uncertainties in this regard. That being said, adaptive control (AC) is a discipline that may allow for accommodating such uncertainties in real-time. In the present work, we derive an AC formulation for linear systems in which all states are observable and apply it to the control of a glycol heat exchanger (GHX) in an IES. Based on prior research in which we quantified the uncertainties of the GHXs system dynamics, we introduced an error of 50% on four terms of the nominal model. In the case where a linear quadratic regulator is used as the nominal control for the reference system, we found that employing AC can reduce the mean absolute error and integral time absolute error by a factor of 30%-75%. This reduction is achieved with minimal computing overhead and control infrastructure, thus underscoring the strength of AC. However, the control effort induced is significant, therefore warranting further study in order to estimate its impact on a physical system. To address further challenges, including partially observable and non-linear dynamics, enhancements of the linear formulation are currently being developed.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Black-Scholes Model, comparison between Analytical Solution and Numerical Analysis</title>
<link>https://arxiv.org/abs/2510.27277</link>
<guid>https://arxiv.org/abs/2510.27277</guid>
<content:encoded><![CDATA[
<div> Keywords: option-pricing model, Black-Scholes, calculus, analytical method, numerical method

Summary: <br /><br />
This article provides an overview of the Black-Scholes option-pricing model, discussing its history, context, and significance in economics. The model's foundation is based on calculus concepts, with explanations on how to derive and solve the equation using both analytical and numerical methods. The article concludes by discussing the practical applications of the Black-Scholes model in today's financial markets. Additionally, two appendices are included to enhance the reader's understanding, one with essential economics concepts and another with code scripts for practical implementation. <div>
arXiv:2510.27277v1 Announce Type: cross 
Abstract: The main purpose of this article is to give a general overview and understanding of the first widely used option-pricing model, the Black-Scholes model. The history and context are presented, with the usefulness and implications in the economics world. A brief review of fundamental calculus concepts is introduced to derive and solve the model. The equation is then resolved using both an analytical (variable separation) and a numerical method (finite differences). Conclusions are drawn in order to understand how Black-Scholes is employed nowadays. At the end a handy appendix (A) is written with some economics notions to ease the reader's comprehension of the paper; furthermore a second appendix (B) is given with some code scripts, to allow the reader to put in practice some concepts.
]]></content:encoded>
<pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reduced order modelling of Hopf bifurcations for the Navier-Stokes equations through invariant manifolds</title>
<link>https://arxiv.org/abs/2510.26542</link>
<guid>https://arxiv.org/abs/2510.26542</guid>
<content:encoded><![CDATA[
<div> parametric simulation-free reduced order model, incompressible flows, Hopf bifurcation, parametrisation method, invariant manifolds<br />
<br />
parametric simulation-free reduced order model for incompressible flows undergoing a Hopf bifurcation is introduced. This method directly operates on the governing equations, eliminating the need for full-order simulations. The model, computed at a specific bifurcation parameter value, remains valid over a range of values. It systematically constructs an invariant manifold and embedded dynamics, providing an efficient reduction of the original system. The proposed model accurately captures pre-critical steady states, the bifurcation point, and post-critical limit cycle oscillations, demonstrating strong agreement with full order simulations and significant computational speed-up. <br /><br />Summary: <div>
arXiv:2510.26542v1 Announce Type: new 
Abstract: This work introduces a parametric simulation-free reduced order model for incompressible flows undergoing a Hopf bifurcation, leveraging the parametrisation method for invariant manifolds. Unlike data-driven approaches, this method operates directly on the governing equations, eliminating the need for full-order simulations. The proposed model is computed at a single value of the bifurcation parameter yet remains valid over a range of values. The approach systematically constructs an invariant manifold and embedded dynamics, providing an accurate and efficient reduction of the original system. The ability to capture pre-critical steady states, the bifurcation point, and post-critical limit cycle oscillations is demonstrated by a strong agreement between the reduced order model and full order simulations, while achieving significant computational speed-up.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fast spectral overlapping domain decomposition method with discretization-independent conditioning bounds</title>
<link>https://arxiv.org/abs/2510.25991</link>
<guid>https://arxiv.org/abs/2510.25991</guid>
<content:encoded><![CDATA[
<div> domain decomposition method, variable-coefficient elliptic PDEs, H-matrix structure, reduced linear system, black-box randomized compression

Summary:
A new domain decomposition method is presented for solving variable-coefficient elliptic partial differential equations on regular domains. The method involves tessellating the domain into thin slabs or shells and creating a reduced linear system connecting the domains. By utilizing H-matrix structure, the method efficiently handles large dense blocks in the system. The formulation ensures well-conditioning, converging to a second kind Fredholm equation with refined local solves. Data sparsity in dense blocks leads to faster H-matrix arithmetic. The reduced linear system is formed using black-box randomized compression, leveraging the efficiency of sparse direct solvers on sub-domains. The solver successfully handles oscillatory 2D and 3D problems with up to 28 million degrees of freedom. <br /><br />Summary: <div>
arXiv:2510.25991v1 Announce Type: cross 
Abstract: A domain decomposition method for the solution of general variable-coefficient elliptic partial differential equations on regular domains is introduced. The method is based on tessellating the domain into overlapping thin slabs or shells, and then explicitly forming a reduced linear system that connects the different domains. Rank-structure ('H-matrix structure') is exploited to handle the large dense blocks that arise in the reduced linear system. Importantly, the formulation used is well-conditioned, as it converges to a second kind Fredholm equation as the precision in the local solves is refined. Moreover, the dense blocks that arise are far more data-sparse than in existing formulations, leading to faster and more efficient H-matrix arithmetic. To form the reduced linear system, black-box randomized compression is used, taking full advantage of the fact that sparse direct solvers are highly efficient on the thin sub-domains. Numerical experiments demonstrate that our solver can handle oscillatory 2D and 3D problems with as many as 28 million degrees of freedom.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Manage Investment Portfolios beyond Simple Utility Functions</title>
<link>https://arxiv.org/abs/2510.26165</link>
<guid>https://arxiv.org/abs/2510.26165</guid>
<content:encoded><![CDATA[
<div> Keywords: investment funds, generative framework, latent representations, U.S. equity mutual funds, market simulation

Summary: 
A generative framework is proposed to learn latent representations of investment fund manager strategies without explicit utility specification. The framework models the conditional probability of a fund's portfolio weights based on various factors. It successfully captures known investment styles and reveals implicit manager objectives. The framework is validated on a dataset of U.S. equity mutual funds, showing heterogeneous realizations for turnover, concentration, and latent factors. Tests are developed to explain and interpret the model, demonstrating that expert labeling is contained in a linear interpretable way. The framework provides a data-driven approach for characterizing investment strategies, with applications in market simulation, strategy attribution, and regulatory oversight. <div>
arXiv:2510.26165v1 Announce Type: cross 
Abstract: While investment funds publicly disclose their objectives in broad terms, their managers optimize for complex combinations of competing goals that go beyond simple risk-return trade-offs. Traditional approaches attempt to model this through multi-objective utility functions, but face fundamental challenges in specification and parameterization. We propose a generative framework that learns latent representations of fund manager strategies without requiring explicit utility specification.
  Our approach directly models the conditional probability of a fund's portfolio weights, given stock characteristics, historical returns, previous weights, and a latent variable representing the fund's strategy. Unlike methods based on reinforcement learning or imitation learning, which require specified rewards or labeled expert objectives, our GAN-based architecture learns directly from the joint distribution of observed holdings and market data.
  We validate our framework on a dataset of 1436 U.S. equity mutual funds. The learned representations successfully capture known investment styles, such as "growth" and "value," while also revealing implicit manager objectives. For instance, we find that while many funds exhibit characteristics of Markowitz-like optimization, they do so with heterogeneous realizations for turnover, concentration, and latent factors.
  To analyze and interpret the end-to-end model, we develop a series of tests that explain the model, and we show that the benchmark's expert labeling are contained in our model's encoding in a linear interpretable way.
  Our framework provides a data-driven approach for characterizing investment strategies for applications in market simulation, strategy attribution, and regulatory oversight.
]]></content:encoded>
<pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stiff Circuit System Modeling via Transformer</title>
<link>https://arxiv.org/abs/2510.24727</link>
<guid>https://arxiv.org/abs/2510.24727</guid>
<content:encoded><![CDATA[
<div> Transformer model, time-series prediction, Kolmogorov-Arnold Networks, stiff circuit, circuit behavior modeling <br />
Summary: <br />
The article introduces a new approach for modeling stiff circuit transient behavior using a combination of Crossformer and Kolmogorov-Arnold Networks (KANs). The proposed method leverages the temporal representation capabilities of Crossformer and the enhanced feature extraction of KANs to predict circuit responses accurately. Experimental evaluations conducted on datasets generated through SPICE simulations of analog-to-digital converter (ADC) circuits demonstrate the effectiveness of the approach. The results show improved fidelity in predicting circuit responses to various input conditions, along with significant reductions in training time and error rates. This new approach presents a promising solution for accurately and efficiently modeling stiff circuits in electronic design automation. <div>
arXiv:2510.24727v1 Announce Type: new 
Abstract: Accurate and efficient circuit behavior modeling is a cornerstone of modern electronic design automation. Among different types of circuits, stiff circuits are challenging to model using previous frameworks. In this work, we propose a new approach using Crossformer, which is a current state-of-the-art Transformer model for time-series prediction tasks, combined with Kolmogorov-Arnold Networks (KANs), to model stiff circuit transient behavior. By leveraging the Crossformer's temporal representation capabilities and the enhanced feature extraction of KANs, our method achieves improved fidelity in predicting circuit responses to a wide range of input conditions. Experimental evaluations on datasets generated through SPICE simulations of analog-to-digital converter (ADC) circuits demonstrate the effectiveness of our approach, with significant reductions in training time and error rates.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Black Box Variational Inference Scheme for Inverse Problems with Demanding Physics-Based Models</title>
<link>https://arxiv.org/abs/2510.25038</link>
<guid>https://arxiv.org/abs/2510.25038</guid>
<content:encoded><![CDATA[
<div> black box variational inference, importance sampling, Bayesian inference, computational costs, non-differentiable models

Summary:
The article introduces a novel Bayesian inference approach based on black box variational inference for addressing inverse problems with costly forward models. The method utilizes importance sampling to estimate the variational objective gradient without the need for forward model gradients, reducing computational costs. A batch-sequential sampling procedure is implemented to determine when new model evaluations are required, optimizing variational parameter updates. This approach is particularly suitable for inverse problems involving non-differentiable, physics-based models. Efficiency gains of the proposed method are demonstrated through benchmarks, including density matching and Bayesian calibration of a nonlinear electro-chemo-mechanical model for solid-state batteries. This method outperforms sequential Monte Carlo and Markov-Chain Monte Carlo, showcasing its effectiveness in managing uncertainties and optimizing computational resources. 

<br /><br />Summary: <div>
arXiv:2510.25038v1 Announce Type: new 
Abstract: Bayesian methods are particularly effective for addressing inverse problems due to their ability to manage uncertainties inherent in the inference process. However, employing these methods with costly forward models poses significant challenges, especially in the context of non-differentiable models, where the absence of likelihood model gradient information can result in high computational costs. To tackle this issue, we develop a novel Bayesian inference approach based on black box variational inference, utilizing importance sampling to reuse existing simulation model calls in the variational objective gradient estimation, without relying on forward model gradients. The novelty lies in a new batch-sequential sampling procedure, which only requires new model evaluations if the currently available model evaluations fail to yield a suitable approximation of the objective gradient. The resulting approach reduces computational costs by leading to variational parameter updates without requiring new model evaluations when possible, while adaptively increasing the number of model calls per iteration as needed. In combination with its black box nature, this new approach is suitable for inverse problems involving demanding physics-based models that lack model gradients. We demonstrate the efficiency gains of the proposed method compared to its baseline version, sequential Monte Carlo, and Markov-Chain Monte Carlo in diverse benchmarks, ranging from density matching to the Bayesian calibration of a nonlinear electro-chemo-mechanical model for solid-state batteries.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Financial Decision-Making: Machine Learning and AI-Powered Predictions and Analysis</title>
<link>https://arxiv.org/abs/2510.25201</link>
<guid>https://arxiv.org/abs/2510.25201</guid>
<content:encoded><![CDATA[
<div> Machine learning algorithms, financial prediction, AI-driven platform, inflation analysis, stock market prediction<br />
Summary:<br />
The proposed system utilizes machine learning algorithms to improve financial prediction accuracy. It includes an AI-driven platform offering inflation analysis, stock market prediction, and an E-learning module with a chatbot. Achieving high accuracy, it demonstrates 0.8% MAE for inflation analysis and 98% and 96% accuracy for Apple and Google stock price predictions, respectively. Key features comprise historical price trends, inflation rates, and short-term stock prediction using real-world financial datasets. The E-learning component helps bridge financial knowledge gaps and aids in making informed decisions. Implemented algorithms include linear regression, ARIMA, and LSTM, with accuracy evaluated using metrics like MAE and RMSE. <br /><br />Summary: <div>
arXiv:2510.25201v1 Announce Type: new 
Abstract: The proposed system aims to use various machine learning algorithms to enhance financial prediction and generate highly accurate analyses. It introduces an AI-driven platform which offers inflation-analysis, stock market prediction, and E-learning module powered by a chatbot. It has achieved high accuracy where the Inflation Analysis depicts 0.8% MAE, 1.2% RMSE and the Stock Prediction shows 98% and 96% accuracy for Apple and Google stock prices respectively. Key features include historical price trends, inflation rates, short-term future stock prediction, where the data has been extracted using real-world financial datasets. Additionally, the E-learning feature contributes to bridging financial gaps and promoting informed decisions. We have implemented algorithms like linear regression, ARIMA, LSTM where the accuracy has been evaluated using metrics such as MAE, RMSE and the like.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related to Metal Casting</title>
<link>https://arxiv.org/abs/2510.25697</link>
<guid>https://arxiv.org/abs/2510.25697</guid>
<content:encoded><![CDATA[
<div> Reframing, Metal casting, 2D operator learning, Surrogate, CFD simulations <br />
Summary:<br /> 
This work presents a novel approach to mold filling in metal casting by utilizing a 2D operator learning surrogate to replace costly transient CFD simulations. The method combines a graph-based encoder for aggregating neighborhood information, a Fourier spectral core for capturing global interactions, and a graph-based decoder for mapping latent fields. The model predicts velocities, pressure, and volume fraction efficiently across various ingate locations and process settings, with 5% mean relative L2 errors. Inference is significantly faster than traditional CFD simulations, enabling rapid design exploration. Ablation studies demonstrate the model's robustness to spatial and temporal subsampling, while reducing training data only slightly affects accuracy. Overall, this approach showcases the effectiveness of neural operators as surrogates for mold filling and enables quick optimization of gating system designs in casting workflows. <br /> <div>
arXiv:2510.25697v1 Announce Type: new 
Abstract: This work reframes mold filling in metal casting as a simplified 2D operator learning surrogate to avoid costly transient CFD simulations. The method combines a graph based encoder that aggregates neighborhood information on an unstructured input mesh to encode geometry and boundary data, a Fourier spectral core that operates on a regular latent grid to capture global interactions, and a graph based decoder that maps latent fields back to a target mesh. The model jointly predicts velocities, pressure, and volume fraction over a fixed horizon and generalizes across varied ingate locations and process settings. On held out geometries and inlet conditions it reproduces large scale advection and the fluid air interface with errors concentrated near steep gradients. Mean relative L2 errors are about 5 percent across all fields. Inference is roughly 100 to 1000 times faster than conventional CFD simulations, thereby enabling rapid in-the-loop design exploration. Ablation studies show accuracy drops monotonically with stronger spatial subsampling of input vertices while temporal subsampling causes a gentler decline. Cutting the training data by 50 percent yields only small error growth. Overall the results demonstrate neural operators as efficient surrogates for 2D mold filling and related filling problems and enable fast exploration and optimization of gating system designs in casting workflows.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework</title>
<link>https://arxiv.org/abs/2510.25402</link>
<guid>https://arxiv.org/abs/2510.25402</guid>
<content:encoded><![CDATA[
<div> quality evaluation, patent content, AI drafting tools, systematic evaluation, improvement suggestions

Summary: 
A proposed framework for evaluating patent quality addresses the lack of research in this area, using modules to assess regulatory compliance, technical coherence, and figure-reference consistency. Results on a dataset of human-authored and AI-generated patents demonstrate high accuracies in detection modules, pointing to the need for attention to figure-text consistency and technical detail precision. Analysis reveals specific issues in Mechanical Engineering and Construction patents, with AI-generated patents showing more structural defects compared to human-authored ones. The latter primarily contain surface-level errors, while the former exhibit issues in figure-text alignment and cross-references. This study highlights the differences between human and AI-generated patents and emphasizes the importance of enhancing the quality of patent content through systematic evaluation and improvement suggestions. 

<br /><br /> <div>
arXiv:2510.25402v1 Announce Type: cross 
Abstract: Despite the surge in patent applications and emergence of AI drafting tools, systematic evaluation of patent content quality has received limited research attention. To address this gap, We propose to evaluate patents using regulatory compliance, technical coherence, and figure-reference consistency detection modules, and then generate improvement suggestions via an integration module. The framework is validated on a comprehensive dataset comprising 80 human-authored and 80 AI-generated patents from two patent drafting tools. Experimental results show balanced accuracies of 99.74\%, 82.12\%, and 91.2\% respectively across the three detection modules when validated against expert annotations. Additional analysis was conducted to examine defect distributions across patent sections, technical domains, and authoring sources. Section-based analysis indicates that figure-text consistency and technical detail precision require particular attention. Mechanical Engineering and Construction show more claim-specification inconsistencies due to complex technical documentation requirements. AI-generated patents show a significant gap compared to human-authored ones. While human-authored patents primarily contain surface-level errors like typos, AI-generated patents exhibit more structural defects in figure-text alignment and cross-references.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics</title>
<link>https://arxiv.org/abs/2510.25683</link>
<guid>https://arxiv.org/abs/2510.25683</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Structural Simulator, Dynamic simulations, Node kinematics, Connectivity radius<br />
Summary:<br />
The article introduces the Graph Network-based Structural Simulator (GNSS) for dynamic structural problems, utilizing GNNs for surrogate modeling in a novel way. GNSS incorporates node kinematics in local frames, employs a sign-aware regression loss, and utilizes a wavelength-informed connectivity radius. In a case study of a beam, GNSS accurately reproduces physics over numerous timesteps and generalizes to new loading conditions. Compared to finite element methods, GNSS offers faster inference speeds while maintaining spatial and temporal accuracy. These results showcase the effectiveness of using locality-preserving GNNs with physics-consistent update rules for dynamic, wave-dominated structural simulations. <br /> <div>
arXiv:2510.25683v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models for numerical simulations. While their applications in computational fluid dynamics have been investigated, little attention has been given to structural problems, especially for dynamic cases. To address this gap, we introduce the Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine learning models, and its design makes it particularly suited for dynamic simulations thanks to three key features: (i) expressing node kinematics in node-fixed local frames, which avoids catastrophic cancellation in finite-difference velocities; (ii) employing a sign-aware regression loss, which reduces phase errors in long rollouts; and (iii) using a wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz Hanning-modulated pulse. The results show that GNSS accurately reproduces the physics of the problem over hundreds of timesteps and generalizes to unseen loading conditions, where existing GNNs fail to converge or deliver meaningful predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial inference speedups while preserving spatial and temporal fidelity. These findings demonstrate that locality-preserving GNNs with physics-consistent update rules are a competitive alternative for dynamic, wave-dominated structural simulations.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMP-Val: Large Language Models Empower Personalized Valuation in Auction</title>
<link>https://arxiv.org/abs/2410.15817</link>
<guid>https://arxiv.org/abs/2410.15817</guid>
<content:encoded><![CDATA[
<div> auctions, personalized valuation, Large Language Models, bidding algorithms, valuation errors

Summary:
The article discusses the importance of incorporating individual users' preferences into auction mechanisms to improve valuation accuracy. It introduces a personalized valuation framework called LaMP-Val, which utilizes Large Language Models to integrate personalized semantic preferences into the valuation process. The framework consists of three components: data, learning, and evaluation, which collectively aim to enhance the accuracy of personalized valuation. Data component focuses on building a novel dataset for fine-tuning LLMs in personalized valuation modeling. Learning component introduces diversity template to improve LLMs' capacity for capturing personal valuation patterns. Evaluation component establishes a closed-loop system for assessing the accuracy of LLM-generated valuations in personalized scenarios. Experimental results demonstrate that LaMP-Val outperforms baseline approaches in capturing personalized values and maximizing profits in auction settings. <div>
arXiv:2410.15817v2 Announce Type: replace 
Abstract: Auctions are a vital economic mechanism used to determine the market value of goods or services through competitive bidding within a specific framework. However, much of the current research primarily focuses on the bidding algorithms used within auction mechanisms. This often neglects the potential benefits of incorporating individual users' unique preferences into the valuation process. Our theoretical and empirical analysis demonstrates that valuation errors can significantly impact the overall utility. To bridge this gap, we propose a personalized valuation framework, namely Large \underline{La}nguage \underline{M}odels-powered \underline{P}ersonalized \underline{Val}uation (LaMP-Val), which integrates Large Language Models to incorporate personalized semantic preference into users valuation process. LaMP-Val integrating three components: data, learning, and evaluation. The data component tackles the challenge of building a novel dataset specifically for LLMs fine-tuning in personalized valuation modeling. The learning component introduces a diversity template to enhance LLMs' capacity for modeling fine-grained personal valuation patterns. The evaluation component establishes a closed-loop system where LLM-generated valuations interact with bidding strategies and auction. It proposes two novel metrics to quantify valuation precision and bidding intention accuracy in personalized scenarios. Extensive experiments show that LaMP-Val more accurately captures personalized values and achieves greater profits than baseline approaches.
]]></content:encoded>
<pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
<link>https://arxiv.org/abs/2510.24106</link>
<guid>https://arxiv.org/abs/2510.24106</guid>
<content:encoded><![CDATA[
<div> Transformer Module, Aerodynamic Simulation, Deep Neural Networks, Joint Training, UniField<br />
Summary:<br />
The article discusses the importance of aerodynamic simulation in engineering and the use of deep neural networks as a cost-effective alternative to traditional CFD simulations for modeling surface pressure fields. Data scarcity is a significant challenge in this field, limiting the application of neural networks. To address this limitation, the authors propose UniField, a model that integrates aerodynamic data from multiple subfields and conducts joint training to learn more general field representations. By consolidating different datasets covering various domains, including automobiles, trains, aircraft, and general shapes, UniField uses a domain-agnostic Transformer module to extract general point cloud features and domain-specific flow-conditioned adapters to adapt to different flow conditions. Results show that jointly training on all data leads to better performance, indicating that the data from different subfields complement each other and help the model learn better flow field representations. This research lays the foundation for broader applications of neural networks in aerodynamic analysis. <br /> <div>
arXiv:2510.24106v1 Announce Type: new 
Abstract: Aerodynamic simulation of the surface pressure field around objects is crucial for many engineering problems. In recent years, deep neural networks have emerged as an efficient alternative to traditional, computationally expensive CFD simulations for modeling surface pressure fields. However, data scarcity remains a fundamental challenge, limiting the application of neural networks. To address this limitation, we propose to integrate aerodynamic data from multiple subfields and conduct joint training to learn more general field representations. We consolidate five different datasets covering various fields, including automobiles, trains, aircraft, and general shapes. Facing significant data differences across different domains, we propose UniField, which employs a domain-agnostic Transformer module to extract general point cloud features and customizes domain-specific flow-conditioned adapters to adapt to the flow information in different subfields. Despite the fact that aerodynamic data from different subfields are typically governed by different equations, we compare models trained jointly on all data with those trained separately on individual datasets and find that the jointly-trained model commonly demonstrates better performance. This indicates that these data complement each other to help the model learn better flow field representations. These results highlight the potential of UniField as a universal flow field representation model and lay the foundation for broader applications of neural networks in aerodynamic analysis.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A data-driven multiscale scheme for anisotropic finite strain magneto-elasticity</title>
<link>https://arxiv.org/abs/2510.24197</link>
<guid>https://arxiv.org/abs/2510.24197</guid>
<content:encoded><![CDATA[
<div> neural network-based, multiscale scheme, structured magnetically soft magnetorheological elastomers, data-driven, decoupled <br />
Summary: 
This work presents a novel approach for modeling structured magnetically soft magnetorheological elastomers using a neural network-based, data-driven, decoupled multiscale scheme. The microscale responses are computed using a mixed finite element formulation and homogenized to create a database for training a macroscopic physics-augmented neural network model. The model enforces physical principles and accurately predicts magnetization, mechanical stress, and total stress within the training data range. For larger magnetic fields, plausible results are obtained. The model is applied to analyze the magnetostrictive behavior of a macroscopic spherical MRE sample, showing contraction along the magnetic field direction when aligned with the material's preferred direction. <div>
arXiv:2510.24197v1 Announce Type: new 
Abstract: In this work, we develop a neural network-based, data-driven, decoupled multiscale scheme for the modeling of structured magnetically soft magnetorheological elastomers (MREs). On the microscale, sampled magneto-mechanical loading paths are imposed on a representative volume element containing spherical particles and an elastomer matrix, and the resulting boundary value problem is solved using a mixed finite element formulation. The computed microscale responses are homogenized to construct a database for the training and testing of a macroscopic physics-augmented neural network model. The proposed model automatically detects the material's preferred direction during training and enforces key physical principles, including objectivity, material symmetry, thermodynamic consistency, and the normalization of free energy, stress, and magnetization. Within the range of the training data, the model enables accurate predictions of magnetization, mechanical stress, and total stress. For larger magnetic fields, the model yields plausible results. Finally, we apply the model to investigate the magnetostrictive behavior of a macroscopic spherical MRE sample, which exhibits contraction along the magnetic field direction when aligned with the material's preferred direction.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization</title>
<link>https://arxiv.org/abs/2510.23667</link>
<guid>https://arxiv.org/abs/2510.23667</guid>
<content:encoded><![CDATA[
<div> predictive modeling, structural topology optimization, deep learning, physics-aware design, generative modeling

Summary: 
The article introduces Optimize Any Topology (OAT), a framework for structural topology optimization that can predict minimum-compliance layouts for various configurations. OAT combines an autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on a large corpus of optimized structures. It outperforms prior models by reducing mean compliance up to 90% and provides fast inference across different resolutions and aspect ratios. OAT is resolution-agnostic and delivers results in less than one second on a single GPU. The framework is general, fast, and supports physics-aware topology optimization. The article also introduces the OpenTO corpus, a dataset of 2.2 million optimized structures, to facilitate further research in generative modeling for inverse design. <div>
arXiv:2510.23667v1 Announce Type: cross 
Abstract: Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales</title>
<link>https://arxiv.org/abs/2510.23993</link>
<guid>https://arxiv.org/abs/2510.23993</guid>
<content:encoded><![CDATA[
<div> GPU, compressible reacting flow solver, AMReX framework, multi-GPU, high-performance <br />
Summary:
A high-speed chemically active flow solver for GPUs targets stiff chemistry challenges by optimizing memory access, workload variability, and multi-GPU load distribution. Utilizing the AMReX framework, it achieves significant performance enhancements for combustion simulations. By optimizing memory access patterns and workload variability, the solver exhibits $2-5\times$ performance gains over initial GPU implementations. It efficiently distributes computational workload across multiple GPUs for adaptive mesh refinement applications. The solver adapts matrix-based chemical kinetics to multigrid contexts, improving arithmetic intensity for convection and chemistry routines. With near-ideal weak scaling across $1-96$ NVIDIA H100 GPUs, the solver demonstrates efficient utilization of GPU resources and memory bandwidth, indicating improved performance for high-speed reacting flow simulations. <div>
arXiv:2510.23993v1 Announce Type: cross 
Abstract: High-speed chemically active flows present significant computational challenges due to their disparate space and time scales, where stiff chemistry often dominates simulation time. While modern supercomputing scientific codes achieve exascale performance by leveraging graphics processing units (GPUs), existing GPU-based compressible combustion solvers face critical limitations in memory management, load balancing, and handling the highly localized nature of chemical reactions. To this end, we present a high-performance compressible reacting flow solver built on the AMReX framework and optimized for multi-GPU settings. Our approach addresses three GPU performance bottlenecks: memory access patterns through column-major storage optimization, computational workload variability via a bulk-sparse integration strategy for chemical kinetics, and multi-GPU load distribution for adaptive mesh refinement applications. The solver adapts existing matrix-based chemical kinetics formulations to multigrid contexts. Using representative combustion applications including hydrogen-air detonations and jet in supersonic crossflow configurations, we demonstrate $2-5\times$ performance improvements over initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4 \times$) routines, confirming efficient utilization of GPU memory bandwidth and computational resources.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HergNet: a Fast Neural Surrogate Model for Sound Field Predictions via Superposition of Plane Waves</title>
<link>https://arxiv.org/abs/2510.24279</link>
<guid>https://arxiv.org/abs/2510.24279</guid>
<content:encoded><![CDATA[
<div> neural network, sound fields, Helmholtz equation, wave phenomena, room acoustics simulation <br />
<br />
Summary: 
A novel neural network architecture is introduced for predicting sound fields efficiently in two and three dimensions. This neural network is uniquely designed to automatically satisfy the Helmholtz equation, ensuring that the predicted sound fields are physically valid. As a result, the neural network can effectively learn solutions to boundary-value problems in various wave phenomena like acoustics, optics, and electromagnetism. Numerical experiments indicate that this approach shows promise in outperforming existing methods, particularly in room acoustics simulation at mid to high frequencies. <div>
arXiv:2510.24279v1 Announce Type: cross 
Abstract: We present a novel neural network architecture for the efficient prediction of sound fields in two and three dimensions. The network is designed to automatically satisfy the Helmholtz equation, ensuring that the outputs are physically valid. Therefore, the method can effectively learn solutions to boundary-value problems in various wave phenomena, such as acoustics, optics, and electromagnetism. Numerical experiments show that the proposed strategy can potentially outperform state-of-the-art methods in room acoustics simulation, in particular in the range of mid to high frequencies.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering</title>
<link>https://arxiv.org/abs/2510.24402</link>
<guid>https://arxiv.org/abs/2510.24402</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, metadata-driven, financial filings, document analysis, RAG architecture

Summary: 
This study explores advanced metadata-driven techniques for Retrieval-Augmented Generation (RAG) in handling long and structured financial filings. The researchers propose a multi-stage RAG architecture that utilizes metadata generated by Large Language Models (LLMs) to improve performance on the FinanceBench dataset. By incorporating contextual embeddings with text ("contextual chunks"), the study shows significant performance gains in RAG. The research highlights the importance of a powerful reranker for precision and the effectiveness of embedding chunk metadata directly with text. The proposed optimal architecture combines LLM-driven pre-retrieval optimizations with contextual embeddings to achieve superior performance. Additionally, a custom metadata reranker is introduced as a cost-effective alternative to commercial solutions, emphasizing the balance between peak performance and operational efficiency in RAG systems for financial document analysis. 

<br /><br />Summary: <div>
arXiv:2510.24402v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) struggles on long, structured financial filings where relevant evidence is sparse and cross-referenced. This paper presents a systematic investigation of advanced metadata-driven Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a novel, multi-stage RAG architecture that leverages LLM-generated metadata. We introduce a sophisticated indexing pipeline to create contextually rich document chunks and benchmark a spectrum of enhancements, including pre-retrieval filtering, post-retrieval reranking, and enriched embeddings, benchmarked on the FinanceBench dataset. Our results reveal that while a powerful reranker is essential for precision, the most significant performance gains come from embedding chunk metadata directly with text ("contextual chunks"). Our proposed optimal architecture combines LLM-driven pre-retrieval optimizations with these contextual embeddings to achieve superior performance. Additionally, we present a custom metadata reranker that offers a compelling, cost-effective alternative to commercial solutions, highlighting a practical trade-off between peak performance and operational efficiency. This study provides a blueprint for building robust, metadata-aware RAG systems for financial document analysis.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-supervised and unsupervised learning for health indicator extraction from guided waves in aerospace composite structures</title>
<link>https://arxiv.org/abs/2510.24614</link>
<guid>https://arxiv.org/abs/2510.24614</guid>
<content:encoded><![CDATA[
<div> deep learning, aerospace structures, health indicators, composite materials, signal processing

Summary: 
- Health indicators (HIs) are crucial for assessing aerospace composite structures' condition.
- Traditional methods face challenges due to material variability, damage evolution, and incidents like bird strikes.
- A data-driven framework with two learning approaches and multi-domain signal processing is proposed.
- The approaches include a diversity deep semi-supervised anomaly detection (Diversity-DeepSAD) and a degradation-trend-constrained variational autoencoder (DTC-VAE).
- Guided waves with multiple excitation frequencies are used to monitor structures under fatigue loading, with results showing improved performance in extracting reliable HIs. 

<br /><br /> <div>
arXiv:2510.24614v1 Announce Type: cross 
Abstract: Health indicators (HIs) are central to diagnosing and prognosing the condition of aerospace composite structures, enabling efficient maintenance and operational safety. However, extracting reliable HIs remains challenging due to variability in material properties, stochastic damage evolution, and diverse damage modes. Manufacturing defects (e.g., disbonds) and in-service incidents (e.g., bird strikes) further complicate this process. This study presents a comprehensive data-driven framework that learns HIs via two learning approaches integrated with multi-domain signal processing. Because ground-truth HIs are unavailable, a semi-supervised and an unsupervised approach are proposed: (i) a diversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach augmented with continuous auxiliary labels used as hypothetical damage proxies, which overcomes the limitation of prior binary labels that only distinguish healthy and failed states while neglecting intermediate degradation, and (ii) a degradation-trend-constrained variational autoencoder (DTC-VAE), in which the monotonicity criterion is embedded via an explicit trend constraint. Guided waves with multiple excitation frequencies are used to monitor single-stiffener composite structures under fatigue loading. Time, frequency, and time-frequency representations are explored, and per-frequency HIs are fused via unsupervised ensemble learning to mitigate frequency dependence and reduce variance. Using fast Fourier transform features, the augmented Diversity-DeepSAD model achieved 81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3% performance, outperforming existing baselines.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative study of Bitcoin and Ripple cryptocurrencies trading using Deep Reinforcement Learning algorithms</title>
<link>https://arxiv.org/abs/2505.07660</link>
<guid>https://arxiv.org/abs/2505.07660</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, automated trading, Deep Reinforcement Learning, Bitcoin, Ripple <br />
<br />
Summary: 
The article explores the use of innovative rule-based strategies combined with Deep Reinforcement Learning (DRL) to address the challenges in forecasting prices of financial assets like Bitcoin and Ripple. Different algorithms such as Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, and Advantage Actor-Critic are applied to train the DRL model to formulate optimal trading policies. The evaluation metrics used are portfolio wealth and trade signals. Results show that Dueling and Double Deep Q-Network perform better in increasing portfolio wealth when trading Ripple. All codes for the project are available on Github for reference and further study. <div>
arXiv:2505.07660v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) has demonstrated remarkable success across various applications. In light of this trend, the field of automated trading has developed a keen interest in leveraging AI techniques to forecast the future prices of financial assets. This interest stems from the need to address trading challenges posed by the inherent volatility and dynamic nature of asset prices. However, crafting a flawless strategy becomes a formidable task when dealing with assets characterized by intricate and ever-changing price dynamics. To surmount these formidable challenges, this research employs an innovative rule-based strategy approach to train Deep Reinforcement Learning (DRL). This application is carried out specifically in the context of trading Bitcoin (BTC) and Ripple (XRP). Our proposed approach hinges on the integration of Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, alongside the Advantage Actor-Critic algorithms. Each of them aims to yield an optimal policy for our application. To evaluate the effectiveness of our Deep Reinforcement Learning (DRL) approach, we rely on portfolio wealth and the trade signal as performance metrics. The experimental outcomes highlight that Duelling and Double Deep Q-Network outperformed when using XRP with the increasing of the portfolio wealth. All codes are available in this \href{https://github.com/VerlonRoelMBINGUI/RL_Final_Projects_AMMI2023}{\color{blue}Github link}.
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data</title>
<link>https://arxiv.org/abs/2505.12638</link>
<guid>https://arxiv.org/abs/2505.12638</guid>
<content:encoded><![CDATA[
<div> scATAC-seq, foundation model, ChromFound, single-cell chromatin accessibility, regulatory mechanisms<br />
<br />
Summary:<br />
The article introduces ChromFound, a foundation model designed for single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) data analysis. It addresses the challenges of high dimensionality and sparsity in scATAC-seq data by using a hybrid architecture and genome-aware tokenization. ChromFound, pretrained on a vast dataset, demonstrates strong performance in various tasks such as cell identification, cell type annotation, and cross-omics prediction. It uncovers enhancer-gene links that were previously undetected, providing insights into disease risk variants in the noncoding genome. ChromFound's ability to generate universal cell representations and its transferability make it a promising tool for comprehensive multi-omics analysis in the field of chromatin accessibility research. <div>
arXiv:2505.12638v3 Announce Type: replace-cross 
Abstract: The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present ChromFound, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error bounded compression for weather and climate applications</title>
<link>https://arxiv.org/abs/2510.22265</link>
<guid>https://arxiv.org/abs/2510.22265</guid>
<content:encoded><![CDATA[
<div> JPEG2000, wavelet transform, SPIHT encoding, error-bounded compression, weather and climate simulations

Summary: 
EBCC (Error Bounded Climate-data Compressor) addresses the challenge of rapidly growing data in weather and climate simulations by utilizing a two-layer compression approach. The base compression layer employs JPEG2000, while the residual compression layer utilizes wavelet transform and SPIHT encoding to eliminate extreme errors. A feedback rate-control mechanism adjusts compression ratios to meet specified error targets. EBCC outperforms other compression methods in benchmarks related to weather and climate science, concentrating errors near zero and achieving compression ratios ranging from 15x to over 300x. In benchmarks such as energy budget closure and Lagrangian trajectory simulation, EBCC maintains errors within natural variability while achieving more than 100x compression. The source code for EBCC is available on github.com/spcl/EBCC. <br /><br />Summary: <div>
arXiv:2510.22265v1 Announce Type: new 
Abstract: As the resolution of weather and climate simulations increases, the amount of data produced is growing rapidly from hundreds of terabytes to tens of petabytes. The huge size becomes a limiting factor for broader adoption, and its fast growth rate will soon exhaust all the available storage devices. To address these issues, we present EBCC (Error Bounded Climate-data Compressor). It follows a two-layer approach: a base compression layer using JPEG2000 to capture the bulk of the data with a high compression ratio, and a residual compression layer using wavelet transform and SPIHT encoding to efficiently eliminate long-tail extreme errors introduced by the base compression layer. It incorporates a feedback rate-control mechanism for both layers that adjusts compression ratios to achieve the specified maximum error target. We evaluate EBCC alongside other established compression methods on benchmarks related to weather and climate science including error statistics, a case study on primitive and derived variables near a hurricane, evaluation of the closure of the global energy budget, and a Lagrangian air parcel trajectory simulation. This is the first time that trajectory simulation is used to benchmark compression methods. Our method concentrates most errors near zero, while others tend to distribute errors uniformly within the error bound. EBCC outperforms other methods in the benchmarks at relative error targets ranging from 0.1% to 10% and achieves compression ratios from 15x to more than 300x. In the energy budget closure and Lagrangian trajectory benchmarks, it can achieve more than 100x compression while keeping errors within natural variability derived from ERA5 uncertainty members. This verifies the effectiveness of EBCC in creating heavily compressed weather and climate datasets suitable for downstream applications. The source code of EBCC is available in github.com/spcl/EBCC.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Sensor Placement: A Correlation-Aware Attribution Framework (CAAF) for Real-world Data Modeling</title>
<link>https://arxiv.org/abs/2510.22517</link>
<guid>https://arxiv.org/abs/2510.22517</guid>
<content:encoded><![CDATA[
<div> sensor placement, machine learning, feature attribution, optimal, real-world

Summary:
The article introduces a Correlation-Aware Attribution Framework (CAAF) for optimal sensor placement (OSP) in complex systems. OSP is crucial for accurate monitoring and control. CAAF uses machine learning to identify OSP by quantifying input contributions through feature attribution. It addresses challenges of highly correlated input data by introducing a clustering step before feature attribution to reduce redundancy and improve generalizability. The framework is validated through various cases and applied to real-world systems like structural health monitoring and airfoil lift prediction. Results demonstrate the effectiveness of CAAF in identifying OSP in systems with nonlinear dynamics, chaotic behavior, and multi-scale interactions. The proposed framework outperforms traditional approaches and enables efficient application of feature attribution in real-world environments.<br /><br />Summary: <div>
arXiv:2510.22517v1 Announce Type: new 
Abstract: Optimal sensor placement (OSP) is critical for efficient, accurate monitoring, control, and inference in complex real-world systems. We propose a machine-learning-based feature attribution framework to identify OSP for the prediction of quantities of interest. Feature attribution quantifies input contributions to a model's output; however, it struggles with highly correlated input data often encountered in real-world applications. To address this, we propose a Correlation-Aware Attribution Framework (CAAF), which introduces a clustering step before performing feature attribution to reduce redundancy and enhance generalizability. We first illustrate the core principles of the proposed framework through a series of validation cases, then demonstrate its effectiveness in real-world dynamical systems, such as structural health monitoring, airfoil lift prediction, and wall-normal velocity estimation for turbulent channel flow. The results show that the CAAF outperforms alternative approaches that typically struggle due to the presence of nonlinear dynamics, chaotic behavior, and multi-scale interactions, and enables the effective application of feature attribution for identifying OSP in real-world environments.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capsule Network-Based Multimodal Fusion for Mortgage Risk Assessment from Unstructured Data Sources</title>
<link>https://arxiv.org/abs/2510.22987</link>
<guid>https://arxiv.org/abs/2510.22987</guid>
<content:encoded><![CDATA[
<div> deep learning; multimodal fusion; mortgage risk assessment; interpretability; sentiment analysis

Summary:
The study proposes a novel multimodal deep learning framework for mortgage risk assessment using publicly available unstructured data sources. The framework leverages BERT for text, VGG for images, and a multilayer perceptron for sentiment-based features in the unimodal phase. In the fusion phase, FusionCapsNet, a capsule-based fusion network, integrates modalities while preserving contextual information. Sentiment analysis across news categories and GradCAM-based visualizations are key components for interpretability. Results show that the FusionCapsNet framework outperforms individual models and benchmark fusion strategies, enhancing both predictive accuracy and interpretability for mortgage risk assessment. <div>
arXiv:2510.22987v1 Announce Type: new 
Abstract: Mortgage risk assessment traditionally relies on structured financial data, which is often proprietary, confidential, and costly. In this study, we propose a novel multimodal deep learning framework that uses cost-free, publicly available, unstructured data sources, including textual information, images, and sentiment scores, to generate credit scores that approximate commercial scorecards. Our framework adopts a two-phase approach. In the unimodal phase, we identify the best-performing models for each modality, i.e. BERT for text, VGG for image data, and a multilayer perceptron for sentiment-based features. In the fusion phase, we introduce the capsule-based fusion network (FusionCapsNet), a novel fusion strategy inspired by capsule networks, but fundamentally redesigned for multimodal integration. Unlike standard capsule networks, our method adapts a specific mechanism in capsule networks to each modality and restructures the fusion process to preserve spatial, contextual, and modality-specific information. It also enables adaptive weighting so that stronger modalities dominate without ignoring complementary signals.
  Our framework incorporates sentiment analysis across distinct news categories to capture borrower and market dynamics and employs GradCAM-based visualizations as an interpretability tool. These components are designed features of the framework, while our results later demonstrate that they effectively enrich contextual understanding and highlight the influential factors driving mortgage risk predictions. Our results show that our multimodal FusionCapsNet framework not only exceeds individual unimodal models but also outperforms benchmark fusion strategies such as addition, concatenation, and cross attention in terms of AUC, partial AUC, and F1 score, demonstrating clear gains in both predictive accuracy and interpretability for mortgage risk assessment.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P1GPT: a multi-agent LLM workflow module for multi-modal financial information analysis</title>
<link>https://arxiv.org/abs/2510.23032</link>
<guid>https://arxiv.org/abs/2510.23032</guid>
<content:encoded><![CDATA[
<div> framework, multi-agent, financial analysis, reasoning workflow, interpretability 

Summary: 
The article introduces P1GPT, a new multi-agent framework for financial analysis that incorporates various data modalities. Unlike existing systems, P1GPT focuses on structured reasoning and communication among agents to provide interpretable trading decision support. Through backtesting on U.S. equities data, P1GPT demonstrates superior cumulative returns, low drawdowns, and transparent causal rationales. The framework systematically integrates technical, fundamental, and news-based insights, showcasing the effectiveness of structured reasoning workflows in achieving explainable and trustworthy financial AI systems. <div>
arXiv:2510.23032v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled multi-agent reasoning systems capable of collaborative decision-making. However, in financial analysis, most frameworks remain narrowly focused on either isolated single-agent predictors or loosely connected analyst ensembles, and they lack a coherent reasoning workflow that unifies diverse data modalities. We introduce P1GPT, a layered multi-agent LLM framework for multi-modal financial information analysis and interpretable trading decision support. Unlike prior systems that emulate trading teams through role simulation, P1GPT implements a structured reasoning pipeline that systematically fuses technical, fundamental, and news-based insights through coordinated agent communication and integration-time synthesis. Backtesting on multi-modal datasets across major U.S. equities demonstrates that P1GPT achieves superior cumulative and risk-adjusted returns, maintains low drawdowns, and provides transparent causal rationales. These findings suggest that structured reasoning workflows, rather than agent role imitation, offer a scalable path toward explainable and trustworthy financial AI systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroupSHAP-Guided Integration of Financial News Keywords and Technical Indicators for Stock Price Prediction</title>
<link>https://arxiv.org/abs/2510.23112</link>
<guid>https://arxiv.org/abs/2510.23112</guid>
<content:encoded><![CDATA[
<div> Keywords: FinBERT, SHAP, GroupSHAP, financial forecasting, news articles

Summary:
- Recent advances in finance-specific language models, such as FinBERT, have allowed for the quantification of public sentiment into index-based measures.
- Using explainable AI techniques like SHAP, researchers have identified influential features, but its computational cost grows exponentially with input features.
- This study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which quantifies contributions of semantically related keyword groups, reducing computational burden.
- News articles from 2015 to 2024 were embedded using FinBERT, clustered into semantic groups, and GroupSHAP was applied to measure each group's contribution to stock price movements.
- GroupSHAP variables across multiple topics were used for one-day-ahead forecasting of the S&amp;P 500 index in 2024, resulting in a 32.2% reduction in MAE and a 40.5% reduction in RMSE compared to benchmark models.
<br /><br />Summary: 
Recent advancements in finance-focused language models have enabled the translation of public sentiment into index-based measures. However, compressing complex language signals into single metrics can overlook contextual nuances. To address this limitation, this study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which identifies influential semantic groupings instead of individual tokens in financial news articles. By employing this approach, the research achieves significant improvements in predictive performance for the S&amp;P 500 index, highlighting the potential for grouped sentiment representations to enhance both interpretability and forecasting accuracy in finance. <div>
arXiv:2510.23112v1 Announce Type: new 
Abstract: Recent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability. To address this limitation, explainable AI techniques, particularly SHAP (SHapley Additive Explanations), have been employed to identify influential features. However, SHAP's computational cost grows exponentially with input features, making it impractical for large-scale text-based financial data. This study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which quantifies contributions of semantically related keyword groups rather than individual tokens, substantially reducing computational burden while preserving interpretability. We employed FinBERT to embed news articles from 2015 to 2024, clustered them into coherent semantic groups, and applied GroupSHAP to measure each group's contribution to stock price movements. The resulting group-level SHAP variables across multiple topics were used as input features for the prediction model. Empirical results from one-day-ahead forecasting of the S&amp;P 500 index throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE and a 40.5% reduction in RMSE compared with benchmark models without the GroupSHAP mechanism. This research presents the first application of GroupSHAP in news-driven financial forecasting, showing that grouped sentiment representations simultaneously enhance interpretability and predictive performance.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms</title>
<link>https://arxiv.org/abs/2510.23166</link>
<guid>https://arxiv.org/abs/2510.23166</guid>
<content:encoded><![CDATA[
<div> Machine learning, scientific machine learning, Common Task Framework, benchmarks, reproducibility<br />
<br />
Summary: 
Machine learning is revolutionizing modeling and control in various scientific fields, but the lack of standardized benchmarks hinders progress. To address this issue, a Common Task Framework (CTF) is proposed, offering curated datasets and metrics for evaluating algorithms. By benchmarking methods on nonlinear systems like Kuramoto-Sivashinsky and Lorenz, the CTF reveals strengths, limitations, and suitability for different problems. A global sea surface temperature dataset competition with a true holdout dataset will foster community engagement. The long-term goal is to replace ad hoc comparisons with standardized evaluations on hidden test sets, enhancing rigor and reproducibility in scientific machine learning. <br /><br /> <div>
arXiv:2510.23166v1 Announce Type: new 
Abstract: Machine learning (ML) is transforming modeling and control in the physical, engineering, and biological sciences. However, rapid development has outpaced the creation of standardized, objective benchmarks - leading to weak baselines, reporting bias, and inconsistent evaluations across methods. This undermines reproducibility, misguides resource allocation, and obscures scientific progress. To address this, we propose a Common Task Framework (CTF) for scientific machine learning. The CTF features a curated set of datasets and task-specific metrics spanning forecasting, state reconstruction, and generalization under realistic constraints, including noise and limited data. Inspired by the success of CTFs in fields like natural language processing and computer vision, our framework provides a structured, rigorous foundation for head-to-head evaluation of diverse algorithms. As a first step, we benchmark methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz. These results illustrate the utility of the CTF in revealing method strengths, limitations, and suitability for specific classes of problems and diverse objectives. Next, we are launching a competition around a global real world sea surface temperature dataset with a true holdout dataset to foster community engagement. Our long-term vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets that raise the bar for rigor and reproducibility in scientific ML.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRO-Based Computation Offloading and Trajectory Design for Low-Altitude Networks</title>
<link>https://arxiv.org/abs/2510.23202</link>
<guid>https://arxiv.org/abs/2510.23202</guid>
<content:encoded><![CDATA[
<div> architecture, computation offloading, UAVs, HAPs, distributionally robust optimization

Summary:<br />
An architecture for Low-altitude networks (LANs) that integrates unmanned aerial vehicles (UAVs) and high-altitude platforms (HAPs) is proposed to meet increasing computation demands. Uncertain task sizes and UAV mobility present challenges in ensuring quality of service. To tackle this, a distributionally robust optimization problem is formulated to minimize worst-case delays by optimizing offloading decisions and UAV trajectories. An algorithm is designed to solve the mixed-integer min-max optimization problem, utilizing an outer-inner layer approach. The algorithm employs Benders decomposition to optimize offloading decisions and UAV trajectories iteratively. Simulation results indicate the proposed algorithm excels over traditional methods in balancing worst-case delay and robustness. <div>
arXiv:2510.23202v1 Announce Type: new 
Abstract: The low-altitude networks (LANs) integrating unmanned aerial vehicles (UAVs) and high-altitude platforms (HAPs) have become a promising solution for the rising computation demands. However, the uncertain task sizes and high mobility of UAVs pose great challenges to guarantee the quality of service. To address these issues, we propose an LAN architecture where UAVs and HAPs collaboratively provide computation offloading for ground users. Moreover, the uncertainty sets are constructed to characterize the uncertain task size, and a distributionally robust optimization problem is formulated to minimize the worst-case delay by jointly optimizing the offloading decisions and UAV trajectories. To solve the mixed-integer min-max optimization problem, we design the distributionally robust computation offloading and trajectories optimization algorithm. Specifically, the original problem is figured out by iteratively solving the outerlayer and inner-layer problems. The convex outer-layer problem with probability distributions is solved by the optimization toolkit. As for the inner-layer mixed-integer problem, we employ the Benders decomposition. The decoupled master problem concerning the binary offloading decisions is solved by the integer solver, and UAV trajectories in the sub-problem are optimized via the successive convex approximation. Simulation results show the proposed algorithm outperforms traditional optimization methods in balancing the worst-case delay and robustness.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the PTM Code through a Coarse-to-Fine, Mechanism-Aware Framework</title>
<link>https://arxiv.org/abs/2510.23492</link>
<guid>https://arxiv.org/abs/2510.23492</guid>
<content:encoded><![CDATA[
<div> Keywords: Post-translational modifications, COMPASS-PTM, enzyme-substrate assignment, protein language models, cellular signaling.

Summary: 
The article introduces COMPASS-PTM, a novel learning framework that combines residue-level PTM profiling with enzyme-substrate assignment. By integrating evolutionary representations, physicochemical priors, and a crosstalk-aware prompting mechanism, COMPASS-PTM effectively deciphers the complex code of PTMs in cellular signaling. The model achieves state-of-the-art performance in multi-label site prediction and enzyme assignment, demonstrating a significant improvement in accuracy. It also shows interpretability by identifying kinase motifs and predicting disease-related PTM alterations due to missense variants. COMPASS-PTM bridges statistical learning with biochemical mechanisms, allowing it to learn the grammar underlying protein regulation and signaling. This unified framework provides a comprehensive understanding of PTMs, shedding light on the intricate regulatory mechanisms within cells.<br /><br />Summary: <div>
arXiv:2510.23492v1 Announce Type: new 
Abstract: Post-translational modifications (PTMs) form a combinatorial "code" that regulates protein function, yet deciphering this code - linking modified sites to their catalytic enzymes - remains a central unsolved problem in understanding cellular signaling and disease. We introduce COMPASS-PTM, a mechanism-aware, coarse-to-fine learning framework that unifies residue-level PTM profiling with enzyme-substrate assignment. COMPASS-PTM integrates evolutionary representations from protein language models with physicochemical priors and a crosstalk-aware prompting mechanism that explicitly models inter-PTM dependencies. This design allows the model to learn biologically coherent patterns of cooperative and antagonistic modifications while addressing the dual long-tail distribution of PTM data. Across multiple proteome-scale benchmarks, COMPASS-PTM establishes new state-of-the-art performance, including a 122% relative F1 improvement in multi-label site prediction and a 54% gain in zero-shot enzyme assignment. Beyond accuracy, the model demonstrates interpretable generalization, recovering canonical kinase motifs and predicting disease-associated PTM rewiring caused by missense variants. By bridging statistical learning with biochemical mechanism, COMPASS-PTM unifies site-level and enzyme-level prediction into a single framework that learns the grammar underlying protein regulation and signaling.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vertex and front-tracking methods for the modeling of microstructure evolution at the solid state: a brief review</title>
<link>https://arxiv.org/abs/2510.21818</link>
<guid>https://arxiv.org/abs/2510.21818</guid>
<content:encoded><![CDATA[
<div> Front-Capturing, Front-Tracking, interface properties, mesoscopic scale, microstructure evolution<br />
Summary:<br />
In mesoscopic scale microstructure evolution modeling, two main numerical frameworks are used: Front-Capturing (FC) and Front-Tracking (FT). FC models indirectly define interfaces by tracking field variable changes, while FT models explicitly define interfaces using interconnected segments or surfaces. FT-type approaches, associated with Lagrangian movement, provide enhanced spatial resolution in 3D and 2D problems. They efficiently handle physical mechanisms related to interface properties and geometries but face challenges with complex topological events in 3D. Recent advancements show their potential in computational efficiency and analyzing mobility and energy properties for potential applications in intragranular phenomena. <div>
arXiv:2510.21818v1 Announce Type: cross 
Abstract: In mesoscopic scale microstructure evolution modeling, two primary numerical frameworks are used: Front-Capturing (FC) and Front-Tracking (FT) ones. FC models, like phase-field or level-set methods, indirectly define interfaces by tracking field variable changes. On the contrary, FT models explicitly define interfaces using interconnected segments or surfaces. In historical FT methodologies, Vertex models were first developed and consider the description of the evolution of polygonal structures in terms of the motion of points where multiple boundaries meet. Globally, FT-type approaches, often associated with Lagrangian movement, enhance spatial resolution in 3D surfacic and 2D lineic problems using techniques derived from finite element meshing and remeshing algorithms. These efficient approaches, by nature, are well adapted to physical mechanisms correlated to interface properties and geometries. They also face challenges in managing complex topological events, especially in 3D. However, recent advances highlight their potential in computational efficiency and analysis of mobility and energy properties, with possible applications in intragranular phenomena.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreditXAI: A Multi-Agent System for Explainable Corporate Credit Rating</title>
<link>https://arxiv.org/abs/2510.22222</link>
<guid>https://arxiv.org/abs/2510.22222</guid>
<content:encoded><![CDATA[
<div> Keywords: corporate credit rating, deep learning, interpretability, Multi-Agent System, credit risk assessment

Summary:
The study introduces CreditXAI, a Multi-Agent System framework that mimics collaborative decision-making in credit analysis. Traditional deep learning methods in corporate credit rating have limitations in interpretability and 'black-box' issues. By incorporating business, financial, and governance risk dimensions, CreditXAI enhances interpretability and accuracy in credit assessments. Experimental results show a 7% improvement in predictive accuracy compared to single-agent models. The framework offers a new approach to building intelligent and transparent credit rating models, addressing the shortcomings of existing methods. <div>
arXiv:2510.22222v1 Announce Type: cross 
Abstract: In the domain of corporate credit rating, traditional deep learning methods have improved predictive accuracy but still suffer from the inherent 'black-box' problem and limited interpretability. While incorporating non-financial information enriches the data and provides partial interpretability, the models still lack hierarchical reasoning mechanisms, limiting their comprehensive analytical capabilities. To address these challenges, we propose CreditXAI, a Multi-Agent System (MAS) framework that simulates the collaborative decision-making process of professional credit analysts. The framework focuses on business, financial, and governance risk dimensions to generate consistent and interpretable credit assessments. Experimental results demonstrate that multi-agent collaboration improves predictive accuracy by more than 7% over the best single-agent baseline, confirming its significant synergistic advantage in corporate credit risk evaluation. This study provides a new technical pathway to build intelligent and interpretable credit rating models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAMP: Data-Efficient Linear Affine Weight-Space Models for Parameter-Controlled 3D Shape Generation and Extrapolation</title>
<link>https://arxiv.org/abs/2510.22491</link>
<guid>https://arxiv.org/abs/2510.22491</guid>
<content:encoded><![CDATA[
<div> align, signed distance function, generation, data-efficient, controllable

Summary: 
LAMP (Linear Affine Mixing of Parametric shapes) is introduced as a data-efficient framework for controllable and interpretable 3D generation, with a focus on generating high-fidelity 3D geometries that satisfy specific parameter constraints. The approach aligns signed distance function (SDF) decoders to overfit exemplars from a shared initialization and synthesizes new geometries by solving a parameter-constrained mixing problem in the aligned weight space. A safety metric is also proposed to detect geometry validity via linearity mismatch. LAMP enables controlled interpolation within bounds with a limited number of samples, safe extrapolation beyond training ranges by up to 100% parameter difference, and physics performance-guided optimization under fixed parameters. The results show LAMP outperforms conditional autoencoder and Deep Network Interpolation (DNI) baselines in both extrapolation and data efficiency, showcasing its advancements in controllable, data-efficient, and safe 3D generation for various applications in design exploration, dataset generation, and performance-driven optimization. <div>
arXiv:2510.22491v1 Announce Type: cross 
Abstract: Generating high-fidelity 3D geometries that satisfy specific parameter constraints has broad applications in design and engineering. However, current methods typically rely on large training datasets and struggle with controllability and generalization beyond the training distributions. To overcome these limitations, we introduce LAMP (Linear Affine Mixing of Parametric shapes), a data-efficient framework for controllable and interpretable 3D generation. LAMP first aligns signed distance function (SDF) decoders by overfitting each exemplar from a shared initialization, then synthesizes new geometries by solving a parameter-constrained mixing problem in the aligned weight space. To ensure robustness, we further propose a safety metric that detects geometry validity via linearity mismatch. We evaluate LAMP on two 3D parametric benchmarks: DrivAerNet++ and BlendedNet. We found that LAMP enables (i) controlled interpolation within bounds with as few as 100 samples, (ii) safe extrapolation by up to 100% parameter difference beyond training ranges, (iii) physics performance-guided optimization under fixed parameters. LAMP significantly outperforms conditional autoencoder and Deep Network Interpolation (DNI) baselines in both extrapolation and data efficiency. Our results demonstrate that LAMP advances controllable, data-efficient, and safe 3D generation for design exploration, dataset generation, and performance-driven optimization.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization</title>
<link>https://arxiv.org/abs/2510.22839</link>
<guid>https://arxiv.org/abs/2510.22839</guid>
<content:encoded><![CDATA[
<div> Framework, Structural parameters, Optimization, Genetic Algorithm, Machine learning<br />
<br />
Summary: 
This study proposes a hybrid data-driven framework that combines a Graph Neural Network (GNN) surrogate model with a Genetic Algorithm (GA) optimizer for optimizing structural parameters in design. The GNN is trained to predict dynamic displacement responses based on mass, stiffness, and damping configurations, reducing computational cost compared to conventional simulations. A dataset of single-degree-of-freedom system responses is used to train the GNN, which enables rapid predictions without solving the system equations repeatedly. The GA then searches for globally optimal parameter sets by minimizing displacements and enhancing dynamic stability. The framework demonstrates strong convergence, robust generalization, and efficient optimization, showcasing the potential of combining machine learning surrogates with evolutionary algorithms for intelligent structural design. <div>
arXiv:2510.22839v1 Announce Type: cross 
Abstract: The optimization of structural parameters, such as mass(m), stiffness(k), and damping coefficient(c), is critical for designing efficient, resilient, and stable structures. Conventional numerical approaches, including Finite Element Method (FEM) and Computational Fluid Dynamics (CFD) simulations, provide high-fidelity results but are computationally expensive for iterative optimization tasks, as each evaluation requires solving the governing equations for every parameter combination. This study proposes a hybrid data-driven framework that integrates a Graph Neural Network (GNN) surrogate model with a Genetic Algorithm (GA) optimizer to overcome these challenges. The GNN is trained to accurately learn the nonlinear mapping between structural parameters and dynamic displacement responses, enabling rapid predictions without repeatedly solving the system equations. A dataset of single-degree-of-freedom (SDOF) system responses is generated using the Newmark Beta method across diverse mass, stiffness, and damping configurations. The GA then searches for globally optimal parameter sets by minimizing predicted displacements and enhancing dynamic stability. Results demonstrate that the GNN and GA framework achieves strong convergence, robust generalization, and significantly reduced computational cost compared to conventional simulations. This approach highlights the effectiveness of combining machine learning surrogates with evolutionary optimization for automated and intelligent structural design.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Generalizable AI for Materials Discovery: Validation through Immersion Coolant Screening</title>
<link>https://arxiv.org/abs/2510.23371</link>
<guid>https://arxiv.org/abs/2510.23371</guid>
<content:encoded><![CDATA[
<div> Framework, AI, Materials discovery, Generalizable, GATE

Summary:
- GATE is a generalizable AI framework that can learn multiple physicochemical properties simultaneously.
- It aligns these properties in a shared geometric space to capture cross-property correlations and reduce bias in multi-criteria screening.
- GATE was applied to the discovery of immersion cooling fluids for data centers without reconfiguration and screened billions of candidates to identify promising molecules.
- Experimentally validated results showed agreement with wet-lab measurements and comparable or superior performance to a commercial coolant.
- GATE is established as a versatile AI platform applicable across various materials discovery tasks. 

<br /><br />Summary: <div>
arXiv:2510.23371v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has emerged as a powerful accelerator of materials discovery, yet most existing models remain problem-specific, requiring additional data collection and retraining for each new property. Here we introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a generalizable AI framework that jointly learns 34 physicochemical properties spanning thermal, electrical, mechanical, and optical domains. By aligning these properties within a shared geometric space, GATE captures cross-property correlations that reduce disjoint-property bias -- a key factor causing false negatives in multi-criteria screening. To demonstrate its generalizability, GATE -- without any problem-specific reconfiguration -- was directly applied to the discovery of immersion cooling fluids for data centers, a stringent real-world challenge defined by the Open Compute Project (OCP). Screening billions of candidates, GATE identified 92,861 molecules as promising for practical deployment. Four were experimentally or literarily validated, showing strong agreement with wet-lab measurements and performance comparable to or exceeding a commercial coolant. These results establish GATE as a ready-to-use, generalizable AI platform readily applicable across diverse materials discovery tasks.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree-Cotree-Based IETI-DP for Eddy Current Problems in Time-Domain</title>
<link>https://arxiv.org/abs/2510.23446</link>
<guid>https://arxiv.org/abs/2510.23446</guid>
<content:encoded><![CDATA[
<div> Keywords: low-frequency electromagnetic problems, eddy current formulations, time-domain simulations, tearing and interconnecting approach, scalability

Summary:
For low-frequency electromagnetic problems, eddy current formulations are commonly used for simplification. However, time-domain simulations can be computationally expensive. This study introduces a tearing and interconnecting approach for eddy currents in the time domain to improve efficiency and scalability. Wave-propagation effects are neglected in this setup, making it suitable for capturing transient startup responses and nonlinear behavior. The proposed method aims to reduce computational costs while maintaining accuracy in simulations. Overall, this approach offers a promising solution for tackling time-domain simulations of eddy current problems in electromagnetic analysis. <div>
arXiv:2510.23446v1 Announce Type: cross 
Abstract: For low-frequency electromagnetic problems, where wave-propagation effects can be neglected, eddy current formulations are commonly used as a simplification of the full Maxwell's equations. In this setup, time-domain simulations, needed to capture transient startup responses or nonlinear behavior, are often computationally expensive. We propose a novel tearing and interconnecting approach for eddy currents in time-domain and investigate its scalability.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error estimation and step size control with minimal subsystem interfaces</title>
<link>https://arxiv.org/abs/2406.17353</link>
<guid>https://arxiv.org/abs/2406.17353</guid>
<content:encoded><![CDATA[
<div> methods, co-simulation, error estimation, black-box subsystems, step size control <br />
Summary: 
This article reviews error estimation methods for co-simulation, focusing on scenarios where subsystems have minimal interfaces. These "black-box" subsystems lack time step rollback capabilities and provide limited internal information, presenting challenges for integration in large-system simulations. The article discusses using error indicators to automatically adjust macro time step sizes for optimal simulation speed and accuracy. It includes pseudocode for implementing the step size control algorithm and offers practical advice on assessing co-simulation quality, avoiding common errors, and configuring the control algorithm effectively. By highlighting the nuances of error estimation in co-simulation, the article aims to guide readers in implementing and testing these methods in their industrial applications. <div>
arXiv:2406.17353v2 Announce Type: replace 
Abstract: We review error estimation methods for co-simulation, in particular methods that are applicable when the subsystems provide minimal interfaces. By this, we mean that subsystems do not support rollback of time steps, do not output derivatives, and do not provide any other information about their internals besides the output variables that are required for coupling with other subsystems. Such "black-box" subsystems are common in industrial applications, and the ability to couple them and run large-system simulations is one of the major attractions of the co-simulation paradigm. We also describe how the resulting error indicators may be used to automatically control macro time step sizes to strike a good balance between simulation speed and accuracy. The various elements of the step size control algorithm are presented in pseudocode so that readers may implement them and test them in their own applications. We provide practicable advice on how to use error indicators to judge the quality of a co-simulation, how to avoid common pitfalls, and how to configure the step size control algorithm.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional Hierarchical Deep Learning Neural Networks-Tensor Decomposition (C-HiDeNN-TD): a scalable surrogate modeling approach for large-scale physical systems</title>
<link>https://arxiv.org/abs/2409.00329</link>
<guid>https://arxiv.org/abs/2409.00329</guid>
<content:encoded><![CDATA[
<div> artificial intelligence, deep learning, neural network, tensor decomposition, large-scale systems

Summary:<br />
- Simulation-driven engineering applications are becoming increasingly large and complex, leading to significant computational time and memory cost issues with classical numerical methods.
- Data-driven surrogates have been explored to accelerate PDE solvers, but often require extensive training data.
- The C-HiDeNN-TD method is proposed in this paper, which can directly obtain surrogate models for large-scale space-time PDE without the need for offline training data.
- The performance of this method is compared to classical numerical methods for extremely large-scale systems.
- The C-HiDeNN-TD method shows promise in providing efficient solutions for complex PDE problems without the need for massive amounts of training data. 

<br /><br />Summary: <div>
arXiv:2409.00329v2 Announce Type: replace 
Abstract: A common trend in simulation-driven engineering applications is the ever-increasing size and complexity of the problem, where classical numerical methods typically suffer from significant computational time and huge memory cost. Methods based on artificial intelligence have been extensively investigated to accelerate partial differential equations (PDE) solvers using data-driven surrogates. However, most data-driven surrogates require an extremely large amount of training data. In this paper, we propose the Convolutional Hierarchical Deep Learning Neural Network-Tensor Decomposition (C-HiDeNN-TD) method, which can directly obtain surrogate models by solving large-scale space-time PDE without generating any offline training data. We compare the performance of the proposed method against classical numerical methods for extremely large-scale systems.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Magneto-thermally Coupled Field Simulation of Homogenized Foil Winding Models</title>
<link>https://arxiv.org/abs/2503.13010</link>
<guid>https://arxiv.org/abs/2503.13010</guid>
<content:encoded><![CDATA[
<div> homogenization technique, foil windings, high frequency applications, magneto-thermal simulation, Joule losses<br />
Summary:
Homogenization techniques are used in the simulation of foil windings to capture their unique properties for high frequency applications. The layered structure of foil windings results in different electromagnetic and thermal characteristics compared to traditional wire windings. A coupled magneto-thermal simulation approach is employed, considering the interaction between electromagnetic and thermal effects through Joule losses and temperature-dependent material properties. Utilizing different time step sizes for electromagnetic and thermal simulations allows for efficient analysis of foil windings. The method is validated using a simple geometry and applied to a pot transformer with both foil and wire windings, demonstrating its effectiveness in capturing the behavior of complex winding structures. <br /><br />Summary: <div>
arXiv:2503.13010v2 Announce Type: replace 
Abstract: Foil windings have, due to their layered structure, different properties than conventional wire windings, which make them advantageous for high frequency applications. Both electromagnetic and thermal analyses are relevant for foil windings. These two physical areas are coupled through Joule losses and temperature dependent material properties. For an efficient simulation of foil windings, homogenization techniques are used to avoid resolving the single turns. Therefore, this paper comprises a coupled magneto-thermal simulation that uses a homogenization method in the electromagnetic and thermal part. A weak coupling with different time step sizes for both parts is presented. The method is verified on a simple geometry and showcased for a pot transformer that uses a foil and a wire winding.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cohort-attention Evaluation Metric against Tied Data: Studying Performance of Classification Models in Cancer Detection</title>
<link>https://arxiv.org/abs/2503.12755</link>
<guid>https://arxiv.org/abs/2503.12755</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, medical screening, imbalanced data, fairness, evaluation metrics

Summary:
The article discusses the limitations of traditional classification metrics in evaluating the accuracy of AI-driven medical screening models, particularly in handling imbalanced data and varying performance across different patient cohorts. To address these challenges, the authors propose the Cohort-Attention Evaluation Metrics (CAT) framework. This framework introduces patient-level assessment, entropy-based distribution weighting, and cohort-weighted sensitivity and specificity metrics to ensure fair and balanced evaluations across diverse populations. Key metrics such as CATSensitivity (CATSen), CATSpecificity (CATSpe), and CATMean are introduced to enhance predictive reliability and interpretability of AI models in medical screening. By incorporating these metrics, the CAT framework aims to provide a more robust evaluation method that takes into account patient-level inconsistencies and ensures fairness and reliability in assessing the performance of AI-driven medical screening models. <div>
arXiv:2503.12755v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) has significantly improved medical screening accuracy, particularly in cancer detection and risk assessment. However, traditional classification metrics often fail to account for imbalanced data, varying performance across cohorts, and patient-level inconsistencies, leading to biased evaluations. We propose the Cohort-Attention Evaluation Metrics (CAT) framework to address these challenges. CAT introduces patient-level assessment, entropy-based distribution weighting, and cohort-weighted sensitivity and specificity. Key metrics like CATSensitivity (CATSen), CATSpecificity (CATSpe), and CATMean ensure balanced and fair evaluation across diverse populations. This approach enhances predictive reliability, fairness, and interpretability, providing a robust evaluation method for AI-driven medical screening models.
]]></content:encoded>
<pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive Alignment</title>
<link>https://arxiv.org/abs/2505.11194</link>
<guid>https://arxiv.org/abs/2505.11194</guid>
<content:encoded><![CDATA[
<div> Keywords: protein function, sequence-to-text model, natural language generation, multimodal learning, contrastive alignment

Summary:
Prot2Text-V2 is a novel multimodal sequence-to-text model that generates natural language descriptions of protein function from amino acid sequences. It combines a protein language model and a decoder-only language model through a modality projector, and includes Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE) to enhance cross-modal learning. The model is trained on curated entries from SwissProt and excels in low-homology conditions, outperforming traditional and LLM-based baselines. The innovative approach of Prot2Text-V2 showcases its ability to generate accurate protein function descriptions directly from protein sequences, showcasing the potential of multimodal learning in predicting protein function.<br /><br />Summary: <div>
arXiv:2505.11194v3 Announce Type: replace 
Abstract: Predicting protein function from sequence is a central challenge in computational biology. While existing methods rely heavily on structured ontologies or similarity-based techniques, they often lack the flexibility to express structure-free functional descriptions and novel biological functions. In this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text model that generates free-form natural language descriptions of protein function directly from amino acid sequences. Our method combines a protein language model as a sequence encoder (ESM-3B) and a decoder-only language model (LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A key innovation is our Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE), which improves cross-modal learning by matching mean- and std-pooled protein embeddings with text representations via contrastive loss. After the alignment phase, we apply instruction-based fine-tuning using LoRA on the decoder to teach the model how to generate accurate protein function descriptions conditioned on the protein sequence. We train Prot2Text-V2 on about 250K curated entries from SwissProt and evaluate it under low-homology conditions, where test sequences have low similarity with training samples. Prot2Text-V2 consistently outperforms traditional and LLM-based baselines across various metrics.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated physics-informed learning and resonance process signature for the prediction of fatigue crack growth for laser-fused alloys</title>
<link>https://arxiv.org/abs/2510.21018</link>
<guid>https://arxiv.org/abs/2510.21018</guid>
<content:encoded><![CDATA[
<div> machine learning, fatigue crack growth, laser-fused components, SS-316L, physics-informed modeling

Summary: 
This study addresses the challenges in predicting fatigue crack growth of laser-fused SS-316L components with random geometrical defects. Traditional statistics-based models struggle to account for the scattering in fatigue behaviors caused by these defects. The proposed approach, a physics-informed machine learning (PIML) model, integrates fatigue laws and constraints with small data to provide a realistic and interpretable prediction. By leveraging resonance process signature data and Paris's law, the model successfully learns the constants and predicts crack growth rates without the need for experimental crack growth data. The results demonstrate that the model can accurately predict crack sizes and provide insights into the fatigue behavior of laser-fused materials. The combination of machine learning and physics-based modeling offers a promising approach for predicting fatigue crack growth in metal components with inherent scattering. <br /><br />Summary: <div>
arXiv:2510.21018v1 Announce Type: new 
Abstract: Fatigue behaviors of metal components by laser fusion suffer from scattering due to random geometrical defects (e.g., porosity, lack of fusion). Monitoring fatigue crack initiation and growth is critical, especially for laser-fused components with significant inherent fatigue scattering. Conventional statistics-based curve-fitting fatigue models have difficulty incorporating significant scattering in their fatigue life due to the random geometrical defects. A scattering-informed predictive method is needed for laser-fused materials' crack size and growth. Current data-driven machine learning could circumvent the issue of deterministic modeling, but results in a black-box function that lacks interpretability. To address these challenges, this study explores a novel nondimensionalized physics-informed machine learning (PIML) model to predict fatigue crack growth of laser-fused SS-316L by integrating fatigue laws and constraints with small data to ensure a realistic and interpretable prediction. Resonance process signature data were leveraged with Paris's law to train the PIML model without experimental crack growth data. The results show that Paris's law constants can be learned with good similarity to comparable data from the literature, and the crack growth rate can be predicted to compute crack sizes.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linked Cell Traversal Algorithms for Three-Body Interactions in Molecular Dynamics</title>
<link>https://arxiv.org/abs/2510.21230</link>
<guid>https://arxiv.org/abs/2510.21230</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular dynamics, three-body interactions, parallel computation, linked cells, Lennard-Jones fluid <br />
Summary: 
This work focuses on developing parallel computation algorithms for three-body interactions in molecular dynamics. The algorithms extend existing traversals for pair interactions to handle interactions between molecules stored across three cells. A general framework for computing three-body interactions in linked cells is described and implemented. The analysis includes considering cutoff conditions, which affect the workload of interaction computations. The algorithms are validated using the Lennard-Jones fluid, with case studies configured for homogeneous and inhomogeneous scenarios. The study evaluates strong scalability and performance in terms of molecule updates at the node level. The results demonstrate the effectiveness and efficiency of the developed algorithms for handling three-body interactions in molecular dynamics. <br /><br />Summary: <div>
arXiv:2510.21230v1 Announce Type: new 
Abstract: In this work, algorithms for the parallel computation of three-body interactions in molecular dynamics are developed. While traversals for the computation of pair interactions are readily available in the literature, here, such traversals are extended to allow for the computation between molecules stored across three cells. A general framework for the computation of three-body interactions in linked cells is described, and then used to implement the corresponding traversals. In addition, our analysis is combined with the commonly used cutoff conditions, because they influence the total workload of the computation of interactions. The combinations between traversals and truncation conditions are validated using the well-known Lennard-Jones fluid. Validation case studies are taken from the literature and configured into homogeneous and inhomogeneous scenarios. Finally, strong scalability and performance in terms of molecule updates are measured at node-level.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crisis-Resilient Portfolio Management via Graph-based Spatio-Temporal Learning</title>
<link>https://arxiv.org/abs/2510.20868</link>
<guid>https://arxiv.org/abs/2510.20868</guid>
<content:encoded><![CDATA[
<div> Keywords: financial time series forecasting, graph-based spatio-temporal learning, crisis-resilient investment, regime-specific predictions, adaptive portfolio allocation

Summary:
CRISP, a novel framework for financial time series forecasting, addresses the challenge of predicting optimal asset allocations during crisis periods. Unlike traditional methods, CRISP adapts to shifting market dynamics by leveraging Graph Convolutional Networks and BiLSTM with self-attention to learn sparse structures through Graph Attention Networks. By filtering out noise and capturing crisis-relevant dependencies, CRISP achieves accurate regime-specific predictions. Trained on data spanning credit and pandemic crises, CRISP demonstrates robust generalization to inflation-driven markets. This adaptive approach enables portfolio allocation that maintains profitability during downturns, outperforming equal-weight baselines and static graph methods. The learned attention weights provide interpretable regime detection, with defensive cluster attention strengthening significantly during crises, reflecting emergent behavior from learning to forecast rather than imposing assumptions.

<br /><br />Summary: <div>
arXiv:2510.20868v1 Announce Type: cross 
Abstract: Financial time series forecasting faces a fundamental challenge: predicting optimal asset allocations requires understanding regime-dependent correlation structures that transform during crisis periods. Existing graph-based spatio-temporal learning approaches rely on predetermined graph topologies--correlation thresholds, sector classifications--that fail to adapt when market dynamics shift across different crisis mechanisms: credit contagion, pandemic shocks, or inflation-driven selloffs.
  We present CRISP (Crisis-Resilient Investment through Spatio-temporal Patterns), a graph-based spatio-temporal learning framework that encodes spatial relationships via Graph Convolutional Networks and temporal dynamics via BiLSTM with self-attention, then learns sparse structures through multi-head Graph Attention Networks. Unlike fixed-topology methods, CRISP discovers which asset relationships matter through attention mechanisms, filtering 92.5% of connections as noise while preserving crisis-relevant dependencies for accurate regime-specific predictions.
  Trained on 2005--2021 data encompassing credit and pandemic crises, CRISP demonstrates robust generalization to 2022--2024 inflation-driven markets--a fundamentally different regime--by accurately forecasting regime-appropriate correlation structures. This enables adaptive portfolio allocation that maintains profitability during downturns, achieving Sharpe ratio 3.76: 707% improvement over equal-weight baselines and 94% improvement over static graph methods. Learned attention weights provide interpretable regime detection, with defensive cluster attention strengthening 49% during crises versus 31% market-wide--emergent behavior from learning to forecast rather than imposing assumptions.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Structural Validation of a Micro-UAV with On-Board Dynamic Route Planning</title>
<link>https://arxiv.org/abs/2510.21648</link>
<guid>https://arxiv.org/abs/2510.21648</guid>
<content:encoded><![CDATA[
<div> Keywords: Micro aerial vehicles, search and rescue operations, lightweight drone, structural durability, adaptive navigation<br />
Summary:<br />
Micro aerial vehicles are vital for search and rescue missions due to their agility and accessibility to hazardous areas. However, many low-cost aerial systems lack structural durability and dynamic path replanning capabilities. This study introduces a custom-built drone that addresses these limitations by using readily available components and materials, focusing on modularity, affordability, and ease of assembly. The drone's frame is reinforced for impact resistance, while the control system operates on free open-source software, enabling real-time perception and adaptive navigation without costly hardware accelerators. This approach offers a practical and cost-effective solution for search and rescue missions, showcasing the importance of lightweight yet durable drone design for navigating rough terrains and responding dynamically to new obstacles or victims.<br /><br />Summary: <div>
arXiv:2510.21648v1 Announce Type: cross 
Abstract: Micro aerial vehicles are becoming increasingly important in search and rescue operations due to their agility, speed, and ability to access confined spaces or hazardous areas. However, designing lightweight aerial systems presents significant structural, aerodynamic, and computational challenges. This work addresses two key limitations in many low-cost aerial systems under two kilograms: their lack of structural durability during flight through rough terrains and inability to replan paths dynamically when new victims or obstacles are detected. We present a fully customised drone built from scratch using only commonly available components and materials, emphasising modularity, low cost, and ease of assembly. The structural frame is reinforced with lightweight yet durable materials to withstand impact, while the onboard control system is powered entirely by free, open-source software solutions. The proposed system demonstrates real-time perception and adaptive navigation capabilities without relying on expensive hardware accelerators, offering an affordable and practical solution for real-world search and rescue missions.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A hybrid framework integrating classical computers and quantum annealers for optimisation of truss structures</title>
<link>https://arxiv.org/abs/2502.19570</link>
<guid>https://arxiv.org/abs/2502.19570</guid>
<content:encoded><![CDATA[
<div> optimisation, quantum computing, hybrid framework, structural engineering, truss design

Summary:
The proposed work introduces a hybrid framework that combines classical computers with quantum annealers for structural optimization. This approach involves formulating two minimization problems at each iteration: one for the mechanical boundary value problem and one for updating the design variables. By leveraging quantum computing, specifically quantum annealing-assisted sequential programming, the framework can efficiently solve these minimization problems. Several case studies on truss optimization demonstrate the framework's effectiveness in utilizing quantum computers for optimization tasks. The framework shows promise for future structural optimization applications, especially in scenarios where classical computers face limitations due to problem complexities. This advancement opens up new possibilities in the field of structural engineering by harnessing the power of quantum computing for design optimization. 

<br /><br />Summary: <div>
arXiv:2502.19570v2 Announce Type: replace 
Abstract: This work proposes a hybrid framework combining classical computers with quantum annealers for structural optimisation. At each optimisation iteration of an iterative process, two minimisation problems are formulated one for the underlying mechanical boundary value problem through the minimisation potential energy principle and one for the minimisation problem to update the design variables. Our hybrid approach leverages the strength of quantum computing to solve these two minimisation problems at each step, thanks to the developed quantum annealing-assisted sequential programming strategy introduced in [Nguyen, Wu, Remacle, and Noels. A quantum annealing-sequential quadratic programming assisted finite element simulation for non-linear and history-dependent mechanical problems. European Journal of Mechanics-A/Solids 105 (2024): 105254]. The applicability of the proposed framework is demonstrated through several case studies of truss optimisation, highlighting its capability to perform optimisation with quantum computers. The proposed framework offers a promising direction for future structural optimisation applications, particularly in scenarios where the quantum computer could resolve the size limitations of the classical computers due to problem complexities.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VENI, VINDy, VICI: a generative reduced-order modeling framework with uncertainty quantification</title>
<link>https://arxiv.org/abs/2405.20905</link>
<guid>https://arxiv.org/abs/2405.20905</guid>
<content:encoded><![CDATA[
<div> machine learning, reduced-order models, variational autoencoders, sparse identification of nonlinear dynamics, uncertainty quantification

Summary:
The article proposes a data-driven framework for building interpretable reduced-order models (ROMs) to solve complex systems of partial differential equations. The framework combines variational autoencoders for dimensionality reduction and a new method called Variational Identification of Nonlinear Dynamics (VINDy) to identify latent variables and dynamics in an interpretable manner. The method, named VENI, VINDy, VICI, uses Variational Encoding of Noisy Inputs (VENI) to determine reduced coordinates' distribution and VINDy to learn the coefficients of candidate functions. The trained model allows for querying new parameter instances and initial conditions to compute full-time solutions. The probabilistic setup enables uncertainty quantification through Variational Inference, providing Certainty Intervals (VICI) during online testing. The effectiveness of the VINDy method is demonstrated on the Roessler system under different noise intensities, followed by testing on PDE benchmarks in structural mechanics and fluid dynamics. <div>
arXiv:2405.20905v2 Announce Type: replace-cross 
Abstract: The simulation of many complex phenomena in engineering and science requires solving expensive, high-dimensional systems of partial differential equations (PDEs). To circumvent this, reduced-order models (ROMs) have been developed to speed up computations. However, when governing equations are unknown or partially known, typically ROMs lack interpretability and reliability of the predicted solutions.
  In this work we present a data-driven, non-intrusive framework for building ROMs where the latent variables and dynamics are identified in an interpretable manner and uncertainty is quantified. Starting from a limited amount of high-dimensional, noisy data the proposed framework constructs an efficient ROM by leveraging variational autoencoders for dimensionality reduction along with a newly introduced, variational version of sparse identification of nonlinear dynamics (SINDy), which we refer to as Variational Identification of Nonlinear Dynamics (VINDy).
  In detail, the method consists of Variational Encoding of Noisy Inputs (VENI) to identify the distribution of reduced coordinates. Simultaneously, we learn the distribution of the coefficients of a pre-determined set of candidate functions by VINDy. Once trained offline, the identified model can be queried for new parameter instances and new initial conditions to compute the corresponding full-time solutions. The probabilistic setup enables uncertainty quantification as the online testing consists of Variational Inference naturally providing Certainty Intervals (VICI). In this work we showcase the effectiveness of the newly proposed VINDy method in identifying interpretable and accurate dynamical system for the Roessler system with different noise intensities and sources. Then the performance of the overall method - named VENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics and fluid dynamics.
]]></content:encoded>
<pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite Element and Machine Learning Modeling of Autogenous Self-Healing Concrete</title>
<link>https://arxiv.org/abs/2510.19839</link>
<guid>https://arxiv.org/abs/2510.19839</guid>
<content:encoded><![CDATA[
<div> Keywords: self-healing concrete, moisture diffusion, damage evolution, finite element modeling, machine learning <br />
Summary: A time-dependent modeling framework was developed for autogenous self-healing concrete, considering moisture diffusion and damage evolution. Two finite element variants, Crack Diffusion Model (CDM) and Crack Membrane Model (CMM), were implemented to simulate healing processes. The orientation and size of initial cracks, diffusion coefficients, healing rate constant, and cement availability smoothing parameter were identified as key parameters affecting healing time. The simulations showed non-monotonic variations in healing time based on crack orientation and size. The CMM model replicated staged moisture penetration but extended total healing time compared to the CDM model. Machine learning classifiers achieved high accuracy in predicting healing outcomes. The framework serves as a valuable tool for guiding experimental studies and practical applications of self-healing concrete. <br /><br />Summary: <div>
arXiv:2510.19839v1 Announce Type: new 
Abstract: A time-dependent modeling framework for autogenous self-healing concrete that couples moisture diffusion with damage evolution was developed. Water transport follows Fick's second law with a damage-dependent diffusivity obtained by power-law interpolation between intact concrete and crack space. Healing reduces damage in proportion to local moisture and a smoothed cement availability field computed via a Helmholtz filter. Two finite element variants were implemented in FEniCSx over time horizons up to $5\times10^6$ seconds: a Crack Diffusion Model (CDM) with standard diffusion and a Crack Membrane Model (CMM) that gates cross-crack transport until a critical moisture threshold is reached. Key control parameters are the initial crack orientation and size, the diffusion coefficients of intact and cracked concrete, the healing rate constant, and the cement availability smoothing parameter. Simulations on a unit square domain show that healing time varies non-monotonically with crack orientation, peaking near $45^\circ$ and $135^\circ$ and minimizing near $90^\circ$, consistent with diffusion distance to crack endpoints dominating the process. The dependence on crack width reverses with material parameters: healing time increases when $D_{\text{cracked}}D_{\text{intact}}$. The CMM reproduces staged moisture penetration with delayed gate activation but lengthens total healing time, whereas the CDM is efficient for parametric sweeps. Machine learning classifiers trained on one million simulation samples predict binary healing outcomes $H(\sigma,\gamma,t)$ (healed or not) with high accuracy (up to 0.998 for neural networks). Although experimental calibration is still required, the framework provides a versatile tool for guiding laboratory studies and implementations of self-healing concrete.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A polygonal Reissner-Mindlin plate element based on the scaled boundary finite element method</title>
<link>https://arxiv.org/abs/2510.20044</link>
<guid>https://arxiv.org/abs/2510.20044</guid>
<content:encoded><![CDATA[
<div> Keywords: polygonal Reissner-Mindlin plate element, scaled boundary finite element method, transverse shear locking, assumed natural strain approach, two-field variational formulation <br />
<br />
Summary: 
This work introduces a polygonal Reissner-Mindlin plate element using a scaled boundary finite element method with linear shape functions. This approach allows the use of non-star-convex-polygonal elements, simplifying meshing. To address transverse shear locking, an assumed natural strain approach is applied. A two-field variational formulation is developed to incorporate three-dimensional material laws while enforcing plane stress assumptions. This enables the use of material models defined in three-dimensional continuum, considering Poisson's thickness locking. The effectiveness of the formulation is demonstrated through various numerical examples. <div>
arXiv:2510.20044v1 Announce Type: new 
Abstract: In this work, a polygonal Reissner-Mindlin plate element is presented. The formulation is based on a scaled boundary finite element method, where in contrast to the original semi-analytical approach, linear shape functions are introduced for the parametrization of the scaling and the radial direction. This yields a fully discretized formulation, which enables the use of non-star-convex-polygonal elements with an arbitrary number of edges, simplifying the meshing process. To address the common effect of transverse shear locking for low-order Reissner-Mindlin elements in the thin-plate limit, an assumed natural strain approach for application on the polygonal scaled boundary finite elements is derived. Further, a two-field variational formulation is introduced to incorporate three-dimensional material laws. Here the plane stress assumptions are enforced on the weak formulation, facilitating the use of material models defined in three-dimensional continuum while considering the effect of Poisson's thickness locking. The effectiveness of the proposed formulation is demonstrated in various numerical examples.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparseEB-gMCR: A Generative Solver for Extreme Sparse Components with Application to Contamination Removal in GC-MS</title>
<link>https://arxiv.org/abs/2510.20364</link>
<guid>https://arxiv.org/abs/2510.20364</guid>
<content:encoded><![CDATA[
<div> decomposability, SparseEB-gMCR, chemical data analysis, contamination removal, analytical chemistry

Summary:
SparseEB-gMCR, a new extension of the EB-gMCR method, has been developed to handle extreme signal sparsity in analytical chemistry instruments. By introducing a fixed EB-select module and making minor adjustments to energy optimization, SparseEB-gMCR demonstrated comparable decomposability and scalability to dense-component EB-gMCR in synthetic datasets. When applied to real GC-MS chromatograms for unsupervised contamination removal, SparseEB-gMCR effectively eliminated siloxane-related pollution signals and improved compound identification reliability. This new extension allows the EB-gMCR family to be applied to a wider range of real-world chemical datasets, providing a general mathematical framework for signal unmixing and contamination elimination in analytical chemistry. <div>
arXiv:2510.20364v1 Announce Type: new 
Abstract: Analytical chemistry instruments provide physically meaningful signals for elucidating analyte composition and play important roles in material, biological, and food analysis. These instruments are valued for strong alignment with physical principles, enabling compound identification through pattern matching with chemical libraries. More reliable instruments generate sufficiently sparse signals for direct interpretation. Generative multivariate curve resolution (gMCR) and its energy-based solver (EB-gMCR) offer powerful tools for decomposing mixed signals suitable for chemical data analysis. However, extreme signal sparsity from instruments such as GC-MS or 1H-NMR can impair EB-gMCR decomposability. To address this, a fixed EB-select module inheriting EB-gMCR's design was introduced for handling extreme sparse components. Combined with minor adjustments to energy optimization, this led to SparseEB-gMCR. In synthetic datasets, SparseEB-gMCR exhibited comparable decomposability and graceful scalability to dense-component EB-gMCR. The sparse variant was applied to real GC-MS chromatograms for unsupervised contamination removal. Analysis showed siloxane-related pollution signals were effectively eliminated, improving compound identification reliability. Results demonstrate that SparseEB-gMCR preserves the decomposability and self-determining component capability of EB-gMCR while extending adaptability to sparse and irregular chemical data. With this sparse extension, the EB-gMCR family becomes applicable to wider ranges of real-world chemical datasets, providing a general mathematical framework for signal unmixing and contamination elimination in analytical chemistry.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers</title>
<link>https://arxiv.org/abs/2510.20066</link>
<guid>https://arxiv.org/abs/2510.20066</guid>
<content:encoded><![CDATA[
<div> liquidity, volatility, spillovers, risk, cryptoassets

Summary:
- The study examines the relationship between liquidity and volatility proxies of core cryptoassets and their impact on forecasting market-wide risk.
- Three statistical layers are integrated: interactions between liquidity and returns, principal-component relations, and volatility-factor projections.
- Various statistical techniques are utilized, including vector autoregression impulse responses, forecast error variance decompositions, and HAR-X models with exogenous regressors.
- The analysis shows significant Granger-causal relationships across layers and moderate predictive accuracy out-of-sample.
- The most informative figures, such as the pipeline overview, Layer A heatmap, Layer C robustness analysis, and precision-recall curve, are presented. Full data and figures are available in the artifact repository. 

Summary:<br /><br /> <div>
arXiv:2510.20066v1 Announce Type: cross 
Abstract: We study whether liquidity and volatility proxies of a core set of cryptoassets generate spillovers that forecast market-wide risk. Our empirical framework integrates three statistical layers: (A) interactions between core liquidity and returns, (B) principal-component relations linking liquidity and returns, and (C) volatility-factor projections that capture cross-sectional volatility crowding. The analysis is complemented by vector autoregression impulse responses and forecast error variance decompositions (see Granger 1969; Sims 1980), heterogeneous autoregressive models with exogenous regressors (HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using temporal splits, early stopping, validation-only thresholding, and SHAP-based interpretation. Using daily data from 2021 to 2025 (1462 observations across 74 assets), we document statistically significant Granger-causal relationships across layers and moderate out-of-sample predictive accuracy. We report the most informative figures, including the pipeline overview, Layer A heatmap, Layer C robustness analysis, vector autoregression variance decompositions, and the test-set precision-recall curve. Full data and figure outputs are provided in the artifact repository.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI PB: A Grounded Generative Agent for Personalized Investment Insights</title>
<link>https://arxiv.org/abs/2510.20099</link>
<guid>https://arxiv.org/abs/2510.20099</guid>
<content:encoded><![CDATA[
<div> Keywords: AI PB, generative agent, retail finance, investment insights, finance-domain embedding model

Summary: 
AI PB is a generative agent deployed in retail finance that proactively generates investment insights. It uses a deterministic orchestration layer to route between LLMs, a hybrid retrieval pipeline, and a multi-stage recommendation mechanism. The system operates on-premises under Korean financial regulations and utilizes Docker Swarm and vLLM on NVIDIA H100 GPUs. Grounded generation with explicit routing and layered safety ensures trustworthy AI insights in high-stakes finance. <div>
arXiv:2510.20099v1 Announce Type: cross 
Abstract: We present AI PB, a production-scale generative agent deployed in real retail finance. Unlike reactive chatbots that answer queries passively, AI PB proactively generates grounded, compliant, and user-specific investment insights. It integrates (i) a component-based orchestration layer that deterministically routes between internal and external LLMs based on data sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the finance-domain embedding model, and (iii) a multi-stage recommendation mechanism combining rule heuristics, sequential behavioral modeling, and contextual bandits. Operating fully on-premises under Korean financial regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100 GPUs. Through human QA and system metrics, we demonstrate that grounded generation with explicit routing and layered safety can deliver trustworthy AI insights in high-stakes finance.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Exchange that Mitigate a Bribery Attack</title>
<link>https://arxiv.org/abs/2510.20645</link>
<guid>https://arxiv.org/abs/2510.20645</guid>
<content:encoded><![CDATA[
<div> HTLC, bribery attack, MAD-HTLC, He-HTLC, Miner-collusion<br />
Summary:<br />
Despite the popularity of Hashed Time-Locked Contracts (HTLCs) in various applications, their use in exchanges is still questionable due to vulnerability to bribery attacks. State-of-the-art solutions like MAD-HTLC and He-HTLC address this issue but have limitations against collusion scenarios. This paper exposes vulnerabilities in these solutions and proposes a stronger attack, demonstrating the need for a more secure protocol. The proposed protocol, \prot, is designed to be game-theoretically secure and resistant to all bribery scenarios by employing a two-phase approach to prevent unauthorized token confiscation. Implementation on Bitcoin and Ethereum shows \prots efficiency in transaction cost and latency. <div>
arXiv:2510.20645v1 Announce Type: cross 
Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of their use in wide areas of applications such as payment channels, atomic swaps, etc, their use in exchange is still questionable. This is because of its incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC (NDSS'23) address this by leveraging miners' profit-driven behaviour to mitigate such attacks. The former is the mitigation against passive miners; however, the latter works against both active and passive miners. However, they consider only two bribing scenarios where either of the parties involved in the transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by presenting a miner-collusion bribery attack with implementation and game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC than He-HTLC, allowing the attacker to earn profits equivalent to attacking naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC protocol resistant to all bribery scenarios. \prot\ employs a two-phase approach, preventing unauthorized token confiscation by third parties, such as miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is executed without manipulation. We demonstrate \prot's efficiency in transaction cost and latency via implementations on Bitcoin and Ethereum.
]]></content:encoded>
<pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter Estimation in River Transport Models With Immobile Phase Exchange Using Dimensional Analysis and Reduced-Order Models</title>
<link>https://arxiv.org/abs/2510.19664</link>
<guid>https://arxiv.org/abs/2510.19664</guid>
<content:encoded><![CDATA[
<div> framework, parameter estimation, river transport models, breakthrough curves, dimensionless

Summary:
The article introduces a novel framework, Dimensionless Synthetic Transport Estimation (DSTE), for parameter estimation in river transport models using breakthrough curve data. The framework incorporates immobile phase exchange through a memory function in a one-dimensional advection-dispersion equation model. By solving the governing equation analytically in the Laplace domain and numerically inverting it, synthetic breakthrough curves are generated for different memory functions and boundary conditions. Through a dimensionless formulation, the estimation of advection velocity is decoupled from other parameters, reducing the required forward solutions. Computational efficiency is improved using a Karhunen-Loeve (KL) expansion to transform the synthetic dataset into a reduced-order space. The DSTE method is benchmarked against various alternatives and proves to deliver accurate parameter estimates when applied to 295 breakthrough curves from 54 tracer tests in 25 rivers. The resulting labeled dataset provides valuable insights into linking transport parameters with hydraulic conditions, site characteristics, and measured concentrations, eliminating the need for additional forward simulations. <br /><br />Summary: <div>
arXiv:2510.19664v1 Announce Type: new 
Abstract: We propose a framework for parameter estimation in river transport models using breakthrough curve data, which we refer to as Dimensionless Synthetic Transport Estimation (DSTE). We utilize this framework to parameterize the one-dimensional advection-dispersion equation model, incorporating immobile phase exchange through a memory function. We solve the governing equation analytically in the Laplace domain and numerically invert it to generate synthetic breakthrough curves for different memory functions and boundary conditions. A dimensionless formulation enables decoupling the estimation of advection velocity from other parameters, significantly reducing the number of required forward solutions. To improve computational efficiency, we apply a Karhunen-Loeve (KL) expansion to transform the synthetic dataset into a reduced-order space. Given a measured breakthrough curve, we estimate the advection velocity by minimizing the distance from the measurement to the synthetic data in KL space, and infer the remaining dimensionless parameters by Projected Barycentric Interpolation (PBI). We benchmark our method against several alternatives, including Laplace domain fitting, moment matching, global random optimization, and variations of the DSTE framework using nearest-neighbor interpolation and neural network-based estimation. Applied to 295 breakthrough curves from 54 tracer tests in 25 rivers, DSTE delivers accurate parameter estimates. The resulting labeled dataset allows researchers to link transport parameters with hydraulic conditions, site characteristics, and measured concentrations. The synthetic dataset can be leveraged for the analysis of new breakthrough curves, eliminating the need for additional forward simulations.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Black Tuesday Attack: how to crash the stock market with adversarial examples to financial forecasting models</title>
<link>https://arxiv.org/abs/2510.18990</link>
<guid>https://arxiv.org/abs/2510.18990</guid>
<content:encoded><![CDATA[
<div> Keywords: stock market crash, adversarial example, financial forecasting models, hostile actors, economic damage <br />
Summary: <br />
The article explores the potential risk of triggering a stock market crash through manipulations of individual stock values to create adversarial examples that deceive financial forecasting models. These subtle interventions could lead to self-fulfilling predictions of a crash, posing a significant threat to financial stability and providing an opportunity for hostile actors to inflict economic harm on adversaries. The scheme, though difficult to detect, could be directed at an entire economy or a specific company. The theoretical basis and potential impacts of such attacks are outlined, emphasizing the urgent need for research on defense strategies. The underappreciated threat highlights the importance of understanding and mitigating vulnerabilities in financial systems. <br /> <div>
arXiv:2510.18990v1 Announce Type: cross 
Abstract: We investigate and defend the possibility of causing a stock market crash via small manipulations of individual stock values that together realize an adversarial example to financial forecasting models, causing these models to make the self-fulfilling prediction of a crash. Such a crash triggered by an adversarial example would likely be hard to detect, since the model's predictions would be accurate and the interventions that would cause it are minor. This possibility is a major risk to financial stability and an opportunity for hostile actors to cause great economic damage to an adversary. This threat also exists against individual stocks and the corresponding valuation of individual companies. We outline how such an attack might proceed, what its theoretical basis is, how it can be directed towards a whole economy or an individual company, and how one might defend against it. We conclude that this threat is vastly underappreciated and requires urgent research on how to defend against it.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wind Variability and Its Effect on Transmission Line Capacity Estimation</title>
<link>https://arxiv.org/abs/2510.19433</link>
<guid>https://arxiv.org/abs/2510.19433</guid>
<content:encoded><![CDATA[
<div> wind velocity averaging, Dynamic Thermal Rating, transmission line, Nusselt number, ampacity

Summary: 
This study examines the impact of wind velocity averaging on Dynamic Thermal Rating (DTR) calculations using high-temporal-resolution wind measurements from a transmission line in Slovenia. Two averaging methods, vector averaging and hybrid averaging, are compared. The study finds that averaging significantly affects Nusselt number and ampacity, with a strong angular dependency on wind direction relative to the line. In cases of parallel wind, averaged data underestimates ampacity, while perpendicular wind can lead to overestimation. The study highlights the importance of considering the averaging method in DTR calculations to ensure safe operation of transmission lines. <div>
arXiv:2510.19433v1 Announce Type: cross 
Abstract: This study investigates the impact of wind velocity averaging on Dynamic Thermal Rating (DTR) calculations. It is based on a high-temporal-resolution (1 second) wind measurements obtained from a transmission line in Slovenia, Europe. Wind speed and direction variability are analysed, and two averaging methods, namely vector averaging, where velocity is averaged as vector, and hybrid averaging, where speed is averaged as scalar, are employed. DTR calculations are performed on both high-resolution data and averaged data (5 minute averaging window). It is demonstrated that averaging has a significant effect on both Nusselt number and ampacity, and the effect exhibits a strong angular dependency on the relative angle of the wind to the line. Therefore, two limit cases are studied: in the case of parallel wind, averaged data underestimates the ampacity, and there is a significant amount of cases where the underestimation is larger than 10 %. In the case of perpendicular wind, the two averaging methods affect the results in different ways, but both result in a substantial amount of cases where ampacity is overestimated, potentially leading to unsafe operation. The main takeaway of the study is that averaging wind velocity has a significant impact on DTR results, and special emphasis should be given to the averaging method, as different methods affect the results in different ways.
]]></content:encoded>
<pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regional heterogeneity in left atrial stiffness impacts passive deformation in a cohort of patient-specific models</title>
<link>https://arxiv.org/abs/2510.18642</link>
<guid>https://arxiv.org/abs/2510.18642</guid>
<content:encoded><![CDATA[
<div> patient-specific models, left atrium, atrial fibrillation, biomechanics, myocardial stiffness
Summary:
Patient-specific models were created from CT images to study the biomechanics of the left atrium (LA) in atrial fibrillation (AF). Regional myocardial stiffness variations in the LA were found to be significant factors in physiological deformation, while anatomical features such as wall thickness and adipose volume were less impactful. The models successfully replicated patient atrial motion and provided insights into the altered biomechanics of the LA in AF. This study highlights the importance of considering regional stiffness in understanding atrial biomechanics and suggests a complex interplay of factors influencing LA function. <div>
arXiv:2510.18642v1 Announce Type: new 
Abstract: The deformation of the left atrium (LA), or its biomechanical function, is closely linked to the health of this cardiac chamber. In atrial fibrillation (AF), atrial biomechanics are significantly altered but the underlying cause of this change is not always clear. Patient-specific models of the LA that replicate patient atrial motion can allow us to understand how factors such as atrial anatomy, myocardial stiffness and physiological constraints are linked to atrial biomechanics. We created patient-specific LA models from CT images. We fitted regional model stiffness to peak CT-derived deformation during the LA reservoir phase ($\pm0.90$ mm) and used the CT deformation transients through the reservoir and conduit phase for model validation (deformation transients fell within $\pm0.38$ mm per unit time of targets). We found that myocardial stiffness varies regionally across the LA. The regional stiffness values were significant factors contributing to regional physiological LA deformation ($p=0.023$) while features of LA anatomy, including regional wall thickness and adipose volume, were less important. These findings provide insight into the underlying causes of altered LA biomechanics in AF.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAISE: A Unified Framework for Responsible AI Scoring and Evaluation</title>
<link>https://arxiv.org/abs/2510.18559</link>
<guid>https://arxiv.org/abs/2510.18559</guid>
<content:encoded><![CDATA[
<div> framework, responsible AI, evaluation, deep learning models, multi-dimensional  

Summary:  
The article introduces RAISE, a framework for evaluating AI models based on four dimensions: explainability, fairness, robustness, and sustainability, creating a holistic Responsibility Score. Three deep learning models were tested on structured datasets from finance, healthcare, and socioeconomics. The Multilayer Perceptron (MLP) showed strong sustainability and robustness, the Feature Tokenizer Transformer excelled in explainability and fairness but at a high environmental cost, and the Tabular ResNet had a balanced profile. This study emphasizes the need for multi-dimensional evaluation in responsible model selection. The implementation of RAISE is available on GitHub. <div>
arXiv:2510.18559v1 Announce Type: cross 
Abstract: As AI systems enter high-stakes domains, evaluation must extend beyond predictive accuracy to include explainability, fairness, robustness, and sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a unified framework that quantifies model performance across these four dimensions and aggregates them into a single, holistic Responsibility Score. We evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular ResNet, and a Feature Tokenizer Transformer, on structured datasets from finance, healthcare, and socioeconomics. Our findings reveal critical trade-offs: the MLP demonstrated strong sustainability and robustness, the Transformer excelled in explainability and fairness at a very high environmental cost, and the Tabular ResNet offered a balanced profile. These results underscore that no single model dominates across all responsibility criteria, highlighting the necessity of multi-dimensional evaluation for responsible model selection. Our implementation is available at: https://github.com/raise-framework/raise.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competitive algorithms for calculating the ground state properties of Bose-Fermi mixtures</title>
<link>https://arxiv.org/abs/2503.13717</link>
<guid>https://arxiv.org/abs/2503.13717</guid>
<content:encoded><![CDATA[
<div> Gross-Pitaevskii equation, Hartree-Fock equation, numerical schemes, Bose-Fermi systems, quantum droplets
Summary:<br />
This work examines various numerical methods for studying the ground state properties of Bose-Fermi systems, specifically mixtures of different atomic species under external forces or self-bound quantum droplets. The bosonic atoms are modeled using the generalized Gross-Pitaevskii equation, while the fermionic atoms are treated individually with wave functions following the Hartree-Fock equation. The study solves the formulated equations using techniques like adiabatic switching of interactions and imaginary time propagation with Gram-Schmidt orthonormalization or Hamiltonian matrix diagonalization. By applying these algorithms to the mixture parameters, including those leading to self-bound quantum droplets, the research demonstrates how different numerical schemes perform in competition. <br /><br />Summary: <div>
arXiv:2503.13717v2 Announce Type: replace 
Abstract: In this work we define, analyze, and compare different numerical schemes that can be used to study the ground state properties of Bose-Fermi systems, such as mixtures of different atomic species under external forces or self-bound quantum droplets. The bosonic atoms are assumed to be condensed and are described by the generalized Gross-Pitaevskii equation. The fermionic atoms, on the other hand, are treated individually, and each atom is associated with a wave function whose evolution follows the Hartree-Fock equation. We solve such a formulated set of equations using a variety of methods, including those based on adiabatic switching of interactions and the imaginary time propagation technique combined with the Gram-Schmidt orthonormalization or the diagonalization of the Hamiltonian matrix. We show how different algorithms compete at the numerical level by studying the mixture in the range of parameters covering the formation of self-bound quantum Bose-Fermi droplets.
]]></content:encoded>
<pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Population-Based Search Method Using Uncertainty-related Pareto Front for Robust Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2510.16386</link>
<guid>https://arxiv.org/abs/2510.16386</guid>
<content:encoded><![CDATA[
<div> framework, robustness, convergence, optimization, multi-objective  
<br />  
Summary:  
The article introduces a new Uncertainty-related Pareto Front (UPF) framework that prioritizes both robustness and convergence in multi-objective optimization. Traditional methods often focus on convergence at the expense of robustness, leading to suboptimal solutions in noisy environments. The UPF framework addresses this by quantifying the impact of noise perturbations on convergence and robustness equally. Building on UPF, the RMOEA-UPF algorithm is proposed, which efficiently calculates and optimizes the UPF during the evolutionary process. Experimental results on various benchmark problems and a real-world application show that RMOEA-UPF consistently outperforms existing methods, providing high-quality solutions for complex, uncertain optimization problems. The algorithm's performance highlights its general applicability and reliability in tackling challenging multi-objective optimization tasks.  
<br /><br />  
 <div>
arXiv:2510.16386v1 Announce Type: new 
Abstract: Traditional robust multi-objective optimization methods typically prioritize convergence while treating robustness as a secondary consideration. This approach can yield solutions that are not genuinely robust optimal under noise-affected scenarios. Furthermore, compared to population-based search methods, determining the robust optimal solution by evaluating the robustness of a single convergence-optimal solution is also inefficient. To address these two limitations,we propose a novel Uncertainty-related Pareto Front (UPF) framework that balances robustness and convergence as equal priorities. Unlike traditional Pareto Front, the UPF explicitly accounts for decision variable with noise perturbation by quantifying their effects on both convergence guarantees and robustness preservation equally within a theoretically grounded and general framework. Building upon UPF, we propose RMOEA-UPF--a population-based search robust multi-objective optimization algorithm. This method enables efficient search optimization by calculating and optimizing the UPF during the evolutionary process.Experiments on nine benchmark problems and a real-world application demonstrate that RMOEA-UPF consistently delivers high-quality results. Our method's consistent top-ranking performance indicates a more general and reliable approach for solving complex, uncertain multi-objective optimization problems. Code is available at: https://github.com/WenxiangJiang-me/RMOEA-UPF.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViT-Transformer: Self-attention mechanism based constitutive modeling for nonlinear heterogeneous materials</title>
<link>https://arxiv.org/abs/2510.16575</link>
<guid>https://arxiv.org/abs/2510.16575</guid>
<content:encoded><![CDATA[
<div> Transformer architectures, machine learning, heterogeneous materials, composites, surrogate model<br />
<br />
Summary: <br />
This study introduces a new surrogate model called ViT-Transformer for multi-scale simulations of nonlinear heterogeneous materials and composites. The model utilizes a Vision Transformer encoder and a Transformer-based decoder with attention mechanisms to capture long-range dependencies and generalization across microstructures. A random extract training algorithm is proposed to enhance training robustness to sequences of variable length. A compact and diverse dataset is constructed via data augmentation to validate the model. The ViT-Transformer model demonstrates effectiveness and accuracy in predicting stress responses from material images and loading scenarios. Multiple numerical examples showcase the model's performance and the effectiveness of the training algorithm. <div>
arXiv:2510.16575v1 Announce Type: new 
Abstract: Multi-scale simulations of nonlinear heterogeneous materials and composites are challenging due to the prohibitive computational costs of high-fidelity simulations. Recently, machine learning (ML) based approaches have emerged as promising alternatives to traditional multiscale methods. However, existing ML surrogate constitutive models struggle in capturing long-range dependencies and generalization across microstructures. The recent advancements in attention-based Transformer architectures open the door to a more powerful class of surrogate models. Attention mechanism has demonstrated remarkable capabilities in natural language processing and computer vision. In this work, we introduce a surrogate (meta) model, namely ViT-Transformer, using a Vision Transformer (ViT) encoder and a Transformer-based decoder which are both driven by the self-attention mechanism. The ViT encoder extracts microstructural features from material images, while the decoder is a masked Transformer encoder that combines the latent geometrical features with the macroscopic strain input sequence to predict the corresponding stress response. To enhance training, we propose a random extract training algorithm that improves robustness to sequences of variable length. We design and construct a compact yet diverse dataset via data augmentation, and validate the surrogate model using various composite material images and loading scenarios. Several numerical examples are provided to show the effectiveness and accuracy of the ViT-Transformer model and the training algorithm.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chem-R: Learning to Reason as a Chemist</title>
<link>https://arxiv.org/abs/2510.16880</link>
<guid>https://arxiv.org/abs/2510.16880</guid>
<content:encoded><![CDATA[
<div> Chem-R, large language models, chemical discovery, reasoning capabilities, molecular tasks, reaction tasks <br />
Summary: <br />
Chem-R is a novel Chemical Reasoning model designed to enhance chemical discovery by integrating core chemical knowledge and advanced reasoning capabilities. The model undergoes a three-phase training framework: Chemical Foundation Training, Chemical Reasoning Protocol Distillation, and Multi-task Group Relative Policy Optimization. This structured approach enables Chem-R to outperform leading large language models like Gemini-2.5-Pro and DeepSeek-R1 by up to 46% on molecular tasks and 66% on reaction tasks. Chem-R also surpasses existing chemical foundation models in performance on molecular and reaction level tasks, showcasing its robust generalization and interpretability. The results demonstrate Chem-R's potential as a cutting-edge AI-driven tool for advancing chemical discovery. <br /> <div>
arXiv:2510.16880v1 Announce Type: new 
Abstract: Although large language models (LLMs) have significant potential to advance chemical discovery, current LLMs lack core chemical knowledge, produce unreliable reasoning trajectories, and exhibit suboptimal performance across diverse chemical tasks. To address these challenges, we propose Chem-R, a generalizable Chemical Reasoning model designed to emulate the deliberative processes of chemists. Chem-R is trained through a three-phase framework that progressively builds advanced reasoning capabilities, including: 1) Chemical Foundation Training, which establishes core chemical knowledge. 2) Chemical Reasoning Protocol Distillation, incorporating structured, expert-like reasoning traces to guide systematic and reliable problem solving. 3) Multi-task Group Relative Policy Optimization that optimizes the model for balanced performance across diverse molecular- and reaction-level tasks. This structured pipeline enables Chem-R to achieve state-of-the-art performance on comprehensive benchmarks, surpassing leading large language models, including Gemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on reaction tasks. Meanwhile, Chem-R also consistently outperforms the existing chemical foundation models across both molecular and reaction level tasks. These results highlight Chem-R's robust generalization, interpretability, and potential as a foundation for next-generation AI-driven chemical discovery.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing data scarcity in structural health monitoring through generative augmentation</title>
<link>https://arxiv.org/abs/2510.16889</link>
<guid>https://arxiv.org/abs/2510.16889</guid>
<content:encoded><![CDATA[
<div> Generative Adversarial Network, Structural Health Monitoring, Bridge Infrastructures, Deep Learning, Data Augmentation

Summary:<br /><br />Structural Health Monitoring is important for bridge safety, but faces challenges like data scarcity and noise. A new study introduces STFTSynth, a custom Generative Adversarial Network model for generating spectrograms from acoustic event signals. STFTSynth outperforms baseline models by creating realistic spectrograms through dense residual blocks and bidirectional gated recurrent units. Evaluation metrics show its superiority in producing high-resolution and temporally consistent spectrograms, especially beneficial for scenarios with rare events like prestressing wire breakage. This approach demonstrates the potential of generative-based data augmentation for enhancing dataset diversity and robustness in Structural Health Monitoring applications. <div>
arXiv:2510.16889v1 Announce Type: new 
Abstract: Structural Health Monitoring plays a crucial role in ensuring the safety, reliability, and longevity of bridge infrastructures through early damage detection. Although recent advances in deep learning-based models have enabled automated event detection, their performance is often limited by data scarcity, environmental noise, and class imbalance. To address these challenges, this study introduces a customized Generative Adversarial Network model, STFTSynth, designed particularly for generating short-time Fourier transform spectrograms derived from acoustic event signals. In contrast to augmentation techniques such as MixUp, generative adversarial networks can synthesize high-quality spectrograms that mimic real-world events, enhancing dataset diversity and robustness. The proposed model integrates dense residual blocks for spatial consistency with bidirectional gated recurrent units for temporal dependency modeling. Model performance is evaluated against three baseline generative models using qualitative inspection and quantitative metrics, including Structural Similarity Index Measure, Peak Signal-to-Noise Ratio, and Fr\'echet Inception Distance. Results show that STFTSynth outperforms baseline models, producing high-resolution, temporally consistent spectrograms that align closely with real-world data. These findings indicate the potential of generative-based data augmentation as a scalable and cost-effective solution for bridge monitoring scenarios where rare events, such as prestressing wire breakage, suffer from data scarcity.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading with the Devil: Risk and Return in Foundation Model Strategies</title>
<link>https://arxiv.org/abs/2510.17165</link>
<guid>https://arxiv.org/abs/2510.17165</guid>
<content:encoded><![CDATA[
<div> financial market, time-series tasks, Foundation models, risk profiles, Capital Asset Pricing Model 

Summary:
The paper introduces an extension to the Capital Asset Pricing Model (CAPM) to analyze the risk profiles of trading strategies built on foundation models in finance. It distinguishes between systematic risk from the shared pretrained model and idiosyncratic risk from custom fine-tuning. By aligning this decomposition with uncertainty disentanglement, it categorizes systematic risk as epistemic uncertainty and idiosyncratic risk as aleatory uncertainty. The study shows how methods like Monte Carlo dropout can measure epistemic risk and provide insights into performance limits, model degradation, and refinements of foundation-model-based strategies in financial markets. The results emphasize the potential and challenges of using large pretrained models for trading strategies. 

<br /><br />Summary: <div>
arXiv:2510.17165v1 Announce Type: new 
Abstract: Foundation models - already transformative in domains such as natural language processing - are now starting to emerge for time-series tasks in finance. While these pretrained architectures promise versatile predictive signals, little is known about how they shape the risk profiles of the trading strategies built atop them, leaving practitioners reluctant to commit serious capital. In this paper, we propose an extension to the Capital Asset Pricing Model (CAPM) that disentangles the systematic risk introduced by a shared foundation model - potentially capable of generating alpha if the underlying model is genuinely predictive - from the idiosyncratic risk attributable to custom fine-tuning, which typically accrues no systematic premium. To enable a practical estimation of these separate risks, we align this decomposition with the concepts of uncertainty disentanglement, casting systematic risk as epistemic uncertainty (rooted in the pretrained model) and idiosyncratic risk as aleatory uncertainty (introduced during custom adaptations). Under the Aleatory Collapse Assumption, we illustrate how Monte Carlo dropout - among other methods in the uncertainty-quantization toolkit - can directly measure the epistemic risk, thereby mapping trading strategies to a more transparent risk-return plane. Our experiments show that isolating these distinct risk factors yields deeper insights into the performance limits of foundation-model-based strategies, their model degradation over time, and potential avenues for targeted refinements. Taken together, our results highlight both the promise and the pitfalls of deploying large pretrained models in competitive financial markets.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StrengthLawExtractor: A Fiji plugin for 3D morphological feature extraction from X-ray micro-CT data</title>
<link>https://arxiv.org/abs/2510.17279</link>
<guid>https://arxiv.org/abs/2510.17279</guid>
<content:encoded><![CDATA[
<div> Keywords: non-destructive methods, micro-computed tomography, morphometric measures, porous materials, predictive modeling  
Summary:  
- Non-destructive methods are crucial for understanding the relationship between porous material microstructure and mechanical behavior.  
- Micro-CT technology allows for high-resolution 3D reconstructions of porous materials, aiding in the quantification of geometric descriptors.  
- Recent advances in morphometric theory suggest that specific measures such as porosity, surface area, mean curvature, and Euler characteristic are essential for predicting strength.  
- A Fiji plugin has been developed to automatically extract these morphometric measures from micro-CT datasets, making analysis reproducible and user-friendly.  
- The extracted descriptors can be used in constitutive models and machine learning algorithms to predict stress-strain behavior and design microstructures.  
<br /><br />Summary: <div>
arXiv:2510.17279v1 Announce Type: new 
Abstract: Non-destructive methods are essential for linking the microstructural geometry of porous materials to their mechanical behavior, as destructive testing is often infeasible due to limited material availability or irreproducible conditions. Micro-computed tomography (micro-CT) provides high resolution three dimensional reconstructions of porous microstructures, enabling direct quantification of geometric descriptors. Recent advances in morphometric theory have demonstrated that four independent morphometric measures (porosity, surface area, mean curvature, and Euler characteristic) are required to capture the relationship between microstructure and strength, thereby forming the basis of generalized strength laws. To facilitate practical application of this framework, a Fiji plugin was developed to extract the four morphometric measures (porosity, surface area, mean curvature, Euler characteristic) from micro-CT datasets automatically. The plugin integrates within the Fiji platform to provide reproducible, accessible, and user friendly analysis. The application of the tool demonstrates that the extracted descriptors can be readily incorporated into constitutive models and machine learning workflows, enabling the forward prediction of stress-strain behavior as well as the inverse design of microstructures. This approach supports non-destructive evaluation, accelerates materials selection, and advances the integration of imaging with predictive modeling in porous media research.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling complexity in system safety: generalizing the D2T2 methodology</title>
<link>https://arxiv.org/abs/2510.17351</link>
<guid>https://arxiv.org/abs/2510.17351</guid>
<content:encoded><![CDATA[
<div> Fault Tree, Event Tree, system safety analysis, complex systems, Dynamic and Dependent Tree Theory 

Summary:
The article discusses the limitations of traditional Fault Tree and Event Tree analysis in capturing the dynamic behavior of complex systems. These techniques struggle to depict the dense network of dependencies within systems, leading to the use of conservative assumptions to compensate for oversimplifications. The proposed solution involves integrating dependency modeling within the conventional Fault and Event Tree framework through the Dynamic and Dependent Tree Theory. This approach combines the combinatorial nature of failure analysis with flexible modeling solutions, allowing for the comprehensive inclusion of any dependency type. By incorporating complex system features while maintaining the effectiveness of traditional safety modeling, this method offers a more versatile and accurate approach to system safety analysis. <div>
arXiv:2510.17351v1 Announce Type: new 
Abstract: Although Fault Tree and Event Tree analysis are still today the standard approach to system safety analysis for many engineering sectors, these techniques lack the capabilities of fully capturing the realistic, dynamic behaviour of complex systems, which results in a dense network of dependencies at any level, i.e. between components, trains of components or subsystems. While these limitations are well recognised across both industry and academia, the shortage of alternative tools able to tackle such challenges while retaining the computational feasibility of the analysis keeps fuelling the long-lived success of Fault Tree and Event Tree modelling. Analysts and regulators often rely on the use of conservative assumptions to mitigate the effect of oversimplifications associated with the use of such techniques. However, this results in the analysis output to be characterised by an unknown level of conservatism, with potential consequences on market competitiveness (i.e., over-conservatism) or safety (i.e., under-conservatism). This study proposes a generalization of the Dynamic and Dependent Tree Theory, which offers theoretical tools for the systematic integration of dependency modelling within the traditional Fault and Event Tree analysis framework. This is achieved by marrying the traditional combinatorial nature of failure analysis, formalised by the Fault and Event Tree language, with more flexible modelling solutions, which provide the flexibility required to capture complex system features. The main advantage of the proposed approach in comparison to existent solutions is the ability to take into account, under the same modelling framework, any type of dependency regardless of its nature and location, while retaining the familiarity and effectiveness of traditional safety modelling.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Volumetric Non-Invasive Cardiac Mapping for Accessible Global Arrhythmia Characterization</title>
<link>https://arxiv.org/abs/2510.17539</link>
<guid>https://arxiv.org/abs/2510.17539</guid>
<content:encoded><![CDATA[
<div> Cardiac arrhythmias; Imageless volumetric ECGI; Non-invasive mapping; 3D activation mapping; Arrhythmia localization<br />
<br />
Summary:<br />
Cardiac arrhythmias pose significant health risks, but traditional mapping techniques are limited to surface reconstructions. A new approach, imageless volumetric ECGI, offers non-invasive 3D activation mapping using Green's functions. By solving an inverse source problem, this method enhances localization accuracy, reducing geodesic errors by 59.3% compared to surface-only methods. Evaluation on patient cases, including ventricular tachycardia and Wolff-Parkinson-White syndrome, demonstrates precise activation pattern alignment with clinical diagnoses. This accessible technique shows promise in preprocedural planning, ablation target guidance, and optimizing cardiac resynchronization therapy. <div>
arXiv:2510.17539v1 Announce Type: new 
Abstract: Cardiac arrhythmias are a major cause of morbidity and mortality increasing the risk of stroke, heart failure, and sudden cardiac death. Imageless electrocardiographic imaging (ECGI) provides a non invasive alternative to electrical mapping from body surface potentials, but conventional ECGI is confined to epicardial reconstructions and can miss arrhythmias originating in deeper myocardium. We address this by reconstructing three dimensional cardiac activity with a volumetric formulation that solves an inverse source problem via Green's functions, enabling full volume activation mapping and improved localization in anatomically complex regions. We evaluate the approach on simulated premature ventricular beats and on four challenging patient cases, a right ventricular outflow tract premature ventricular contraction, a left bundle branch block, a ventricular tachycardia, and Wolff Parkinson White, and additionally assess performance on an open source myocardial infarction dataset. Results show that volumetric ECGI recovers 3D activation and sharpens arrhythmia origin localization, achieving a 59.3% reduction in geodesic error between estimated and simulated origins relative to surface only methods; in patient cases, activation patterns align with clinical diagnoses. Overall, imageless volumetric ECGI offers accessible, non invasive 3D activation mapping that overcomes a core limitation of surface restricted techniques and may improve preprocedural planning, ablation target guidance, and selection or optimization of cardiac resynchronization therapy.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter</title>
<link>https://arxiv.org/abs/2510.15954</link>
<guid>https://arxiv.org/abs/2510.15954</guid>
<content:encoded><![CDATA[
<div> diffusion model, Ensemble Score Filter, data assimilation, wildfire spread predictions, numerical investigations
<br />
Summary: 
This paper explores the use of the Ensemble Score Filter (EnSF) algorithm for data assimilation in real-time active wildfire spread predictions. By integrating observations like remote-sensing data with fire predictions from numerical models, EnSF enhances forecasting accuracy. The EnSF algorithm, based on a score-based generative diffusion model, offers superior accuracy for high-dimensional nonlinear filtering problems, making it well-suited for wildfire spread models. The study provides technical details and demonstrates through numerical investigations that EnSF outperforms other methods in terms of accuracy, stability, and computational efficiency. The robustness and practicality of EnSF make it a valuable tool for wildfire data assimilation. The public availability of the code used in this study encourages further research and application of EnSF in wildfire management. 
<br /><br />Summary: <div>
arXiv:2510.15954v1 Announce Type: cross 
Abstract: As wildfires become increasingly destructive and expensive to control, effective management of active wildfires requires accurate, real-time fire spread predictions. To enhance the forecasting accuracy of active fires, data assimilation plays a vital role by integrating observations (such as remote-sensing data) and fire predictions generated from numerical models. This paper provides a comprehensive investigation on the application of a recently proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter (EnSF) -- to the data assimilation problem for real-time active wildfire spread predictions. Leveraging a score-based generative diffusion model, EnSF has been shown to have superior accuracy for high-dimensional nonlinear filtering problems, making it an ideal candidate for the filtering problems of wildfire spread models. Technical details are provided, and our numerical investigations demonstrate that EnSF provides superior accuracy, stability, and computational efficiency, establishing it as a robust and practical method for wildfire data assimilation. Our code has been made publicly available.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia</title>
<link>https://arxiv.org/abs/2510.16066</link>
<guid>https://arxiv.org/abs/2510.16066</guid>
<content:encoded><![CDATA[
<div> Keywords: MSMEs, financing, bank statement data, credit assessment, financial inclusion<br />
<br />
Summary: 
This study addresses the challenge of financing faced by Micro, Small, and Medium Enterprises (MSMEs) in Malaysia through the use of bank statement data for credit assessment. The research proposes a cash flow-based underwriting pipeline that leverages machine learning credit scoring on bank transaction data. A novel dataset of 611 loan applicants from a Malaysian lending institution is introduced for model development and evaluation. The study demonstrates that incorporating bank transaction-derived features improves credit scoring models for new-to-lending MSMEs. The results highlight the potential of using alternative data sources for credit assessment to promote financial inclusion in emerging markets. The anonymised bank transaction dataset will be released to support further research on MSMEs' financial inclusion in Malaysia's economy. <div>
arXiv:2510.16066v1 Announce Type: cross 
Abstract: Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established or young businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. Firstly, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end to end data extraction and machine learning credit scoring. Secondly, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Thirdly, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Lastly, we intend to release the anonymised bank transaction dataset to facilitate further research on MSMEs financial inclusion within Malaysia's emerging economy.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A hierarchical Bayesian approach for population-based structural health monitoring in ship hull structures</title>
<link>https://arxiv.org/abs/2510.16316</link>
<guid>https://arxiv.org/abs/2510.16316</guid>
<content:encoded><![CDATA[
<div> Hierarchical Bayesian model, Structural health monitoring, Population-based SHM, Deflection amplitudes, Markov Chain Monte Carlo<br />
<br />
Summary: 
This work explores utilizing a hierarchical Bayesian model for assessing structural health monitoring data in a population of similar structures. The model aims to detect excessive initial deflections in plate elements, which can lead to unexpected events if not monitored. Using Finite Element modeling to generate strain response data, Bayesian inference with Markov Chain Monte Carlo is applied, with a surrogate model used to calculate the likelihood function. The hierarchical approach is compared to an independent model for a plate component with limited data, showing that the hierarchical model offers more robust results in uncertainty estimation under data sparsity conditions. This enhancement is crucial for decision-making tasks in structural health monitoring strategies. <div>
arXiv:2510.16316v1 Announce Type: cross 
Abstract: Structural health monitoring (SHM) strategies involve the processing of structural response data to indirectly assess an asset's condition. These strategies can be enhanced for a group of structures, especially when they are similar, since mutual underlying physics are expected to exist. The concept behind population-based SHM exploits the sharing of data among individuals, so that data-rich members can support data-scarce ones. One approach to population-level modeling is the hierarchical Bayesian method, where the model is structured hierarchically in terms of its parameters, and correlation among learning tasks is enabled by conditioning on shared latent variables.
  This work investigates the application of a hierarchical Bayesian model to infer expected distributions of deflection amplitudes at both the population and domain levels, with the aim of detecting excessive initial deflections in a population of plate elements. Although these damages are typically localized, they can trigger unexpected events, if not properly monitored. The work is conducted in a numerical setting using a Finite Element model to generate strain response data, which serve as the monitoring data. Bayesian inference was conducted using Markov Chain Monte Carlo (MCMC), with a surrogate model employed to calculate the likelihood function. The hierarchical approach was compared to an independent model for a plate component with few data. The results revealed that, under data sparsity conditions, the hierarchical model can offer more robust results in terms of uncertainty, which is essential for decision-making tasks.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2510.16658</link>
<guid>https://arxiv.org/abs/2510.16658</guid>
<content:encoded><![CDATA[
<div> neuroscience, artificial intelligence, large-scale models, computational challenges, clinical applications
Summary:
The paper discusses the transformative effects of large-scale artificial intelligence (AI) models on various neuroscience domains, including neuroimaging, brain-computer interfaces, molecular neuroscience, clinical assistance, and disease-specific applications. These models revolutionize traditional computational methods by enabling end-to-end learning from raw brain signals and neural data. They address key computational challenges in neuroscience, such as integrating multimodal neural data, interpreting spatiotemporal patterns, and developing translational frameworks for clinical deployment. The interaction between neuroscience and AI is becoming more reciprocal, with biologically inspired architectural constraints enhancing interpretability and efficiency of models. The review emphasizes the importance of rigorous evaluation frameworks, domain knowledge integration, and ethical guidelines for clinical use. The paper also provides a systematic listing of critical neuroscience datasets used to develop and validate large-scale AI models across diverse research applications. <div>
arXiv:2510.16658v1 Announce Type: cross 
Abstract: The advent of large-scale artificial intelligence (AI) models has a transformative effect on neuroscience research, which represents a paradigm shift from the traditional computational methods through the facilitation of end-to-end learning from raw brain signals and neural data. In this paper, we explore the transformative effects of large-scale AI models on five major neuroscience domains: neuroimaging and data processing, brain-computer interfaces and neural decoding, molecular neuroscience and genomic modeling, clinical assistance and translational frameworks, and disease-specific applications across neurological and psychiatric disorders. These models are demonstrated to address major computational neuroscience challenges, including multimodal neural data integration, spatiotemporal pattern interpretation, and the derivation of translational frameworks for clinical deployment. Moreover, the interaction between neuroscience and AI has become increasingly reciprocal, as biologically informed architectural constraints are now incorporated to develop more interpretable and computationally efficient models. This review highlights both the notable promise of such technologies and key implementation considerations, with particular emphasis on rigorous evaluation frameworks, effective domain knowledge integration, and comprehensive ethical guidelines for clinical use. Finally, a systematic listing of critical neuroscience datasets used to derive and validate large-scale AI models across diverse research applications is provided.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSight: Towards Real-World Financial Deep Research</title>
<link>https://arxiv.org/abs/2510.16844</link>
<guid>https://arxiv.org/abs/2510.16844</guid>
<content:encoded><![CDATA[
<div> agent, financial reports, AI systems, visualization, analysis

Summary: 
FinSight is a multi-agent framework designed to automate the generation of professional financial reports. The Code Agent with Variable Memory (CAVM) architecture integrates data, tools, and agents into a flexible space for data collection, analysis, and report generation through executable code. The Iterative Vision-Enhanced Mechanism refines visual outputs into high-quality financial charts. A two-stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports. Experiments demonstrate that FinSight outperforms existing deep research systems in terms of factual accuracy, analytical depth, and presentation quality. The framework shows promising results in generating reports that approach human-expert quality. <div>
arXiv:2510.16844v1 Announce Type: cross 
Abstract: Generating professional financial reports is a labor-intensive and intellectually demanding process that current AI systems struggle to fully automate. To address this challenge, we introduce FinSight (Financial InSight), a novel multi agent framework for producing high-quality, multimodal financial reports. The foundation of FinSight is the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, enabling flexible data collection, analysis and report generation through executable code. To ensure professional-grade visualization, we propose an Iterative Vision-Enhanced Mechanism that progressively refines raw visual outputs into polished financial charts. Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports, ensuring both analytical depth and structural consistency. Experiments on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing</title>
<link>https://arxiv.org/abs/2510.17088</link>
<guid>https://arxiv.org/abs/2510.17088</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial anomalies, adaptive graph learning, expert networks, interpretability, temporal evolution detection <br />
Summary: 
This article introduces a novel framework for detecting financial anomalies by addressing three main challenges. These challenges include static graph structures that cannot adapt to market correlations, uniform detection mechanisms missing type-specific signatures, and lack of actionable guidance on anomaly mechanisms. The proposed framework utilizes adaptive graph learning with specialized expert networks to provide interpretability. Multi-scale temporal dependencies are captured through BiLSTM with self-attention, combining temporal and spatial information with cross-modal attention. Dynamic graphs are learned through neural multi-source interpolation, balancing dynamics with structural priors via stress-modulated fusion. Anomalies are routed to mechanism-specific experts for detection. The framework achieved a high detection rate of major events with lead time and outperformed baseline methods. A case study demonstrated the tracking of anomaly evolution, showcasing automatic temporal mechanism identification without labeled supervision. <div>
arXiv:2510.17088v1 Announce Type: cross 
Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity freezes, contagion cascades, regime shifts), but existing detectors treat all anomalies uniformly, producing scalar scores without revealing which mechanism is failing, where risks concentrate, or how to intervene. This opacity prevents targeted regulatory responses. Three unsolved challenges persist: (1) static graph structures cannot adapt when market correlations shift during regime changes; (2) uniform detection mechanisms miss type-specific signatures across multiple temporal scales while failing to integrate individual behaviors with network contagion; (3) black-box outputs provide no actionable guidance on anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks that provide built-in interpretability. Our framework captures multi-scale temporal dependencies through BiLSTM with self-attention, fuses temporal and spatial information via cross-modal attention, learns dynamic graphs through neural multi-source interpolation, adaptively balances learned dynamics with structural priors via stress-modulated fusion, routes anomalies to four mechanism-specific experts, and produces dual-level interpretable attributions. Critically, interpretability is embedded architecturally rather than applied post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley Bank case study demonstrates anomaly evolution tracking: Price-Shock expert weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48 (66% above baseline) one week later, revealing automatic temporal mechanism identification without labeled supervision.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation</title>
<link>https://arxiv.org/abs/2510.17146</link>
<guid>https://arxiv.org/abs/2510.17146</guid>
<content:encoded><![CDATA[
<div> Keywords: HVAC systems, anomaly detection, Large Language Models, Physics-Informed LLM, smart building systems

Summary: 
PILLM is a novel framework for anomaly detection in HVAC systems that combines the benefits of Large Language Models (LLMs) with physics-based constraints. By incorporating thermodynamic and control-theoretic principles, PILLM generates adaptive and physically grounded rules for anomaly detection. The framework operates within an evolutionary loop to automatically refine detection rules, leading to improved performance on the Building Fault Detection dataset. PILLM bridges the gap between interpretability and predictive power, offering transparent and actionable diagnostic rules for smart building systems. This approach represents a significant advancement in the field of AI for HVAC systems, enabling efficient energy use and reduced emissions while maintaining reliability and trustworthiness. <div>
arXiv:2510.17146v1 Announce Type: cross 
Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a substantial share of global building energy use, making reliable anomaly detection essential for improving efficiency and reducing emissions. Classical rule-based approaches offer explainability but lack adaptability, while deep learning methods provide predictive power at the cost of transparency, efficiency, and physical plausibility. Recent attempts to use Large Language Models (LLMs) for anomaly detection improve interpretability but largely ignore the physical principles that govern HVAC operations. We present PILLM, a Physics-Informed LLM framework that operates within an evolutionary loop to automatically generate, evaluate, and refine anomaly detection rules. Our approach introduces physics-informed reflection and crossover operators that embed thermodynamic and control-theoretic constraints, enabling rules that are both adaptive and physically grounded. Experiments on the public Building Fault Detection dataset show that PILLM achieves state-of-the-art performance while producing diagnostic rules that are interpretable and actionable, advancing trustworthy and deployable AI for smart building systems.
]]></content:encoded>
<pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Temporal Disturbance Estimations For Magnetic Anomaly Navigation and Mapping</title>
<link>https://arxiv.org/abs/2510.15113</link>
<guid>https://arxiv.org/abs/2510.15113</guid>
<content:encoded><![CDATA[
<div> Reference Station Model, Magnetic Anomaly Mapping, Long-range Aeromagnetic Surveys, Regression Models, Space Weather<br />
Summary:<br />
The article introduces the Extended Reference Station Model (ERSM) to address the challenges faced by slow-moving vehicles relying on crustal magnetic anomaly navigation. This model aims to eliminate the need for fixed ground stations within 100 km by utilizing an extended reference ground magnetometer station. By applying longitudinal correction and regression models, such as linear regression, k-nearest neighbors, and neural-network regression, ERSM successfully estimates the local temporal disturbance field with performance benefits. The results demonstrate low root mean square errors and median performance below 5nT, particularly with the kNN and neural-net models for longer distances. Factors such as space-weather events, water-body separation, and proximity to polar regions are also considered in determining the model's performance based on ERS selection. <div>
arXiv:2510.15113v1 Announce Type: new 
Abstract: Slow-moving vehicles relying on crustal magnetic anomaly navigation (MagNav) or vehicles revisiting the same location in a short time - such as those used for surveys in magnetic anomaly mapping - require fixed ground stations within 100 km of the vehicle's trajectory to measure and remove the geomagnetic disturbance field from magnetic readings. This approach is impractical due to the limited network of fixed-ground magnetometer stations, making long-range (several hundred kilometers long) aeromagnetic surveys for anomaly map-making infeasible. To address these challenges, we developed the Extended Reference Station Model (ERSM). ERSM applies a longitudinal correction and regression model to an extended reference ground magnetometer station (ERS) to produce an estimate of the local temporal disturbance field. ERSM is regression model-agnostic, so we implemented a linear regression, a k-nearest neighbors (kNN) regression, and a neural-network regression model to assess performance benefits. Our results show typical performance below 10nT root mean square error and median performance below 5nT for typical use with the kNN and neural-net model for farther distances and below 5nT performance using the linear regression model on stations with proximity. We also consider how space-weather events, water-body separation, and proximity to polar regions affect the model performance based on ERS selection.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Black Scholes for Prediction Markets: A Unified Kernel and Market Maker's Handbook</title>
<link>https://arxiv.org/abs/2510.15205</link>
<guid>https://arxiv.org/abs/2510.15205</guid>
<content:encoded><![CDATA[
<div> prediction markets, Polymarket, logit jump-diffusion, risk factors, calibration pipeline <br />
Summary:<br />
The article introduces a new stochastic kernel for prediction markets, specifically focusing on platforms like Polymarket. The proposed logit jump-diffusion model aims to provide a unified framework for market makers to manage belief volatility, jump, and cross-event risks. By treating traded probabilities as Q-martingales and incorporating risk factors such as belief volatility and jump intensity, the model enables quoting and hedging strategies. A calibration pipeline is developed to filter noise, separate diffusion from jumps, and establish a stable belief-volatility surface. Additionally, the model supports the creation of derivative instruments analogous to options, allowing for the transfer of belief risk. Experimental results show improved short-horizon belief-variance forecast accuracy compared to existing approaches, demonstrating the model's utility in enhancing market efficiency and risk management in prediction markets like Polymarket. <br /> <div>
arXiv:2510.15205v1 Announce Type: new 
Abstract: Prediction markets, such as Polymarket, aggregate dispersed information into tradable probabilities, but they still lack a unifying stochastic kernel comparable to the one options gained from Black-Scholes. As these markets scale with institutional participation, exchange integrations, and higher volumes around elections and macro prints, market makers face belief volatility, jump, and cross-event risks without standardized tools for quoting or hedging. We propose such a foundation: a logit jump-diffusion with risk-neutral drift that treats the traded probability p_t as a Q-martingale and exposes belief volatility, jump intensity, and dependence as quotable risk factors. On top, we build a calibration pipeline that filters microstructure noise, separates diffusion from jumps using expectation-maximization, enforces the risk-neutral drift, and yields a stable belief-volatility surface. We then define a coherent derivative layer (variance, correlation, corridor, and first-passage instruments) analogous to volatility and correlation products in option markets. In controlled experiments on synthetic risk-neutral paths and real event data, the model reduces short-horizon belief-variance forecast error relative to diffusion-only and probability-space baselines, supporting both causal calibration and economic interpretability. Conceptually, the logit jump-diffusion kernel supplies an implied-volatility analogue for prediction markets: a tractable, tradable language for quoting, hedging, and transferring belief risk across venues such as Polymarket.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Market Microstructure for Decentralized Prediction Markets (DePMs)</title>
<link>https://arxiv.org/abs/2510.15612</link>
<guid>https://arxiv.org/abs/2510.15612</guid>
<content:encoded><![CDATA[
<div> Keywords: DePMs, decentralized prediction markets, Polymarket, Truthcoin, Augur

Summary:
Decentralized prediction markets (DePMs) have evolved over the years, with modern platforms like Polymarket showcasing significant differences from earlier designs such as Truthcoin and Augur v1. The article reviews the history of DePMs, spanning back to 2011 and encompassing numerous proposals. A modular workflow consisting of seven stages is outlined, including underlying infrastructure, market topic, share structure and pricing, trading, market resolution, settlement, and archiving. Design variants for each module are discussed, with a focus on decentralization, expressiveness, and manipulation resistance trade-offs. The article also highlights open challenges for researchers in the DePM ecosystem. <div>
arXiv:2510.15612v1 Announce Type: new 
Abstract: Decentralized prediction markets (DePMs) allow open participation in event-based wagering without fully relying on centralized intermediaries. We review the history of DePMs which date back to 2011 and includes hundreds of proposals. Perhaps surprising, modern DePMs like Polymarket deviate materially from earlier designs like Truthcoin and Augur v1. We use our review to present a modular workflow comprising seven stages: underlying infrastructure, market topic, share structure and pricing, trading, market resolution, settlement, and archiving. For each module, we enumerate the design variants, analyzing trade-offs around decentralization, expressiveness, and manipulation resistance. We also identify open problems for researchers interested in this ecosystem.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepAries: Adaptive Rebalancing Interval Selection for Enhanced Portfolio Selection</title>
<link>https://arxiv.org/abs/2510.14985</link>
<guid>https://arxiv.org/abs/2510.14985</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, portfolio management, deep learning, financial markets, transaction costs
Summary:<br /><br />DeepAries is introduced as a novel deep reinforcement learning framework for dynamic portfolio management that optimizes both timing and allocation decisions. It addresses the limitation of fixed rebalancing intervals by adaptively selecting optimal intervals and portfolio weights based on market conditions. The framework combines a Transformer-based state encoder with Proximal Policy Optimization to generate simultaneous discrete and continuous actions for rebalancing and asset allocations. Extensive experiments on real-world financial markets show that DeepAries outperforms traditional strategies in terms of risk-adjusted returns, transaction costs, and drawdowns. A live demo and open-source code and dataset are provided, demonstrating the framework's interpretability and effectiveness in producing adaptive portfolio management decisions aligned with changing market conditions. Overall, DeepAries offers a new approach to portfolio management by integrating timing and allocation decisions in a unified framework. <div>
arXiv:2510.14985v1 Announce Type: cross 
Abstract: We propose DeepAries , a novel deep reinforcement learning framework for dynamic portfolio management that jointly optimizes the timing and allocation of rebalancing decisions. Unlike prior reinforcement learning methods that employ fixed rebalancing intervals regardless of market conditions, DeepAries adaptively selects optimal rebalancing intervals along with portfolio weights to reduce unnecessary transaction costs and maximize risk-adjusted returns. Our framework integrates a Transformer-based state encoder, which effectively captures complex long-term market dependencies, with Proximal Policy Optimization (PPO) to generate simultaneous discrete (rebalancing intervals) and continuous (asset allocations) actions. Extensive experiments on multiple real-world financial markets demonstrate that DeepAries significantly outperforms traditional fixed-frequency and full-rebalancing strategies in terms of risk-adjusted returns, transaction costs, and drawdowns. Additionally, we provide a live demo of DeepAries at https://deep-aries.github.io/, along with the source code and dataset at https://github.com/dmis-lab/DeepAries, illustrating DeepAries' capability to produce interpretable rebalancing and allocation decisions aligned with shifting market regimes. Overall, DeepAries introduces an innovative paradigm for adaptive and practical portfolio management by integrating both timing and allocation into a unified decision-making process.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning reveals key predictors of thermal conductivity in covalent organic frameworks</title>
<link>https://arxiv.org/abs/2409.06457</link>
<guid>https://arxiv.org/abs/2409.06457</guid>
<content:encoded><![CDATA[
<div> machine learning, thermal conductivity, covalent organic frameworks, dangling branches, molecular dynamics simulations

Summary:
The study explores the relationship between the structure and thermal conductivity of covalent organic frameworks (COFs), a class of nanoporous polymeric materials. Traditional features do not reliably predict thermal conductivity in COFs, prompting the development of an attention-based machine learning model that accurately predicts thermal conductivities even for structures beyond the training set. The model's analysis highlights dangling molecular branches as a significant predictor, leading to the definition of the dangling mass ratio (DMR) as a descriptor for thermal conductivity prediction. Feature importance assessments confirm the importance of DMR in predicting thermal conductivity. Molecular dynamics simulations support the observation that COFs with dangling functional groups have lower thermal transfer capabilities, attributed to significant mismatches in vibrational density of states due to the presence of dangling branches.<br /><br />Summary: <div>
arXiv:2409.06457v3 Announce Type: replace 
Abstract: The thermal conductivity of covalent organic frameworks (COFs), an emerging class of nanoporous polymeric materials, is crucial for many applications, yet the link between their structure and thermal properties remains poorly understood. Analysis of a dataset containing over 2,400 COFs reveals that conventional features such as density, pore size, void fraction, and surface area do not reliably predict thermal conductivity. To address this, an attention-based machine learning model was trained, accurately predicting thermal conductivities even for structures outside the training set. The attention mechanism was then utilized to investigate the model's success. The analysis identified dangling molecular branches as a key predictor of thermal conductivity, leading us to define the dangling mass ratio (DMR), a descriptor that quantifies the fraction of atomic mass in dangling branches relative to the total COF mass. Feature importance assessments on regression models confirm the significance of DMR in predicting thermal conductivity. These findings indicate that COFs with dangling functional groups exhibit lower thermal transfer capabilities. Molecular dynamics simulations support this observation, revealing significant mismatches in the vibrational density of states due to the presence of dangling branches.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Symmetry-Aware Efficient Simulation of Quantum Systems and Beyond</title>
<link>https://arxiv.org/abs/2303.11409</link>
<guid>https://arxiv.org/abs/2303.11409</guid>
<content:encoded><![CDATA[
<div> tensor networks, quantum simulation, symmetry, machine learning, scalable approaches <br />
<br />
Summary: Physics-informed tensor networks are key in efficiently simulating complex quantum systems. By incorporating symmetry, such as $U(1)$-symmetric tensor networks, computational costs are reduced, enabling larger simulations. These networks are accelerated on GPUs and scaled to supercomputers, making them essential for quantum simulation, computation, and machine learning. Physics-informed design extends beyond symmetry, including hybrid tensor networks and parallel sequential circuits, to enhance efficiency through different principles. This Perspective emphasizes the importance of physics-informed tensor networks, incorporating both symmetry and beyond-symmetry insights, as unifying strategies for scalable approaches in quantum simulation, computation, and machine learning. <br /><br /> <div>
arXiv:2303.11409v2 Announce Type: replace-cross 
Abstract: The efficient simulation of complex quantum systems remains a central challenge due to the exponential growth of Hilbert space with system size. Tensor network methods have long been established as powerful approximation schemes, and their efficiency can be further enhanced by incorporating physics-informed priors. A prominent example is symmetry: recent progress on $U(1)$-symmetric tensor networks, accelerated on GPUs and scaled to supercomputers, shows how conserved charges induce block-sparse structures that reduce computational cost and enable larger simulations. The same principle extends to general symmetries, inspiring equivariant neural networks in machine learning and guiding symmetry-preserving ansatze in variational quantum algorithms. Beyond symmetry, physics-informed design also includes strategies such as hybrid tensor networks and parallel sequential circuits, which pursue efficiency from complementary principles. This Perspective argues that physics-informed tensor networks, grounded in both symmetry and beyond-symmetry insights, provide unifying strategies for scalable approaches in quantum simulation, computation, and machine learning.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing trust in automated reasoning</title>
<link>https://arxiv.org/abs/2309.12351</link>
<guid>https://arxiv.org/abs/2309.12351</guid>
<content:encoded><![CDATA[
<div> automated reasoning, machine learning, trust, reviewability, science<br />
<br />
Summary: 
The article discusses the importance of automated reasoning in scientific research and the shift towards formulating rules through machine learning techniques. It highlights the need for trust in these systems and the results they produce, an issue often overlooked by practitioners. The focus is on independent reviewing as a key factor in building trust in automated reasoning systems. The article identifies the characteristics that affect the reviewability of these systems and proposes a combination of technical and social measures to enhance their trustworthiness. By emphasizing the importance of transparency and accountability in automated reasoning, the article aims to bridge the gap between the philosophical discussions on trust in science and practical considerations in research. <div>
arXiv:2309.12351v2 Announce Type: replace-cross 
Abstract: Since its beginnings in the 1940s, automated reasoning by computers has become a tool of ever growing importance in scientific research. So far, the rules underlying automated reasoning have mainly been formulated by humans, in the form of program source code. Rules derived from large amounts of data, via machine learning techniques, are a complementary approach currently under intense development. The question of why we should trust these systems, and the results obtained with their help, has been discussed by philosophers of science but has so far received little attention by practitioners. The present work focuses on independent reviewing, an important source of trust in science, and identifies the characteristics of automated reasoning systems that affect their reviewability. It also discusses possible steps towards increasing reviewability and trustworthiness via a combination of technical and social measures.
]]></content:encoded>
<pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach</title>
<link>https://arxiv.org/abs/2510.13937</link>
<guid>https://arxiv.org/abs/2510.13937</guid>
<content:encoded><![CDATA[
<div> machine learning, rock classification, mineral composition, 1D-CNN, geological expertise

Summary:
A new study introduces a novel approach for automated rock classification based on mineral composition, addressing a significant challenge in geological applications. The study combines geological domain expertise with spectral analysis to enhance deep learning models. Evaluation of machine learning methods, including 1D-CNN and its uncertainty-aware variant, shows excellent performance in mineral classification. However, the classification of rock types from mineral assemblages poses challenges, especially for rocks with similar mineral compositions. Results vary across different lithologies, with limestone classification achieving optimal accuracy. The study underscores the complexity of automated geological classification systems and highlights the need for methodological advancements in material characterization and sorting technologies. <div>
arXiv:2510.13937v1 Announce Type: new 
Abstract: Automated rock classification from mineral composition presents a significant challenge in geological applications, with critical implications for material recycling, resource management, and industrial processing. While existing methods using One dimensional Convolutional Neural Network (1D-CNN) excel at mineral identification through Raman spectroscopy, the crucial step of determining rock types from mineral assemblages remains unsolved, particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance (98.37+-0.006% and 97.75+-0.010% respectively). The integrated system's evaluation on rock samples revealed variable performance across lithologies, with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading</title>
<link>https://arxiv.org/abs/2510.14264</link>
<guid>https://arxiv.org/abs/2510.14264</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model (LLM), automated trading, reinforcement learning (RL), transparency, interpretable reasoning

Summary:<br /><br />
The article introduces AlphaQuanter, a single-agent framework using reinforcement learning to learn a dynamic policy for automated trading. Unlike multi-agent frameworks, AlphaQuanter addresses inefficiency and inconsistency issues by enabling a single agent to orchestrate tools and acquire information proactively. The framework is built on a transparent, tool-augmented decision workflow, allowing for end-to-end optimization and coherent strategy learning from market feedback. Through extensive experiments, AlphaQuanter achieves state-of-the-art performance on financial metrics and its interpretable reasoning reveals sophisticated trading strategies. The framework provides novel insights for human traders and enhances transparency and audibility in the trading process. The code for data acquisition and agent training is publicly available on GitHub at: https://github.com/AlphaQuanter/AlphaQuanter. <div>
arXiv:2510.14264v1 Announce Type: new 
Abstract: While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Neural ODE Approach for Real Time Evaluation of AC Losses in 3D Superconducting Tapes</title>
<link>https://arxiv.org/abs/2510.14487</link>
<guid>https://arxiv.org/abs/2510.14487</guid>
<content:encoded><![CDATA[
<div> Keywords: High Temperature Superconductors, Reduced-order modeling, Integral Equation Method, Proper Orthogonal Decomposition, Neural Ordinary Differential Equation <br />
Summary: 
This article explores efficient modeling techniques for High Temperature Superconductors (HTS) to improve real-time quench monitoring. Conventional full-order electromagnetic simulations are costly due to strong nonlinearities. The study investigates reduced-order strategies for HTS systems using Integral Equation Method (IEM). The researchers apply Proper Orthogonal Decomposition (POD) and Discrete Empirical Interpolation Method (DEIM) to IEM-based HTS models. They also introduce a Structured Neural Ordinary Differential Equation (Neural ODE) approach to learn nonlinear dynamics in the reduced space. Benchmark results demonstrate that the Neural ODE outperforms POD-DEIM in efficiency and accuracy, making it a promising tool for real-time superconducting simulations. <br /><br />Summary: <div>
arXiv:2510.14487v1 Announce Type: new 
Abstract: Efficient modeling of High Temperature Superconductors (HTS) is crucial for real-time quench monitoring; however, full-order electromagnetic simulations remain prohibitively costly due to the strong nonlinearities. Conventional reduced-order methods, such as the Proper Orthogonal Decomposition (POD) and Discrete Empirical Interpolation Method (DEIM), alleviate this cost but are limited by intrusive implementation and by the need for many interpolation points. This work investigates reduced-order strategies for Integral Equation Method (IEM) of HTS systems. We present the first application of POD-DEIM to IEM-based HTS models, and introduce a Structured Neural Ordinary Differential Equation (Neural ODE) approach that learns nonlinear dynamics directly in the reduced space. Benchmark results show that the Neural ODE outperforms POD-DEIM in both efficiency and accuracy, highlighting its potential for real-time superconducting simulations.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics</title>
<link>https://arxiv.org/abs/2510.14944</link>
<guid>https://arxiv.org/abs/2510.14944</guid>
<content:encoded><![CDATA[
<div> benchmark, metabolomics, large language models, evaluation, AI systems

Summary:
MetaBench is introduced as the first benchmark for evaluating the capabilities of Large Language Models (LLMs) in the specialized scientific domain of metabolomics. The benchmark assesses five essential capabilities for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Results from evaluating 25 LLMs show that while models perform well on text generation tasks, they struggle with cross-database identifier grounding and long-tail metabolites with sparse annotations. The study highlights the challenges LLMs face in leveraging complex biochemical pathways, heterogeneous identifier systems, and fragmented databases in metabolomics research. MetaBench provides crucial infrastructure for the development and evaluation of AI systems in metabolomics, aiming to facilitate progress towards reliable computational tools for metabolomics research. 

<br /><br />Summary: <div>
arXiv:2510.14944v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.
]]></content:encoded>
<pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Program of Thoughts for Financial Reasoning: Leveraging Dynamic In-Context Examples and Generative Retrieval</title>
<link>https://arxiv.org/abs/2510.13157</link>
<guid>https://arxiv.org/abs/2510.13157</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, financial numerical reasoning, FINDER, state-of-the-art, performance improvement

Summary:
FINDER is a new two-step framework designed to enhance large language models' (LLMs) performance in financial numerical reasoning tasks. The first step involves using a generative retriever to extract relevant information from unstructured data, including text and tables. In the second step, context-aware Program of Thought prompting is applied with dynamic selection of in-context examples. This approach significantly improves performance on financial numerical reasoning datasets like FinQA and ConvFinQA, surpassing previous benchmarks with execution accuracy improvements of 5.98% and 4.05%, respectively. FINDER's success demonstrates the effectiveness of combining information extraction with context-aware prompting techniques in enhancing LLMs' capabilities in dealing with complex numerical reasoning tasks. <br /><br />Summary: FINDER, a two-step framework, utilizes a generative retriever to extract relevant facts and context-aware prompting to improve LLMs' performance in financial numerical reasoning, achieving state-of-the-art results on FinQA and ConvFinQA datasets. <div>
arXiv:2510.13157v1 Announce Type: new 
Abstract: Despite continuous advancements in the capabilities of large language models (LLMs), numerical reasoning remains a challenging area. Techniques like chain-of-thought prompting, tree-of-thought prompting, and program-of-thought prompting guide LLMs through intermediate reasoning steps. Although in-context learning with few-shot prompting has improved performance, LLMs still lag behind state-of-the-art models on financial numerical reasoning datasets such as FinQA and ConvFinQA. In this work, we introduce FINDER, a novel two-step framework, to enhance LLMs' capabilities in financial numerical reasoning. The first step utilizes a generative retriever to extract relevant facts from unstructured data, including both text and tables. This is followed by context-aware Program of Thought prompting with dynamic selection of in-context examples. Our model FINDER achieves a new state-of-the-art performance on both the FinQA and ConvFinQA datasets, surpassing previous benchmarks with execution accuracy improvements of 5.98% and 4.05%, respectively.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Constitutive Model Discovery from Sparse and Noisy Data</title>
<link>https://arxiv.org/abs/2510.13559</link>
<guid>https://arxiv.org/abs/2510.13559</guid>
<content:encoded><![CDATA[
<div> Framework, Virtual Fields Method, statFEM, unsupervised constitutive model discovery, isotropic hyperelastic materials <br />
Summary: <br />
This article introduces a new framework, statFEM--EUCLID, that combines the statistical finite element method (statFEM) with the EUCLID approach for unsupervised constitutive model discovery. The aim is to address issues of measurement noise and data sparsity that affect VFM-based approaches. By integrating statFEM with EUCLID, the framework is able to reconstruct displacement fields while enforcing consistency with equilibrium and constitutive laws. The study focuses on isotropic hyperelastic materials and shows that the integration reduces sensitivity to noise and data sparsity. The results demonstrate that statFEM--EUCLID provides a more robust approach to constitutive model discovery in the presence of uncertainties, offering both reliable field reconstruction and interpretable constitutive models. <div>
arXiv:2510.13559v1 Announce Type: new 
Abstract: Recently, unsupervised constitutive model discovery has gained attention through frameworks based on the Virtual Fields Method (VFM), most prominently the EUCLID approach. However, the performance of VFM-based approaches, including EUCLID, is affected by measurement noise and data sparsity, which are unavoidable in practice. The statistical finite element method (statFEM) offers a complementary perspective by providing a Bayesian framework for assimilating noisy and sparse measurements to reconstruct the full-field displacement response, together with quantified uncertainty. While statFEM recovers displacement fields under uncertainty, it does not strictly enforce consistency with constitutive relations or aim to yield interpretable constitutive models. In this work, we couple statFEM with unsupervised constitutive model discovery in the EUCLID framework, yielding statFEM--EUCLID. The framework is demonstrated for isotropic hyperelastic materials. The results show that this integration reduces sensitivity to noise and data sparsity, while ensuring that the reconstructed fields remain consistent with both equilibrium and constitutive laws.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs</title>
<link>https://arxiv.org/abs/2510.12839</link>
<guid>https://arxiv.org/abs/2510.12839</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Factuality Evaluation, Claim Extraction, Evidence Collection, Verification <br />
Summary: <br />
The article introduces a new evaluation framework called FastFact to assess the factuality of long-form generations from Large Language Models (LLMs). It addresses the inefficiency and ineffectiveness of existing methods by implementing chunk-level claim extraction with pre-verification and document-level evidence collection. FastFact significantly reduces the cost of web searching and verification while ensuring reliability, making it the most efficient and accurate among current approaches. The framework collects evidence from crawled webpages and selectively retrieves it during verification, overcoming the insufficiency of evidence found in previous pipelines. Extensive experiments on a manually annotated benchmark showcase the effectiveness and efficiency of FastFact in evaluating the factuality of LLM-generated text. The code and benchmark data for FastFact are available on GitHub for further exploration and usage. <br /> <div>
arXiv:2510.12839v1 Announce Type: cross 
Abstract: Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to accuracy issues and costly human assessment. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to complex pipeline components unsuitable for long LLM outputs, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence collection of one-line snippets.
  To address these limitations, we propose \name, a fast and strong evaluation framework that achieves the highest alignment with human evaluation and efficiency among existing baselines. \name first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the cost of web searching and inference calling while ensuring reliability. For searching and verification, it collects document-level evidence from crawled webpages and selectively retrieves it during verification, addressing the evidence insufficiency problem in previous pipelines.
  Extensive experiments based on an aggregated and manually annotated benchmark demonstrate the reliability of \name in both efficiently and effectively evaluating the factuality of long-form LLM generations. Code and benchmark data is available at https://github.com/Yingjia-Wan/FastFact.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the alignment problem in transportation policy making: an LLM approach</title>
<link>https://arxiv.org/abs/2510.13139</link>
<guid>https://arxiv.org/abs/2510.13139</guid>
<content:encoded><![CDATA[
<div> model-driven decision tools, transportation planning, large language models, multi-agent simulation, collective preferences <br />
Summary: <br />
The article explores using large language models (LLMs) to address the misalignment between traveler preferences and transportation policies. By creating a multi-agent simulation with LLMs representing residents in a city, the study analyzes how LLMs can inform transit policy decisions. Results show that LLM agents can approximate collective preferences and adapt to local contexts, but also exhibit behavioral biases and slight deviations from optimization-based benchmarks. This highlights both the potential and limitations of LLMs in solving the alignment problem in transportation planning. <div>
arXiv:2510.13139v1 Announce Type: cross 
Abstract: A key challenge in transportation planning is that the collective preferences of heterogeneous travelers often diverge from the policies produced by model-driven decision tools. This misalignment frequently results in implementation delays or failures. Here, we investigate whether large language models (LLMs), noted for their capabilities in reasoning and simulating human decision-making, can help inform and address this alignment problem. We develop a multi-agent simulation in which LLMs, acting as agents representing residents from different communities in a city, participate in a referendum on a set of transit policy proposals. Using chain-of-thought reasoning, LLM agents provide ranked-choice or approval-based preferences, which are aggregated using instant-runoff voting (IRV) to model democratic consensus. We implement this simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago and Houston. Our findings suggest that LLM agents are capable of approximating plausible collective preferences and responding to local context, while also displaying model-specific behavioral biases and modest divergences from optimization-based benchmarks. These capabilities underscore both the promise and limitations of LLMs as tools for solving the alignment problem in transportation decision-making.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GO-Diff: Data-free and amortized global structure optimization</title>
<link>https://arxiv.org/abs/2510.13448</link>
<guid>https://arxiv.org/abs/2510.13448</guid>
<content:encoded><![CDATA[
<div> Keywords: GO-Diff, global structure optimization, diffusion-based method, energy function, Boltzmann-weighted score-matching loss

Summary: 
GO-Diff is a novel diffusion-based method that optimizes global atomic structures without requiring prior data or explicit relaxation. It utilizes a Boltzmann-weighted score-matching loss to guide the generation of low-energy configurations. The method operates in a self-sampling and model refinement loop, continuously improving its ability to target favorable structures. Compared to traditional optimization pipelines, GO-Diff achieves competitive results with fewer energy evaluations. Additionally, it supports amortized optimization by reusing pretrained models across similar systems, enabling faster convergence on new tasks without retraining from scratch. <div>
arXiv:2510.13448v1 Announce Type: cross 
Abstract: We introduce GO-Diff, a diffusion-based method for global structure optimization that learns to directly sample low-energy atomic configurations without requiring prior data or explicit relaxation. GO-Diff is trained from scratch using a Boltzmann-weighted score-matching loss, leveraging only the known energy function to guide generation toward thermodynamically favorable regions. The method operates in a two-stage loop of self-sampling and model refinement, progressively improving its ability to target low-energy structures. Compared to traditional optimization pipelines, GO-Diff achieves competitive results with significantly fewer energy evaluations. Moreover, by reusing pretrained models across related systems, GO-Diff supports amortized optimization - enabling faster convergence on new tasks without retraining from scratch.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile Coverage Analysis using Crowdsourced Data</title>
<link>https://arxiv.org/abs/2510.13459</link>
<guid>https://arxiv.org/abs/2510.13459</guid>
<content:encoded><![CDATA[
<div> coverage analysis, mobile network, weak spot analysis, QoE data, crowdsourced

Summary:
The paper introduces a novel framework for mobile network coverage assessment and weak spot identification using crowdsourced QoE data. It focuses on analyzing coverage at the individual cell level, then aggregating to the site level using geolocation data. A One-Class Support Vector Machine (OC-SVM) algorithm is employed to calculate mobile network coverage, effectively mapping coverage areas for cells and sites. This methodology is extended to analyze crowdsourced service loss reports, pinpointing localized weak spots. The research demonstrates the framework's accuracy in mapping mobile coverage and highlighting signal deficiencies, especially in urban areas. The framework offers a targeted approach to enhancing user Quality of Experience (QoE) by identifying and addressing specific areas of network improvement. 

<br /><br />Summary: <div>
arXiv:2510.13459v1 Announce Type: cross 
Abstract: Effective assessment of mobile network coverage and the precise identification of service weak spots are paramount for network operators striving to enhance user Quality of Experience (QoE). This paper presents a novel framework for mobile coverage and weak spot analysis utilising crowdsourced QoE data. The core of our methodology involves coverage analysis at the individual cell (antenna) level, subsequently aggregated to the site level, using empirical geolocation data. A key contribution of this research is the application of One-Class Support Vector Machine (OC-SVM) algorithm for calculating mobile network coverage. This approach models the decision hyperplane as the effective coverage contour, facilitating robust calculation of coverage areas for individual cells and entire sites. The same methodology is extended to analyse crowdsourced service loss reports, thereby identifying and quantifying geographically localised weak spots. Our findings demonstrate the efficacy of this novel framework in accurately mapping mobile coverage and, crucially, in highlighting granular areas of signal deficiency, particularly within complex urban environments.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multifractality and its sources in the digital currency market</title>
<link>https://arxiv.org/abs/2510.13785</link>
<guid>https://arxiv.org/abs/2510.13785</guid>
<content:encoded><![CDATA[
<div> Keywords: Multifractality, Time series analysis, Digital currency markets, Long-range temporal correlations, Heavy-tailed distributions

Summary: 
Multifractality in time series analysis involves multiple scaling exponents, indicating complex dynamical behaviors beyond simple models. In digital currency markets, multifractal properties arise from long-range temporal correlations and heavy-tailed return distributions. Multifractal analysis enhances understanding of market inefficiencies and aids in volatility forecasting. The study applied the method of disentangling sources of multifractality to Bitcoin, Ethereum, decentralized exchanges, and non-fungible tokens. Results show that heavy tails play a significant role in generating a broad multifractal spectrum. Temporal correlations are the primary source of multifractality, independent of tail thickness. This observation in the digital currency market supports the validity of the disentangling methodology for understanding multifractality in time series.<br /><br />Summary: <div>
arXiv:2510.13785v1 Announce Type: cross 
Abstract: Multifractality in time series analysis characterizes the presence of multiple scaling exponents, indicating heterogeneous temporal structures and complex dynamical behaviors beyond simple monofractal models. In the context of digital currency markets, multifractal properties arise due to the interplay of long-range temporal correlations and heavy-tailed distributions of returns, reflecting intricate market microstructure and trader interactions. Incorporating multifractal analysis into the modeling of cryptocurrency price dynamics enhances the understanding of market inefficiencies, may improve volatility forecasting and facilitate the detection of critical transitions or regime shifts. Based on the multifractal cross-correlation analysis (MFCCA) whose spacial case is the multifractal detrended fluctuation analysis (MFDFA), as the most commonly used practical tools for quantifying multifractality, in the present contribution a recently proposed method of disentangling sources of multifractality in time series was applied to the most representative instruments from the digital market. They include Bitcoin (BTC), Ethereum (ETH), decentralized exchanges (DEX) and non-fungible tokens (NFT). The results indicate the significant role of heavy tails in generating a broad multifractal spectrum. However, they also clearly demonstrate that the primary source of multifractality are temporal correlations in the series, and without them, multifractality fades out. It appears characteristic that these temporal correlations, to a large extent, do not depend on the thickness of the tails of the fluctuation distribution. These observations, made here in the context of the digital currency market, provide a further strong argument for the validity of the proposed methodology of disentangling sources of multifractality in time series.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical reduced order modelling for the parametric Helmholtz equation</title>
<link>https://arxiv.org/abs/2407.04438</link>
<guid>https://arxiv.org/abs/2407.04438</guid>
<content:encoded><![CDATA[
<div> statistical finite element method, predictive modeling, reduced order model, sensor data, uncertainty

Summary: 
The paper discusses the challenge of predictive modeling using simulation and sensor data, focusing on the statistical finite element method (statFEM). It addresses the mismatch between simulation and sensor data due to uncertainty sources and proposes a reduced order statFEM framework using Krylov-based moment matching. This framework includes a data model that considers the bias induced by the reduced order approximation, estimated by an error indicator. The method aims to improve accuracy and convergence speed compared to the standard statFEM procedure applied to a reduced order model. Results from numerical examples show better accuracy and faster convergence across different frequency ranges. The proposed approach enhances the efficiency and effectiveness of predictive modeling in computational science by integrating a reduced order model with statistical methods to handle uncertainty in simulations. 

<br /><br />Summary: <div>
arXiv:2407.04438v2 Announce Type: replace 
Abstract: Predictive modeling involving simulation and sensor data at the same time, is a growing challenge in computational science. Even with large-scale finite element models, a mismatch to the sensor data often remains, which can be attributed to different sources of uncertainty. For such a scenario, the statistical finite element method (statFEM) can be used to condition a simulated field on given sensor data. This yields a posterior solution which resembles the data much better and additionally provides consistent estimates of uncertainty, including model misspecification. For frequency or parameter dependent problems, occurring, e.g. in acoustics or electromagnetism, solving the full order model at the frequency grid and conditioning it on data quickly results in a prohibitive computational cost. In this case, the introduction of a surrogate in form of a reduced order model yields much smaller systems of equations. In this paper, we propose a reduced order statFEM framework relying on Krylov-based moment matching. We introduce a data model which explicitly includes the bias induced by the reduced approximation, which is estimated by an inexpensive error indicator. The results of the new statistical reduced order method are compared to the standard statFEM procedure applied to a ROM prior, i.e. without explicitly accounting for the reduced order bias. The proposed method yields better accuracy and faster convergence throughout a given frequency range for different numerical examples.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deciphering the Crypto-shopper: Knowledge and Preferences of Consumers Using Cryptocurrencies for Purchases</title>
<link>https://arxiv.org/abs/2310.02911</link>
<guid>https://arxiv.org/abs/2310.02911</guid>
<content:encoded><![CDATA[
<div> Keywords: cryptocurrency, shopping habits, knowledge levels, purchase frequency, demographic segmentation <br />
<br />
Summary: 
The study examines the knowledge, expertise, and purchasing behavior of individuals using cryptocurrencies. Of the 516 participants surveyed, there was a range of knowledge levels, with around 30% displaying high purchase frequency despite limited knowledge. While domain knowledge was found to influence purchasing frequency to some extent, it only explained a small portion of the variance. Through K-means cluster analysis, respondents were divided into three distinct groups based on knowledge levels and shopping tendencies. The findings challenge the notion that extensive knowledge directly correlates with increased cryptocurrency usage, indicating other factors come into play. Businesses need to understand this diverse crypto-shopper demographic to tailor strategies and enhance user experiences. This research provides insights into current crypto-shopping behaviors and suggests future studies to explore broader impacts and potential shifts in the crypto-consumer landscape. <br /><br /> <div>
arXiv:2310.02911v5 Announce Type: replace-cross 
Abstract: The fast-growing cryptocurrency sector presents both challenges and opportunities for businesses and consumers alike. This study investigates the knowledge, expertise, and buying habits of people who shop using cryptocurrencies. Our survey of 516 participants shows that knowledge levels vary from beginners to experts. Interestingly, a segment of respondents, nearly 30%, showed high purchase frequency despite their limited knowledge. Regression analyses indicated that while domain knowledge plays a role, it only accounts for 11.6% of the factors affecting purchasing frequency. A K-means cluster analysis further segmented the respondents into three distinct groups, each having unique knowledge levels and purchasing tendencies. These results challenge the conventional idea linking extensive knowledge to increased cryptocurrency usage, suggesting other factors at play. Understanding this varying crypto-shopper demographic is pivotal for businesses, emphasizing the need for tailored strategies and user-friendly experiences. This study offers insights into current crypto-shopping behaviors and discusses future research exploring the broader impacts and potential shifts in the crypto-consumer landscape.
]]></content:encoded>
<pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAID-0e: A Resilient Striping Array Architecture for Balanced Performance and Availability</title>
<link>https://arxiv.org/abs/2510.12139</link>
<guid>https://arxiv.org/abs/2510.12139</guid>
<content:encoded><![CDATA[
<div> RAID-0e, disk array architecture, fault tolerance layer, data protection, RAID 0, operational resilience <br />
Summary: <br />
This paper introduces RAID-0e, a novel disk array architecture that enhances data resilience by incorporating a separate parity domain to protect the primary data domain in traditional RAID 0. RAID-0e mitigates the risk of array-wide data loss from common media failures while maintaining the read performance advantages of RAID 0. The architecture's operational workflows, performance characteristics, failure mode analysis, and security considerations are comprehensively discussed. RAID-0e is designed for environments that prioritize I/O performance, storage cost, and data resilience over full drive failure concerns. It offers a pragmatic solution for balancing performance and data protection in scenarios where non-catastrophic media failures are a primary concern. <div>
arXiv:2510.12139v1 Announce Type: new 
Abstract: This paper introduces a novel disk array architecture, designated RAID-0e (Resilient Striping Array), designed to superimpose a low-overhead fault tolerance layer upon traditional RAID 0 (striping). By employing a logically and physically separate parity domain to protect a primary data domain, RAID-0e mitigates the risk of array-wide data loss from common, non-catastrophic media failures, such as isolated bad blocks, transient read errors, or sector-level corruption. The architecture is engineered to preserve the intrinsic read performance advantages of RAID 0 while significantly enhancing data availability and operational resilience. This document provides a comprehensive exposition of the architectural principles, operational workflows, performance characteristics, failure mode analysis, and security considerations of RAID-0e. It is presented as an experimental yet pragmatic solution for environments seeking a new equilibrium between I/O performance, storage cost, and data resilience, particularly where full drive failure is a secondary concern to media degradation.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Simulation of a Financial Market with Large Language Models</title>
<link>https://arxiv.org/abs/2510.12189</link>
<guid>https://arxiv.org/abs/2510.12189</guid>
<content:encoded><![CDATA[
<div> Keywords: stock markets, chart patterns, path dependence, human loss aversion, large language models

Summary:
In the study, the authors explore the presence of path dependence in stock markets, where investor decisions are influenced by historical price movements in addition to current market conditions. The phenomenon suggests the role of human loss aversion, anchored to individual reference points, in shaping investor behavior. To capture these subtle behavioral tendencies, the authors propose the Fundamental-Chartist-LLM-Agent framework, which uses large language models to emulate human-like trading decisions. Through simulations, the FCLAgents demonstrate the ability to reproduce path-dependent patterns that traditional agents miss. Additionally, an analysis of FCLAgents' behavior reveals that reference points guiding loss aversion vary with market trajectories, indicating the potential of LLM-based agents to model nuanced investor behavior. This research sheds light on the complex interactions between human psychology and market dynamics in real-world stock markets. 

<br /><br />Summary: <div>
arXiv:2510.12189v1 Announce Type: new 
Abstract: In real-world stock markets, certain chart patterns -- such as price declines near historical highs -- cannot be fully explained by fundamentals alone. These phenomena suggest the presence of path dependence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices leading up to the present. Path dependence has drawn attention in behavioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow standard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Furthermore, an analysis of FCLAgents' behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Sensing and Reliable State Estimation with Shallow Recurrent Decoders on a TRIGA Mark II Reactor</title>
<link>https://arxiv.org/abs/2510.12368</link>
<guid>https://arxiv.org/abs/2510.12368</guid>
<content:encoded><![CDATA[
<div> deep learning, state estimation, engineering systems, nuclear reactors, Shallow Recurrent Decoder networks

Summary: 
Shallow Recurrent Decoder networks are a data-driven methodology for accurate state estimation in engineering systems like nuclear reactors. This technique maps sparse measurements to the full state space, handling noisy data without hyperparameter tuning. The study applies this approach to a fluid dynamics model of the TRIGA Mark II research reactor, using synthetic and experimental temperature data. The objectives are to reconstruct the full system state and assess correction capabilities. Results show the architecture can accurately reconstruct characteristic fields in real-time, making it suitable for monitoring and control in a reactor digital twin. <div>
arXiv:2510.12368v1 Announce Type: new 
Abstract: Shallow Recurrent Decoder networks are a novel data-driven methodology able to provide accurate state estimation in engineering systems, such as nuclear reactors. This deep learning architecture is a robust technique designed to map the temporal trajectories of a few sparse measures to the full state space, including unobservable fields, which is agnostic to sensor positions and able to handle noisy data through an ensemble strategy, leveraging the short training times and without the need for hyperparameter tuning. Following its application to a novel reactor concept, this work investigates the performance of Shallow Recurrent Decoders when applied to a real system. The underlying model is represented by a fluid dynamics model of the TRIGA Mark II research reactor; the architecture will use both synthetic temperature data coming from the numerical model and leveraging experimental temperature data recorded during a previous campaign. The objective of this work is, therefore, two-fold: 1) assessing if the architecture can reconstruct the full state of the system (temperature, velocity, pressure, turbulence quantities) given sparse data located in specific, low-dynamics channels and 2) assessing the correction capabilities of the architecture (that is, given a discrepancy between model and data, assessing if sparse measurements can provide some correction to the architecture output). As will be shown, the accurate reconstruction of every characteristic field, using both synthetic and experimental data, in real-time makes this approach suitable for interpretable monitoring and control purposes in the framework of a reactor digital twin.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proceedings of the International Workshop on Verification of Scientific Software</title>
<link>https://arxiv.org/abs/2510.12314</link>
<guid>https://arxiv.org/abs/2510.12314</guid>
<content:encoded><![CDATA[
<div> workshop, Verification of Scientific Software, challenges, correctness, reliability<br />
Summary: 
The VSS 2025 workshop, held at McMaster University, focused on addressing challenges in ensuring the correctness and reliability of large-scale scientific codes. The event featured five peer-reviewed papers, three invited contributions, and challenge problems, covering topics such as deductive verification, floating-point error analysis, and domain-aware testing. The workshop built on previous Correctness Workshop series and the 2023 NSF/DOE report on scientific software correctness. It showcased a variety of perspectives, problems, and solutions in progress, with the potential for challenge problems to facilitate collaboration among different verification tools. Overall, VSS serves as an important snapshot of the intersection between software verification and scientific computing, highlighting ongoing efforts to enhance the reliability and accuracy of scientific codes. <br /><br />  <div>
arXiv:2510.12314v1 Announce Type: cross 
Abstract: This volume contains the proceedings of the Verification of Scientific Software (VSS 2025) workshop, held on 4 May 2025 at McMaster University, Canada, as part of ETAPS 2025. VSS brings together researchers in software verification and scientific computing to address challenges in ensuring the correctness and reliability of large-scale scientific codes. The program featured five peer-reviewed papers, three invited contributions, and a set of challenge problems, covering themes such as deductive verification, floating-point error analysis, specification of coupled models, and domain-aware testing. VSS builds on the Correctness Workshop series at Supercomputing and the 2023 NSF/DOE report on scientific software correctness. It serves as yet another snapshot of this important area, showcasing a wide range of perspectives, problems and their solutions in progress, with the challenge problems having the potential to bring together separate verification tools into concerted action.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Health-promoting Potential of Parks in 35 Cities Worldwide</title>
<link>https://arxiv.org/abs/2407.15770</link>
<guid>https://arxiv.org/abs/2407.15770</guid>
<content:encoded><![CDATA[
<div> Keywords: urban parks, health-related activities, park quality, city centers, public health

Summary:
Urban parks play a crucial role in public health by supporting various health-related activities such as physical exercise, mindfulness, nature appreciation, environmental education, social interaction, and cultural experiences. A study analyzing 23,477 parks in 35 global cities revealed distinct patterns in park design and quality. Parks in North America tend to focus more on physical activities, while European parks offer more opportunities for nature-based experiences. City center parks generally outperform suburban parks in terms of supporting health-promoting activities. Disparities in park quality were noted between cities, with Tokyo and Paris exhibiting more equal access to health-promoting spaces compared to Copenhagen and Rio de Janeiro. These findings highlight the need for cities to create more equitable urban parks that cater to a diverse range of health-enhancing activities, ultimately improving public health outcomes. 

<br /><br />Summary: <div>
arXiv:2407.15770v2 Announce Type: replace-cross 
Abstract: Urban parks are important for public health, but the role of specific spaces, such as playgrounds or lakes, and elements, such as benches or sports equipment, in supporting well-being is not well understood. Based on expert input and a review of the literature, we defined six types of health-related activities: physical, mindfulness, nature appreciation, environmental, social, and cultural. We built a lexicon that links each activity to specific elements and spaces within parks present in OpenStreetMap. Using this data, we scored 23,477 parks across 35 cities worldwide based on their ability to support these activities. We found clear patterns: parks in North America focus more on physical activity, while those in Europe offer more chances to enjoy nature. Parks near city centers support health-promoting activities better than those farther out. Suburban parks in many cities lack the spaces and equipment needed for nature-based, social, and cultural activities. We also found large gaps in park quality between cities. Tokyo and Paris provide more equal access, while Copenhagen and Rio de Janeiro show sharp contrasts. These results can help cities create fairer parks that better support public health.
]]></content:encoded>
<pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Glaucoma Report Generation via Dual-Attention Semantic Parallel-LSTM and Multimodal Clinical Data Integration</title>
<link>https://arxiv.org/abs/2510.10037</link>
<guid>https://arxiv.org/abs/2510.10037</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, glaucoma diagnostic, multimodal architectures, Dual-Attention Semantic Parallel-LSTM Network, clinical reports

Summary: 
The article introduces the Dual-Attention Semantic Parallel-LSTM Network (DA-SPL) to improve automated glaucoma diagnostic report generation. The challenges of content redundancy and inadequate highlighting of key features in traditional models are addressed through DA-SPL's advanced framework. DA-SPL utilizes a dual-attention mechanism in the encoder, parallelized LSTM decoder architecture, and label enhancement module for accurate report generation. Evaluation on glaucoma datasets shows DA-SPL outperforming existing models in extracting pathological indicators and generating precise clinical reports aligned with expert annotations. The model's ability to process fundus imaging and supplementary visual inputs enhances its diagnostic accuracy and semantic consistency in producing comprehensive reports.<br /><br />Summary: <div>
arXiv:2510.10037v1 Announce Type: new 
Abstract: Generative AI for automated glaucoma diagnostic report generation faces two predominant challenges: content redundancy in narrative outputs and inadequate highlighting of pathologically significant features including optic disc cupping, retinal nerve fiber layer defects, and visual field abnormalities. These limitations primarily stem from current multimodal architectures' insufficient capacity to extract discriminative structural-textural patterns from fundus imaging data while maintaining precise semantic alignment with domain-specific terminology in comprehensive clinical reports. To overcome these constraints, we present the Dual-Attention Semantic Parallel-LSTM Network (DA-SPL), an advanced multimodal generation framework that synergistically processes both fundus imaging and supplementary visual inputs. DA-SPL employs an Encoder-Decoder structure augmented with the novel joint dual-attention mechanism in the encoder for cross-modal feature refinement, the parallelized LSTM decoder architecture for enhanced temporal-semantic consistency, and the specialized label enhancement module for accurate disease-relevant term generation. Rigorous evaluation on standard glaucoma datasets demonstrates DA-SPL's consistent superiority over state-of-the-art models across quantitative metrics. DA-SPL exhibits exceptional capability in extracting subtle pathological indicators from multimodal inputs while generating diagnostically precise reports that exhibit strong concordance with clinical expert annotations.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GrifFinNet: A Graph-Relation Integrated Transformer for Financial Predictions</title>
<link>https://arxiv.org/abs/2510.10387</link>
<guid>https://arxiv.org/abs/2510.10387</guid>
<content:encoded><![CDATA[
<div> Transformer, financial predictions, graph modeling, spatio-temporal dynamics, market behavior

Summary: 
GrifFinNet is a novel model proposed for predicting stock returns in the financial market by integrating multi-relational graph modeling with Transformer-based temporal encoding. The model constructs inter-stock relation graphs based on industry sectors and institutional ownership, enhancing its ability to capture spatial dependencies and temporal patterns. GrifFinNet incorporates an adaptive gating mechanism to dynamically integrate relational data, providing a comprehensive representation of market dynamics. Experimental results on Chinese A-share indices demonstrate that GrifFinNet outperforms baseline models consistently, offering valuable and interpretable insights into financial market behavior. The code and data for GrifFinNet are available for further exploration and analysis. <div>
arXiv:2510.10387v1 Announce Type: new 
Abstract: Predicting stock returns remains a central challenge in quantitative finance, transitioning from traditional statistical methods to contemporary deep learning techniques. However, many current models struggle with effectively capturing spatio-temporal dynamics and integrating multiple relational data sources. This study proposes GrifFinNet, a Graph-Relation Integrated Transformer for Financial Predictions, which combines multi-relational graph modeling with Transformer-based temporal encoding. GrifFinNet constructs inter-stock relation graphs based on industry sectors and institutional ownership, and incorporates an adaptive gating mechanism to dynamically integrate relational data in response to changing market conditions. This approach enables the model to jointly capture spatial dependencies and temporal patterns, offering a comprehensive representation of market dynamics. Extensive experiments on two Chinese A-share indices show that GrifFinNet consistently outperforms several baseline models and provides valuable, interpretable insights into financial market behavior. The code and data are available at: https://www.healthinformaticslab.org/supp/.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameterized crack modelling based on a localized non-intrusive reduced basis method</title>
<link>https://arxiv.org/abs/2510.10624</link>
<guid>https://arxiv.org/abs/2510.10624</guid>
<content:encoded><![CDATA[
<div> model order reduction, parametric modelling, cracks, spline discretizations, non-intrusive reduced basis methods

Summary: 
This article introduces a model order reduction strategy for fast parametric modelling of problems involving cracks on spline discretizations. The approach focuses on using non-intrusive reduced basis methods and a localization strategy tailored to parametric problems with moving discontinuities. By efficiently separating the offline and online simulation processes, the proposed framework allows for fast computations and accurate results. Tests conducted on linear elastic problems using splines and the extended isogeometric method demonstrate the effectiveness of the reduced order models in terms of both accuracy and real-time efficiency. This method shows promise for applications in damage detection and other areas where fast and accurate simulations are required. <div>
arXiv:2510.10624v1 Announce Type: new 
Abstract: This contribution presents a model order reduction strategy for fast parametric modelling of problems with cracks formulated on spline discretizations. In the context of damage detection, parametric reduced order models (ROMs) are well suited for fast computations by establishing an efficient offline/online split of the simulation process. The problems of interest focus on geometric parameters that describe the crack configuration and may pose challenges to constructing efficient ROMs. This work proposes a framework based on non-intrusive reduced basis methods and a localization strategy tailored to parametric problems with moving discontinuities. The combined benefits of non-intrusive ROMs and localization enable accurate and efficient reduction with low online cost. We demonstrate the applicability of the ROM approach with benchmark tests on linear elastic problems discretized with splines and the extended isogeometric method (XIGA) for crack modelling. The results we obtain show the accuracy and real-time efficiency of the constructed reduced order models.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence of coronary plaque morphology on local mechanical states and associated in-stent restenosis</title>
<link>https://arxiv.org/abs/2510.10763</link>
<guid>https://arxiv.org/abs/2510.10763</guid>
<content:encoded><![CDATA[
<div> Keywords: in-stent restenosis, morphological characteristics, local mechanical factors, stress distributions, computational simulations<br />
<br />
Summary: 
This study focuses on the relationship between specific morphological characteristics of coronary artery lesions and the occurrence of in-stent restenosis after percutaneous coronary intervention. By conducting computational simulations based on patient-specific coronary artery models, the researchers analyze the impact of plaque composition on local mechanical factors, particularly stress distributions in the artery wall during and after stent implantation. The findings suggest that morphological features like circumferential or asymmetric block calcifications can lead to higher stresses in the surrounding tissue, increasing the risk of in-stent restenosis. Therefore, understanding and evaluating local tensional stresses are crucial for assessing individual in-stent restenosis risk. This research sheds light on the importance of considering both morphological characteristics and local mechanical factors in predicting and managing in-stent restenosis. <br /><br /> <div>
arXiv:2510.10763v1 Announce Type: new 
Abstract: In-stent restenosis after percutaneous coronary intervention is a multifactorial process. Specific morphological lesion characteristics were observed to contribute to the occurrence of in-stent restenosis. Local mechanical factors, such as stresses and strains, are known to influence tissue adaptation after stent implantation. However, the influence of morphological features on those local mechanical states and, hence, on the occurrence of in-stent restenosis remains understudied. This work investigates the correlation between local mechanical quantities and in-stent restenosis by evaluating the stress distributions in the artery wall during and after stent implantation for informative lesion morphologies. We perform computational simulations of the stenting procedure with physics-based patient-specific coronary artery models. Different morphologies are assessed using the spatial plaque composition information from high-resolution coronary computed tomography angiography data. We quantify the correlation between in-stent restenosis and local tensional stresses. We found that specific morphological characteristics like circumferential or asymmetric block calcifications result in higher stresses in the surrounding tissue. This study concludes that local stresses are critical for assessing the individual in-stent restenosis risk.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Earth-Observing Satellite Sampling Effectiveness Using Kullback-Leibler Divergence</title>
<link>https://arxiv.org/abs/2510.10859</link>
<guid>https://arxiv.org/abs/2510.10859</guid>
<content:encoded><![CDATA[
<div> Earth-Observing Satellites; Sampling Effectiveness; Kullback-Leibler Divergence; Observational Representativeness; Monsoon
Summary:
This work presents a methodology for assessing the representativeness of geophysical variables sampled by Earth-observing satellites. The study evaluates the effectiveness of 20 satellite configurations for observing convective storm activity in the Southwestern U.S. during the North American Monsoon season. Results indicate that a two-satellite sun-synchronous system with an 8:00 PM LTAN achieved the most representative observation of storm clusters. Single-satellite configurations, especially those with late-night LTANs, had significantly higher KL divergence. The study concludes that dual-satellite configurations in sun-synchronous orbits with evening LTANs outperform single-satellite and inclined configurations in capturing representative convective storm activity. <div>
arXiv:2510.10859v1 Announce Type: new 
Abstract: This work presents an objective, repeatable, automatic, and fast methodology for assessing the representativeness of geophysical variables sampled by Earth-observing satellites. The primary goal is to identify and mitigate potential sampling biases attributed to orbit selection during pre-Phase A mission studies. This methodology supports current incubation activities for a future Planetary Boundary Layer observing system by incorporating a sampling effectiveness measure into a broader architectural study. The study evaluates the effectiveness of 20 satellite configurations for observing convective storm activity in the Southwestern U.S. during the North American Monsoon (NAM) season. The primary design variables are the number of satellites, orbit type (sun-synchronous or inclined), and Local Time of Ascending Node (LTAN). Using Kullback-Leibler (KL) divergence to assess observational representativeness and Kernel Density Estimation (KDE) to estimate probability density functions, the study quantifies the discrepancy between observed and ground truth storm features. Results indicate that a two-satellite sun-synchronous system with an 8:00 PM LTAN, achieved the lowest KL divergence, signifying the most representative observation of storm clusters. In contrast, single-satellite configurations, particularly those with late-night LTANs (e.g., 12:00 AM), demonstrated significantly higher KL divergence. The study concludes that dual-satellite configurations in sun-synchronous orbits with evening LTANs outperform single-satellite and inclined configurations in capturing representative convective storm activity. Keywords: Earth-Observing Satellites; Sampling Effectiveness; Kullback-Leibler Divergence; Observational Representativeness; Monsoon
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Evaluation of Neural Network Architectures for Generalizable Human Spatial Preference Prediction in Unseen Built Environments</title>
<link>https://arxiv.org/abs/2510.10954</link>
<guid>https://arxiv.org/abs/2510.10954</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Convolutional Neural Networks, deep learning, preference modeling, built environments <br />
<br />
Summary: 
This study focuses on predicting human spatial preferences within built environments to develop Cyber-Physical-Social Infrastructure Systems. The research compares Graph Neural Networks, Convolutional Neural Networks, and standard feedforward Neural Networks using synthetic data from a pocket park environment. The goal is to assess the generalizability of preference models to unseen layouts. The models are evaluated based on their ability to predict preferences influenced by physical, environmental, and social features. A generalizability score is calculated using the area under the precision-recall curve for seen and unseen layouts. This score is suitable for imbalanced data and provides insights into which neural network architecture is most effective for preference-aware human behavior modeling in unfamiliar built environments. <div>
arXiv:2510.10954v1 Announce Type: new 
Abstract: The capacity to predict human spatial preferences within built environments is instrumental for developing Cyber-Physical-Social Infrastructure Systems (CPSIS). A significant challenge in this domain is the generalizability of preference models, particularly their efficacy in predicting preferences within environmental configurations not encountered during training. While deep learning models have shown promise in learning complex spatial and contextual dependencies, it remains unclear which neural network architectures are most effective at generalizing to unseen layouts. To address this, we conduct a comparative study of Graph Neural Networks, Convolutional Neural Networks, and standard feedforward Neural Networks using synthetic data generated from a simplified and synthetic pocket park environment. Beginning with this illustrative case study, allows for controlled analysis of each model's ability to transfer learned preference patterns to unseen spatial scenarios. The models are evaluated based on their capacity to predict preferences influenced by heterogeneous physical, environmental, and social features. Generalizability score is calculated using the area under the precision-recall curve for the seen and unseen layouts. This generalizability score is appropriate for imbalanced data, providing insights into the suitability of each neural network architecture for preference-aware human behavior modeling in unseen built environments.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Physics-Enhanced Bayesian Inverse Analysis: Information Gain from Additional Fields</title>
<link>https://arxiv.org/abs/2510.11095</link>
<guid>https://arxiv.org/abs/2510.11095</guid>
<content:encoded><![CDATA[
<div> Bayesian inverse analysis, computational models, multi-physics data, uncertainty reduction,
information gain <br />
<br />
Summary: Many inverse problems in real-world applications suffer from limited data, leading to uncertainty in parameter estimation using Bayesian analysis. This study proposes enhancing the inverse analysis by incorporating data from additional physical fields into computational models. By extending the models to include these fields, even weakly or one-way coupled, the information gain from the prior to the posterior can be significantly increased. The research demonstrates the effectiveness of this multi-physics-enhanced approach using both simple and complex models. The results show that even a small amount of data from an additional physical field can greatly reduce uncertainty. This method presents a cost- and time-saving alternative to setting up additional experimental setups, making it a valuable tool across various scientific and industrial applications. <br /><br /> <div>
arXiv:2510.11095v1 Announce Type: new 
Abstract: Many real-world inverse problems suffer from limited data, often because they rely on measurements of a single physical field. Such data frequently fail to sufficiently reduce parameter uncertainty in Bayesian inverse analysis. Incorporating easily available data from additional physical fields can substantially decrease this uncertainty. We focus on Bayesian inverse analyses based on computational models, e.g., those using the finite element method. To incorporate data from additional physical fields, the computational model must be extended to include these fields. While this model extension may have little to no effect on forward model predictions, it can greatly enhance inverse analysis by leveraging the multi-physics data. Our work proposes this multi-physics-enhanced inverse approach and demonstrates its potential using two models: a simple model with one-way coupled fields and a complex computational model with fully coupled fields. We quantify the uncertainty reduction by comparing the effect of single-physics and multi-physics data on the information gain from the prior to the posterior. Our results show that even a few or noisy data points from an additional physical field can considerably increase the information gain, even if this field is weakly or one-way coupled. Although multi-physics data are often readily available, it is remarkable that their potential has been largely neglected in model calibration so far. Instead, costly and time-consuming additional experimental setups are often pursued. In contrast, incorporating multi-physics data requires minimal effort when multi-physics models are readily available or easy to implement, as is the case with uncoupled and one-way coupled models. This work proposes and promotes the future use of multi-physics-enhanced Bayesian inverse analysis as a cost- and time-saving game-changer across various fields of science and industry.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A mathematical model for pricing perishable goods for quick-commerce applications</title>
<link>https://arxiv.org/abs/2510.11360</link>
<guid>https://arxiv.org/abs/2510.11360</guid>
<content:encoded><![CDATA[
<div> q-commerce, informal employment, perishable goods, mathematical model, simulation

Summary:
The paper discusses the rapid growth of quick commerce (q-commerce) in India, providing informal employment to a large number of workers. The industry primarily deals with perishable goods, leading to high order volumes and repetitive purchases. A key challenge for retailers is finding the optimal pricing strategy for these goods to maximize revenue while avoiding unsold inventory. A mathematical model is proposed to address this pricing dilemma, aiming to improve the unit economics of q-commerce firms and potentially benefit gig workers. The simulation results will be presented in a future study. This research is significant not only for the industry's financial sustainability but also for its social impact on gig workers. <div>
arXiv:2510.11360v1 Announce Type: new 
Abstract: Quick commerce (q-commerce) is one of the fastest growing sectors in India. It provides informal employment to approximately 4,50,000 workers, and it is estimated to become a USD 200 Billion industry by 2026. A significant portion of this industry deals with perishable goods. (e.g. milk, dosa batter etc.) These are food items which are consumed relatively fresh by the consumers and therefore their order volume is high and repetitive even when the average basket size is relatively small. The fundamental challenge for the retailer is that, increasing selling price would hamper sales and would lead to unsold inventory. On the other hand setting a price less, would lead to forgoing of potential revenue. This paper attempts to propose a mathematical model which formalizes this dilemma. The problem statement is not only important for improving the unit economics of the perennially loss making quick commerce firms, but also would lead to a trickle-down effect in improving the conditions of the gig workers as observed in [4]. The sections below describe the mathematical formulation. The results from the simulation would be published in a follow-up study.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LRQ-Solver: A Transformer-Based Neural Operator for Fast and Accurate Solving of Large-scale 3D PDEs</title>
<link>https://arxiv.org/abs/2510.11636</link>
<guid>https://arxiv.org/abs/2510.11636</guid>
<content:encoded><![CDATA[
<div> low-rank query attention, PDE solver, physics-integrated framework, scalability, efficiency
Summary:
The article introduces LRQ-Solver, a framework designed for fast, accurate, and scalable simulations of complex three-dimensional geometries. It combines Parameter Conditioned Lagrangian Modeling (PCLM) to link physical states with design parameters, enhancing generalization, and a Low-Rank Query Attention (LR-QA) module to reduce computational complexity. LRQ-Solver reduces error rates on benchmark datasets, achieves significant training speedups, and handles up to 2 million points on a single GPU efficiently. By providing a powerful solution for multi-configuration physics simulations, LRQ-Solver demonstrates state-of-the-art performance in accuracy, scalability, and efficiency. The code for experiments can also be accessed on GitHub at https://github.com/LilaKen/LRQ-Solver.<br /><br />Summary: <div>
arXiv:2510.11636v1 Announce Type: new 
Abstract: Solving large-scale Partial Differential Equations (PDEs) on complex three-dimensional geometries represents a central challenge in scientific and engineering computing, often impeded by expensive pre-processing stages and substantial computational overhead. We introduce Low-Rank Query-based PDE Solver (LRQ-Solver), a physics-integrated framework engineered for rapid, accurate, and highly scalable simulations of industrial-grade models. This framework is built upon two primary technical innovations. First, our Parameter Conditioned Lagrangian Modeling (PCLM) approach explicitly couples local physical states with global design parameters, enabling robust predictions across varied simulation configurations. By embedding physical consistency directly into the learning architecture, PCLM ensures that predictions remain physically meaningful even under unseen design conditions, significantly enhancing generalization and reliability. Second, the Low-Rank Query Attention (LR-QA) module leverages the second-order statistics of physical fields to construct a global coherence kernel, reducing the computational complexity of attention from O(N2) to O(NC2 + C3). By replacing point-wise clustering with covariance decomposition, LRQ-Solver achieves exceptional scalability efficiently processing up to 2 million points on a single GPU. Validated on standard benchmarks, LRQ-Solver achieves a 38.9% error reduction on the DrivAer++ dataset and 28.76% on the 3D Beam dataset, alongside a training speedup of up to 50 times. Our results establish that LRQ-Solver offers a powerful paradigm for multi-configuration physics simulations, delivering a SOTA combination of accuracy, scalability, and efficiency. Code to reproduce the experiments is available at https://github.com/LilaKen/LRQ-Solver.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Relationship between Space-Time Accessibility and Leisure Activity Participation</title>
<link>https://arxiv.org/abs/2510.10307</link>
<guid>https://arxiv.org/abs/2510.10307</guid>
<content:encoded><![CDATA[
<div> Keywords: accessibility, leisure activities, space-time, capability approach, urban mobility

Summary: 
The study introduces a space-time accessibility metric based on the capability approach to assess how accessibility influences participation in leisure activities in urban areas. Using GPS data from residents in the Paris region, the research examines how space-time accessibility impacts travel time and diversity of leisure activity locations. Spatial patterns indicate that individuals, particularly active transport users, select destinations based on their opportunity sets defined by space-time accessibility. Structural equation modeling shows that space-time accessibility directly enhances leisure diversity while reducing travel time, which in turn decreases diversity. The findings emphasize the importance of person-centered, capability-informed accessibility metrics for understanding urban mobility inequalities and guiding transport planning initiatives aimed at expanding opportunities for social participation across diverse population groups. 

Summary: <div>
arXiv:2510.10307v1 Announce Type: cross 
Abstract: Understanding how accessibility shapes participation in leisure activities is central to promoting inclusive and vibrant urban life. Conventional accessibility measures often focus on potential access from fixed home locations, overlooking the constraints and opportunities embedded in daily routines. In this study, we introduce a space-time accessibility (SPA) metric rooted in the capability approach, capturing feasible leisure opportunities between home and work given a certain time budget, individual transport modes, and urban infrastructure. Using high-resolution GPS data from 2,415 residents in the Paris region, we assess how SPA influences total travel time and leisure participation, measured as the diversity of leisure activity locations. Spatial patterns show that most individuals-especially active transport users-choose destinations aligned with their SPA-defined opportunity sets, underscoring the metric's validity in capturing capability sets. Structural equation modeling reveals that SPA directly fosters leisure diversity but also reduces travel time, which in turn is associated with lower diversity. These findings highlight the value of person-centered, capability-informed accessibility metrics for understanding inequalities in urban mobility and informing transport planning strategies that expand real freedoms to participate in social life across diverse population groups.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance</title>
<link>https://arxiv.org/abs/2510.10402</link>
<guid>https://arxiv.org/abs/2510.10402</guid>
<content:encoded><![CDATA[
<div> diffusion, graph generation, Monte Carlo Tree Search, controllable, scalability

Summary:
TreeDiff is a novel framework for controllable graph generation using Monte Carlo Tree Search (MCTS). It addresses limitations of existing methods by introducing a macro-step expansion strategy, dual-space denoising, and a dual-space verifier. These innovations allow TreeDiff to efficiently explore long-horizon paths, maintain structural fidelity, and predict long-term rewards for partial graph denoising. Experimental results on molecular generation benchmarks demonstrate that TreeDiff outperforms existing methods, particularly in scaling up with additional computation. The framework shows superior performance in both unconditional and conditional settings, showcasing its potential for diverse applications in graph learning domains. Its scalability and controllability make it a promising approach for graph generation tasks, providing a valuable tool for a range of applications requiring high-quality graph synthesis. 

<br /><br />Summary: <div>
arXiv:2510.10402v1 Announce Type: cross 
Abstract: Graph generation is a fundamental problem in graph learning with broad applications across Web-scale systems, knowledge graphs, and scientific domains such as drug and material discovery. Recent approaches leverage diffusion models for step-by-step generation, yet unconditional diffusion offers little control over desired properties, often leading to unstable quality and difficulty in incorporating new objectives. Inference-time guidance methods mitigate these issues by adjusting the sampling process without retraining, but they remain inherently local, heuristic, and limited in controllability. To overcome these limitations, we propose TreeDiff, a Monte Carlo Tree Search (MCTS) guided dual-space diffusion framework for controllable graph generation. TreeDiff is a plug-and-play inference-time method that expands the search space while keeping computation tractable. Specifically, TreeDiff introduces three key designs to make it practical and scalable: (1) a macro-step expansion strategy that groups multiple denoising updates into a single transition, reducing tree depth and enabling long-horizon exploration; (2) a dual-space denoising mechanism that couples efficient latent-space denoising with lightweight discrete correction in graph space, ensuring both scalability and structural fidelity; and (3) a dual-space verifier that predicts long-term rewards from partially denoised graphs, enabling early value estimation and removing the need for full rollouts. Extensive experiments on 2D and 3D molecular generation benchmarks, under both unconditional and conditional settings, demonstrate that TreeDiff achieves state-of-the-art performance. Notably, TreeDiff exhibits favorable inference-time scaling: it continues to improve with additional computation, while existing inference-time methods plateau early under limited resources.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying and Quantifying Financial Bubbles with the Hyped Log-Periodic Power Law Model</title>
<link>https://arxiv.org/abs/2510.10878</link>
<guid>https://arxiv.org/abs/2510.10878</guid>
<content:encoded><![CDATA[
<div> Keywords: Hyped Log-Periodic Power Law Model, financial bubbles, sentiment scores, dual-stream transformer model, backtesting

Summary:
The Hyped Log-Periodic Power Law Model (HLPPL) is proposed as a novel approach to quantifying and detecting financial bubbles, incorporating bubble labels, sentiment scores, and a hype index. A dual-stream transformer model is trained using market data and machine learning methods, producing a Bubble Score time series. The model demonstrates an average annualized return of 34.13% when backtested on U.S. equities from 2018 to 2024, with strong generalization across industry sectors. Its conservative bias reduces false positives, making it useful for market signaling. The framework effectively identifies extreme overpricing and underpricing phases in a unified structure, offering real-time bubble identification and measurement using HLPPL signals. <br /><br />Summary: The HLPPL model combines theoretical and empirical advancements to quantify and detect financial bubbles, showing promising results in backtesting on U.S. equities. Its dual-stream transformer approach, incorporating sentiment analysis and hype index, contributes to accurate positive and negative bubble identification with a conservative bias to minimize false positives, enhancing market decision-making capabilities. <div>
arXiv:2510.10878v1 Announce Type: cross 
Abstract: We propose a novel model, the Hyped Log-Periodic Power Law Model (HLPPL), to the problem of quantifying and detecting financial bubbles, an ever-fascinating one for academics and practitioners alike. Bubble labels are generated using a Log-Periodic Power Law (LPPL) model, sentiment scores, and a hype index we introduced in previous research on NLP forecasting of stock return volatility. Using these tools, a dual-stream transformer model is trained with market data and machine learning methods, resulting in a time series of confidence scores as a Bubble Score. A distinctive feature of our framework is that it captures phases of extreme overpricing and underpricing within a unified structure.
  We achieve an average yield of 34.13 percentage annualized return when backtesting U.S. equities during the period 2018 to 2024, while the approach exhibits a remarkable generalization ability across industry sectors. Its conservative bias in predicting bubble periods minimizes false positives, a feature which is especially beneficial for market signaling and decision-making. Overall, this approach utilizes both theoretical and empirical advances for real-time positive and negative bubble identification and measurement with HLPPL signals.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Structural Engineering Workflows with Large Language Model Agents</title>
<link>https://arxiv.org/abs/2510.11004</link>
<guid>https://arxiv.org/abs/2510.11004</guid>
<content:encoded><![CDATA[
<div> Keywords: MASSE, Multi-Agent System, Structural Engineering, Large Language Model, Automation

Summary:
MASSE is introduced as the first Multi-Agent System for Structural Engineering, integrating large language model (LLM)-based agents with engineering workflows. Structural engineering, a traditionally stagnant domain, stands to benefit from recent advancements in LLMs for tasks like interpreting design codes and load calculations. A proof-of-concept demonstrates full automation of real-world engineering workflows with MASSE, reducing expert workload significantly. The system offers immediate deployment in professional environments and enhances reliability and accuracy in engineering scenarios. MASSE showcases the potential for LLM-based automation to revolutionize structural engineering practices. <div>
arXiv:2510.11004v1 Announce Type: cross 
Abstract: We introduce $\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hot-Starting Quantum Portfolio Optimization</title>
<link>https://arxiv.org/abs/2510.11153</link>
<guid>https://arxiv.org/abs/2510.11153</guid>
<content:encoded><![CDATA[
<div> Combinatorial optimization, convex objective function, adiabatic quantum optimization, quantum annealer, discrete solutions<br />
Summary:<br />
This study addresses the challenge of combinatorial optimization with a smooth and convex objective function. It focuses on applications like discrete mean-variance portfolio optimization, where assets are traded in integer quantities. Existing quantum optimization methods struggle to utilize optimal solutions efficiently. The study introduces a novel approach that constructs a compact Hilbert space to restrict the search space to discrete solutions near the continuous optimum, reducing the required number of qubits. Experiments on software solvers and a D-Wave Advantage quantum annealer showcase the superiority of this method over current techniques. The approach not only outperforms state-of-the-art methods, but also integrates insights from the relaxed continuous solution into the QUBO formulation, paving the way for improved quantum optimization strategies. <br />Summary: <div>
arXiv:2510.11153v1 Announce Type: cross 
Abstract: Combinatorial optimization with a smooth and convex objective function arises naturally in applications such as discrete mean-variance portfolio optimization, where assets must be traded in integer quantities. Although optimal solutions to the associated smooth problem can be computed efficiently, existing adiabatic quantum optimization methods cannot leverage this information. Moreover, while various warm-starting strategies have been proposed for gate-based quantum optimization, none of them explicitly integrate insights from the relaxed continuous solution into the QUBO formulation. In this work, a novel approach is introduced that restricts the search space to discrete solutions in the vicinity of the continuous optimum by constructing a compact Hilbert space, thereby reducing the number of required qubits. Experiments on software solvers and a D-Wave Advantage quantum annealer demonstrate that our method outperforms state-of-the-art techniques.
]]></content:encoded>
<pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Molecular Property Prediction: A Survey</title>
<link>https://arxiv.org/abs/2510.08900</link>
<guid>https://arxiv.org/abs/2510.08900</guid>
<content:encoded><![CDATA[
<div> few-shot learning, molecular property prediction, AI, drug discovery, materials design

Summary:
This article introduces the concept of few-shot molecular property prediction (FSMPP), which addresses the challenge of limited labeled data in real-world molecules for AI-assisted prediction. The core challenges identified are cross-property generalization under distribution shifts and cross-molecule generalization under structural heterogeneity. The article presents a unified taxonomy to categorize existing methods for FSMPP based on data, model, and learning paradigm levels. It compares representative methods, discusses benchmark datasets, and evaluation protocols. The article concludes by highlighting key trends and suggesting future directions for research in FSMPP. <div>
arXiv:2510.08900v1 Announce Type: new 
Abstract: AI-assisted molecular property prediction has become a promising technique in early-stage drug discovery and materials design in recent years. However, due to high-cost and complex wet-lab experiments, real-world molecules usually experience the issue of scarce annotations, leading to limited labeled data for effective supervised AI model learning. In light of this, few-shot molecular property prediction (FSMPP) has emerged as an expressive paradigm that enables learning from only a few labeled examples. Despite rapidly growing attention, existing FSMPP studies remain fragmented, without a coherent framework to capture methodological advances and domain-specific challenges. In this work, we present the first comprehensive and systematic survey of few-shot molecular property prediction. We begin by analyzing the few-shot phenomenon in molecular datasets and highlighting two core challenges: (1) cross-property generalization under distribution shifts, where each task corresponding to each property, may follow a different data distribution or even be inherently weakly related to others from a biochemical perspective, requiring the model to transfer knowledge across heterogeneous prediction tasks, and (2) cross-molecule generalization under structural heterogeneity, where molecules involved in different or same properties may exhibit significant structural diversity, making model difficult to achieve generalization. Then, we introduce a unified taxonomy that organizes existing methods into data, model, and learning paradigm levels, reflecting their strategies for extracting knowledge from scarce supervision in few-shot molecular property prediction. Next, we compare representative methods, summarize benchmark datasets and evaluation protocols. In the end, we identify key trends and future directions for advancing the continued research on FSMPP.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Uncertainty Sets: Dependence of Uncertain Parameters via a Simple Polyhedral Set</title>
<link>https://arxiv.org/abs/2510.08843</link>
<guid>https://arxiv.org/abs/2510.08843</guid>
<content:encoded><![CDATA[
<div> Algorithm, Robust optimization, Uncertainty set, Pairwise differences, Numerical experiments <br /> 
<br />Summary: 
The article introduces a novel polyhedral uncertainty set called the smooth uncertainty set for robust optimization. This set captures dependencies between uncertain parameters by constraining their pairwise differences. The bounds on these differences can be determined based on expert knowledge or correlations. Specialized solution methods are explored, including compact reformulations and a column generation algorithm. Numerical experiments show that the smooth uncertainty set model performs similarly to the ellipsoidal uncertainty model but with shorter running times. Additionally, a column-generation algorithm proves to be more efficient than traditional approaches in terms of solution time and memory consumption. <div>
arXiv:2510.08843v1 Announce Type: cross 
Abstract: We propose a novel polyhedral uncertainty set for robust optimization, termed the smooth uncertainty set, which captures dependencies of uncertain parameters by constraining their pairwise differences. The bounds on these differences may be dictated by the underlying physics of the problem and may be expressed by domain experts. When correlations are available, the bounds can be set
  to ensure that the associated probabilistic constraints are satisfied for any given probability. We explore specialized solution methods for the resulting optimization problems, including compact reformulations that exploit special structures when
  they appear, a column generation algorithm, and a reformulation of the adversarial problem as a minimum-cost flow problem. Our numerical experiments, based on problems from literature, illustrate (i) that the performance of the smooth uncertainty set model solution is similar to that of the ellipsoidal uncertainty model solution, albeit, it is computed within significantly shorter running times, and (ii) our column-generation algorithm can outperform the classical cutting plane algorithm and dualized reformulation, respectively in terms of solution time and memory consumption.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-fidelity Batch Active Learning for Gaussian Process Classifiers</title>
<link>https://arxiv.org/abs/2510.08865</link>
<guid>https://arxiv.org/abs/2510.08865</guid>
<content:encoded><![CDATA[
<div> Gaussian Process, Multi-fidelity, Batch Active Learning, Mutual Information, Binary Simulation Output  
Summary:  
- The study focuses on optimizing simulation budgets using a multi-fidelity approach with a Gaussian Process (GP) model in binary simulation output scenarios.
- Bernoulli Parameter Mutual Information (BPMI) is introduced as a batch active learning algorithm for multi-fidelity GP classifiers, improving efficiency in parameter space exploration.
- BPMI addresses the challenge of calculating mutual information in probability space by utilizing a first-order Taylor expansion of the link function.
- Evaluation on synthetic and real-world simulation cases, such as a laser-ignited rocket combustor, shows that BPMI outperforms baseline methods by achieving higher predictive accuracy within a fixed computational budget.
- The results demonstrate the effectiveness of BPMI in accelerating the exploration of parameter space in expensive computational simulations. 

Summary: <div>
arXiv:2510.08865v1 Announce Type: cross 
Abstract: Many science and engineering problems rely on expensive computational simulations, where a multi-fidelity approach can accelerate the exploration of a parameter space. We study efficient allocation of a simulation budget using a Gaussian Process (GP) model in the binary simulation output case. This paper introduces Bernoulli Parameter Mutual Information (BPMI), a batch active learning algorithm for multi-fidelity GP classifiers. BPMI circumvents the intractability of calculating mutual information in the probability space by employing a first-order Taylor expansion of the link function. We evaluate BPMI against several baselines on two synthetic test cases and a complex, real-world application involving the simulation of a laser-ignited rocket combustor. In all experiments, BPMI demonstrates superior performance, achieving higher predictive accuracy for a fixed computational budget.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs</title>
<link>https://arxiv.org/abs/2510.08886</link>
<guid>https://arxiv.org/abs/2510.08886</guid>
<content:encoded><![CDATA[
<div> benchmark, financial auditing, LLMs, structured reasoning, taxonomy<br />
Summary:<br />
The article introduces FinAuditing, a benchmark for evaluating large language models (LLMs) on financial auditing tasks. It focuses on structured auditing reasoning using real US-GAAP-compliant XBRL filings, defining subtasks for semantic, relational, and numerical consistency. The evaluation framework integrates metrics across these subtasks. Experiments on 13 LLMs show inconsistent performance, particularly in reasoning over hierarchical structures, with accuracy drops up to 60-90%. The study reveals limitations in current LLMs for financial reasoning and highlights the need for structure-aware systems. FinAuditing aims to develop reliable financial intelligence systems aligned with regulations. The benchmark dataset is accessible through Hugging Face. 

<br />Summary: <div>
arXiv:2510.08886v1 Announce Type: cross 
Abstract: The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creation of the Chinese Adaptive Policy Communication Corpus</title>
<link>https://arxiv.org/abs/2510.08986</link>
<guid>https://arxiv.org/abs/2510.08986</guid>
<content:encoded><![CDATA[
<div> Keywords: CAPC-CG, Chinese policy directives, annotation, adaptive policy communication, NLP research<br />
Summary:<br />
- The Chinese Adaptive Policy Communication (Central Government) Corpus (CAPC-CG) is introduced as the first open dataset of Chinese policy directives annotated with a five-color taxonomy of clear and ambiguous language categories.
- The corpus spans from 1949 to 2023, including national laws, administrative regulations, and ministerial rules issued by China's top authorities, totaling 3.3 million units.
- Comprehensive metadata, a two-round labeling framework, and a gold-standard annotation set developed by expert and trained coders are provided alongside the corpus.
- Inter-annotator agreement achieves high reliability for supervised modeling, indicated by Fleiss's kappa of K = 0.86 on directive labels.
- Baseline classification results using large language models (LLMs) are presented, along with an annotation codebook and patterns from the dataset. This release aims to support downstream tasks and multilingual NLP research in policy communication.<br /> 
Summary: <div>
arXiv:2510.08986v1 Announce Type: cross 
Abstract: We introduce CAPC-CG, the Chinese Adaptive Policy Communication (Central Government) Corpus, the first open dataset of Chinese policy directives annotated with a five-color taxonomy of clear and ambiguous language categories, building on Ang's theory of adaptive policy communication. Spanning 1949-2023, this corpus includes national laws, administrative regulations, and ministerial rules issued by China's top authorities. Each document is segmented into paragraphs, producing a total of 3.3 million units. Alongside the corpus, we release comprehensive metadata, a two-round labeling framework, and a gold-standard annotation set developed by expert and trained coders. Inter-annotator agreement achieves a Fleiss's kappa of K = 0.86 on directive labels, indicating high reliability for supervised modeling. We provide baseline classification results with several large language models (LLMs), together with our annotation codebook, and describe patterns from the dataset. This release aims to support downstream tasks and multilingual NLP research in policy communication.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised full-field Bayesian inference of orthotropic hyperelasticity from a single biaxial test: a myocardial case study</title>
<link>https://arxiv.org/abs/2510.09498</link>
<guid>https://arxiv.org/abs/2510.09498</guid>
<content:encoded><![CDATA[
<div> discovery, constitutive models, parameter identification, orthotropic, uncertainty quantification <br />
<br />
Summary: In the study, the authors address the challenge of capturing complex tissue behavior in traditional testing methods by using heterogeneous deformation profiles. They employ the EUCLID method, along with Bayesian inference and three-dimensional continuum elements, to identify material model parameters for highly nonlinear, orthotropic constitutive models. The approach demonstrates success in quantitatively inferring these parameters from a single heterogeneous biaxial stretch test, even in the presence of noise. The results show good agreement with ground-truth simulations and credibility intervals, highlighting the potential for characterizing such material models with uncertainty quantification from a single test. This method offers a promising avenue for efficient parameter estimation in tissue testing, reducing the need for multiple samples and extensive manipulations. <div>
arXiv:2510.09498v1 Announce Type: cross 
Abstract: Fully capturing this behavior in traditional homogenized tissue testing requires the excitation of multiple deformation modes, i.e. combined triaxial shear tests and biaxial stretch tests. Inherently, such multimodal experimental protocols necessitate multiple tissue samples and extensive sample manipulations. Intrinsic inter-sample variability and manipulation-induced tissue damage might have an adverse effect on the inversely identified tissue behavior. In this work, we aim to overcome this gap by focusing our attention to the use of heterogeneous deformation profiles in a parameter estimation problem. More specifically, we adapt EUCLID, an unsupervised method for the automated discovery of constitutive models, towards the purpose of parameter identification for highly nonlinear, orthotropic constitutive models using a Bayesian inference approach and three-dimensional continuum elements. We showcase its strength to quantitatively infer, with varying noise levels, the material model parameters of synthetic myocardial tissue slabs from a single heterogeneous biaxial stretch test. This method shows good agreement with the ground-truth simulations and with corresponding credibility intervals. Our work highlights the potential for characterizing highly nonlinear and orthotropic material models from a single biaxial stretch test with uncertainty quantification.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Rigorous Modeling of Antenna--Medium Interactions Above Planar Stratified Media via the Generalized Scattering Matrix</title>
<link>https://arxiv.org/abs/2504.12613</link>
<guid>https://arxiv.org/abs/2504.12613</guid>
<content:encoded><![CDATA[
<div> Keywords: reflection coefficients, antennas, planar layered media, spherical vector wave functions, numerical implementation <br />
Summary: 
A rigorous method for evaluating reflection coefficients of antennas above planar layered media is presented. The approach uses the antenna's generalized scattering matrix (GSM) expressed in spherical vector wave functions (SVWFs) and spherical-to-planar vector wave transformations to model interaction with the layered structure. This formulation reduces algebraic complexity, enabling fast numerical implementation with high accuracy. Each evaluation for a specific configuration can be completed within milliseconds, offering significant speed improvement over full-wave solvers like FEKO. The method is suitable for real-time electromagnetic characterization and inverse modeling in planar layered environments. <br /><br />Summary: <div>
arXiv:2504.12613v2 Announce Type: replace 
Abstract: A rigorous and computationally efficient method is presented for evaluating the reflection coefficients of antennas operating above planar layered media. The approach reformulates the problem within the framework of the antenna's generalized scattering matrix (GSM), expressed in terms of spherical vector wave functions (SVWFs). The mutual interaction between the antenna and the layered structure is modeled through spherical-to-planar vector wave transformations that incorporate the exact Fresnel reflection response of the medium, without introducing any simplifying approximations. This formulation dramatically reduces algebraic complexity and enables fast, stable numerical implementation. Excluding the one-time preprocessing required to obtain the antenna's free-space GSM, each evaluation for a given layered configuration can be completed within milliseconds -- achieving several orders of magnitude speed improvement over full-wave solvers such as FEKO, while maintaining virtually identical accuracy. The proposed framework thus provides a powerful foundation for real-time electromagnetic characterization and inverse modeling involving planar layered environments.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IKNet: Interpretable Stock Price Prediction via Keyword-Guided Integration of News and Technical Indicators</title>
<link>https://arxiv.org/abs/2510.07661</link>
<guid>https://arxiv.org/abs/2510.07661</guid>
<content:encoded><![CDATA[
<div> interpretability, stock forecasting, news articles, contextual analysis, FinBERT <br />
Summary: 
The article introduces a new forecasting model, IKNet, which focuses on the interpretability of stock price movements based on news articles. IKNet utilizes keyword analysis to identify salient keywords through FinBERT-based contextual analysis. It then integrates these keywords with technical indicators to predict next-day closing prices. The model outperforms existing models, reducing RMSE by 32.9% and improving cumulative returns by 18.5% on S&amp;P 500 data from 2015 to 2024. IKNet not only provides accurate predictions but also offers quantifiable and interpretable attributions for the impact of each keyword on the forecasts. This approach enhances transparency by offering context-aware explanations for volatility events driven by public sentiment. <div>
arXiv:2510.07661v1 Announce Type: new 
Abstract: The increasing influence of unstructured external information, such as news articles, on stock prices has attracted growing attention in financial markets. Despite recent advances, most existing newsbased forecasting models represent all articles using sentiment scores or average embeddings that capture the general tone but fail to provide quantitative, context-aware explanations of the impacts of public sentiment on predictions. To address this limitation, we propose an interpretable keyword-guided network (IKNet), which is an explainable forecasting framework that models the semantic association between individual news keywords and stock price movements. The IKNet identifies salient keywords via FinBERTbased contextual analysis, processes each embedding through a separate nonlinear projection layer, and integrates their representations with the time-series data of technical indicators to forecast next-day closing prices. By applying Shapley Additive Explanations the model generates quantifiable and interpretable attributions for the contribution of each keyword to predictions. Empirical evaluations of S&amp;P 500 data from 2015 to 2024 demonstrate that IKNet outperforms baselines, including recurrent neural networks and transformer models, reducing RMSE by up to 32.9% and improving cumulative returns by 18.5%. Moreover, IKNet enhances transparency by offering contextualized explanations of volatility events driven by public sentiment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Forecasting of Network Dynamics through Weight Flow Matching</title>
<link>https://arxiv.org/abs/2510.07957</link>
<guid>https://arxiv.org/abs/2510.07957</guid>
<content:encoded><![CDATA[
<div> forecasting, network systems, deep learning, coefficient shift, zero-shot accuracy

Summary:
- Forecasting state evolution of network systems is crucial for effective policy interventions and resource management.
- Deep learning models face challenges in adapting to shifting propagation dynamics without retraining.
- Zero-Shot Forecasting of Network Dynamics through Weight Flow Matching (FNFM) is presented as a solution.
- FNFM is a generative framework that generates dynamic model weights for unseen coefficients, enabling zero-shot forecasting.
- The framework utilizes a Variational Encoder and Conditional Flow Matching module to achieve accurate predictions without gradient-based optimization.
- Empirical results show that FNFM outperforms baseline methods, especially under significant coefficient shifts. 

<br /><br />Summary: <div>
arXiv:2510.07957v1 Announce Type: new 
Abstract: Forecasting state evolution of network systems, such as the spread of information on social networks, is significant for effective policy interventions and resource management. However, the underlying propagation dynamics constantly shift with new topics or events, which are modeled as changing coefficients of the underlying dynamics. Deep learning models struggle to adapt to these out-of-distribution shifts without extensive new data and retraining. To address this, we present Zero-Shot Forecasting of Network Dynamics through Weight Flow Matching (FNFM), a generative, coefficient-conditioned framework that generates dynamic model weights for an unseen target coefficient, enabling zero-shot forecasting. Our framework utilizes a Variational Encoder to summarize the forecaster weights trained in observed environments into compact latent tokens. A Conditional Flow Matching (CFM) module then learns a continuous transport from a simple Gaussian distribution to the empirical distribution of these weights, conditioned on the dynamical coefficients. This process is instantaneous at test time and requires no gradient-based optimization. Across varied dynamical coefficients, empirical results indicate that FNFM yields more reliable zero-shot accuracy than baseline methods, particularly under pronounced coefficient shift.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse Supply Chain Network Design of a Polyurethane Waste Upcycling System</title>
<link>https://arxiv.org/abs/2510.08097</link>
<guid>https://arxiv.org/abs/2510.08097</guid>
<content:encoded><![CDATA[
<div> Keywords: supply chain, plastic waste, upcycling, mathematical programming, economic potential <br />
Summary:<br />
This paper presents a mathematical programming framework for optimizing supply chain infrastructures for upcycling plastic waste, using a multi-product, multi-echelon, multi-period mixed-integer linear programming model. The objective is to minimize costs from waste collection to high-value polymer production, considering various constraints. The framework aids in strategic planning by determining optimal facility numbers, sizes, and material transportation. A case study on rigid polyurethane foam waste upcycling in Germany shows economic feasibility is currently lacking compared to fossil-based feedstock. However, with appropriate incentives, there is potential for competitive value chains once technology and economic frameworks stabilize. <br /><br />Summary: <div>
arXiv:2510.08097v1 Announce Type: new 
Abstract: This paper presents a general mathematical programming framework for the design and optimization of supply chain infrastructures for the upcycling of plastic waste. For this purpose, a multi-product, multi-echelon, multi-period mixed-integer linear programming (MILP) model has been formulated. The objective is to minimize the cost of the entire circular supply chain starting from the collection of post-consumer plastic waste to the production of virgin-equivalent high value polymers, satisfying a large number of constraints from collection quota to the quality of the feedstock. The framework aims to support the strategic planning of future circular supply chains by determining the optimal number, locations and sizes of various types of facilities as well as the amounts of materials to be transported between the nodes of the supply chain network over a specified period. The functionality of the framework has been tested with a case study for the upcycling of rigid polyurethane foam waste coming from construction sites in Germany. The economic potential and infrastructure requirements are evaluated, and it has been found that from a solely economic perspective, the current status of the value chain is not competitive with fossil-based feedstock or incineration. However, with the right economic incentives, there is a considerable potential to establish such value chains, once the upcycling technology is ready and the economic framework conditions have stabilized.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Poisson Energy Formulation for Floorplanning: Variational Analysis and Mathematical Foundations</title>
<link>https://arxiv.org/abs/2510.08126</link>
<guid>https://arxiv.org/abs/2510.08126</guid>
<content:encoded><![CDATA[
<div> floorplanning, placement, Poisson energy, spectral framework, optimization

Summary:
This paper introduces a variational and spectral framework for Poisson energy-based floorplanning and placement in physical design, focusing on arranging modules in VLSI circuits without overlap. The Poisson energy is derived as the squared H^{-1} Sobolev norm of the density residual, acting as a low-pass filter to enforce uniformity. It serves as a smooth surrogate for nonoverlap constraints, with a lower bound relating it to geometric overlap area. Projected gradient descent converges globally to stationary points and exhibits local linear convergence near regular minima. The continuous-time dynamics are interpreted as a Wasserstein-2 gradient flow, highlighting nonlocality and global balancing behavior. These findings provide a mathematically principled foundation for PDE-regularized optimization in large-scale floorplanning and related geometric layout problems.


<br /><br />Summary: <div>
arXiv:2510.08126v1 Announce Type: new 
Abstract: Arranging many modules within a bounded domain without overlap, central to the Electronic Design Automation (EDA) of very large-scale integrated (VLSI) circuits, represents a broad class of discrete geometric optimization problems with physical constraints. This paper develops a variational and spectral framework for Poisson energy-based floorplanning and placement in physical design. We show that the Poisson energy, defined via a Neumann Poisson equation, is exactly the squared H^{-1} Sobolev norm of the density residual, providing a functional-analytic interpretation of the classical electrostatic analogy. Through spectral analysis, we demonstrate that the energy acts as an intrinsic low-pass filter, suppressing high-frequency fluctuations while enforcing large-scale uniformity. Under a mild low-frequency dominance assumption, we establish a quantitative linear lower bound relating the Poisson energy to the geometric overlap area, thereby justifying its use as a smooth surrogate for the hard nonoverlap constraint. We further show that projected gradient descent converges globally to stationary points and exhibits local linear convergence near regular minima. Finally, we interpret the continuous-time dynamics as a Wasserstein-2 gradient flow, revealing the intrinsic nonlocality and global balancing behavior of the model. These results provide a mathematically principled foundation for PDE-regularized optimization in large-scale floorplanning and related geometric layout problems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design of chemical recycling processes for PUR foam under uncertainty</title>
<link>https://arxiv.org/abs/2510.08301</link>
<guid>https://arxiv.org/abs/2510.08301</guid>
<content:encoded><![CDATA[
<div> Keywords: Optimization, Chemical process design, Uncertainty, Process simulation software, Evolutionary strategy

Summary:
- Optimization problems in chemical process design involve both discrete and continuous decisions, especially when considering uncertainties.
- The combination of commercial process simulation software with an evolutionary strategy can help address the complexity of design problems.
- Two-stage optimization problems, where some decisions are fixed at the design stage and others can be adapted during plant operation, pose challenges in exploration.
- The proposed algorithm outperformed a manually designed robust process for designing a downstream process for isolating valuable products from pyrolysis oil.
- Analysis of different scenarios provided valuable insights into potential changes in the overall layout of the recycling process. 

<br /><br />Summary: <div>
arXiv:2510.08301v1 Announce Type: new 
Abstract: Optimization problems in chemical process design involve a significant number of discrete and continuous decisions. When taking into account uncertainties, the search space is very difficult to explore, even for experienced engineers. Moreover, it should be taken into account that while some decisions are fixed at the design stage, other parameters can be adapted to the realization of the uncertainty during the operation of the plant. This leads to a two-stage optimization problem which is difficult to solve. To address this challenge, we propose to combine commercial process simulation software with an evolutionary strategy. This approach is applied to designing a downstream process to isolate valuable products from pyrolysis oil produced by the catalytic pyrolysis of rigid polyurethane foam. The suggested algorithm consistently performed better than a manually designed robust process. Additionally, the analysis of different scenarios provided insight into promising changes in the overall layout of the recycling process.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing the Value-at-Risk of Loan Portfolio via Deep Neural Networks</title>
<link>https://arxiv.org/abs/2510.07444</link>
<guid>https://arxiv.org/abs/2510.07444</guid>
<content:encoded><![CDATA[
<div> Neural network, peer-to-peer lending, risk management, Value-at-Risk, Conditional Value-at-Risk
Summary:
- Risk management in peer-to-peer lending is crucial, with diversification being an effective strategy to reduce exposure.
- Two deep neural network models, DeNN and DSNN, are proposed to minimize the Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) of loan portfolios.
- The models predict default probabilities and time to default, leading to significant reductions in portfolio VaRs at different confidence levels compared to benchmarks.
- The low degree of freedom model, DeNN, demonstrates superior performance over DSNN in most scenarios.
- Implementing these neural network models can enhance risk mitigation and decision-making in peer-to-peer lending environments. 
<br /><br />Summary: <div>
arXiv:2510.07444v1 Announce Type: cross 
Abstract: Risk management is a prominent issue in peer-to-peer lending. An investor may naturally reduce his risk exposure by diversifying instead of putting all his money on one loan. In that case, an investor may want to minimize the Value-at-Risk (VaR) or Conditional Value-at-Risk (CVaR) of his loan portfolio. We propose a low degree of freedom deep neural network model, DeNN, as well as a high degree of freedom model, DSNN, to tackle the problem. In particular, our models predict not only the default probability of a loan but also the time when it will default. The experiments demonstrate that both models can significantly reduce the portfolio VaRs at different confidence levels, compared to benchmarks. More interestingly, the low degree of freedom model, DeNN, outperforms DSNN in most scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Meet Virtual Cell: A Survey</title>
<link>https://arxiv.org/abs/2510.07706</link>
<guid>https://arxiv.org/abs/2510.07706</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, virtual cell modeling, cellular representation, perturbation prediction, gene regulation inference

Summary:<br /><br />Large language models (LLMs) are revolutionizing cellular biology by enabling the creation of virtual cells through computational systems. This review categorizes existing methods into two paradigms: LLMs as Oracles for direct cellular modeling and LLMs as Agents for orchestrating complex scientific tasks. The core tasks identified include cellular representation, perturbation prediction, and gene regulation inference. The review covers models, datasets, evaluation benchmarks, and challenges such as scalability, generalizability, and interpretability. This comprehensive overview highlights the transformative potential of LLMs in cellular biology and showcases the advancements in virtual cell modeling enabled by these models. <div>
arXiv:2510.07706v1 Announce Type: cross 
Abstract: Large language models (LLMs) are transforming cellular biology by enabling the development of "virtual cells"--computational systems that represent, predict, and reason about cellular states and behaviors. This work provides a comprehensive review of LLMs for virtual cell modeling. We propose a unified taxonomy that organizes existing methods into two paradigms: LLMs as Oracles, for direct cellular modeling, and LLMs as Agents, for orchestrating complex scientific tasks. We identify three core tasks--cellular representation, perturbation prediction, and gene regulation inference--and review their associated models, datasets, evaluation benchmarks, as well as the critical challenges in scalability, generalizability, and interpretability.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfitted hybrid high-order methods stabilized by polynomial extension for elliptic interface problems</title>
<link>https://arxiv.org/abs/2503.11397</link>
<guid>https://arxiv.org/abs/2503.11397</guid>
<content:encoded><![CDATA[
<div> Hybrid High-Order Method, Unfitted Meshes, Polynomial Unknowns, Interface, Gradient Reconstruction Operator <br />
<br />
Summary: <br />
This work introduces a novel hybrid high-order (HHO) method for unfitted meshes, utilizing polynomial unknowns attached to mesh faces and cells. In this method, the interface can traverse mesh cells in a general manner, with doubled polynomial unknowns in cut cells and faces. To address ill-conditioning issues from small cut cells, the method employs polynomial extensions in the gradient reconstruction operator. Stability and consistency are proven, yielding optimal error estimates. Numerical experiments validate the theory's applicability. <div>
arXiv:2503.11397v2 Announce Type: replace-cross 
Abstract: In this work, we study the design and analysis of a novel hybrid high-order (HHO) method on unfitted meshes. HHO methods rely on a pair of unknowns, combining polynomials attached to the mesh faces and the mesh cells. In the unfitted framework, the interface can cut through the mesh cells in a very general fashion, and the polynomial unknowns are doubled in the cut cells and the cut faces. In order to avoid the ill-conditioning issues caused by the presence of small cut cells, the novel approach introduced herein is to use polynomial extensions in the definition of the gradient reconstruction operator. Stability and consistency results are established, leading to optimally decaying error estimates. The theory is illustrated by numerical experiments.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-Enhanced Reinforcement Learning for Dynamic Portfolio Optimization</title>
<link>https://arxiv.org/abs/2510.06466</link>
<guid>https://arxiv.org/abs/2510.06466</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, dynamic portfolio optimization, Dirichlet policy, cross-sectional attention mechanisms, portfolio management<br />
<br />
Summary: 
This article presents a deep reinforcement learning framework for dynamic portfolio optimization. The framework combines a Dirichlet policy with cross-sectional attention mechanisms to ensure feasible portfolio weights, natural handling of tradability constraints, and stable exploration of the allocation space. By integrating per-asset temporal encoders with a global attention layer, the model can capture sector relationships, factor spillovers, and other cross-asset dependencies. The reward function considers transaction costs and portfolio variance penalties, linking the learning objective to traditional mean variance trade-offs. Results demonstrate that attention-based Dirichlet policies outperform equal-weight and standard reinforcement learning benchmarks in terms of terminal wealth and Sharpe ratio, while maintaining realistic turnover and drawdown levels. This study highlights the benefits of combining principled action design with attention-based representations for improving the stability and interpretability of reinforcement learning in portfolio management. <br /><br /> <div>
arXiv:2510.06466v1 Announce Type: new 
Abstract: We develop a deep reinforcement learning framework for dynamic portfolio optimization that combines a Dirichlet policy with cross-sectional attention mechanisms. The Dirichlet formulation ensures that portfolio weights are always feasible, handles tradability constraints naturally, and provides a stable way to explore the allocation space. The model integrates per-asset temporal encoders with a global attention layer, allowing it to capture sector relationships, factor spillovers, and other cross asset dependencies. The reward function includes transaction costs and portfolio variance penalties, linking the learning objective to traditional mean variance trade offs. The results show that attention based Dirichlet policies outperform equal-weight and standard reinforcement learning benchmarks in terms of terminal wealth and Sharpe ratio, while maintaining realistic turnover and drawdown levels. Overall, the study shows that combining principled action design with attention-based representations improves both the stability and interpretability of reinforcement learning for portfolio management.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Higher-Order Time Domain Boundary Element Formulation based on Isogeometric Analysis and the Convolution Quadrature Method</title>
<link>https://arxiv.org/abs/2510.06804</link>
<guid>https://arxiv.org/abs/2510.06804</guid>
<content:encoded><![CDATA[
<div> isogeometric boundary element method, scattering problems, wave equation, elastodynamics, convolution quadratures

Summary:
The article presents an isogeometric boundary element method for solving scattering problems in isotropic homogeneous media, focusing on wave problems governed by the scalar wave equation and the Lam\'e-Navier equations for elastodynamics. The method approximates time-dependent convolution integrals using multi-stage Runge-Kutta convolution quadratures based on steady-state solutions in the Laplace domain. Spatial discretization is done using isogeometric analysis with a patchwise smooth spline basis. The implementation scheme uses local, uniform Bernstein polynomials as basis functions and localizes them on the elements defined by the non-empty knot spans in the knot vectors. The solutions of mixed problems are approximated using a symmetric Galerkin variational formulation and a collocation method. Convergence rates of the approximations are investigated in a mixed space and time error norm. <div>
arXiv:2510.06804v1 Announce Type: new 
Abstract: An isogeometric boundary element method (BEM) is presented to solve scattering problems in an isotropic homogeneous medium. We consider wave problems governed by the scalar wave equation as in acoustics and the Lam\'e-Navier equations for elastodynamics considering the theory of linear elasticity. The underlying boundary integral equations imply time-dependent convolution integrals and allow us to determine the sought quantities in the bounded interior or the unbounded exterior after solving for the unknown Cauchy data. In the present work, the time-dependent convolution integrals are approximated by multi-stage Runge-Kutta (RK) based convolution quadratures that involve steady-state solutions in the Laplace domain. The proposed method discretizes the spatial variables in the framework of isogeometric analysis (IGA), entailing a patchwise smooth spline basis. Overall, it enables high convergence rates in space and time. The implementation scheme follows an element structure defined by the non-empty knot spans in the knot vectors and local, uniform Bernstein polynomials as basis functions. The algorithms to localize the basis functions on the elements are outlined and explained. The solutions of the mixed problems are approximated by the BEM based on a symmetric Galerkin variational formulation and a collocation method. We investigate convergence rates of the approximative solutions in a mixed space and time error norm.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Measuring How News Topics Drive Stock Movement</title>
<link>https://arxiv.org/abs/2510.06864</link>
<guid>https://arxiv.org/abs/2510.06864</guid>
<content:encoded><![CDATA[
<div> framework, news topics, stock price movements, sentiment analysis, financial markets

Summary: 
This paper introduces a novel framework that analyzes the impact of different news topics on stock price movements in financial markets. By encoding news headlines into dense semantic embeddings and clustering them into distinct topics, the framework uncovers the specific themes that influence daily stock returns. Through an ordinary least squares regression, the framework quantifies the effects of these topics on next-day stock returns. Applied to Apple Inc., the framework identifies certain topics that significantly affect stock returns, while others show no measurable impact. These findings underscore the significance of conducting topic-level analysis to understand the relationship between news content and market responses. The proposed framework offers a scalable approach for researchers and practitioners to evaluate the informational value of news topics, suggesting a promising avenue for enhancing predictive models of stock price movements. 

<br /><br />Summary: <div>
arXiv:2510.06864v1 Announce Type: new 
Abstract: In modern financial markets, news plays a critical role in shaping investor sentiment and influencing stock price movements. However, most existing studies aggregate daily news sentiment into a single score, potentially overlooking important variations in topic content and relevance. This simplification may mask nuanced relationships between specific news themes and market responses. To address this gap, this paper proposes a novel framework to examine how different news topics influence stock price movements. The framework encodes individual news headlines into dense semantic embeddings using a pretrained sentence transformer, then applies K-means clustering to identify distinct news topics. Topic exposures are incorporated as explanatory variables in an ordinary least squares regression to quantify their impact on daily stock returns. Applied to Apple Inc., the framework reveals that certain topics are significantly associated with positive or negative next-day returns, while others have no measurable effect. These findings highlight the importance of topic-level analysis in understanding the relationship between news content and financial markets. The proposed framework provides a scalable approach for both researchers and practitioners to assess the informational value of different news topics and suggests a promising direction for improving predictive models of stock price movement.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOMATOES: Topology and Material Optimization for Latent Heat Thermal Energy Storage Devices</title>
<link>https://arxiv.org/abs/2510.07057</link>
<guid>https://arxiv.org/abs/2510.07057</guid>
<content:encoded><![CDATA[
<div> optimize, latent heat thermal energy storage, topology optimization, material design, phase change material

Summary:
The article introduces a new approach for optimizing latent heat thermal energy storage (LHTES) systems by concurrently optimizing material choice and topology. Traditional topology optimization methods focus on optimizing geometry for pre-selected materials, limiting the exploration of novel materials. To address this limitation, the authors develop an automated design framework that integrates material databases using a data-driven variational autoencoder (VAE) to project discrete materials onto continuous latent spaces. This approach enables the co-design of material and geometry for LHTES systems, leading to improved performance. The framework is demonstrated on a problem of maximizing discharged energy within a specified time while considering cost constraints. Several illustrative examples validate the effectiveness of the proposed method. The novel approach offers a promising solution for developing efficient and cost-effective LHTES systems. 

<br /><br />Summary: <div>
arXiv:2510.07057v1 Announce Type: new 
Abstract: Latent heat thermal energy storage (LHTES) systems are compelling candidates for energy storage, primarily owing to their high storage density. Improving their performance is crucial for developing the next-generation efficient and cost effective devices. Topology optimization (TO) has emerged as a powerful computational tool to design LHTES systems by optimally distributing a high-conductivity material (HCM) and a phase change material (PCM). However, conventional TO typically limits to optimizing the geometry for a fixed, pre-selected materials. This approach does not leverage the large and expanding databases of novel materials. Consequently, the co-design of material and geometry for LHTES remains a challenge and unexplored.
  To address this limitation, we present an automated design framework for the concurrent optimization of material choice and topology. A key challenge is the discrete nature of material selection, which is incompatible with the gradient-based methods used for TO. We overcome this by using a data-driven variational autoencoder (VAE) to project discrete material databases for both the HCM and PCM onto continuous and differentiable latent spaces. These continuous material representations are integrated into an end-to-end differentiable, transient nonlinear finite-element solver that accounts for phase change. We demonstrate this framework on a problem aimed at maximizing the discharged energy within a specified time, subject to cost constraints. The effectiveness of the proposed method is validated through several illustrative examples.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models</title>
<link>https://arxiv.org/abs/2510.06439</link>
<guid>https://arxiv.org/abs/2510.06439</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, hyperparameter tuning, uncertainty, stochastic models, computational engineering <br />
Summary: 
The paper introduces a new Bayesian optimization framework designed for hyperparameter tuning in uncertain systems. The approach focuses on optimizing scale- or precision-type parameters in stochastic models. By utilizing a statistical surrogate for the random variable, the method allows for analytical evaluation of the expectation operator. A closed-form expression for the optimizer of the random acquisition function is derived, reducing computational costs significantly. Compared to traditional Monte Carlo-based optimization schemes, the proposed method requires 40 times fewer data points, leading to a substantial reduction in computational expenses. The effectiveness of the approach is demonstrated through numerical examples in computational engineering, showcasing its potential for efficiently optimizing hyperparameters in uncertain systems. <br /> <div>
arXiv:2510.06439v1 Announce Type: cross 
Abstract: Hyperparameter tuning is a challenging problem especially when the system itself involves uncertainty. Due to noisy function evaluations, optimization under uncertainty can be computationally expensive. In this paper, we present a novel Bayesian optimization framework tailored for hyperparameter tuning under uncertainty, with a focus on optimizing a scale- or precision-type parameter in stochastic models. The proposed method employs a statistical surrogate for the underlying random variable, enabling analytical evaluation of the expectation operator. Moreover, we derive a closed-form expression for the optimizer of the random acquisition function, which significantly reduces computational cost per iteration. Compared with a conventional one-dimensional Monte Carlo-based optimization scheme, the proposed approach requires 40 times fewer data points, resulting in up to a 40-fold reduction in computational cost. We demonstrate the effectiveness of the proposed method through two numerical examples in computational engineering.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEAorta: A Fully Automated Framework for Finite Element Analysis of the Aorta From 3D CT Images</title>
<link>https://arxiv.org/abs/2510.06621</link>
<guid>https://arxiv.org/abs/2510.06621</guid>
<content:encoded><![CDATA[
<div> Keywords: Aortic aneurysm, biomechanics, Finite Element Analysis, PyTorch, deep neural network

Summary:<br />
Aortic aneurysm disease is a common cause of death in the U.S. population, with thoracic aortic aneurysm being a significant contributor. Rupture occurs when the stress on the aortic wall exceeds its strength, which can be determined through biomechanical analyses like Finite Element Analysis (FEA). However, current methods for patient-specific risk assessment face challenges due to manual segmentation and computational burden. To address these issues, a PyTorch FEA library and FEA-DNN integration framework were developed, significantly reducing computation time. This study focuses on overcoming the labor-intensive 3D reconstruction barrier by creating a deep neural network that can generate patient-specific finite element meshes directly from 3D CT images. The integration of deep learning and FEA techniques offers a promising solution for efficient and accurate risk assessment of thoracic aortic aneurysms. <br /><br /> <div>
arXiv:2510.06621v1 Announce Type: cross 
Abstract: Aortic aneurysm disease ranks consistently in the top 20 causes of death in the U.S. population. Thoracic aortic aneurysm is manifested as an abnormal bulging of thoracic aortic wall and it is a leading cause of death in adults. From the perspective of biomechanics, rupture occurs when the stress acting on the aortic wall exceeds the wall strength. Wall stress distribution can be obtained by computational biomechanical analyses, especially structural Finite Element Analysis. For risk assessment, probabilistic rupture risk of TAA can be calculated by comparing stress with material strength using a material failure model. Although these engineering tools are currently available for TAA rupture risk assessment on patient specific level, clinical adoption has been limited due to two major barriers: labor intensive 3D reconstruction current patient specific anatomical modeling still relies on manual segmentation, making it time consuming and difficult to scale to a large patient population, and computational burden traditional FEA simulations are resource intensive and incompatible with time sensitive clinical workflows. The second barrier was successfully overcome by our team through the development of the PyTorch FEA library and the FEA DNN integration framework. By incorporating the FEA functionalities within PyTorch FEA and applying the principle of static determinacy, we reduced the FEA based stress computation time to approximately three minutes per case. Moreover, by integrating DNN and FEA through the PyTorch FEA library, our approach further decreases the computation time to only a few seconds per case. This work focuses on overcoming the first barrier through the development of an end to end deep neural network capable of generating patient specific finite element meshes of the aorta directly from 3D CT images.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios</title>
<link>https://arxiv.org/abs/2510.07099</link>
<guid>https://arxiv.org/abs/2510.07099</guid>
<content:encoded><![CDATA[
<div> framework, portfolio optimisation, financial markets, Diffusion-Augmented Reinforcement Learning, DDPMs <br />
Summary:<br />
The article introduces a novel framework called Diffusion-Augmented Reinforcement Learning (DARL) for portfolio management in financial markets. It combines Denoising Diffusion Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) to better capture market dynamics and investor preferences. DARL uses DDPMs to generate synthetic market crash scenarios, enhancing training data robustness. Empirical testing shows DARL outperforms traditional methods, delivering higher risk-adjusted returns and resilience during crises such as the 2025 Tariff Crisis. This innovative approach offers a practical solution for enhancing stress resilience in DRL-driven financial applications. <br />Summary: <div>
arXiv:2510.07099v1 Announce Type: cross 
Abstract: In the ever-changing and intricate landscape of financial markets, portfolio optimisation remains a formidable challenge for investors and asset managers. Conventional methods often struggle to capture the complex dynamics of market behaviour and align with diverse investor preferences. To address this, we propose an innovative framework, termed Diffusion-Augmented Reinforcement Learning (DARL), which synergistically integrates Denoising Diffusion Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for portfolio management. By leveraging DDPMs to generate synthetic market crash scenarios conditioned on varying stress intensities, our approach significantly enhances the robustness of training data. Empirical evaluations demonstrate that DARL outperforms traditional baselines, delivering superior risk-adjusted returns and resilience against unforeseen crises, such as the 2025 Tariff Crisis. This work offers a robust and practical methodology to bolster stress resilience in DRL-driven financial applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Default Resilience and Worst-Case Effects in Financial Networks</title>
<link>https://arxiv.org/abs/2403.10631</link>
<guid>https://arxiv.org/abs/2403.10631</guid>
<content:encoded><![CDATA[
<div> price fluctuations, network of banks, default contagion, default resilience margin, systemic loss

Summary:
This paper explores the resilience of a network of banks to joint price fluctuations of shared external assets and the impact on default contagion. It introduces the concept of a default resilience margin, $\epsilon^*$, which represents the maximum amplitude of asset price fluctuations that the network can withstand without defaults occurring. The threshold value $\epsilon^*$ is determined by considering different measures of price fluctuations. When the perturbation exceeds $\epsilon^*$, defaults may occur, leading to the calculation of the worst-case systemic loss. This involves determining the total unpaid debt under the most severe price variation scenario. The analysis involves solving linear programming problems to compute the threshold level $\epsilon^*$ and evaluate the worst-case loss and corresponding asset price scenario. <div>
arXiv:2403.10631v2 Announce Type: replace-cross 
Abstract: In this paper we analyze the resilience of a network of banks to joint price fluctuations of the external assets in which they have shared exposures, and evaluate the worst-case effects of the possible default contagion. Indeed, when the prices of certain external assets either decrease or increase, all banks exposed to them experience varying degrees of simultaneous shocks to their balance sheets. These coordinated and structured shocks have the potential to exacerbate the likelihood of defaults. In this context, we introduce first a concept of {default resilience margin}, $\epsilon^*$, i.e., the maximum amplitude of asset prices fluctuations that the network can tolerate without generating defaults. Such threshold value is computed by considering two different measures of price fluctuations, one based on the maximum individual variation of each asset, and the other based on the sum of all the asset's absolute variations. For any price perturbation having amplitude no larger than $\epsilon^*$, the network absorbs the shocks remaining default free. When the perturbation amplitude goes beyond $\epsilon^*$, however, defaults may occur. In this case we find the worst-case systemic loss, that is, the total unpaid debt under the most severe price variation of given magnitude. Computation of both the threshold level $\epsilon^*$ and of the worst-case loss and of a corresponding worst-case asset price scenario, amounts to solving suitable linear programming problems.}
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Ensemble Topology (GET): A New Explicit and Inherently Smooth Framework for Manufacture-Ready Topology Optimization</title>
<link>https://arxiv.org/abs/2510.05572</link>
<guid>https://arxiv.org/abs/2510.05572</guid>
<content:encoded><![CDATA[
<div> Gaussian Ensemble Topology, explicit framework, topology optimization, anisotropic Gaussian functions, smooth designs<br />
<br />
Summary: 
The article introduces the Gaussian Ensemble Topology (GET) method for topology optimization, utilizing an explicit framework with anisotropic Gaussian functions to represent design geometries. The method combines Gaussian descriptions with a Heaviside projection to generate smooth designs without the need for post-processing steps. Validated on compliance-minimization and compliant mechanism benchmarks, GET produces optimized designs with comparable objective values to classical approaches but with refined boundaries. The framework offers advantages such as mesh independence, strong geometric expressiveness, and control over smoothness, discreteness, and complexity through parameter tuning. GET provides a robust and manufacture-ready solution for explicit topology optimization, allowing for the tackling of advanced and complex design problems. <br /><br /> <div>
arXiv:2510.05572v1 Announce Type: new 
Abstract: We introduce the Gaussian Ensemble Topology (GET) method, a new explicit and manufacture-ready framework for topology optimization in which design geometries are represented as superpositions of anisotropic Gaussian functions. By combining explicit Gaussian descriptions with a level-set-like Heaviside projection, GET inherently generates smooth, curvature-continuous designs without requiring post-processing steps such as mesh or corner smoothing and feature extraction. The method is validated on standard compliance-minimization and compliant mechanism benchmarks in two and three dimensions. The optimized designs achieve objective values comparable to those obtained with classical Moving Morphable Component (MMC) approaches, but with geometrically consistent, refined boundaries. Numerical examples demonstrate additional advantages of the GET framework, including mesh independence inherent to explicit parameterizations, strong geometric expressiveness, and effective control over smoothness, discreteness, and structural complexity through parameter tuning. As a robust and manufacture-ready approach to explicit topology optimization, GET opens avenues for tackling advanced and complex design problems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy</title>
<link>https://arxiv.org/abs/2510.05747</link>
<guid>https://arxiv.org/abs/2510.05747</guid>
<content:encoded><![CDATA[
<div> transformer, T-cell receptors, physicochemical descriptors, generative modeling, cellular therapy

Summary:
PhysicoGPTCR is a generative protein transformer model designed for rapid and accurate generation of T-cell receptor (TCR) sequences. It is dual-conditioned on peptide and HLA context and trained to incorporate physicochemical descriptors in the sequence generation process. The model outperforms baseline models like ANN, GPTCR, LSTM, and VAE in terms of edit-distance, similarity, and longest-common-subsequence scores, generating a more diverse set of TCR sequences. In in-silico experiments, PhysicoGPTCR demonstrates a higher proportion of binding-competent clones compared to baselines, showcasing the benefits of context conditioning and physicochemical awareness. This physics-grounded generative modeling approach significantly accelerates the design of functional TCR candidates, reducing the discovery timeline from months to minutes while maintaining wet-lab verifiability.<br /><br />Summary: <div>
arXiv:2510.05747v1 Announce Type: new 
Abstract: Physicochemically informed biological sequence generation has the potential to accelerate computer-aided cellular therapy, yet current models fail to \emph{jointly} ensure novelty, diversity, and biophysical plausibility when designing variable regions of T-cell receptors (TCRs). We present \textbf{PhysicoGPTCR}, a large generative protein Transformer that is \emph{dual-conditioned} on peptide and HLA context and trained to autoregressively synthesise TCR sequences while embedding residue-level physicochemical descriptors. The model is optimised on curated TCR--peptide--HLA triples with a maximum-likelihood objective and compared against ANN, GPTCR, LSTM, and VAE baselines. Across multiple neoantigen benchmarks, PhysicoGPTCR substantially improves edit-distance, similarity, and longest-common-subsequence scores, while populating a broader region of sequence space. Blind in-silico docking and structural modelling further reveal a higher proportion of binding-competent clones than the strongest baseline, validating the benefit of explicit context conditioning and physicochemical awareness. Experimental results demonstrate that dual-conditioned, physics-grounded generative modelling enables end-to-end design of functional TCR candidates, reducing the discovery timeline from months to minutes without sacrificing wet-lab verifiability.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Smell Detection via Pearson Correlation and ML Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2510.05835</link>
<guid>https://arxiv.org/abs/2510.05835</guid>
<content:encoded><![CDATA[
<div> ML algorithms, code smells, software quality analysis, SMOTE, feature selection
Summary:
This study introduces a machine learning (ML) based model to detect code smells in large-scale software systems. The model utilizes eight diverse ML algorithms such as XGBoost and AdaBoost, along with techniques like SMOTE for class imbalance and Pearson correlation for feature selection. The methodology includes preprocessing data, balancing the dataset with SMOTE, reducing redundancy with Pearson correlation, training ML algorithms, and tuning hyperparameters. AdaBoost, Random Forest, and XGBoost exhibit the best performance, achieving accuracies of 100%, 99%, and 99%, respectively. This approach offers a scalable solution for accurately identifying code smells, enhancing software quality assurance. Overall, the study showcases the effectiveness of a comprehensive ML approach in detecting code smells and improving software quality. 
<br /><br />Summary: <div>
arXiv:2510.05835v1 Announce Type: new 
Abstract: This study addresses the challenge of detecting code smells in large-scale software systems using machine learning (ML). Traditional detection methods often suffer from low accuracy and poor generalization across different datasets. To overcome these issues, we propose a machine learning-based model that automatically and accurately identifies code smells, offering a scalable solution for software quality analysis. The novelty of our approach lies in the use of eight diverse ML algorithms, including XGBoost, AdaBoost, and other classifiers, alongside key techniques such as the Synthetic Minority Over-sampling Technique (SMOTE) for class imbalance and Pearson correlation for efficient feature selection. These methods collectively improve model accuracy and generalization. Our methodology involves several steps: first, we preprocess the data and apply SMOTE to balance the dataset; next, Pearson correlation is used for feature selection to reduce redundancy; followed by training eight ML algorithms and tuning hyperparameters through Grid Search, Random Search, and Bayesian Optimization. Finally, we evaluate the models using accuracy, F-measure, and confusion matrices. The results show that AdaBoost, Random Forest, and XGBoost perform best, achieving accuracies of 100%, 99%, and 99%, respectively. This study provides a robust framework for detecting code smells, enhancing software quality assurance, and demonstrating the effectiveness of a comprehensive, optimized ML approach.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive comparison of neural operators for 3D industry-scale engineering designs</title>
<link>https://arxiv.org/abs/2510.05995</link>
<guid>https://arxiv.org/abs/2510.05995</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural operators, engineering design, comparison, datasets, performance

Summary:
Neural operators are widely used in engineering design for predicting complex dynamics. To facilitate model selection, six 3D industry-scale engineering design datasets have been standardized, covering various problem settings. These datasets are preprocessed and ready for use with different neural operator architectures. Four types of neural operator variants are compared, including Branch-Trunk-based, Graph-based, Grid-based, and Point-based models, each with practical enhancements for different engineering scenarios. The comparison evaluates predictive performance, computational efficiency, memory usage, and deployment complexity of each model. The study provides valuable insights for future neural operator development, guiding researchers in optimizing their models for engineering design applications. 

<br /><br />Summary: <div>
arXiv:2510.05995v1 Announce Type: new 
Abstract: Neural operators have emerged as powerful tools for learning nonlinear mappings between function spaces, enabling real-time prediction of complex dynamics in diverse scientific and engineering applications. With their growing adoption in engineering design evaluation, a wide range of neural operator architectures have been proposed for various problem settings. However, model selection remains challenging due to the absence of fair and comprehensive comparisons. To address this, we propose and standardize six representative 3D industry-scale engineering design datasets spanning thermal analysis, linear elasticity, elasto-plasticity, time-dependent plastic problems, and computational fluid dynamics. All datasets include fully preprocessed inputs and outputs for model training, making them directly usable across diverse neural operator architectures. Using these datasets, we conduct a systematic comparison of four types of neural operator variants, including Branch-Trunk-based Neural Operators inspired by DeepONet, Graph-based Neural Operators inspired by Graph Neural Networks, Grid-based Neural Operators inspired by Fourier Neural Operators, and Point-based Neural Operators inspired by PointNet. We further introduce practical enhancements to adapt these models to different engineering settings, improving the fairness of the comparison. Our benchmarking study evaluates each model strengths and limitations in terms of predictive performance, computational efficiency, memory usage, and deployment complexity. The findings provide actionable insights to guide future neural operator development.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models</title>
<link>https://arxiv.org/abs/2510.05121</link>
<guid>https://arxiv.org/abs/2510.05121</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, structured knowledge extraction, Economics, trade agreement texts, triples <br />
Summary: 
This study explores the effectiveness of Large Language Models (LLMs) in extracting structured knowledge, specifically Subject-Predicate-Object triples, focusing on the domain of Economics. The research investigates the use of LLMs for creating economic trade knowledge graphs from natural language legal trade agreement texts. By applying zero-shot, one-shot, and few-shot prompting techniques with positive and negative examples, the study evaluates the performance of the model in extracting trade-related information triples. The Llama 3.1 model is utilized to process unstructured regional trade agreement texts and extract triples, leading to insights on the challenges and future directions of using language models in economic applications. The findings emphasize the potential of LLMs in extracting structured knowledge in various scenarios, highlighting their significance in economic contexts. <br /><br />Summary: <div>
arXiv:2510.05121v1 Announce Type: cross 
Abstract: This study investigates the effectiveness of Large Language Models (LLMs) for the extraction of structured knowledge in the form of Subject-Predicate-Object triples. We apply the setup for the domain of Economics application. The findings can be applied to a wide range of scenarios, including the creation of economic trade knowledge graphs from natural language legal trade agreement texts. As a use case, we apply the model to regional trade agreement texts to extract trade-related information triples. In particular, we explore the zero-shot, one-shot and few-shot prompting techniques, incorporating positive and negative examples, and evaluate their performance based on quantitative and qualitative metrics. Specifically, we used Llama 3.1 model to process the unstructured regional trade agreement texts and extract triples. We discuss key insights, challenges, and potential future directions, emphasizing the significance of language models in economic applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auditing Algorithmic Bias in Transformer-Based Trading</title>
<link>https://arxiv.org/abs/2510.05140</link>
<guid>https://arxiv.org/abs/2510.05140</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer models, financial applications, risk, biases, prediction confidence

Summary:
The study focuses on auditing the reliance of transformer models in financial applications on volatile data and quantifies the impact of frequency of price movements on prediction confidence. Using a transformer model for prediction, the researchers introduce a metric based on Partial Information Decomposition (PID) to assess the influence of each asset on the model's decision-making process. The analysis uncovers two key findings: firstly, the model ignores data volatility completely; and secondly, it displays bias towards data exhibiting lower-frequency price movements. These observations shed light on potential gaps in the model's decision-making process in financial scenarios, highlighting a need for further exploration and refinement to mitigate risks and biases in transformer model applications. 

<br /><br />Summary: <div>
arXiv:2510.05140v1 Announce Type: cross 
Abstract: Transformer models have become increasingly popular in financial applications, yet their potential risk making and biases remain under-explored. The purpose of this work is to audit the reliance of the model on volatile data for decision-making, and quantify how the frequency of price movements affects the model's prediction confidence. We employ a transformer model for prediction, and introduce a metric based on Partial Information Decomposition (PID) to measure the influence of each asset on the model's decision making. Our analysis reveals two key observations: first, the model disregards data volatility entirely, and second, it is biased toward data with lower-frequency price movements.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2510.05158</link>
<guid>https://arxiv.org/abs/2510.05158</guid>
<content:encoded><![CDATA[
<div> Physics-informed neural networks (PINNs), a powerful method for solving partial differential equations (PDEs, can be labor-intensive and error-prone to construct. Lang-PINN, a multi-agent system driven by large language models, processes natural language task descriptions to build trainable PINNs. It consists of a PDE Agent, PINN Agent, Code Agent, and Feedback Agent to transform informal tasks into executable PINN code. Lang-PINN outperforms baselines by reducing mean squared error, improving execution success, and decreasing time overhead. Keywords: Physics-informed neural networks, PINN, Partial differential equations, Large language models, Multi-agent system<br /><br />Summary: Lang-PINN, powered by large language models, translates natural language task descriptions into trainable physics-informed neural networks. It outperforms existing methods by significantly reducing errors, improving success rates in execution, and decreasing time overhead. By coordinating multiple agents to parse, select architectures, generate code, and provide feedback for refinement, Lang-PINN transforms informal problem statements into precise and efficient executable code for solving partial differential equations. <div>
arXiv:2510.05158v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) provide a powerful approach for solving partial differential equations (PDEs), but constructing a usable PINN remains labor-intensive and error-prone. Scientists must interpret problems as PDE formulations, design architectures and loss functions, and implement stable training pipelines. Existing large language model (LLM) based approaches address isolated steps such as code generation or architecture suggestion, but typically assume a formal PDE is already specified and therefore lack an end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system that builds trainable PINNs directly from natural language task descriptions. Lang-PINN coordinates four complementary agents: a PDE Agent that parses task descriptions into symbolic PDEs, a PINN Agent that selects architectures, a Code Agent that generates modular implementations, and a Feedback Agent that executes and diagnoses errors for iterative refinement. This design transforms informal task statements into executable and verifiable PINN code. Experiments show that Lang-PINN achieves substantially lower errors and greater robustness than competitive baselines: mean squared error (MSE) is reduced by up to 3--5 orders of magnitude, end-to-end execution success improves by more than 50\%, and reduces time overhead by up to 74\%.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentZero++: Modeling Fear-Based Behavior</title>
<link>https://arxiv.org/abs/2510.05185</link>
<guid>https://arxiv.org/abs/2510.05185</guid>
<content:encoded><![CDATA[
<div> AgentZero++, agent-based model, collective violence, cognitive mechanisms, emotional mechanisms, social mechanisms <br />
<br />
Summary: <br />
AgentZero++ is a new agent-based model that incorporates cognitive, emotional, and social mechanisms to simulate decentralized collective violence in spatially distributed systems. The model includes eight behavioral enhancements that allow agents to adapt based on internal states, previous experiences, and social feedback. These additions result in emergent dynamics such as protest asymmetries, escalation cycles, and localized retaliation. By implementing features such as memory-based risk estimation, affect-cognition coupling, and multi-agent coordination, the model demonstrates how small variations in memory, reactivity, and affective alignment can amplify or dampen unrest through feedback loops. Overall, AgentZero++ provides a flexible platform for analyzing affective contagion and psychologically grounded collective action, with implications for understanding the role of emotional thresholds, identity-driven behavior, and adaptive networks in shaping conflict patterns. <div>
arXiv:2510.05185v1 Announce Type: cross 
Abstract: We present AgentZero++, an agent-based model that integrates cognitive, emotional, and social mechanisms to simulate decentralized collective violence in spatially distributed systems. Building on Epstein's Agent\_Zero framework, we extend the original model with eight behavioral enhancements: age-based impulse control; memory-based risk estimation; affect-cognition coupling; endogenous destructive radius; fight-or-flight dynamics; affective homophily; retaliatory damage; and multi-agent coordination. These additions allow agents to adapt based on internal states, previous experiences, and social feedback, producing emergent dynamics such as protest asymmetries, escalation cycles, and localized retaliation. Implemented in Python using the Mesa ABM framework, AgentZero++ enables modular experimentation and visualization of how micro-level cognitive heterogeneity shapes macro-level conflict patterns. Our results highlight how small variations in memory, reactivity, and affective alignment can amplify or dampen unrest through feedback loops. By explicitly modeling emotional thresholds, identity-driven behavior, and adaptive networks, this work contributes a flexible and extensible platform for analyzing affective contagion and psychologically grounded collective action.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium</title>
<link>https://arxiv.org/abs/2510.05711</link>
<guid>https://arxiv.org/abs/2510.05711</guid>
<content:encoded><![CDATA[
<div> Keywords: Time-bound stablecoins, Liquidity-of-Time Premium, no-arbitrage pricing model, dynamic risk-control mechanism, empirical proxies.

Summary: 
Time-bound stablecoins tokenize traditional securities during market off-hours to provide continuous cross-market liquidity. The Liquidity-of-Time Premium (TLP) is introduced as the additional return or cost of providing liquidity when the primary market is closed. A no-arbitrage pricing model is developed to estimate fair values over different expiries, with a dynamic risk-control mechanism adjusting loan-to-value (LTV) ratios in real time to manage TLP. By blending financial engineering and empirical finance, the analysis measures TLP under time-zone frictions and proposes an LTV policy akin to a central bank adjusting rates. Empirical proxies for TLP include ADR premiums, index futures divergence, and pre-market gaps. Results indicate TLP increases with closure length and volatility but can be managed through adaptive LTV. The study provides insights for protocol design and suggests time-bound stablecoins as a tool to mitigate temporal market inefficiencies. 

Summary: <div>
arXiv:2510.05711v1 Announce Type: cross 
Abstract: Time-bound stablecoins are DeFi assets that temporarily tokenize traditional securities during market off-hours, enabling continuous cross-market liquidity. We introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of providing liquidity when the primary market is closed. We build a no-arbitrage pricing model that yields a band for fair values over different expiries, and a dynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real time to keep TLP within a target range. Our analysis blends financial engineering (no-arbitrage conditions, option-style pricing) with empirical finance (event studies on cross-listed stocks and futures) to measure TLP under time-zone frictions. We define TLP formally, derive closed-form expressions for its term structure under idealized assumptions, and simulate scenarios that vary volatility and collateralization. We then propose an LTV policy that raises or lowers collateral to expand or curtail time-bound stablecoin supply, analogous to a central bank adjusting rates to defend a peg. We outline empirical proxies for TLP, including ADR premiums, overseas index futures versus cash index divergence, and pre-market versus official close gaps. Results show that TLP grows with closure length and volatility, yet can be contained by adaptive LTV. We provide backtests and figures (term-structure curves, capital-efficiency versus tail-risk trade-offs, time-liquidity heatmaps) and discuss protocol design (vault structure, closing-price oracles, on-chain auction liquidations). The findings position time-bound stablecoins as a tool to reduce temporal market inefficiencies and inform future research and deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2510.06107</link>
<guid>https://arxiv.org/abs/2510.06107</guid>
<content:encoded><![CDATA[
<div> hallucination, language models, distributional semantics tracing, commitment layer, dual-process theory<br />
Summary:<br />
Large Language Models (LLMs) often generate factually incorrect statements known as hallucinations. This study investigates the underlying causes of this issue within the Transformer architecture. The authors introduce Distributional Semantics Tracing (DST), a framework that maps a model's reasoning and identifies a "commitment layer" where hallucinations become inevitable. They discover a conflict between fast, heuristic pathways and slow, deliberate pathways in the model, leading to predictable failure modes. By quantifying the coherence of the contextual pathway, they find a strong negative correlation with hallucination rates, linking these failures to internal semantic weakness. This mechanistic account provides insights into when and why hallucinations occur, shedding light on the architectural origins of this problem. <br /> <div>
arXiv:2510.06107v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary contributions.First, to enable the reliable tracing of internal semantic failures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific \textbf{commitment layer} where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic \textbf{associative pathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway} (akin to System 2), leading to predictable failure modes such as \textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation ($\rho = -0.863$) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Report of the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science</title>
<link>https://arxiv.org/abs/2510.03413</link>
<guid>https://arxiv.org/abs/2510.03413</guid>
<content:encoded><![CDATA[
<div> workshop, scientific computing, AI, cross-disciplinary collaboration, workforce development

Summary: 
The report discusses insights from the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing, focusing on harnessing community, software, and AI for cross-disciplinary team science. The participants emphasized the need for agile, robust ecosystems in scientific software development through socio-technical co-design. Recommendations include building trustworthy AI-enabled scientific software systems, integrating AI systems into workflows while preserving human creativity and rigor, and developing innovative training pipelines. Pilot projects were identified as catalysts for progress, with priorities on hybrid AI/HPC infrastructure, collaboration, responsible AI guidelines, and public-private partnerships. The vision presented aims to intertwine AI, software, hardware, and human expertise to drive discovery, enhance access, bolster the workforce, and hasten scientific advancements. <div>
arXiv:2510.03413v1 Announce Type: new 
Abstract: This report summarizes insights from the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science, which convened more than 40 experts from national laboratories, academia, industry, and community organizations to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems. To address urgent challenges at the intersection of high-performance computing (HPC), AI, and scientific software, participants envisioned agile, robust ecosystems built through socio-technical co-design--the intentional integration of social and technical components as interdependent parts of a unified strategy. This approach combines advances in AI, HPC, and software with new models for cross-disciplinary collaboration, training, and workforce development. Key recommendations include building modular, trustworthy AI-enabled scientific software systems; enabling scientific teams to integrate AI systems into their workflows while preserving human creativity, trust, and scientific rigor; and creating innovative training pipelines that keep pace with rapid technological change. Pilot projects were identified as near-term catalysts, with initial priorities focused on hybrid AI/HPC infrastructure, cross-disciplinary collaboration and pedagogy, responsible AI guidelines, and prototyping of public-private partnerships. This report presents a vision of next-generation ecosystems for scientific computing where AI, software, hardware, and human expertise are interwoven to drive discovery, expand access, strengthen the workforce, and accelerate scientific progress.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight and Data-Efficient MultivariateTime Series Forecasting using Residual-Stacked Gaussian (RS-GLinear) Architecture</title>
<link>https://arxiv.org/abs/2510.03788</link>
<guid>https://arxiv.org/abs/2510.03788</guid>
<content:encoded><![CDATA[
<div> Transformer, time-series forecasting, Gaussian Linear, RSGL model, prediction accuracy

Summary:
The study evaluates the Gaussian-based Linear architecture for time-series forecasting and proposes an enhanced version called the Residual Stacked Gaussian Linear (RSGL) model. The RSGL model is assessed for its performance in long-term forecasting tasks and its applicability in financial time series and epidemiological data. Experimental results show that the RSGL model outperforms both the baseline Gaussian Linear and Transformer-based models in terms of prediction accuracy and robustness. The study addresses the mixed results reported in previous research and demonstrates the effectiveness of the RSGL model in capturing both short- and long-term dependencies in forecasting tasks. Additionally, the study highlights the broader applicability of the RSGL model across different domains, showcasing its versatility in handling various types of time-series data. <div>
arXiv:2510.03788v1 Announce Type: new 
Abstract: Following the success of Transformer architectures in language modeling, particularly their ability to capture long-range dependencies, researchers have explored how these architectures can be adapted for time-series forecasting. Transformer-based models have been proposed to handle both short- and long-term dependencies when predicting future values from historical data. However, studies such as those by Zeng et al. (2022) and Rizvi et al. (2025) have reported mixed results in long-term forecasting tasks. In this work, we evaluate the Gaussian-based Linear architecture introduced by Rizvi et al. (2025) and present an enhanced version called the Residual Stacked Gaussian Linear (RSGL) model. We also investigate the broader applicability of the RSGL model in additional domains, including financial time series and epidemiological data. Experimental results show that the RSGL model achieves improved prediction accuracy and robustness compared to both the baseline Gaussian Linear and Transformer-based models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nystr\"om-Accelerated Primal LS-SVMs: Breaking the $O(an^3)$ Complexity Bottleneck for Scalable ODEs Learning</title>
<link>https://arxiv.org/abs/2510.04094</link>
<guid>https://arxiv.org/abs/2510.04094</guid>
<content:encoded><![CDATA[
<div> Kernel-based methods, LS-SVMs, ordinary differential equations, computational complexity, Nystrm-accelerated framework<br />
<br />
Summary: 
This paper introduces a novel Nystrm-accelerated LS-SVMs framework for solving linear and nonlinear ordinary differential equations efficiently. By reformulating ODEs as primal-space constraints and deriving an explicit Nystrm-based mapping to a higher-dimensional feature space, the proposed method significantly reduces computational complexity. Numerical experiments on sixteen benchmark ODEs demonstrate faster computation (up to 6000 times) compared to classical LS-SVMs and PINNs, with comparable accuracy and scalability to a high number of time steps. The framework achieves accuracy improvements over PINNs by 72% in RMSE while maintaining a precision level similar to LS-SVMs. This work presents a breakthrough in kernel-based ODEs learning, providing a more efficient and scalable approach without compromising solution accuracy.<br /><br />Summary: <div>
arXiv:2510.04094v1 Announce Type: new 
Abstract: A major problem of kernel-based methods (e.g., least squares support vector machines, LS-SVMs) for solving linear/nonlinear ordinary differential equations (ODEs) is the prohibitive $O(an^3)$ ($a=1$ for linear ODEs and 27 for nonlinear ODEs) part of their computational complexity with increasing temporal discretization points $n$. We propose a novel Nystr\"om-accelerated LS-SVMs framework that breaks this bottleneck by reformulating ODEs as primal-space constraints. Specifically, we derive for the first time an explicit Nystr\"om-based mapping and its derivatives from one-dimensional temporal discretization points to a higher $m$-dimensional feature space ($1< m\le n$), enabling the learning process to solve linear/nonlinear equation systems with $m$-dependent complexity. Numerical experiments on sixteen benchmark ODEs demonstrate: 1) $10-6000$ times faster computation than classical LS-SVMs and physics-informed neural networks (PINNs), 2) comparable accuracy to LS-SVMs ($<0.13\%$ relative MAE, RMSE, and $\left \| y-\hat{y} \right \| _{\infty } $difference) while maximum surpassing PINNs by 72\% in RMSE, and 3) scalability to $n=10^4$ time steps with $m=50$ features. This work establishes a new paradigm for efficient kernel-based ODEs learning without significantly sacrificing the accuracy of the solution.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Complement to Neural Networks for Anisotropic Inelasticity at Finite Strains</title>
<link>https://arxiv.org/abs/2510.04187</link>
<guid>https://arxiv.org/abs/2510.04187</guid>
<content:encoded><![CDATA[
<div> neural networks, anisotropy, inelasticity, finite strains, dual potential<br />
Summary:<br />
The article introduces a novel approach that combines neural networks with material principles to capture anisotropy and inelasticity at finite strains. A dual potential is utilized to govern dissipation and incorporate anisotropy, while also satisfying the dissipation inequality without requiring convexity. The neural network architecture employs invariant-based input representations and introduces Input Monotonic Neural Networks to expand potential class. Recurrent Liquid Neural Networks are used to bypass time integration issues in the finite strain regime and stabilize training of inelastic materials. The method is evaluated at material point and structural scales, delivering accurate and stable performance beyond the training regime. The implementation is available as open-source and accessible to the public. <div>
arXiv:2510.04187v1 Announce Type: new 
Abstract: We propose a complement to constitutive modeling that augments neural networks with material principles to capture anisotropy and inelasticity at finite strains. The key element is a dual potential that governs dissipation, consistently incorporates anisotropy, and-unlike conventional convex formulations-satisfies the dissipation inequality without requiring convexity.
  Our neural network architecture employs invariant-based input representations in terms of mixed elastic, inelastic and structural tensors. It adapts Input Convex Neural Networks, and introduces Input Monotonic Neural Networks to broaden the admissible potential class. To bypass exponential-map time integration in the finite strain regime and stabilize the training of inelastic materials, we employ recurrent Liquid Neural Networks.
  The approach is evaluated at both material point and structural scales. We benchmark against recurrent models without physical constraints and validate predictions of deformation and reaction forces for unseen boundary value problems. In all cases, the method delivers accurate and stable performance beyond the training regime. The neural network and finite element implementations are available as open-source and are accessible to the public via https://doi.org/10.5281/zenodo.17199965.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-source FDTD solvers: The applicability of Elecode, gprMax and MEEP for simulations of lightning EM fields</title>
<link>https://arxiv.org/abs/2510.04262</link>
<guid>https://arxiv.org/abs/2510.04262</guid>
<content:encoded><![CDATA[
<div> FDTD solvers, gprMax, Elecode, MEEP, lightning electromagnetic field propagation, simulation accuracy <br />
<br />
Summary: 
This study evaluates the open-source FDTD solvers gprMax, Elecode, and MEEP for lightning electromagnetic field propagation simulations. The solvers are tested in various scenarios and compared to reference field results over conducting and lossy ground. Results show that all solvers perform satisfactorily, but careful consideration of spatial discretization and simulation cell boundaries is necessary to avoid numerical dispersion and reflections. Improper parameter choices can lead to inaccurate results, as demonstrated in some cases. The study highlights the features, performance, limitations, advantages, and drawbacks of the solvers. Scripts for initializing and running simulations are provided in a publicly accessible repository to aid the community in using the solvers' programmatic interfaces. <div>
arXiv:2510.04262v1 Announce Type: new 
Abstract: In this study, the open-source finite-difference time-domain (FDTD) solvers gprMax, Elecode and MEEP are investigated for their suitability to compute lightning electromagnetic field propagation. Several simulations are performed to reproduce the results of typical field propagation scenarios that can be found in the literature. The results of the presented solvers are validated through comparison with reference field results corresponding to propagation over perfectly conducting and lossy ground. In most of the tested scenarios, all solvers reproduce the reference fields with satisfactory accuracy. However, close attention must be paid to the proper choice of the spatial discretization to avoid artificial numerical dispersion, and the application of the simulation cell boundaries, which can cause significant impairment of the results due to undesired reflections. Some cases of inaccurate FDTD results due to improper choices of parameters are demonstrated. Further, the features, the performance and limitations, and the advantages and drawbacks of the presented solvers are highlighted. For familiarization with the solvers' programmatical interfaces to initialize and run the simulations, the developed scripts are made available to the community in an openly accessible repository.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fast Option Pricing PDE Solvers Powered by PIELM</title>
<link>https://arxiv.org/abs/2510.04322</link>
<guid>https://arxiv.org/abs/2510.04322</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Partial Differential Equations, Finance, Physics-Informed, Extreme Learning Machines<br />
Summary:<br />
Physics-Informed Extreme Learning Machines (PIELMs) are proposed as a fast alternative to Physics-Informed Neural Networks (PINNs) for solving forward and inverse problems in financial PDEs. PIELMs utilize least-squares solve instead of iterative optimization, leading to efficient and deterministic training. PIELMs are benchmarked on Black-Scholes and Heston-Hull-White models, demonstrating comparable accuracy to PINNs but with up to 30 times faster computation. They excel in inverse model calibration, accurately recovering volatility and interest rate parameters from noisy data. The potential of PIELMs for real-time financial modeling is highlighted, offering a promising solution for efficient and accurate option pricing and risk evaluation in quantitative finance applications.<br /> <div>
arXiv:2510.04322v1 Announce Type: new 
Abstract: Partial differential equation (PDE) solvers underpin modern quantitative finance, governing option pricing and risk evaluation. Physics-Informed Neural Networks (PINNs) have emerged as a promising approach for solving the forward and inverse problems of partial differential equations (PDEs) using deep learning. However they remain computationally expensive due to their iterative gradient descent based optimization and scale poorly with increasing model size. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs) as fast alternative to PINNs for solving both forward and inverse problems in financial PDEs. PIELMs replace iterative optimization with a single least-squares solve, enabling deterministic and efficient training. We benchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward pricing and demonstrate its capability in inverse model calibration to recover volatility and interest rate parameters from noisy data. From experiments we observe that PIELM achieve accuracy comparable to PINNs while being up to $30\times$ faster, highlighting their potential for real-time financial modeling.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep vs. Shallow: Benchmarking Physics-Informed Neural Architectures on the Biharmonic Equation</title>
<link>https://arxiv.org/abs/2510.04490</link>
<guid>https://arxiv.org/abs/2510.04490</guid>
<content:encoded><![CDATA[
<div> Mesh-based solvers, finite difference, finite volume, finite element, physics-informed neural networks (PINNs), RBF-PIELM<br />
Summary:<br />
This paper presents a benchmark of RBF-PIELM, an extreme learning machine with radial-basis activations, for solving higher-order partial differential equations (PDEs). Compared to traditional mesh-based approaches and physics-informed neural networks (PINNs), RBF-PIELM offers faster training (350 times faster than PINNs) and requires fewer parameters (10 times fewer for comparable accuracy). However, RBF-PIELM still falls short of mature mesh-based solvers' accuracy and struggles with highly oscillatory solutions. The study focuses on the fourth-order biharmonic equation and tests RBF-PIELM on lid-driven cavity flow and an oscillatory solution. This research highlights the potential of RBF-PIELM as a rapid and efficient PDE solver but also identifies the need for further development to address challenges in practical applications. <br /> <div>
arXiv:2510.04490v1 Announce Type: new 
Abstract: Partial differential equation (PDE) solvers are fundamental to engineering simulation. Classical mesh-based approaches (finite difference/volume/element) are fast and accurate on high-quality meshes but struggle with higher-order operators and complex, hard-to-mesh geometries. Recently developed physics-informed neural networks (PINNs) and their variants are mesh-free and flexible, yet compute-intensive and often less accurate. This paper systematically benchmarks RBF-PIELM, a rapid PINN variant-an extreme learning machine with radial-basis activations-for higher-order PDEs. RBF-PIELM replaces PINNs' time-consuming gradient descent with a single-shot least-squares solve. We test RBF-PIELM on the fourth-order biharmonic equation using two benchmarks: lid-driven cavity flow (streamfunction formulation) and a manufactured oscillatory solution. Our results show up to $(350\times)$ faster training than PINNs and over $(10\times)$ fewer parameters for comparable solution accuracy. Despite surpassing PINNs, RBF-PIELM still lags mature mesh-based solvers and its accuracy degrades on highly oscillatory solutions, highlighting remaining challenges for practical deployment.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems</title>
<link>https://arxiv.org/abs/2510.01396</link>
<guid>https://arxiv.org/abs/2510.01396</guid>
<content:encoded><![CDATA[
<div> surrogate framework, neural network, Cartesian coordinates, automatic differentiation, free energy reconstruction

Summary: 
The article presents a neural network surrogate framework that learns collective variables (CVs) directly from Cartesian coordinates, eliminating the need for analytical Jacobians in free energy reconstruction methods like Gaussian Process Regression (GPR). This innovative approach allows for the use of complex or machine-learned CVs and provides accurate Jacobians through automatic differentiation. The framework was tested on an MgCl2 ion-pairing system, demonstrating high accuracy with both simple distance CVs and complex coordination-number CVs. Additionally, the errors in Jacobians were found to follow a near-Gaussian distribution, making them suitable for GPR pipelines. By enabling gradient-based free energy methods to incorporate complex CVs, this framework expands the capabilities of biochemistry and materials simulations.<br /><br />Summary: <div>
arXiv:2510.01396v1 Announce Type: cross 
Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR) require Jacobians of the collective variables (CVs), a bottleneck that restricts the use of complex or machine-learned CVs. We introduce a neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to provide Jacobians, bypassing analytical forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for both a simple distance CV and a complex coordination-number CV. Moreover, Jacobian errors also followed a near-Gaussian distribution, making them suitable for GPR pipelines. This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening the scope of biochemistry and materials simulations.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark</title>
<link>https://arxiv.org/abs/2510.03261</link>
<guid>https://arxiv.org/abs/2510.03261</guid>
<content:encoded><![CDATA[
<div> Neural networks, thermal errors, machine tools, data-driven compensation, high-fidelity prediction <br />
Summary:  
Neural networks are utilized to predict temperature and heat flux fields in machine tools, offering a more versatile approach to thermal error correction. This method allows for the computation and correction of various error types using interchangeable components. By training the neural network on finite element method data under different initial conditions, accurate predictions can be made with minimal hardware requirements. Various time-series neural network architectures are benchmarked to determine their effectiveness in predicting temperature and heat flux fields. The results demonstrate the ability to predict these fields accurately and affordably, setting the stage for flexible and adaptable thermal error correction methods in machine tool environments. <br /> <div>
arXiv:2510.03261v1 Announce Type: cross 
Abstract: Thermal errors in machine tools significantly impact machining precision and productivity. Traditional thermal error correction/compensation methods rely on measured temperature-deformation fields or on transfer functions. Most existing data-driven compensation strategies employ neural networks (NNs) to directly predict thermal errors or specific compensation values. While effective, these approaches are tightly bound to particular error types, spatial locations, or machine configurations, limiting their generality and adaptability. In this work, we introduce a novel paradigm in which NNs are trained to predict high-fidelity temperature and heat flux fields within the machine tool. The proposed framework enables subsequent computation and correction of a wide range of error types using modular, swappable downstream components. The NN is trained using data obtained with the finite element method under varying initial conditions and incorporates a correlation-based selection strategy that identifies the most informative measurement points, minimising hardware requirements during inference. We further benchmark state-of-the-art time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit, Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal Convolutional Network, by training both specialised models, tailored for specific initial conditions, and general models, capable of extrapolating to unseen scenarios. The results show accurate and low-cost prediction of temperature and heat flux fields, laying the basis for enabling flexible and generalisable thermal error correction in machine tool environments.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions</title>
<link>https://arxiv.org/abs/2510.03370</link>
<guid>https://arxiv.org/abs/2510.03370</guid>
<content:encoded><![CDATA[
<div> fine-tuning, protein language models, multimodal, mutation-effect prediction, ESM2 <br />
Summary: <br />
The authors introduce a fine-tuning framework named InstructPLM-mu for improving mutation-effect prediction in protein language models. They investigate whether fine-tuning a pretrained sequence-only model with structural inputs can match the performance of end-to-end trained models. Surprisingly, experiments demonstrate that fine-tuning ESM2 with structural inputs can achieve performance comparable to ESM3. The study systematically compares different feature-fusion designs and fine-tuning recipes, highlighting the significant impact of fusion methods and tuning strategies on final accuracy. These findings suggest that the fine-tuning process is critical and nontrivial. The work aims to provide practical insights into integrating structure into pretrained protein language models and encourages further exploration of advanced fusion mechanisms and fine-tuning protocols. <br /> <div>
arXiv:2510.03370v1 Announce Type: cross 
Abstract: Multimodal protein language models deliver strong performance on mutation-effect prediction, but training such models from scratch demands substantial computational resources. In this paper, we propose a fine-tuning framework called InstructPLM-mu and try to answer a question: \textit{Can multimodal fine-tuning of a pretrained, sequence-only protein language model match the performance of models trained end-to-end? } Surprisingly, our experiments show that fine-tuning ESM2 with structural inputs can reach performance comparable to ESM3. To understand how this is achieved, we systematically compare three different feature-fusion designs and fine-tuning recipes. Our results reveal that both the fusion method and the tuning strategy strongly affect final accuracy, indicating that the fine-tuning process is not trivial. We hope this work offers practical guidance for injecting structure into pretrained protein language models and motivates further research on better fusion mechanisms and fine-tuning protocols.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design</title>
<link>https://arxiv.org/abs/2510.03650</link>
<guid>https://arxiv.org/abs/2510.03650</guid>
<content:encoded><![CDATA[
<div> point sets, digital sequences, quasi-Monte Carlo, program synthesis, evolutionary algorithm
Summary:
Quasi-Monte Carlo (QMC) methods for high-dimensional integration rely on low-discrepancy point sets and digital sequences. This study presents an approach using an evolutionary algorithm guided by program synthesis to tackle two QMC design problems. The first problem involves constructing finite 2D/3D point sets with low star discrepancy, while the second problem focuses on selecting Sobol' direction numbers to minimize randomized QMC error. The approach combines constructive code proposals with numerical refinement to achieve optimal results. The study successfully rediscovers known optimal designs and sets new benchmarks for 2D and 3D point sets. Additionally, evolving Sobol' parameters leads to reduced error in rQMC for 32-dimensional tasks, surpassing the performance of traditional parameters. The results highlight the effectiveness of using evolutionary program synthesis to automate the discovery of high-quality QMC constructions, improving upon existing designs and optimizing for specific integration tasks. Data and code resources are available for further exploration and implementation. 
<br /><br />Summary: <div>
arXiv:2510.03650v1 Announce Type: cross 
Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo (QMC) methods for high-dimensional integration. We cast two long-standing QMC design problems as program synthesis and solve them with an LLM-guided evolutionary loop that mutates and selects code under task-specific fitness: (i) constructing finite 2D/3D point sets with low star discrepancy, and (ii) choosing Sobol' direction numbers that minimize randomized QMC error on downstream integrands. Our two-phase procedure combines constructive code proposals with iterative numerical refinement. On finite sets, we rediscover known optima in small 2D cases and set new best-known 2D benchmarks for N >= 40, while matching most known 3D optima up to the proven frontier (N <= 8) and reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol' parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC) mean-squared error for several 32-dimensional option-pricing tasks relative to widely used Joe--Kuo parameters, while preserving extensibility to any sample size and compatibility with standard randomizations. Taken together, the results demonstrate that LLM-driven evolutionary program synthesis can automate the discovery of high-quality QMC constructions, recovering classical designs where they are optimal and improving them where finite-N structure matters. Data and code are available at https://github.com/hockeyguy123/openevolve-star-discrepancy.git.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling</title>
<link>https://arxiv.org/abs/2510.04204</link>
<guid>https://arxiv.org/abs/2510.04204</guid>
<content:encoded><![CDATA[
<div> keywords: Large Reasoning Models, domain adaptation, optimization modeling, CALM, STORM

Summary: 
Large Reasoning Models (LRMs) have advanced capabilities in multi-step reasoning, but traditional domain adaptation methods are ineffective for leveraging these abilities. Direct fine-tuning on non-reflective datasets yields limited improvements in LRMs' reasoning patterns. To address this issue, the framework CALM (Corrective Adaptation with Lightweight Modification) is proposed, which refines LRMs within their native reasoning modes for optimization modeling tasks. CALM involves an expert providing corrective hints to guide the model in producing improved reasoning trajectories with minimal token modification. This approach, combined with reinforcement learning, results in the development of STORM (Smart Thinking Optimization Reasoning Model), a high-performing LRM with 4B parameters. STORM achieves a state-of-the-art average accuracy of 68.9% across multiple optimization modeling benchmarks, demonstrating the effectiveness of hint-based data synthesis in preserving and enhancing LRMs' reasoning abilities for expert-level performance. 

<br /><br />Summary: <div>
arXiv:2510.04204v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional \textit{non-reflective} datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose \textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCR-EML: Explainable Model Layers for TCR-pMHC Prediction</title>
<link>https://arxiv.org/abs/2510.04377</link>
<guid>https://arxiv.org/abs/2510.04377</guid>
<content:encoded><![CDATA[
<div> Machine Learning, TCR-pMHC binding, Explainability, Vaccine design, Cancer immunotherapy <br />
Summary: <br />
This article introduces a new approach, TCR-EML, for modeling T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes using explainable model layers. These layers incorporate known binding mechanisms to provide high-quality explanations for predicted TCR-pMHC binding. The approach, based on prototype layers for amino acid residue contacts, improves upon existing black-box transformer models by offering insights into the biochemical mechanisms underlying TCR-pMHC interactions. Experimental results on large datasets show competitive predictive accuracy and generalization, with improved explainability compared to current methods. The proposed explainable model layers have potential applications in vaccine design, cancer immunotherapy, and autoimmune disease research, enhancing our understanding of TCR-pMHC binding and enabling the development of more effective therapies. <div>
arXiv:2510.04377v1 Announce Type: cross 
Abstract: T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is a central component of adaptive immunity, with implications for vaccine design, cancer immunotherapy, and autoimmune disease. While recent advances in machine learning have improved prediction of TCR-pMHC binding, the most effective approaches are black-box transformer models that cannot provide a rationale for predictions. Post-hoc explanation methods can provide insight with respect to the input but do not explicitly model biochemical mechanisms (e.g. known binding regions), as in TCR-pMHC binding. ``Explain-by-design'' models (i.e., with architectural components that can be examined directly after training) have been explored in other domains, but have not been used for TCR-pMHC binding. We propose explainable model layers (TCR-EML) that can be incorporated into protein-language model backbones for TCR-pMHC modeling. Our approach uses prototype layers for amino acid residue contacts drawn from known TCR-pMHC binding mechanisms, enabling high-quality explanations for predicted TCR-pMHC binding. Experiments of our proposed method on large-scale datasets demonstrate competitive predictive accuracy and generalization, and evaluation on the TCR-XAI benchmark demonstrates improved explainability compared with existing approaches.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overlapping Schwarz Scheme for Linear-Quadratic Programs in Continuous Time</title>
<link>https://arxiv.org/abs/2510.04478</link>
<guid>https://arxiv.org/abs/2510.04478</guid>
<content:encoded><![CDATA[
<div> framework, linear-quadratic optimal control, time-inhomogeneous ODEs, Hamiltonian systems, numerical integration <br />
<br />
Summary: 
The article presents an optimize-then-discretize framework for solving linear-quadratic optimal control problems governed by time-inhomogeneous ordinary differential equations. The method utilizes a modified overlapping Schwarz decomposition based on the Pontryagin Minimum Principle to partition the temporal domain into overlapping intervals and solve Hamiltonian systems independently. The convergence is ensured by updating the boundary conditions of the individual Hamiltonian dynamics. The analysis proves that the exponential decay of sensitivity seen in discrete-time OCPs extends to the continuous-time setting. Unlike the approach of discretize-then-optimize, this method can incorporate various numerical integration methods for solving the resulting Hamiltonian two-point boundary-value subproblems, including adaptive-time integrators. A numerical experiment on a linear-quadratic OCP demonstrates the practicality and versatility of this approach in diverse scientific applications. <br /> <div>
arXiv:2510.04478v1 Announce Type: cross 
Abstract: We present an optimize-then-discretize framework for solving linear-quadratic optimal control problems (OCP) governed by time-inhomogeneous ordinary differential equations (ODEs). Our method employs a modified overlapping Schwarz decomposition based on the Pontryagin Minimum Principle, partitioning the temporal domain into overlapping intervals and independently solving Hamiltonian systems in continuous time. We demonstrate that the convergence is ensured by appropriately updating the boundary conditions of the individual Hamiltonian dynamics. The cornerstone of our analysis is to prove that the exponential decay of sensitivity (EDS) exhibited in discrete-time OCPs carries over to the continuous-time setting. Unlike the discretize-then-optimize approach, our method can flexibly incorporate different numerical integration methods for solving the resulting Hamiltonian two-point boundary-value subproblems, including adaptive-time integrators. A numerical experiment on a linear-quadratic OCP illustrates the practicality of our approach in broad scientific applications.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering</title>
<link>https://arxiv.org/abs/2510.04514</link>
<guid>https://arxiv.org/abs/2510.04514</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal LLMs, visual question answering, ChartAgent, visual reasoning, chart understanding <br />
Summary:<br />
- The study introduces ChartAgent, a framework for visual reasoning on chart images, which outperforms previous methods on benchmarks by up to 16.07% on average and 17.31% for complex queries.<br />
- ChartAgent breaks down queries into visual subtasks and manipulates chart images using specialized actions, mimicking human cognitive strategies for chart comprehension.<br />
- The framework achieves high accuracy across various types of charts and complexity levels, proving its effectiveness in visually grounded reasoning.<br />
- ChartAgent serves as a plug-and-play tool that enhances the performance of different multimodal language models (LLMs) in chart-based visual question answering tasks.<br />
- This work showcases the potential of visually grounded reasoning in improving chart interpretation and understanding through tool-augmented multimodal agents.<br /> <div>
arXiv:2510.04514v1 Announce Type: cross 
Abstract: Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Time Series Foundation Models for Short-Term Household Electricity Load Forecasting</title>
<link>https://arxiv.org/abs/2410.09487</link>
<guid>https://arxiv.org/abs/2410.09487</guid>
<content:encoded><![CDATA[
<div> Time series foundation models, household electricity STLF, Chronos, TimesFM, Time-MoE <br />
Summary: <br />
Accurate short-term load forecasting for household electricity is crucial for sustainable energy systems. While various approaches have been used for this task, newer time series foundation models like Chronos and Time-MoE offer a different approach, trained on extensive time series data for zero-shot learning. This study compares the forecasting performance of these foundation models with Transformer-based approaches. Results show that foundation models perform similarly to Transformer models, with certain foundation models outperforming Transformers as input size increases. Foundation models also require less effort as they do not need domain-specific training and only limited contextual data for inference. <div>
arXiv:2410.09487v2 Announce Type: replace 
Abstract: Accurate household electricity short-term load forecasting (STLF) is key to future and sustainable energy systems. While various studies have analyzed statistical, machine learning, or deep learning approaches for household electricity STLF, recently proposed time series foundation models such as Chronos, TimesFM or Time-MoE promise a new approach for household electricity STLF. These models are trained on a vast amount of time series data and are able to forecast time series without explicit task-specific training (zero-shot learning). In this study, we benchmark the forecasting capabilities of time series foundation models compared to Trained-from-Scratch (TFS) Transformer-based approaches. Our results suggest that foundation models perform comparably to TFS Transformer models, while certain time series foundation models outperform all TFS models when the input size increases. At the same time, they require less effort, as they need no domain-specific training and only limited contextual data for inference.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGRDN-Data learned sparsification of graph reaction-diffusion networks</title>
<link>https://arxiv.org/abs/2303.11943</link>
<guid>https://arxiv.org/abs/2303.11943</guid>
<content:encoded><![CDATA[
<div> sparsification, graph, reaction-diffusion systems, Reduced Order Model, eigenmodes <br />
<br />
Summary: <br />
Graph sparsification is a challenging task in computer science and mathematics, aimed at reducing the number of edges while preserving graph properties. A new method called SGRDN extends sparsification to complex reaction-diffusion systems, preserving their dynamics. By framing sparsification as a data assimilation problem within a Reduced Order Model space, SGRDN enforces constraints to conserve the eigenmodes of the Laplacian matrix. Efficient eigenvalue and eigenvector approximations for perturbed Laplacian matrices are derived and integrated into the optimization process. An experiment on Neural Ordinary Differential Equations demonstrates SGRDN's ability to achieve parameter sparsity. This method broadens the applicability of sparsification techniques to complex systems while maintaining spectral preservation constraints. <div>
arXiv:2303.11943v4 Announce Type: replace-cross 
Abstract: Graph sparsification is an area of interest in computer science and applied mathematics. Sparsification of a graph, in general, aims to reduce the number of edges in the network while preserving specific properties of the graph, like cuts and subgraph counts. Computing the sparsest cuts of a graph is known to be NP-hard, and sparsification routines exist for generating linear-sized sparsifiers in almost quadratic running time $O(n^{2 + \epsilon})$. Consequently, obtaining a sparsifier can be a computationally demanding task, and the complexity varies based on the level of sparsity required. We propose SGRDN to extend sparsification to complex reaction-diffusion systems. This approach seeks to sparsify the graph such that the inherent reaction-diffusion dynamics are strictly preserved on the resulting structure. By selectively considering a subset of trajectories, we frame the network sparsification issue as a data assimilation problem within a Reduced Order Model (ROM) space, imposing constraints to conserve the eigenmodes of the Laplacian matrix ($L = D - A$), the difference between the degree matrix ($D$) and the adjacency matrix ($A$) despite perturbations. We derive computationally efficient eigenvalue and eigenvector approximations for perturbed Laplacian matrices and integrate these as spectral preservation constraints in the optimization problem. To further validate the method's broad applicability, we conducted an additional experiment on Neural Ordinary Differential Equations (neural ODEs), where SGRDN successfully achieved parameter sparsity.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Graph Representation of Stiffened Panels with Non-Uniform Boundary Conditions and Loads</title>
<link>https://arxiv.org/abs/2510.02472</link>
<guid>https://arxiv.org/abs/2510.02472</guid>
<content:encoded><![CDATA[
<div> Graph representation, heterogeneous graph neural network, stiffened panels, structural analysis, optimization

Summary:
The study introduces a heterogeneous graph representation using heterogeneous graph neural networks (HGNNs) for analyzing and optimizing stiffened panels. The structure is divided into distinct units with varying node types to consider geometrical variability, boundary conditions, and loading scenarios. Edge heterogeneity is incorporated to capture local orientations and spatial relationships. The proposed heterogeneous graph representations are implemented into a heterogeneous graph transformer (HGT) to predict stress and displacement fields across stiffened panels based on loading and boundary conditions. Numerical tests on panels and box beams demonstrate the superior performance of the heterogeneous graph representation compared to a homogeneous counterpart. The approach shows strong predictive accuracy for both displacement and stress, effectively capturing structural behavior patterns and maximum values.<br /><br />Summary: <div>
arXiv:2510.02472v1 Announce Type: new 
Abstract: Surrogate models are essential in structural analysis and optimization. We propose a heterogeneous graph representation of stiffened panels that accounts for geometrical variability, non-uniform boundary conditions, and diverse loading scenarios, using heterogeneous graph neural networks (HGNNs). The structure is partitioned into multiple structural units, such as stiffeners and the plates between them, with each unit represented by three distinct node types: geometry, boundary, and loading nodes. Edge heterogeneity is introduced by incorporating local orientations and spatial relationships of the connecting nodes. Several heterogeneous graph representations, each with varying degrees of heterogeneity, are proposed and analyzed. These representations are implemented into a heterogeneous graph transformer (HGT) to predict von Mises stress and displacement fields across stiffened panels, based on loading and degrees of freedom at their boundaries. To assess the efficacy of our approach, we conducted numerical tests on panels subjected to patch loads and box beams composed of stiffened panels under various loading conditions. The heterogeneous graph representation was compared with a homogeneous counterpart, demonstrating superior performance. Additionally, an ablation analysis was performed to evaluate the impact of graph heterogeneity on HGT performance. The results show strong predictive accuracy for both displacement and von Mises stress, effectively capturing structural behavior patterns and maximum values.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisitHGNN: Heterogeneous Graph Neural Networks for Modeling Point-of-Interest Visit Patterns</title>
<link>https://arxiv.org/abs/2510.02702</link>
<guid>https://arxiv.org/abs/2510.02702</guid>
<content:encoded><![CDATA[
<div> Keywords: urban travel, origin-destination flow, graph neural network, mobility data, public health

Summary: 
- The study focuses on understanding urban residents' travel patterns for transportation planning, mobility management, and public health.
- VisitHGNN, a graph neural network, predicts visit probabilities at individual Points of interest (POIs) using historical flow patterns and spatial, temporal, and functional relations.
- POIs and census block groups (CBGs) are linked through spatial adjacency and distance-annotated cross-type edges.
- The model achieves strong predictive performance with mean KL divergence of 0.287, MAE of 0.008, and high accuracy metrics, outperforming baseline models.
- The model closely mirrors observed travel behavior, demonstrating its potential for decision support in urban planning, transportation policy, mobility system design, and public health. 

<br /><br />Summary: <div>
arXiv:2510.02702v1 Announce Type: new 
Abstract: Understanding how urban residents travel between neighborhoods and destinations is critical for transportation planning, mobility management, and public health. By mining historical origin-to-destination flow patterns with spatial, temporal, and functional relations among urban places, we estimate probabilities of visits from neighborhoods to specific destinations. These probabilities capture neighborhood-level contributions to citywide vehicular and foot traffic, supporting demand estimation, accessibility assessment, and multimodal planning. Particularly, we introduce VisitHGNN, a heterogeneous, relation-specific graph neural network designed to predict visit probabilities at individual Points of interest (POIs). POIs are characterized using numerical, JSON-derived, and textual attributes, augmented with fixed summaries of POI--POI spatial proximity, temporal co-activity, and brand affinity, while census block groups (CBGs) are described with 72 socio-demographic variables. CBGs are connected via spatial adjacency, and POIs and CBGs are linked through distance-annotated cross-type edges. Inference is constrained to a distance-based candidate set of plausible origin CBGs, and training minimizes a masked Kullback-Leibler (KL) divergence to yield probability distribution across the candidate set. Using weekly mobility data from Fulton County, Georgia, USA, VisitHGNN achieves strong predictive performance with mean KL divergence of 0.287, MAE of 0.008, Top-1 accuracy of 0.853, and R-square of 0.892, substantially outperforming pairwise MLP and distance-only baselines, and aligning closely with empirical visitation patterns (NDCG@50 = 0.966); Recall@5 = 0.611). The resulting distributions closely mirror observed travel behavior with high fidelity, highlighting the model's potential for decision support in urban planning, transportation policy, mobility system design, and public health.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Hit Moving Targets? Tracking Evolving Signals in Corporate Disclosures</title>
<link>https://arxiv.org/abs/2510.03195</link>
<guid>https://arxiv.org/abs/2510.03195</guid>
<content:encoded><![CDATA[
<div> entity recognition, target extraction, strategic shifting, key performance metrics, stock underperformance

Summary:
- The study focuses on moving targets in management, where shifting of key performance metrics predicts stock underperformance.
- Limitations in the original method, such as noise in extracted targets and loss of contextual information, were identified due to named entity recognition (NER) use.
- An LLM-based target extraction method with a new metric was proposed to capture semantic context better.
- The approach preserves semantic context and yields higher predictive power compared to the original method.
- Overall, the proposed approach enhances the granularity and accuracy of predicting financial performance based on text analysis.

<br /><br />Summary: <div>
arXiv:2510.03195v1 Announce Type: new 
Abstract: Moving targets -- managers' strategic shifting of key performance metrics when the original targets become difficult to achieve -- have been shown to predict subsequent stock underperformance. However, our work reveals that the method employed in that study exhibits two key limitations that hinder the accuracy -- noise in the extracted targets and loss of contextual information -- both of which stem primarily from the use of a named entity recognition (NER). To address these two limitations, we propose an LLM-based target extraction} method with a newly defined metric that better captures semantic context. This approach preserves semantic context beyond simple entity recognition and yields consistently higher predictive power than the original approach. Overall, our approach enhances the granularity and accuracy of financial text-based performance prediction.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully automated inverse co-optimization of templates and block copolymer blending recipes for DSA lithography</title>
<link>https://arxiv.org/abs/2510.02715</link>
<guid>https://arxiv.org/abs/2510.02715</guid>
<content:encoded><![CDATA[
<div> Gaussian descriptor, template shape, self-assembly, block copolymers, Bayesian optimization <br />
Summary: <br />
The article discusses the directed self-assembly (DSA) of block copolymers (BCPs) for fabricating contact holes at sub-7nm technology nodes. A Gaussian descriptor with two parameters is proposed to characterize template shapes for guiding self-assembly. AB/AB binary blends are suggested for improved adaptability to template shapes. Bayesian optimization is used to co-optimize the binary blend and template shape, leading to optimal templates for various multi-hole patterns. Templates are optimized with constraints on curvature variation for manufacturability. Key parameters of the blend have wide tunable windows for high precision requirements. This research provides insights for advancing DSA technology and its practical applications. <br /> <div>
arXiv:2510.02715v1 Announce Type: cross 
Abstract: The directed self-assembly (DSA) of block copolymers (BCPs) offers a highly promising approach for the fabrication of contact holes or vertical interconnect access at sub-7nm technology nodes. To fabricate circular holes with precisely controlled size and positions, the self-assembly of block copolymers requires guidance from a properly designed template. Effectively parameterizing the template shape to enable efficient optimization remains a critical yet challenging problem. Moreover, the optimized template must possess excellent manufacturability for practical applications. In this work, we propose a Gaussian descriptor for characterizing the template shape with only two parameters. We further propose to use AB/AB binary blends instead of pure diblock copolymer to improve the adaptability of the block copolymer system to the template shape. The Bayesian optimization (BO) is applied to co-optimize the binary blend and the template shape. Our results demonstrate that BO based on the Gaussian descriptor can efficiently yield the optimal templates for diverse multi-hole patterns, all leading to highly matched self-assembled morphologies. Moreover, by imposing constraints on the variation of curvature of the template during optimization, superior manufacturability is ensured for each optimized template. It is noteworthy that each key parameter of the blend exhibits a relatively wide tunable window under the requirement of rather high precision. Our work provides valuable insights for advancing DSA technology, and thus potentially propels its practical applications forward.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories</title>
<link>https://arxiv.org/abs/2510.03152</link>
<guid>https://arxiv.org/abs/2510.03152</guid>
<content:encoded><![CDATA[
<div> modeling, human mobility, urban planning, spatiotemporal trajectories, Patterns of Life (PoLs) 

Summary:
The article introduces Markovian Reeb Graphs, a novel framework for simulating spatiotemporal trajectories that accurately capture Patterns of Life (PoLs) learned from baseline data. By incorporating individual- and population-level mobility structures in a probabilistic topological model, the approach generates realistic future trajectories that maintain both consistency and variability in daily life. Evaluations on the Urban Anomalies dataset (Atlanta and Berlin subsets) demonstrate that the method achieves high fidelity using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics. The results show that Markovian Reeb Graphs are data- and compute-efficient while providing a scalable framework for trajectory simulation suitable for various urban environments. <div>
arXiv:2510.03152v1 Announce Type: cross 
Abstract: Accurately modeling human mobility is critical for urban planning, epidemiology, and traffic management. In this work, we introduce Markovian Reeb Graphs, a novel framework for simulating spatiotemporal trajectories that preserve Patterns of Life (PoLs) learned from baseline data. By combining individual- and population-level mobility structures within a probabilistic topological model, our approach generates realistic future trajectories that capture both consistency and variability in daily life. Evaluations on the Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon Divergence (JSD) across population- and agent-level metrics demonstrate that the proposed method achieves strong fidelity while remaining data- and compute-efficient. These results position Markovian Reeb Graphs as a scalable framework for trajectory simulation with broad applicability across diverse urban environments.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation</title>
<link>https://arxiv.org/abs/2510.01225</link>
<guid>https://arxiv.org/abs/2510.01225</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial digests, data extraction, automated generation, actionable insights 

Summary: 
Large Language Models (LLMs), such as Google's Gemini Pro, offer a new framework for automatically generating insightful financial digests. By extracting data from OpenAlex, strategically engineering prompts, and utilizing LLM-driven analysis, this approach provides comprehensive digests that highlight key findings and emerging trends. This method overcomes traditional analysis limitations by efficiently processing vast amounts of unstructured data and delivering actionable insights in a user-friendly format. The paper explains the workings of LLMs in simple terms and demonstrates how their power can help researchers and scholars save time and stay informed on current trends. The study details the step-by-step process, from data acquisition and JSON construction to interacting with Gemini and automating PDF report generation. A link to the project's GitHub repository is provided for broader accessibility and continued development. 

<br /><br />Summary: <div>
arXiv:2510.01225v1 Announce Type: new 
Abstract: The exponential growth of information presents a significant challenge for researchers and professionals seeking to remain at the forefront of their fields and this paper introduces an innovative framework for automatically generating insightful financial digests using the power of Large Language Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of data extraction from OpenAlex, strategic prompt engineering, and LLM-driven analysis, we demonstrate the automated example of creating a comprehensive digests that generalize key findings, identify emerging trends. This approach addresses the limitations of traditional analysis methods, enabling the efficient processing of vast amounts of unstructured data and the delivery of actionable insights in an easily digestible format. This paper describes how LLMs work in simple words and how we can use their power to help researchers and scholars save their time and stay informed about current trends. Our study includes step-by-step process, from data acquisition and JSON construction to interaction with Gemini and the automated generation of PDF reports, including a link to the project's GitHub repository for broader accessibility and further development.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioRAG: A Retrieval-Augmented Generation Framework for Multimodal Chagas Disease Detection</title>
<link>https://arxiv.org/abs/2510.01558</link>
<guid>https://arxiv.org/abs/2510.01558</guid>
<content:encoded><![CDATA[
<div> AI-enhanced electrocardiogram (ECG) screening is proposed for Chagas disease diagnosis in regions with limited testing capabilities. A retrieval-augmented generation framework, CardioRAG, integrates large language models with ECG-based clinical features for improved accuracy and interpretability. Key features include right bundle branch block, left anterior fascicular block, and heart rate variability metrics. Variational autoencoder-learned representations aid in semantic case retrieval to guide clinical reasoning. Evaluation results show high recall performance (89.80%) and a maximum F1 score of 0.68, effectively identifying positive cases for prioritized serological testing. CardioRAG offers an evidence-based, interpretable approach suitable for resource-constrained settings, showcasing the potential of incorporating clinical indicators in medical AI systems. 

Keywords: Chagas disease, AI-enhanced ECG screening, clinical features, interpretation, interpretable medical AI systems

<br /><br />Summary: 
- Chagas disease diagnosis in regions with limited testing capabilities
- Integration of ECG-based clinical features for accuracy and interpretability
- Key features: right bundle branch block, left anterior fascicular block, heart rate variability metrics
- Utilization of variational autoencoder-learned representations for semantic case retrieval
- High recall performance and effective identification of positive cases for prioritized serological testing 
- Resource-constrained settings benefit from an evidence-based, interpretable approach
- Potential of incorporating clinical indicators in medical AI systems <div>
arXiv:2510.01558v1 Announce Type: new 
Abstract: Chagas disease affects nearly 6 million people worldwide, with Chagas cardiomyopathy representing its most severe complication. In regions where serological testing capacity is limited, AI-enhanced electrocardiogram (ECG) screening provides a critical diagnostic alternative. However, existing machine learning approaches face challenges such as limited accuracy, reliance on large labeled datasets, and more importantly, weak integration with evidence-based clinical diagnostic indicators. We propose a retrieval-augmented generation framework, CardioRAG, integrating large language models with interpretable ECG-based clinical features, including right bundle branch block, left anterior fascicular block, and heart rate variability metrics. The framework uses variational autoencoder-learned representations for semantic case retrieval, providing contextual cases to guide clinical reasoning. Evaluation demonstrated high recall performance of 89.80%, with a maximum F1 score of 0.68 for effective identification of positive cases requiring prioritized serological testing. CardioRAG provides an interpretable, clinical evidence-based approach particularly valuable for resource-limited settings, demonstrating a pathway for embedding clinical indicators into trustworthy medical AI systems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShapeGen3DCP: A Deep Learning Framework for Layer Shape Prediction in 3D Concrete Printing</title>
<link>https://arxiv.org/abs/2510.02009</link>
<guid>https://arxiv.org/abs/2510.02009</guid>
<content:encoded><![CDATA[
<div> ShapeGen3DCP, deep learning, filament cross-sectional geometry, 3D Concrete Printing, neural network<br />
<br />
Summary:<br />
ShapeGen3DCP introduces a deep learning framework for accurately predicting filament cross-sectional geometry in 3D Concrete Printing. The framework utilizes a neural network architecture that incorporates material properties and process parameters to predict extruded layer shapes. By reformulating inputs into dimensionless parameters and using Fourier descriptors to represent predicted geometries, the framework achieves compact and smooth profiles. The training dataset is generated synthetically using a PFEM model, enabling validation against numerical and experimental cases. This approach enhances generalization and accuracy of predictions, offering practical applications such as pre-calibrating print settings and optimizing toolpaths. Future developments may involve integrating the framework with simulations and sensor feedback, enabling real-time optimization, defect detection, and adaptive control in 3DCP processes. <div>
arXiv:2510.02009v1 Announce Type: new 
Abstract: This work introduces ShapeGen3DCP, a deep learning framework for fast and accurate prediction of filament cross-sectional geometry in 3D Concrete Printing (3DCP). The method is based on a neural network architecture that takes as input both material properties in the fluid state (density, yield stress, plastic viscosity) and process parameters (nozzle diameter, nozzle height, printing and flow velocities) to directly predict extruded layer shapes. To enhance generalization, some inputs are reformulated into dimensionless parameters that capture underlying physical principles. Predicted geometries are compactly represented using Fourier descriptors, which enforce smooth, closed, and symmetric profiles while reducing the prediction task to a small set of coefficients. The training dataset was synthetically generated using a well-established Particle Finite Element (PFEM) model of 3DCP, overcoming the scarcity of experimental data. Validation against diverse numerical and experimental cases shows strong agreement, confirming the framework's accuracy and reliability. This opens the way to practical uses ranging from pre-calibration of print settings, minimizing or even eliminating trial-and-error adjustments, to toolpath optimization for more advanced designs. Looking ahead, coupling the framework with simulations and sensor feedback could enable closed-loop digital twins for 3DCP, driving real-time process optimization, defect detection, and adaptive control of printing parameters.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Copula-Based Variational Autoencoder for Uncertainty Quantification in Inverse Problems: Application to Damage Identification in an Offshore Wind Turbine</title>
<link>https://arxiv.org/abs/2510.02013</link>
<guid>https://arxiv.org/abs/2510.02013</guid>
<content:encoded><![CDATA[
<div> varitational autoencoder, floating offshore wind turbines, structural health monitoring, damage identification, copula<br />
<br />
Summary: <br />
Structural Health Monitoring of Floating Offshore Wind Turbines (FOWTs) is crucial for safety and efficiency. This study addresses the challenge of identifying damage in components like mooring systems from limited sensor data by proposing a Variational Autoencoder (VAE) architecture. The VAE models the inverse operator through the encoder and the forward operator through the decoder, with a probabilistic representation of uncertainties in the estimates. A novel Copula-based VAE architecture is introduced, offering a flexible method for representing complex, correlated posterior distributions. Comparative analysis shows that the Copula VAE outperforms Gaussian Mixture alternatives in high-dimensional spaces with fewer parameters, making it a promising tool for uncertainty-aware damage identification in FOWT mooring systems. While the study is limited to a two-dimensional space, scalability to higher dimensions is anticipated based on the favorable performance of the Copula VAE. <div>
arXiv:2510.02013v1 Announce Type: new 
Abstract: Structural Health Monitoring of Floating Offshore Wind Turbines (FOWTs) is critical for ensuring operational safety and efficiency. However, identifying damage in components like mooring systems from limited sensor data poses a challenging inverse problem, often characterized by multimodal solutions where various damage states could explain the observed response. To overcome it, we propose a Variational Autoencoder (VAE) architecture, where the encoder approximates the inverse operator, while the decoder approximates the forward. The posterior distribution of the latent space variables is probabilistically modeled, describing the uncertainties in the estimates. This work tackles the limitations of conventional Gaussian Mixtures used within VAEs, which can be either too restrictive or computationally prohibitive for high-dimensional spaces. We propose a novel Copula-based VAE architecture that decouples the marginal distribution of the variables from their dependence structure, offering a flexible method for representing complex, correlated posterior distributions. We provide a comprehensive comparison of three different approaches for approximating the posterior: a Gaussian Mixture with a diagonal covariance matrix, a Gaussian Mixture with a full covariance matrix, and a Gaussian Copula. Our analysis, conducted on a high-fidelity synthetic dataset, demonstrates that the Copula VAE offers a promising and tractable solution in high-dimensional spaces. Although the present work remains in the two-dimensional space, the results suggest efficient scalability to higher dimensions. It achieves superior performance with significantly fewer parameters than the Gaussian Mixture alternatives, whose parametrization grows prohibitively with the dimensionality. The results underscore the potential of Copula-based VAEs as a tool for uncertainty-aware damage identification in FOWT mooring systems.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast training of accurate physics-informed neural networks without gradient descent</title>
<link>https://arxiv.org/abs/2405.20836</link>
<guid>https://arxiv.org/abs/2405.20836</guid>
<content:encoded><![CDATA[
<div> PINNs, time-dependent Partial Differential Equations, Frozen-PINN, space-time separation, training efficiency 

Summary: 
Frozen-PINN introduces a novel approach to solving time-dependent Partial Differential Equations (PDEs) by leveraging space-time separation and random features instead of gradient descent for training. This method addresses the limitations of traditional Physics-Informed Neural Networks (PINNs) by incorporating temporal causality and achieving superior training efficiency and accuracy on eight challenging PDE benchmarks. The Frozen-PINN outperforms state-of-the-art PINNs by several orders of magnitude, providing a quick, accurate, and causal PDE solver. By challenging the reliance on stochastic gradient descent and specialized hardware, this approach presents a paradigm shift in PINN training and serves as a benchmark for the community. <div>
arXiv:2405.20836v2 Announce Type: cross 
Abstract: Solving time-dependent Partial Differential Equations (PDEs) is one of the most critical problems in computational science. While Physics-Informed Neural Networks (PINNs) offer a promising framework for approximating PDE solutions, their accuracy and training speed are limited by two core barriers: gradient-descent-based iterative optimization over complex loss landscapes and non-causal treatment of time as an extra spatial dimension. We present Frozen-PINN, a novel PINN based on the principle of space-time separation that leverages random features instead of training with gradient descent, and incorporates temporal causality by construction. On eight PDE benchmarks, including challenges such as extreme advection speeds, shocks, and high dimensionality, Frozen-PINNs achieve superior training efficiency and accuracy over state-of-the-art PINNs, often by several orders of magnitude. Our work addresses longstanding training and accuracy bottlenecks of PINNs, delivering quickly trainable, highly accurate, and inherently causal PDE solvers, a combination that prior methods could not realize. Our approach challenges the reliance of PINNs on stochastic gradient-descent-based methods and specialized hardware, leading to a paradigm shift in PINN training and providing a challenging benchmark for the community.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Enhanced, Data-Driven Personalized and Equitable Clinician Scheduling: A Predict-then-Optimize Approach</title>
<link>https://arxiv.org/abs/2510.02047</link>
<guid>https://arxiv.org/abs/2510.02047</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinician scheduling, Large language models, Predict-then-optimize framework, Schedule optimization, Clinician well-being<br />
Summary: <br />
Clinician scheduling in academic anesthesiology departments is a challenging task due to limited resources and varying demands. A new framework is proposed that combines availability predictions from large language models with a schedule optimization model. By leveraging unstructured data such as free-text notes, the framework aims to improve schedule alignment, reduce burnout, and optimize resource utilization. The framework considers four key objectives: ensuring compliance with full-time equivalent regulations, balancing workload distribution, maximizing clinician availability for shifts, and promoting schedule consistency. By integrating the interpretive power of large language models with mathematical optimization, the framework offers a data-driven solution to enhance operational efficiency and support equity and clinician well-being. <br /> <div>
arXiv:2510.02047v1 Announce Type: cross 
Abstract: Clinician scheduling remains a persistent challenge due to limited clinical resources and fluctuating demands. This complexity is especially acute in large academic anesthesiology departments as physicians balance responsibilities across multiple clinical sites with conflicting priorities. Further, scheduling must account for individual clinical and lifestyle preferences to ensure job satisfaction and well-being. Traditional approaches, often based on statistical or rule-based optimization models, rely on structured data and explicit domain knowledge. However, these methods often overlook unstructured information, e.g., free-text notes from routinely administered clinician well-being surveys and scheduling platforms. These notes may reveal implicit and underutilized clinical resources. Neglecting such information can lead to misaligned schedules, increased burnout, overlooked staffing flexibility, and suboptimal utilization of available resources. To address this gap, we propose a predict-then-optimize framework that integrates classification-based clinician availability predictions with a mixed-integer programming schedule optimization model. Large language models (LLMs) are employed to extract actionable preferences and implicit constraints from unstructured schedule notes, enhancing the reliability of availability predictions. These predictions then inform the schedule optimization considering four objectives: first, ensuring clinical full-time equivalent compliance, second, reducing workload imbalances by enforcing equitable proportions of shift types, third, maximizing clinician availability for assigned shifts, and fourth, schedule consistency. By combining the interpretive power of LLMs with the rigor of mathematical optimization, our framework provides a robust, data-driven solution that enhances operational efficiency while supporting equity and clinician well-being.
]]></content:encoded>
<pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow of Knowledge: Federated Fine-Tuning of LLMs in Healthcare under Non-IID Conditions</title>
<link>https://arxiv.org/abs/2510.00543</link>
<guid>https://arxiv.org/abs/2510.00543</guid>
<content:encoded><![CDATA[
<div> privacy-preserving, federated learning, healthcare, large language models, LoRA 
Summary: 
Federated fine-tuning approach based on Low-Rank Adaptation (LoRA) is introduced for large language models (LLMs) in healthcare to overcome data privacy restrictions and facilitate cross-institution collaboration. The method combines local LoRA adaptation and global parameter aggregation, enabling knowledge sharing without exposing raw data. A blockchain identity scheme is utilized for LLM identification in distributed networks. Experiments on non-IID medical text datasets demonstrate that federated LoRA enhances cross-client generalization and boosts the performance of the weakest client, ensuring stable convergence and fairer outcomes. This approach offers a practical and effective paradigm for adapting LLMs in healthcare, opening up new possibilities for multi-center medical AI collaboration. 
<br /><br />Summary: <div>
arXiv:2510.00543v1 Announce Type: new 
Abstract: Large language models (LLMs) show great promise in healthcare, but their applications are hindered by data privacy restrictions and the challenges of cross-institution collaboration. Sensitive medical data cannot be centralized, while non-independent and identically distributed (non-IID) characteristics across institutions further complicate convergence and fairness. To address these issues, we present a federated fine-tuning approach based on Low-Rank Adaptation (LoRA), enabling privacy-preserving knowledge flow across institutions. The method iteratively combines local LoRA adaptation with global parameter aggregation, allowing efficient knowledge sharing without exposing raw data. A blockchain identity scheme is used for identifying individual LLM in such a distributed network. We evaluate this approach on heterogeneous and highly non-IID medical text datasets, where experiments demonstrate that federated LoRA not only enhances cross-client generalization but also improves the performance of the weakest client, achieving stable convergence and fairer outcomes. These findings highlight federated LoRA fine-tuning as a practical and effective paradigm for adapting LLMs in healthcare, offering a new path for multi-center medical AI collaboration.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal Classification Recovery Across Domains Using Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2510.00589</link>
<guid>https://arxiv.org/abs/2510.00589</guid>
<content:encoded><![CDATA[
<div> Adversarial learning, statistical distance alignment, stochastic modeling, unsupervised domain adaptation, deep neural networks <br />
Summary:
This paper investigates the use of unsupervised domain adaptation techniques to improve the generalization of signal classification models trained on controlled datasets to real-world scenarios with varying channel environments. The study focuses on aligning representations between simulated and over-the-air signal domains using methods such as adversarial learning, statistical distance alignment, and stochastic modeling. By deliberately generating modulated signals with realistic channel impairments, the researchers evaluate classification performance under different scenarios, including cross-SNR and SNR-matched cross-domain situations. The results demonstrate that unsupervised domain adaptation methods, specifically stochastic classifier (STAR) and joint adaptive networks (JAN), offer significant performance improvements over baseline models. These findings suggest the potential of these techniques for enhancing the deployment of deep neural networks in wireless systems. <br /> <div>
arXiv:2510.00589v1 Announce Type: new 
Abstract: Signal classification models based on deep neural networks are typically trained on datasets collected under controlled conditions, either simulated or over-the-air (OTA), which are constrained to specific channel environments with limited variability, such as fixed signal-to-noise ratio (SNR) levels. As a result, these models often fail to generalize when deployed in real-world scenarios where the feature distribution significantly differs from the training domain. This paper explores unsupervised domain adaptation techniques to bridge the generalization gap between mismatched domains. Specifically, we investigate adaptation methods based on adversarial learning, statistical distance alignment, and stochastic modeling to align representations between simulated and OTA signal domains. To emulate OTA characteristics, we deliberately generate modulated signals subjected to realistic channel impairments without demodulation. We evaluate classification performance under three scenarios, i.e., cross-SNR, SNR-matched cross-domain, and stepwise adaptation involving both SNR and domain shifts. Experimental results show that unsupervised domain adaptation methods, particularly stochastic classifier (STAR) and joint adaptive networks (JAN), enable consistent and substantial performance gains over baseline models, which highlight their promise for real-world deployment in wireless systems.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Economic Impact of DeFi Crime Events on Decentralized Autonomous Organizations (DAOs)</title>
<link>https://arxiv.org/abs/2510.00669</link>
<guid>https://arxiv.org/abs/2510.00669</guid>
<content:encoded><![CDATA[
<div> Decentralized Finance, DeFi, autonomous organizations, governance assets, crime events<br />
<br />
Summary:
Crime events in the Decentralized Finance (DeFi) ecosystem have resulted in over $10 billion in direct losses, triggering broader market reactions. Decentralized Autonomous Organizations (DAOs) govern DeFi applications through tradable governance assets, similar to corporate shares. A study conducted on 22 crime events between 2020 and 2022 examined their economic impact on governance asset prices, trading volumes, and market capitalization using a dynamic difference-in-differences (DiD) framework. Results indicate that 55% of crime events lead to significant negative price impacts, with an average decline of 14%. Furthermore, 68% of crime events result in increased trading volume of governance assets. The indirect economic losses estimated from these impacts exceed $1.3 billion in DAO market capitalization, surpassing direct victim costs and accounting for 74% of total losses. This study provides valuable insights into how crime events influence market dynamics and affect DAOs, offering a reproducible methodological approach applicable to other cryptoassets. <br /><br /> <div>
arXiv:2510.00669v1 Announce Type: new 
Abstract: The Decentralized Finance (DeFi) ecosystem has experienced over \$10 billion in direct losses due to crime events. Beyond these immediate losses, such events often trigger broader market reactions, including price declines, trading activity changes, and reductions in market capitalization. Decentralized Autonomous Organizations (DAOs) govern DeFi applications through tradable governance assets that function like corporate shares for voting and decision-making. Leveraging DeFi's granular trading data, we conduct an event study on 22 crime events between 2020 and 2022 to assess their economic impact on governance asset prices, trading volumes, and market capitalization. Using a dynamic difference-in-differences (DiD) framework with counterfactual governance assets, we aim for causal inference of intraday temporal effects. Our results show that 55% of crime events lead to significant negative price impacts, with an average decline of about 14%. Additionally, 68% of crime events lead to increased governance asset trading volume. Based on these impacts, we estimate indirect economic losses of over $1.3 billion in DAO market capitalization, far exceeding direct victim costs and accounting for 74% of total losses. Our study provides valuable insights into how crime events shape market dynamics and affect DAOs. Moreover, our methodological approach is reproducible and applicable beyond DAOs, offering a framework to assess the indirect economic impact on other cryptoassets.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMMET: orders-of-magnitude speed-up in finite element method via batch-vectorized neural constitutive updates</title>
<link>https://arxiv.org/abs/2510.00884</link>
<guid>https://arxiv.org/abs/2510.00884</guid>
<content:encoded><![CDATA[
<div> Keywords: Constitutive evaluations, Neural constitutive models, Finite element simulations, Computational mechanics, High-fidelity simulations

Summary:<br /><br />Constitutive evaluations in finite element simulations can be costly when using complex material models. Neural constitutive models (NCMs) provide a flexible framework for modeling such behavior in solid mechanics but have limited practical adoption due to high computational costs. The COMMET FE framework is introduced to address this issue, featuring a redesigned architecture that accelerates costly constitutive updates. It includes a novel assembly algorithm supporting batched and vectorized evaluations, optimized derivatives, and distributed-memory parallelism via MPI to reduce runtime significantly. The framework demonstrates speed-ups over traditional implementations, making it possible to efficiently perform large-scale simulations with high fidelity. These advancements primarily target NCMs but can be applied more broadly to improve performance where constitutive updates or assembly processes limit computational efficiency in computational mechanics. <div>
arXiv:2510.00884v1 Announce Type: new 
Abstract: Constitutive evaluations often dominate the computational cost of finite element (FE) simulations whenever material models are complex. Neural constitutive models (NCMs) offer a highly expressive and flexible framework for modeling complex material behavior in solid mechanics. However, their practical adoption in large-scale FE simulations remains limited due to significant computational costs, especially in repeatedly evaluating stress and stiffness. NCMs thus represent an extreme case: their large computational graphs make stress and stiffness evaluations prohibitively expensive, restricting their use to small-scale problems. In this work, we introduce COMMET, an open-source FE framework whose architecture has been redesigned from the ground up to accelerate high-cost constitutive updates. Our framework features a novel assembly algorithm that supports batched and vectorized constitutive evaluations, compute-graph-optimized derivatives that replace automatic differentiation, and distributed-memory parallelism via MPI. These advances dramatically reduce runtime, with speed-ups exceeding three orders of magnitude relative to traditional non-vectorized automatic differentiation-based implementations. While we demonstrate these gains primarily for NCMs, the same principles apply broadly wherever for-loop based assembly or constitutive updates limit performance, establishing a new standard for large-scale, high-fidelity simulations in computational mechanics.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Market States with Clustering and State Machines</title>
<link>https://arxiv.org/abs/2510.00953</link>
<guid>https://arxiv.org/abs/2510.00953</guid>
<content:encoded><![CDATA[
<div> probabilistic state machine, financial markets, market states, transition matrix, asset returns<br />
Summary:
This study presents a novel framework for modeling financial markets using an interpretable probabilistic state machine. By clustering historical returns based on momentum and risk features across different time horizons, distinct market states representing various regimes such as expansion, contraction, crisis, and recovery are identified. A transition matrix is then constructed to capture the dynamics between these states, creating a probabilistic state machine that models the market's temporal evolution. This state machine allows for the generation of a customized distribution of returns by combining Gaussian components weighted by state frequencies. Results show that this approach outperforms traditional methods in capturing key statistical properties of asset returns, including skewness and kurtosis. Robustness is confirmed through experiments across various assets and time periods. <div>
arXiv:2510.00953v1 Announce Type: new 
Abstract: This work introduces a new framework for modeling financial markets through an interpretable probabilistic state machine. By clustering historical returns based on momentum and risk features across multiple time horizons, we identify distinct market states that capture underlying regimes, such as expansion phase, contraction, crisis, or recovery. From a transition matrix representing the dynamics between these states, we construct a probabilistic state machine that models the temporal evolution of the market. This state machine enables the generation of a custom distribution of returns based on a mixture of Gaussian components weighted by state frequencies. We show that the proposed benchmark significantly outperforms the traditional approach in capturing key statistical properties of asset returns, including skewness and kurtosis, and our experiments across random assets and time periods confirm its robustness.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets</title>
<link>https://arxiv.org/abs/2510.00332</link>
<guid>https://arxiv.org/abs/2510.00332</guid>
<content:encoded><![CDATA[
<div> blind spot, adversarial environments, misinformation, financial decisions, model evaluation
<br />
Summary: 
The article introduces CAIA, a benchmark that highlights the inadequacy of state-of-the-art AI models in handling adversarial, high-stakes environments where misinformation is rampant and errors are irreversible. The benchmark evaluates 17 models on tasks related to distinguishing truth from manipulation, navigating fragmented information landscapes, and making financial decisions under adversarial pressure, using crypto markets as a testbed. The findings reveal a significant capability gap, with models achieving only 28% accuracy compared to a human baseline of 80%. Despite access to professional resources, model performance plateaus at 67.4% due to a preference for unreliable sources like web search over authoritative data. The benchmark also uncovers dangerous trial-and-error behavior in model decision-making, emphasizing the importance of adversarial robustness in AI deployment across various domains. <div>
arXiv:2510.00332v1 Announce Type: cross 
Abstract: We present CAIA, a benchmark exposing a critical blind spot in AI evaluation: the inability of state-of-the-art models to operate in adversarial, high-stakes environments where misinformation is weaponized and errors are irreversible. While existing benchmarks measure task completion in controlled settings, real-world deployment demands resilience against active deception. Using crypto markets as a testbed where $30 billion was lost to exploits in 2024, we evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish truth from manipulation, navigate fragmented information landscapes, and make irreversible financial decisions under adversarial pressure.
  Our results reveal a fundamental capability gap: without tools, even frontier models achieve only 28% accuracy on tasks junior analysts routinely handle. Tool augmentation improves performance but plateaus at 67.4% versus 80% human baseline, despite unlimited access to professional resources. Most critically, we uncover a systematic tool selection catastrophe: models preferentially choose unreliable web search over authoritative data, falling for SEO-optimized misinformation and social media manipulation. This behavior persists even when correct answers are directly accessible through specialized tools, suggesting foundational limitations rather than knowledge gaps. We also find that Pass@k metrics mask dangerous trial-and-error behavior for autonomous deployment.
  The implications extend beyond crypto to any domain with active adversaries, e.g. cybersecurity, content moderation, etc. We release CAIA with contamination controls and continuous updates, establishing adversarial robustness as a necessary condition for trustworthy AI autonomy. The benchmark reveals that current models, despite impressive reasoning scores, remain fundamentally unprepared for environments where intelligence must survive active opposition.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous Graphs for Urban Microclimate Prediction</title>
<link>https://arxiv.org/abs/2510.00457</link>
<guid>https://arxiv.org/abs/2510.00457</guid>
<content:encoded><![CDATA[
<div> framework, physics-informed, urban microclimates, spatio-temporal graphs, heterogeneous dynamic graphs
Summary:<br />
- The article introduces UrbanGraph, a framework for predicting urban microclimates that addresses shortcomings of existing approaches by integrating heterogeneous and dynamic spatio-temporal graphs.
- UrbanGraph encodes key physical processes like vegetation evapotranspiration, shading, and convective diffusion, while modeling complex spatial dependencies among urban entities and their temporal evolution.
- Evaluation on the UMC4/12 dataset shows that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% compared to baselines, with heterogeneous and dynamic graphs contributing to the gains.
- The dataset used provides a high-resolution benchmark for spatio-temporal microclimate modeling, allowing for more accurate predictions.
- UrbanGraph's approach can be extended to other urban heterogeneous dynamic computing tasks, highlighting its potential impact beyond microclimate modeling.
<br />Summary: <div>
arXiv:2510.00457v1 Announce Type: cross 
Abstract: With rapid urbanization, predicting urban microclimates has become critical, as it affects building energy demand and public health risks. However, existing generative and homogeneous graph approaches fall short in capturing physical consistency, spatial dependencies, and temporal variability. To address this, we introduce UrbanGraph, a physics-informed framework integrating heterogeneous and dynamic spatio-temporal graphs. It encodes key physical processes -- vegetation evapotranspiration, shading, and convective diffusion -- while modeling complex spatial dependencies among diverse urban entities and their temporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based simulation dataset covering diverse urban configurations and climates. Results show that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% over all baselines, with heterogeneous and dynamic graphs contributing 3.5% and 7.1% gains. Our dataset provides the first high-resolution benchmark for spatio-temporal microclimate modeling, and our method extends to broader urban heterogeneous dynamic computing tasks.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques</title>
<link>https://arxiv.org/abs/2510.00836</link>
<guid>https://arxiv.org/abs/2510.00836</guid>
<content:encoded><![CDATA[
<div> SMOTE, ensemble learning models, pump and dump, cryptocurrency markets, manipulation <br />
<br />
Summary: This study focuses on detecting pump and dump (P&amp;D) manipulation in cryptocurrency markets, addressing the challenge of severe class imbalance. By applying the Synthetic Minority Oversampling Technique (SMOTE) and evaluating advanced ensemble learning models, the researchers successfully identified manipulative trading behavior from normal market activity. The experimental results demonstrated that using SMOTE significantly improved the ability of all models to detect P&amp;D events, enhancing recall and achieving a better balance between precision and recall. Specifically, XGBoost and LightGBM stood out with high recall rates and strong F1-scores, along with fast computational performance suitable for near real-time surveillance. The integration of data balancing techniques with ensemble methods proved to be effective in early detection of manipulative activities, contributing to a more fair, transparent, and stable cryptocurrency market. 
<br /><br /> <div>
arXiv:2510.00836v1 Announce Type: cross 
Abstract: This study aims to detect pump and dump (P&amp;D) manipulation in cryptocurrency markets, where the scarcity of such events causes severe class imbalance and hinders accurate detection. To address this issue, the Synthetic Minority Oversampling Technique (SMOTE) was applied, and advanced ensemble learning models were evaluated to distinguish manipulative trading behavior from normal market activity. The experimental results show that applying SMOTE greatly enhanced the ability of all models to detect P&amp;D events by increasing recall and improving the overall balance between precision and recall. In particular, XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%, respectively) with strong F1-scores and demonstrated fast computational performance, making them suitable for near real time surveillance. These findings indicate that integrating data balancing techniques with ensemble methods significantly improves the early detection of manipulative activities, contributing to a fairer, more transparent, and more stable cryptocurrency market.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Machine Learning Approach in Augmenting RANS Models Using DNS Data and DeepInsight Method on FDA Nozzle</title>
<link>https://arxiv.org/abs/2510.01091</link>
<guid>https://arxiv.org/abs/2510.01091</guid>
<content:encoded><![CDATA[
<div> machine learning, turbulence modeling, OpenFOAM, Reynolds stress tensor, DNS 

Summary:
- The study introduces a data-driven framework for turbulence modeling in flow prediction in the FDA nozzle, using a hybrid implicit-explicit approach.
- New variables are introduced, and a solver is developed within the OpenFOAM framework, incorporating a machine learning module to estimate these variables.
- Invariant input features are derived based on Hilbert's basis theorem, and the machine learning model's outputs are obtained through eigenvalue-vector decomposition of the Reynolds stress tensor.
- Validation is performed using DNS data for turbulent flow in a square channel at various Reynolds numbers.
- A Deep-Insight network trained on benchmark DNS datasets as images demonstrates improved prediction of turbulence structures in the FDA blood nozzle, showcasing the potential of data-driven augmentation in turbulence modeling. 

Summary:<br />
Keywords: machine learning, turbulence modeling, OpenFOAM, Reynolds stress tensor, DNS <br /> <div>
arXiv:2510.01091v1 Announce Type: cross 
Abstract: We present a data-driven framework for turbulence modeling, applied to flow prediction in the FDA nozzle. In this study, the standard RANS equations have been modified using an implicit-explicit hybrid approach. New variables were introduced, and a solver was developed within the OpenFOAM framework, integrating a machine learning module to estimate these variables. The invariant input features were derived based on Hilbert's basis theorem, and the outputs of the machine learning model were obtained through eigenvalue-vector decomposition of the Reynolds stress tensor. Validation was performed using DNS data for turbulent flow in a square channel at various Reynolds numbers. A baseline MLP was first trained at $Re=2900$ and tested at $Re=3500$ to assess its ability to reproduce turbulence anisotropy and secondary flows. To further enhance generalization, three benchmark DNS datasets were transformed into images via the Deep-Insight method, enabling the use of convolutional neural networks. The trained Deep-Insight network demonstrated improved prediction of turbulence structures in the FDA blood nozzle, highlighting the promise of data-driven augmentation in turbulence modeling.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-patch isogeometric neural solver for partial differential equations on computer-aided design domains</title>
<link>https://arxiv.org/abs/2509.25450</link>
<guid>https://arxiv.org/abs/2509.25450</guid>
<content:encoded><![CDATA[
<div> neural networks, isogeometric analysis, partial differential equations, computational framework, finite element solvers
Summary:
This work presents a computational framework that combines physics-informed neural networks with multi-patch isogeometric analysis to solve partial differential equations on complex computer-aided design geometries. The method utilizes patch-local neural networks operating on the reference domain of isogeometric analysis and enforces Dirichlet boundary conditions with a custom output layer. Interface neural networks ensure solution conformity across non-uniform rational B-spline patch interfaces. Training follows a variational framework by minimizing the energy functional. The method's effectiveness is demonstrated on two challenging use-cases, a 2D magnetostatics model and a 3D nonlinear solid and contact mechanics model, showing excellent agreement with high-fidelity finite element solver solutions. This neural solver shows promise in addressing complex engineering problems with corresponding computer-aided design models. <br /><br />Summary: <div>
arXiv:2509.25450v1 Announce Type: new 
Abstract: This work develops a computational framework that combines physics-informed neural networks with multi-patch isogeometric analysis to solve partial differential equations on complex computer-aided design geometries. The method utilizes patch-local neural networks that operate on the reference domain of isogeometric analysis. A custom output layer enables the strong imposition of Dirichlet boundary conditions. Solution conformity across interfaces between non-uniform rational B-spline patches is enforced using dedicated interface neural networks. Training is performed using the variational framework by minimizing the energy functional derived after the weak form of the partial differential equation. The effectiveness of the suggested method is demonstrated on two highly non-trivial and practically relevant use-cases, namely, a 2D magnetostatics model of a quadrupole magnet and a 3D nonlinear solid and contact mechanics model of a mechanical holder. The results show excellent agreement to reference solutions obtained with high-fidelity finite element solvers, thus highlighting the potential of the suggested neural solver to tackle complex engineering problems given the corresponding computer-aided design models.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource Allocation under Stochastic Demands using Shrinking Horizon Optimization</title>
<link>https://arxiv.org/abs/2509.25412</link>
<guid>https://arxiv.org/abs/2509.25412</guid>
<content:encoded><![CDATA[
<div> method, resource allocation, revenue maximization, stochastic demands, shrinking horizon algorithm

Summary:
The article discusses the optimal allocation of limited resources over time to maximize revenue in the face of stochastic demands. It has applications in various control areas like supply chain management, healthcare operations, and power grid energy allocation. The proposed bisection method aims to solve the static optimization problem efficiently. The authors extend this approach to a shrinking horizon algorithm for sequential problems, updating future allocations based on observed demand values. A synthetic example with jointly log-normal demands illustrates the method's performance, demonstrating results close to those obtained by solving the prescient problem. This research provides valuable insights for industries seeking to optimize resource allocation in uncertain environments. <br /><br />Summary: <div>
arXiv:2509.25412v1 Announce Type: cross 
Abstract: We consider the problem of optimally allocating a limited number of resources across time to maximize revenue under stochastic demands. This formulation is relevant in various areas of control, such as supply chain, ticket revenue maximization, healthcare operations, and energy allocation in power grids. We propose a bisection method to solve the static optimization problem and extend our approach to a shrinking horizon algorithm for the sequential problem. The shrinking horizon algorithm computes future allocations after updating the distribution of future demands by conditioning on the observed values of demand. We illustrate the method on a simple synthetic example with jointly log-normal demands, showing that it achieves performance close to a bound obtained by solving the prescient problem.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quasi-Monte Carlo methods for uncertainty quantification of tumor growth modeled by a parametric semi-linear parabolic reaction-diffusion equation</title>
<link>https://arxiv.org/abs/2509.25753</link>
<guid>https://arxiv.org/abs/2509.25753</guid>
<content:encoded><![CDATA[
<div> tumor growth, quasi-Monte Carlo method, partial differential equations, uncertainty, mathematical models

Summary:<br />
This study focuses on applying a quasi-Monte Carlo method to analyze semi-linear parabolic reaction-diffusion equations used in modeling tumor growth. Tumor growth models are complex due to factors like patient variability, disease heterogeneity, and sparse data, leading to uncertainty in model parameters. Efficiently propagating these uncertainties is essential for computing quantities of interest (QoIs) to guide clinical decisions. The research demonstrates that quasi-Monte Carlo methods are effective in computing QoIs, with theoretical error bounds established for uniform random fields, showing a superior linear error rate compared to standard Monte Carlo. Numerical validations support this finding, with promising results for lognormal random fields prompting further investigation. The study provides a valuable contribution to the field of tumor growth modeling and uncertainty quantification. 

<br /><br /> <div>
arXiv:2509.25753v1 Announce Type: cross 
Abstract: We study the application of a quasi-Monte Carlo (QMC) method to a class of semi-linear parabolic reaction-diffusion partial differential equations used to model tumor growth. Mathematical models of tumor growth are largely phenomenological in nature, capturing infiltration of the tumor into surrounding healthy tissue, proliferation of the existing tumor, and patient response to therapies, such as chemotherapy and radiotherapy. Considerable inter-patient variability, inherent heterogeneity of the disease, sparse and noisy data collection, and model inadequacy all contribute to significant uncertainty in the model parameters. It is crucial that these uncertainties can be efficiently propagated through the model to compute quantities of interest (QoIs), which in turn may be used to inform clinical decisions. We show that QMC methods can be successful in computing expectations of meaningful QoIs. Well-posedness results are developed for the model and used to show a theoretical error bound for the case of uniform random fields. The theoretical linear error rate, which is superior to that of standard Monte Carlo, is verified numerically. Encouraging computational results are also provided for lognormal random fields, prompting further theoretical development.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding</title>
<link>https://arxiv.org/abs/2509.25803</link>
<guid>https://arxiv.org/abs/2509.25803</guid>
<content:encoded><![CDATA[
<div> Transformer models, financial transactions, analysis, LLMs, proprietary models<br />
Summary:<br />
The paper explores the use of Transformer-based models in understanding financial transactions. Three types of models were evaluated: Encoder-Only, Decoder-Only, and Encoder-Decoder, with a focus on pretrained LLMs, fine-tuned LLMs, and small proprietary models. While LLMs like LLaMA3-8b, Flan-T5, and SBERT excel in natural language processing tasks, they do not surpass small proprietary models in financial transaction understanding in terms of speed and cost efficiency. Customized proprietary models tailored to transaction data requirements prove to be more suitable for real-time applications in the financial sector. The implementation of a proprietary decoder-only model led to a 14% increase in transaction coverage and over $13 million in annual cost savings. <div>
arXiv:2509.25803v1 Announce Type: cross 
Abstract: Analyzing financial transactions is crucial for ensuring regulatory compliance, detecting fraud, and supporting decisions. The complexity of financial transaction data necessitates advanced techniques to extract meaningful insights and ensure accurate analysis. Since Transformer-based models have shown outstanding performance across multiple domains, this paper seeks to explore their potential in understanding financial transactions. This paper conducts extensive experiments to evaluate three types of Transformer models: Encoder-Only, Decoder-Only, and Encoder-Decoder models. For each type, we explore three options: pretrained LLMs, fine-tuned LLMs, and small proprietary models developed from scratch. Our analysis reveals that while LLMs, such as LLaMA3-8b, Flan-T5, and SBERT, demonstrate impressive capabilities in various natural language processing tasks, they do not significantly outperform small proprietary models in the specific context of financial transaction understanding. This phenomenon is particularly evident in terms of speed and cost efficiency. Proprietary models, tailored to the unique requirements of transaction data, exhibit faster processing times and lower operational costs, making them more suitable for real-time applications in the financial sector. Our findings highlight the importance of model selection based on domain-specific needs and underscore the potential advantages of customized proprietary models over general-purpose LLMs in specialized applications. Ultimately, we chose to implement a proprietary decoder-only model to handle the complex transactions that we previously couldn't manage. This model can help us to improve 14% transaction coverage, and save more than \$13 million annual cost.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning</title>
<link>https://arxiv.org/abs/2509.26116</link>
<guid>https://arxiv.org/abs/2509.26116</guid>
<content:encoded><![CDATA[
<div> Keywords: metagenomic binning, probabilistic embedding, DNA fragments, microbial communities, scalability<br />
<br />
Summary: 
This article introduces UncertainGen, a probabilistic embedding approach for metagenomic binning that represents DNA fragments as probability distributions in latent space. Unlike existing deterministic methods, UncertainGen captures the uncertainty inherent in DNA sequences due to inter-species DNA sharing. The approach offers theoretical guarantees on embedding distinguishability and enables more flexible separation of bins/clusters by introducing a data-adaptive metric in the latent space. Experiments with real metagenomic datasets demonstrate the superiority of UncertainGen over deterministic k-mer and LLM-based embeddings for the binning task. The probabilistic embedding framework not only enhances scalability but also provides a lightweight solution for large-scale metagenomic analysis. <div>
arXiv:2509.26116v1 Announce Type: cross 
Abstract: Metagenomic binning aims to cluster DNA fragments from mixed microbial samples into their respective genomes, a critical step for downstream analyses of microbial communities. Existing methods rely on deterministic representations, such as k-mer profiles or embeddings from large language models, which fail to capture the uncertainty inherent in DNA sequences arising from inter-species DNA sharing and from fragments with highly similar representations. We present the first probabilistic embedding approach, UncertainGen, for metagenomic binning, representing each DNA fragment as a probability distribution in latent space. Our approach naturally models sequence-level uncertainty, and we provide theoretical guarantees on embedding distinguishability. This probabilistic embedding framework expands the feasible latent space by introducing a data-adaptive metric, which in turn enables more flexible separation of bins/clusters. Experiments on real metagenomic datasets demonstrate the improvements over deterministic k-mer and LLM-based embeddings for the binning task by offering a scalable and lightweight solution for large-scale metagenomic analysis.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bubble, Bubble, AI's Rumble: Why Global Financial Regulatory Incident Reporting is Our Shield Against Systemic Stumbles</title>
<link>https://arxiv.org/abs/2509.26150</link>
<guid>https://arxiv.org/abs/2509.26150</guid>
<content:encoded><![CDATA[
<div> systemic risks, AI incident database, algorithmic trading, regulatory-grade global database, financial stability<br />
<br />Summary: 
The article discusses the need for a regulatory-grade global database to address the lack of transparency in AI-driven financial markets. It highlights the challenges posed by opaque AI systems and the potential systemic risks associated with algorithmic trading. The proposed database incorporates frameworks from healthcare and aviation industries to document AI incidents, allowing for cross-jurisdictional analysis of emerging risks. It employs a data omission technique to protect confidential information while enabling thorough analysis. Synthetic data validation reveals patterns such as market manipulation clusters and the influence of AI system typology on trading behavior, transcending geographical boundaries. The solution aims to empower regulators, financial institutions, and investors with enhanced oversight and transparency in AI-driven financial markets, promoting global financial stability. Immediate action is urged to strengthen risk management and resilience against AI-driven systemic risks. <div>
arXiv:2509.26150v1 Announce Type: cross 
Abstract: "Double, double toil and trouble; Fire burn and cauldron bubble." As Shakespeare's witches foretold chaos through cryptic prophecies, modern capital markets grapple with systemic risks concealed by opaque AI systems. According to IMF, the August 5, 2024, plunge in Japanese and U.S. equities can be linked to algorithmic trading yet ab-sent from existing AI incidents database exemplifies this transparency crisis. Current AI incident databases, reliant on crowdsourcing or news scraping, systematically over-look capital market anomalies, particularly in algorithmic and high-frequency trading. We address this critical gap by proposing a regulatory-grade global database that elegantly synthesises post-trade reporting frameworks with proven incident documentation models from healthcare and aviation. Our framework's temporal data omission technique masking timestamps while preserving percent-age-based metrics enables sophisticated cross-jurisdictional analysis of emerging risks while safeguarding confidential business information. Synthetic data validation (modelled after real life published incidents , sentiments, data) reveals compelling pat-terns: systemic risks transcending geographical boundaries, market manipulation clusters distinctly identifiable via K-means algorithms, and AI system typology exerting significantly greater influence on trading behaviour than geographical location, This tripartite solution empowers regulators with unprecedented cross-jurisdictional oversight, financial institutions with seamless compliance integration, and investors with critical visibility into previously obscured AI-driven vulnerabilities. We call for immediate action to strengthen risk management and foster resilience in AI-driven financial markets against the volatile "cauldron" of AI-driven systemic risks., promoting global financial stability through enhanced transparency and coordinated oversight.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing BEV Suitability and Charging Strategies Using Italian Driving Data</title>
<link>https://arxiv.org/abs/2509.26262</link>
<guid>https://arxiv.org/abs/2509.26262</guid>
<content:encoded><![CDATA[
<div> telemetry data, Battery Electric Vehicles, charging scenarios, mobility needs, BEV autonomy
Summary:<br /><br />Battery Electric Vehicles (BEVs) are becoming a popular choice for private transportation, replacing Internal Combustion Engine (ICE) vehicles. However, barriers such as range anxiety and charging station inconvenience persist. A study in Italy analyzed data from 10,441 ICE vehicle users to determine the feasibility of switching to BEVs without changing travel habits. By simulating trips and charging events, the study found that with overnight charging, at least 35% of users could switch to low-capacity BEVs without altering their mobility needs. The analysis highlights the importance of charging behaviors and BEV autonomy in transitioning to electric vehicles. <div>
arXiv:2509.26262v1 Announce Type: cross 
Abstract: Battery Electric Vehicles (BEVs) are rapidly evolving from a niche alternative to an established option for private transportation, often replacing Internal Combustion Engine (ICE) vehicles. Despite growing interest, significant barriers remain, including range anxiety, the inconvenience associated with public charging stations, and higher costs. This study analyses extensive telemetry data collected from 10,441 users using ICE vehicles in an Italian province to assess the potential for switching to BEVs without changing current travel behaviour. We evaluate to what extent the BEV models can fulfil their mobility needs under different charging scenarios. To do so, we replicate trips and parking events, simulating and monitoring the battery state of charge. The analysis reveals the compromises between charging behaviours and limited BEV autonomy. Assuming access to overnight charging, at least 35% of the users could already adopt even low-capacity BEVs.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Importance of localized dilatation and distensibility in identifying determinants of thoracic aortic aneurysm with neural operators</title>
<link>https://arxiv.org/abs/2509.26576</link>
<guid>https://arxiv.org/abs/2509.26576</guid>
<content:encoded><![CDATA[
<div> finite element framework, synthetic TAAs, neural networks, mechanobiological drivers, personalized treatment strategies
Summary:
The study investigates Thoracic aortic aneurysms (TAAs) development by simulating heterogeneous insults using a finite element framework. Neural networks are trained to predict the combined insult leading to TAA formation. Various network architectures are compared, with UNet showing the highest accuracy in predicting insults. Results emphasize the importance of including both dilatation and distensibility data for accurate predictions. Acquiring full-field measurements of these parameters is crucial for assessing the mechanobiological drivers of TAAs and developing personalized treatment strategies.<br /><br />Summary: <div>
arXiv:2509.26576v1 Announce Type: cross 
Abstract: Thoracic aortic aneurysms (TAAs) arise from diverse mechanical and mechanobiological disruptions to the aortic wall that increase the risk of dissection or rupture. Evidence links TAA development to dysfunctions in the aortic mechanotransduction axis, including loss of elastic fiber integrity and cell-matrix connections. Because distinct insults create different mechanical vulnerabilities, there is a critical need to identify interacting factors that drive progression. Here, we use a finite element framework to generate synthetic TAAs from hundreds of heterogeneous insults spanning varying degrees of elastic fiber damage and impaired mechanosensing. From these simulations, we construct spatial maps of localized dilatation and distensibility to train neural networks that predict the initiating combined insult. We compare several architectures (Deep Operator Networks, UNets, and Laplace Neural Operators) and multiple input data formats to define a standard for future subject-specific modeling. We also quantify predictive performance when networks are trained using only geometric data (dilatation) versus both geometric and mechanical data (dilatation plus distensibility). Across all networks, prediction errors are significantly higher when trained on dilatation alone, underscoring the added value of distensibility information. Among the tested models, UNet consistently provides the highest accuracy across all data formats. These findings highlight the importance of acquiring full-field measurements of both dilatation and distensibility in TAA assessment to reveal the mechanobiological drivers of disease and support the development of personalized treatment strategies.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of nested geometry treatments within GPU-based Monte Carlo neutron transport simulations of fission reactors</title>
<link>https://arxiv.org/abs/2406.13849</link>
<guid>https://arxiv.org/abs/2406.13849</guid>
<content:encoded><![CDATA[
<div> GPU-based neutron transport, Monte Carlo simulation, reactor physics, tracking algorithms, supercomputing 
Summary:<br />
- Monte Carlo neutron transport provides detailed estimates of radiological quantities in fission reactors by tracking individual neutrons through computational geometry.
- CPU-based MC codes utilize multiple tracker types with different algorithms, but virtual function calls have high GPU overhead.
- Shift MC code was modified to support GPU-based tracking using dynamic polymorphism, static polymorphism, and single tracker type with tree-based acceleration.
- Results on the Frontier supercomputer show efficient tracking rates using all three methods, suitable for typical reactor problems.
- The single tracker method demonstrates flexibility in handling hexagonal-grid microreactor problems without specific tracking routines, providing significant speedup over CPU execution. <br /><br />Summary: <div>
arXiv:2406.13849v2 Announce Type: replace-cross 
Abstract: Monte Carlo (MC) neutron transport provides detailed estimates of radiological quantities within fission reactors. This involves tracking individual neutrons through a computational geometry. CPU-based MC codes use multiple polymorphic tracker types with different tracking algorithms to exploit the repeated configurations of reactors, but virtual function calls have high overhead on the GPU. The Shift MC code was modified to support GPU-based tracking with three strategies: dynamic polymorphism with virtual functions, static polymorphism, and a single tracker type with tree-based acceleration. On the Frontier supercomputer these methods achieve 77.8%, 91.2%, and 83.4%, respectively, of the tracking rate obtained using a specialized tracker optimized for rectilinear-grid-based reactors. This indicates that all three methods are suitable for typical reactor problems in which tracking does not dominate runtime. The flexibility of the single tracker method is highlighted with a hexagonal-grid microreactor problem, performed without hexagonal-grid-specific tracking routines, providing a 2.19$\times$ speedup over CPU execution.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel, Trend and Periodic-Wise Representation Learning for Multivariate Long-term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.23583</link>
<guid>https://arxiv.org/abs/2509.23583</guid>
<content:encoded><![CDATA[
<div> Keywords: downsampling-based methods, time series forecasting, CTPNet, multi-head attention mechanism, Transformer

Summary:
CTPNet is a novel framework for time series forecasting that addresses limitations in downsampling-based methods by explicitly learning representations from three perspectives. It captures inter-channel dependencies using a temporal query-based multi-head attention mechanism, models intra-subsequence dependencies with a Transformer, and extracts inter-subsequence dependencies with residual connections to capture global patterns. By integrating these levels, CTPNet provides a more holistic representation of temporal dynamics, leading to improved forecasting accuracy. Experimental results demonstrate the superiority of CTPNet compared to existing methods. <div>
arXiv:2509.23583v1 Announce Type: new 
Abstract: Downsampling-based methods for time series forecasting have attracted increasing attention due to their superiority in capturing sequence trends. However, this approaches mainly capture dependencies within subsequences but neglect inter-subsequence and inter-channel interactions, which limits forecasting accuracy. To address these limitations, we propose CTPNet, a novel framework that explicitly learns representations from three perspectives: i) inter-channel dependencies, captured by a temporal query-based multi-head attention mechanism; ii) intra-subsequence dependencies, modeled via a Transformer to characterize trend variations; and iii) inter-subsequence dependencies, extracted by reusing the encoder with residual connections to capture global periodic patterns. By jointly integrating these levels, proposed method provides a more holistic representation of temporal dynamics. Extensive experiments demonstrate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-Code Generation for Modular Building Layouts in Building Information Modeling</title>
<link>https://arxiv.org/abs/2509.23713</link>
<guid>https://arxiv.org/abs/2509.23713</guid>
<content:encoded><![CDATA[
<div> Keywords: Text2MBL, text-to-code generation, Building Information Modeling (BIM), modular building layout (MBL), hierarchical structure

Summary: 
Text2MBL is a framework that generates executable BIM code from textual descriptions of MBL designs, moving beyond conventional 2D layout generation. It produces parametric BIM layouts through on-the-fly code instantiation, addressing the challenges of MBL's hierarchical structure. The framework uses an object-oriented code architecture and large language models to output structured action sequences in code format. A dataset of paired descriptions and ground truth layouts from modular housing projects was used to train and evaluate Text2MBL, ensuring executable validity, semantic fidelity, and geometric consistency. By integrating natural language understanding with BIM code generation, Text2MBL creates a scalable pipeline for modular construction workflows. The implementation of Text2MBL is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2509.23713v1 Announce Type: new 
Abstract: We present Text2MBL, a text-to-code generation framework that generates executable Building Information Modeling (BIM) code directly from textual descriptions of modular building layout (MBL) design. Unlike conventional layout generation approaches that operate in 2D space, Text2MBL produces fully parametric, semantically rich BIM layouts through on-the-fly code instantiation. To address MBLs' unique challenges due to their hierarchical three-tier structure: modules (physical building blocks), units (self-contained dwellings), and rooms (functional spaces), we developed an object-oriented code architecture and fine-tuned large language models to output structured action sequences in code format. To train and evaluate the framework, we curated a dataset of paired descriptions and ground truth layouts drawn from real-world modular housing projects. Performance was assessed using metrics for executable validity, semantic fidelity, and geometric consistency. By tightly unifying natural language understanding with BIM code generation, Text2MBL establishes a scalable pipeline from high-level conceptual design to automation-ready modular construction workflows. Our implementation is available at https://github.com/CI3LAB/Text2MBL.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid DNN Transformer AE Framework for Corporate Tax Risk Supervision and Risk Level Assessment</title>
<link>https://arxiv.org/abs/2509.23862</link>
<guid>https://arxiv.org/abs/2509.23862</guid>
<content:encoded><![CDATA[
<div> deep learning, tax risk supervision, Transformer, Autoencoder, risk level assessment

Summary:
The paper presents a hybrid deep learning framework, DNN-Transformer-Autoencoder, for corporate tax risk supervision. The framework combines a Deep Neural Network (DNN) for static enterprise attributes, a Transformer for long-term dependencies in financial time series, and an Autoencoder for detecting anomalous tax behaviors. By integrating these modules, the framework generates a comprehensive risk score and assigns discrete risk levels. Experimental results on a real-world tax dataset show the framework's effectiveness with an accuracy of 0.91 and a Macro F1-score of 0.88. The hybrid model not only enhances classification performance but also improves interpretability and applicability in tax regulation scenarios. This study contributes to methodological innovation and provides regulatory implications for intelligent tax risk management.<br /><br />Summary: <div>
arXiv:2509.23862v1 Announce Type: new 
Abstract: Tax risk supervision has become a critical component of modern financial governance, as irregular tax behaviors and hidden compliance risks pose significant challenges to regulatory authorities and enterprises alike. Traditional rule-based methods often struggle to capture complex and dynamic tax-related anomalies in large-scale enterprise data. To address this issue, this paper proposes a hybrid deep learning framework (DNN-Transformer-Autoencoder) for corporate tax risk supervision and risk level assessment. The framework integrates three complementary modules: a Deep Neural Network (DNN) for modeling static enterprise attributes, a Transformer-based architecture for capturing long-term dependencies in historical financial time series, and an Autoencoder (AE) for unsupervised detection of anomalous tax behaviors. The outputs of these modules are fused to generate a comprehensive risk score, which is further mapped into discrete risk levels (high, medium, low). Experimental evaluations on a real-world enterprise tax dataset demonstrate the effectiveness of the proposed framework, achieving an accuracy of 0.91 and a Macro F1-score of 0.88. These results indicate that the hybrid model not only improves classification performance but also enhances interpretability and applicability in practical tax regulation scenarios. This study provides both methodological innovation and regulatory implications for intelligent tax risk management.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying the Multimodal Hierarchy of Public Transit Systems Using Trip Chain Data</title>
<link>https://arxiv.org/abs/2509.24220</link>
<guid>https://arxiv.org/abs/2509.24220</guid>
<content:encoded><![CDATA[
<div> hierarchy, urban mobility, multimodal trips, smart card data, Seoul<br />
Summary:<br />
The article introduces the concept of a macroscopic multimodal hierarchy to understand interactions between different modes of urban transportation. Trips follow an ascending-descending order starting and ending with lower hierarchical modes like walking for high accessibility and utilizing higher modes for efficiency. A methodology to identify the multimodal hierarchy using smart card trip data is proposed and demonstrated with data from Seoul, South Korea. This approach helps in understanding how traditional and emerging modes of transportation interact in complex public transit systems, influencing users' multimodal itineraries. <div>
arXiv:2509.24220v1 Announce Type: new 
Abstract: As urban mobility integrates traditional and emerging modes, public transit systems are becoming increasingly complex. Some modes complement each other, while others compete, influencing users' multimodal itineraries. To provide a clear, high-level understanding of these interactions, we introduce the concept of a macroscopic multimodal hierarchy. In this framework, trips follow an "ascending-descending" order, starting and ending with lower hierarchical modes (e.g., walking) that offer high accessibility, while utilizing higher modes (e.g., subways) for greater efficiency. We propose a methodology to identify the multimodal hierarchy of a city using multimodal smart card trip chain data and demonstrate its application with actual data collected from Seoul and the surrounding metropolitan area in South Korea.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparison of Surrogate Constitutive Models for Viscoplastic Creep Simulation of HT-9 Steel</title>
<link>https://arxiv.org/abs/2509.22667</link>
<guid>https://arxiv.org/abs/2509.22667</guid>
<content:encoded><![CDATA[
<div> Keywords: microstructure, constitutive models, surrogate modeling, viscoplastic response, data-driven approach

Summary:
- Mechanistic microstructure-informed constitutive models for polycrystals are essential in computational materials science but can be computationally expensive.
- Data-driven surrogate models offer a solution by learning constitutive relations directly from data.
- Two local surrogate models, including a piecewise response surface method and a mixture of experts model, were developed for the viscoplastic response of steel to balance accuracy and computational efficiency.
- The surrogates adapt to varying material behavior based on parameters or conditions and were tested on HT-9 steel for creep simulations.
- Test metrics show that the mixture of experts model outperforms the piecewise response surface method in accuracy for predicting viscoplastic material behavior.<br /><br />Summary:Mechanistic microstructure models are crucial in computational materials science but can be computationally intensive. Data-driven surrogate models offer a promising solution. Two models were developed for steel's viscoplastic response, adapting to varying behavior and tested on HT-9 for creep simulations. The mixture of experts model was found to outperform the piecewise response surface method in accuracy. <div>
arXiv:2509.22667v1 Announce Type: cross 
Abstract: Mechanistic microstructure-informed constitutive models for the mechanical response of polycrystals are a cornerstone of computational materials science. However, as these models become increasingly more complex - often involving coupled differential equations describing the effect of specific deformation modes - their associated computational costs can become prohibitive, particularly in optimization or uncertainty quantification tasks that require numerous model evaluations. To address this challenge, surrogate constitutive models that balance accuracy and computational efficiency are highly desirable. Data-driven surrogate models, that learn the constitutive relation directly from data, have emerged as a promising solution. In this work, we develop two local surrogate models for the viscoplastic response of a steel: a piecewise response surface method and a mixture of experts model. These surrogates are designed to adapt to complex material behavior, which may vary with material parameters or operating conditions. The surrogate constitutive models are applied to creep simulations of HT-9 steel, an alloy of considerable interest to the nuclear energy sector due to its high tolerance to radiation damage, using training data generated from viscoplastic self-consistent (VPSC) simulations. We define a set of test metrics to numerically assess the accuracy of our surrogate models for predicting viscoplastic material behavior, and show that the mixture of experts model outperforms the piecewise response surface method in terms of accuracy.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents</title>
<link>https://arxiv.org/abs/2509.24063</link>
<guid>https://arxiv.org/abs/2509.24063</guid>
<content:encoded><![CDATA[
<div> agent-based simulation, distributed execution, serialization mechanism, delta encoding, extreme-scale simulations

Summary:
TeraAgent is introduced as a distributed agent-based simulation engine to address the scalability limitations of existing platforms like BioDynaMo. The key challenge of exchanging agent information across servers is tackled through a tailored serialization mechanism and the use of delta encoding to reduce data transfer. These solutions enable TeraAgent to support extreme-scale simulations with half a trillion agents, a significant improvement in scalability. The platform also facilitates faster time-to-result by utilizing additional compute nodes, enhances interoperability with third-party tools, and provides users with more hardware flexibility. TeraAgent marks a significant advancement in the field of agent-based simulation, offering a promising solution for studying complex systems on a massive scale. 

<br /><br />Summary: <div>
arXiv:2509.24063v1 Announce Type: cross 
Abstract: Agent-based simulation is an indispensable paradigm for studying complex systems. These systems can comprise billions of agents, requiring the computing resources of multiple servers to simulate. Unfortunately, the state-of-the-art platform, BioDynaMo, does not scale out across servers due to its shared-memory-based implementation.
  To overcome this key limitation, we introduce TeraAgent, a distributed agent-based simulation engine. A critical challenge in distributed execution is the exchange of agent information across servers, which we identify as a major performance bottleneck. We propose two solutions: 1) a tailored serialization mechanism that allows agents to be accessed and mutated directly from the receive buffer, and 2) leveraging the iterative nature of agent-based simulations to reduce data transfer with delta encoding.
  Built on our solutions, TeraAgent enables extreme-scale simulations with half a trillion agents (an 84x improvement), reduces time-to-result with additional compute nodes, improves interoperability with third-party tools, and provides users with more hardware flexibility.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting the Structure of Press Releases for Predicting Earnings Announcement Returns</title>
<link>https://arxiv.org/abs/2509.24254</link>
<guid>https://arxiv.org/abs/2509.24254</guid>
<content:encoded><![CDATA[
<div> Keywords: press releases, stock returns, earnings announcement, FinBERT, language analysis

Summary: 
Press releases are analyzed to predict stock returns on earnings announcement days. Traditional bag-of-words and BERT-based embeddings are compared, with FinBERT showing the highest predictive power. The study finds that press release content is as informative as earnings surprise in predicting stock returns. Combining models improves explanatory strength and interpretability of press release content. Stock prices reflect press release content at market open, and leaked press releases offer predictive advantage. Topic analysis reveals self-serving bias in managerial narratives. The framework supports real-time return prediction through online learning integration, providing interpretability and highlighting the role of language in price formation. <div>
arXiv:2509.24254v1 Announce Type: cross 
Abstract: We examine how textual features in earnings press releases predict stock returns on earnings announcement days. Using over 138,000 press releases from 2005 to 2023, we compare traditional bag-of-words and BERT-based embeddings. We find that press release content (soft information) is as informative as earnings surprise (hard information), with FinBERT yielding the highest predictive power. Combining models enhances explanatory strength and interpretability of the content of press releases. Stock prices fully reflect the content of press releases at market open. If press releases are leaked, it offers predictive advantage. Topic analysis reveals self-serving bias in managerial narratives. Our framework supports real-time return prediction through the integration of online learning, provides interpretability and reveals the nuanced role of language in price formation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from RNA-Seq Data</title>
<link>https://arxiv.org/abs/2509.24840</link>
<guid>https://arxiv.org/abs/2509.24840</guid>
<content:encoded><![CDATA[
<div> Keywords: single-cell RNA sequencing, Cell2Text, generative framework, gene expression, natural language descriptions

Summary:
Cell2Text is a multimodal generative framework that translates single-cell RNA sequencing profiles into structured natural language descriptions. By combining gene-level embeddings from single-cell foundation models with large language models, Cell2Text generates coherent summaries that capture cellular identity, tissue origin, disease associations, and pathway activity. This innovative approach outperforms baseline methods in classification accuracy, demonstrates strong ontological consistency, and achieves high semantic fidelity in text generation. The integration of expression data with natural language not only improves predictive performance but also provides inherently interpretable outputs. This advancement in technology offers a scalable solution for efficiently characterizing unseen cells with minimal label requirements. <div>
arXiv:2509.24840v1 Announce Type: cross 
Abstract: Single-cell RNA sequencing has transformed biology by enabling the measurement of gene expression at cellular resolution, providing information for cell types, states, and disease contexts. Recently, single-cell foundation models have emerged as powerful tools for learning transferable representations directly from expression profiles, improving performance on classification and clustering tasks. However, these models are limited to discrete prediction heads, which collapse cellular complexity into predefined labels that fail to capture the richer, contextual explanations biologists need. We introduce Cell2Text, a multimodal generative framework that translates scRNA-seq profiles into structured natural language descriptions. By integrating gene-level embeddings from single-cell foundation models with pretrained large language models, Cell2Text generates coherent summaries that capture cellular identity, tissue origin, disease associations, and pathway activity, generalizing to unseen cells. Empirically, Cell2Text outperforms baselines on classification accuracy, demonstrates strong ontological consistency using PageRank-based similarity metrics, and achieves high semantic fidelity in text generation. These results demonstrate that coupling expression data with natural language offers both stronger predictive performance and inherently interpretable outputs, pointing to a scalable path for label-efficient characterization of unseen cells.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction</title>
<link>https://arxiv.org/abs/2509.25075</link>
<guid>https://arxiv.org/abs/2509.25075</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, Cryo-electron microscopy, 3D reconstruction, Neural radiance fields, Training efficiency
<br />
Summary:<br />
The article introduces GEM, a cryo-EM reconstruction framework based on 3D Gaussian Splatting (3DGS), aimed at improving efficiency and accuracy in high-resolution structural biology. GEM operates directly in real-space by representing proteins with compact 3D Gaussians, significantly reducing memory and training costs. By implementing a novel gradient computation method for 3D Gaussians, GEM achieves up to 48% faster training and 12% lower memory usage compared to existing methods. Additionally, GEM improves local resolution by up to 38.8%, demonstrating its practicality and scalability in cryo-EM reconstruction. The framework unifies speed, efficiency, and high-resolution accuracy, making it a valuable tool in the field. The code for GEM is openly available on GitHub for researchers to utilize and further develop. 
<br /> <div>
arXiv:2509.25075v1 Announce Type: cross 
Abstract: Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive. Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead. Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency. Instead of modeling the entire density volume, GEM represents proteins with compact 3D Gaussians, each parameterized by only 11 values. To further improve the training efficiency, we designed a novel gradient computation to 3D Gaussians that contribute to each voxel. This design substantially reduced both memory footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to 48% faster training and 12% lower memory usage compared to state-of-the-art methods, while improving local resolution by as much as 38.8%. These results establish GEM as a practical and scalable paradigm for cryo-EM reconstruction, unifying speed, efficiency, and high-resolution accuracy. Our code is available at https://github.com/UNITES-Lab/GEM.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A bound-preserving multinumerics scheme for steady-state convection-diffusion equations</title>
<link>https://arxiv.org/abs/2509.25181</link>
<guid>https://arxiv.org/abs/2509.25181</guid>
<content:encoded><![CDATA[
<div> adaptive partitioning, finite volume, discontinuous Galerkin, convection-diffusion equation, bound-preserving<br />
Summary:<br />
This study proposes a novel approach to solving the convection-diffusion equation by combining cell-centered finite volume (FV) and discontinuous Galerkin (DG) methods. The domain is divided into FV and DG subdomains, connected through an interface term. An adaptive partitioning strategy is introduced, automatically selecting FV or DG based on solution accuracy. Whenever a cell average violates bounds, FV is applied in the vicinity of the element until all averages are bound-preserving within a specified tolerance. This process, akin to $p$-adaptivity, improves efficiency and accuracy in convection-dominated regimes. Standard benchmarks validate the effectiveness of this adaptive technique, showcasing its potential for accurate and computationally efficient convection-diffusion simulations. <div>
arXiv:2509.25181v1 Announce Type: cross 
Abstract: We solve the convection-diffusion equation using a coupling of cell-centered finite volume (FV) and discontinuous Galerkin (DG) methods. The domain is divided into disjoint regions assigned to FV or DG, and the two methods are coupled through an interface term. DG is stable and resolves sharp layers in convection-dominated regimes, but it can produce sizable spurious oscillations and is computationally expensive; FV (two-point flux) is low-order and monotone, but inexpensive. We propose a novel adaptive partitioning strategy that automatically selects FV and DG subdomains: whenever the solution's cell average violates the bounds, we switch to FV on a small neighborhood of that element. Viewed as a natural analog of $p$-adaptivity, this process is repeated until all cell averages are bound-preserving (up to some specified tolerance). Thereafter, standard conservative limiters may be applied to ensure the full solution is bound-preserving. Standard benchmarks confirm the effectiveness of the adaptive technique.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time</title>
<link>https://arxiv.org/abs/2412.18848</link>
<guid>https://arxiv.org/abs/2412.18848</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryptocurrency markets, pump-and-dump schemes, Telegram groups, real-time prediction pipeline, natural language processing

Summary: 
Cryptocurrency markets are susceptible to manipulation through pump-and-dump schemes orchestrated by large Telegram groups, leading to artificial inflation of coin prices. These groups exploit information asymmetry by selling inside information, posing financial risks to subscribers and all investors. A real-time prediction pipeline incorporating advanced natural language processing technology has been developed to forecast target coins and alert investors to potential pump-and-dump schemes. In a case study on Poloniex, the model successfully identified the target coin in 55.81% of observed pump-and-dump events. By analyzing Telegram messages, the pipeline has identified over 2,000 past pump events and can detect new schemes as they emerge. This innovative approach aims to mitigate the impact of pump-and-dump schemes on cryptocurrency markets and provide investors with timely information to make informed decisions. 

<br /><br />Summary: <div>
arXiv:2412.18848v2 Announce Type: replace 
Abstract: Cryptocurrency markets often face manipulation through prevalent pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some exceeding two million members, artificially inflate target cryptocurrency prices. These groups sell premium access to inside information, worsening information asymmetry and financial risks for subscribers and all investors. This paper presents a real-time prediction pipeline to forecast target coins and alert investors to possible P&amp;D schemes. In a Poloniex case study, the model accurately identified the target coin among the top five from 50 random coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural language processing (NLP) to classify Telegram messages, identifying 2,079 past pump events and detecting new ones in real-time.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of spatial coarsening on Parareal convergence for the linear advection equation</title>
<link>https://arxiv.org/abs/2111.10228</link>
<guid>https://arxiv.org/abs/2111.10228</guid>
<content:encoded><![CDATA[
<div> hyperbolic partial differential equations, Parareal method, parallel-in-time integration, spatial resolution, numerical time stepping

Summary: 
The Parareal method, commonly used for parallel-in-time integration, faces challenges when applied to hyperbolic partial differential equations, especially with reduced spatial resolution. For linear problems, it is difficult to predict convergence using the 2-norm of the Parareal iteration matrix. However, a theorem suggests that the pseudo-spectral radius may reliably indicate whether a Parareal configuration will exhibit transient growth or monotonic convergence. Numerical results support this, showing that the pseudo-spectral radius can also estimate the convergence rate in initial Parareal iterations. This research sheds light on distinguishing configurations where Parareal errors decrease steadily from those where errors initially increase significantly, providing insights for efficient application in hyperbolic problems. <div>
arXiv:2111.10228v3 Announce Type: replace-cross 
Abstract: The Parareal parallel-in-time integration method often performs poorly when applied to hyperbolic partial differential equations. This effect is even more pronounced when the coarse propagator uses a reduced spatial resolution. However, some combinations of spatial discretization and numerical time stepping nevertheless allow for Parareal to converge with monotonically decreasing errors. This raises the question how these configurations can be distinguished theoretically from those where the error initially increases, sometimes over many orders of magnitude. For linear problems, we prove a theorem that implies that the 2-norm of the Parareal iteration matrix is not a suitable tool to predict convergence for hyperbolic problems when spatial coarsening is used. We then show numerical results that suggest that the pseudo-spectral radius can reliably indicate if a given configuration of Parareal will show transient growth or monotonic convergence. For the studied examples, it also provides a good quantitative estimate of the convergence rate in the first few Parareal iterations.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Occasional to Steady: Habit Formation Insights From a Comprehensive Fitness Study</title>
<link>https://arxiv.org/abs/2501.01779</link>
<guid>https://arxiv.org/abs/2501.01779</guid>
<content:encoded><![CDATA[
<div> gym attendance, habit formation, survival metric, clusters, personalized guidance<br />
<br />
Summary: 
This study analyzes gym attendance data to investigate factors influencing the formation of exercise habits. It identifies critical periods for habit formation and segments gym-goers into clusters based on visit patterns. The research highlights the importance of personalized guidance and social dynamics in sustaining long-term engagement. Results show that specific interventions like group classes and personal trainer sessions have varied impacts on different subgroups. Causal inference analysis confirms the significance of tailored, multi-dimensional approaches in promoting exercise habits. By considering individual characteristics, social dynamics, and strategic interventions, the study emphasizes the need for a personalized approach to support consistent exercise routines. <div>
arXiv:2501.01779v2 Announce Type: replace-cross 
Abstract: Regular exercise is widely recognized as a cornerstone of health, yet sustaining consistent exercise habits remains challenging. Understanding the factors that influence the formation of these habits is crucial for developing effective interventions. This study utilizes data from Mars Athletic Club, T\"urkiye's largest sports chain, to investigate the dynamics of gym attendance and habit formation. The general problem addressed by this study is identifying the critical periods and factors that contribute to the successful establishment of consistent exercise routines among gym-goers. We show that specific periods of attendance are most crucial for habit formation. By developing a survival metric based on gym attendance patterns, we pinpoint these key phases and segment members into distinct clusters based on their visit patterns. Our analysis reveals significant differences in how various subgroups respond to interventions, such as group classes, personal trainer sessions, and visiting different clubs. Using causal inference analysis, we demonstrate that personalized guidance and social dynamics are key drivers of sustained long-term engagement. By systematically examining these variables and considering the specific characteristics of different clusters, our research highlights the importance of a tailored, multi-dimensional approach to promoting exercise habits, which integrates social dynamics, personalized guidance, and strategic interventions to sustain long-term engagement.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2502.01678</link>
<guid>https://arxiv.org/abs/2502.01678</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, Alzheimer's disease, deep learning, subject-level detection, pre-training <br />
Summary: <br />
The study focuses on using EEG for Alzheimer's disease detection, highlighting the challenges faced by existing methods. The authors introduce the largest EEG-AD dataset comprising 2,255 subjects and propose LEAD, a novel deep learning model for subject-level AD detection. The model includes a preprocessing pipeline, a subject-regularized spatio-temporal transformer with a unique loss function, and AD-guided contrastive pre-training. LEAD outperforms 10 baseline models in subject-level detection, achieving a Sensitivity of 90.91% on the ADFTD dataset using leave-one-subject-out cross-validation. The results demonstrate the efficacy of this approach for real-world EEG-based Alzheimer's disease detection. The source code for LEAD is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2502.01678v3 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) provides a non-invasive, highly accessible, and cost-effective approach for detecting Alzheimer's disease (AD). However, existing methods, whether based on handcrafted feature engineering or standard deep learning, face two major challenges: 1) the lack of large-scale EEG-AD datasets for robust representation learning, and 2) the absence of a dedicated deep learning pipeline for subject-level detection, which is more clinically meaningful than the commonly used sample-level detection. To address these gaps, we have curated the world's largest EEG-AD corpus to date, comprising 2,255 subjects. Leveraging this unique data corpus, we propose LEAD, the first large-scale foundation model for EEG analysis in dementia. Our approach provides an innovative framework for subject-level AD detection, including: 1) a comprehensive preprocessing pipeline such as artifact removal, resampling, and filtering, and a newly proposed multi-scale segmentation strategy, 2) a subject-regularized spatio-temporal transformer trained with a novel subject-level cross-entropy loss and an indices group-shuffling algorithm, and 3) AD-guided contrastive pre-training. We pre-train on 12 datasets (3 AD-related and 9 non-AD) and fine-tune/test on 4 AD datasets. Compared with 10 baselines, LEAD consistently obtains superior subject-level detection performance under the challenging subject-independent cross-validation protocol. On the benchmark ADFTD dataset, our model achieves an impressive subject-level Sensitivity of 90.91% under the leave-one-subject-out (LOSO) setting. These results strongly validate the effectiveness of our method for real-world EEG-based AD detection. Source code: https://github.com/DL4mHealth/LEAD
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sci2Pol: Evaluating and Fine-tuning LLMs on Scientific-to-Policy Brief Generation</title>
<link>https://arxiv.org/abs/2509.21493</link>
<guid>https://arxiv.org/abs/2509.21493</guid>
<content:encoded><![CDATA[
<div> Keywords: benchmark, training dataset, large language models, policy brief generation, fine-tuning

Summary: 
Sci2Pol-Bench and Sci2Pol-Corpus are introduced as benchmarks and training datasets for evaluating and fine-tuning large language models (LLMs) on policy brief generation from scientific papers. The Sci2Pol-Bench is structured based on a five-stage taxonomy mirroring the human writing process, featuring 18 tasks in various formats. Evaluation of LLMs using this benchmark reveals key limitations in current models. The new LLM-based evaluation metric introduced aligns with expert judgment, better capturing the quality of brief writing compared to existing metrics like BERTScore and ROUGE scores. The Sci2Pol-Corpus, curated for fine-tuning, pairs cited scientific papers with corresponding policy documents, filtered and polished using an LLM-as-a-judge and expert-written samples. Fine-tuning three models on the corpus leads to consistent performance improvements across the benchmark tasks. Notably, Gemma-27B outperforms larger models like GPT-4o and DeepSeek-V3 after fine-tuning, demonstrating the effectiveness of the corpus in bridging science and policy. 

<br /><br />Summary: <div>
arXiv:2509.21493v1 Announce Type: new 
Abstract: We propose Sci2Pol-Bench and Sci2Pol-Corpus, the first benchmark and training dataset for evaluating and fine-tuning large language models (LLMs) on policy brief generation from a scientific paper. We build Sci2Pol-Bench on a five-stage taxonomy to mirror the human writing process: (i) Autocompletion, (ii) Understanding, (iii) Summarization, (iv) Generation, and (v) Verification. It features 18 tasks in multiple-choice and open-ended formats. Specifically, for the Generation stage, we show that BERTScore and ROUGE scores fail to capture the quality of brief writing, and introduce a new LLM-based evaluation metric aligned with expert judgement. Using this benchmark, we evaluate 13 leading open-source and commercial LLMs to uncover key limitations. To improve LLM performance on brief writing, we curate the Sci2Pol-Corpus for fine-tuning. We start by linking each cited scientific paper to its corresponding policy document, drawn from 5.6 million policy records. This produces 140,000 candidate pairs. We then employ an LLM-as-a-judge to filter high-quality examples, followed by in-context polishing using three expert-written samples as references. This process yields a final set of 639 new pairs. Finally, we fine-tune three models on Sci2Pol-Corpus: LLaMA-3.1-8B, Gemma-12B, and Gemma-27B. Fine-tuning leads to consistent performance improvements across Sci2Pol-Bench. Notably, after fine-tuning, Gemma-27B surpasses the much larger GPT-4o and DeepSeek-V3 (671B). These demonstrate the effectiveness of our corpus in bridging the gap between science and policy.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantMind: A Context-Engineering Based Knowledge Framework for Quantitative Finance</title>
<link>https://arxiv.org/abs/2509.21507</link>
<guid>https://arxiv.org/abs/2509.21507</guid>
<content:encoded><![CDATA[
<div> Keywords: QuantMind, knowledge extraction, structured knowledge, quantitative finance, semantic search 

Summary: 
Quantitative research in finance increasingly relies on unstructured financial content like filings and research notes. Existing pipelines struggle with accuracy, evidence attribution, and integration into research workflows. To address these challenges, QuantMind is introduced as an intelligent knowledge extraction and retrieval framework tailored to quantitative finance. It consists of a two-stage architecture: knowledge extraction transforms diverse documents into structured knowledge through parsing text, tables, and formulas, while intelligent retrieval integrates semantic search with multi-hop reasoning for auditable outputs. A user study demonstrates that QuantMind enhances factual accuracy and user experience compared to unaided reading and generic AI assistance, showcasing the importance of domain-specific context engineering in finance.

Summary:  <div>
arXiv:2509.21507v1 Announce Type: new 
Abstract: Quantitative research increasingly relies on unstructured financial content such as filings, earnings calls, and research notes, yet existing LLM and RAG pipelines struggle with point-in-time correctness, evidence attribution, and integration into research workflows. To tackle this, We present QuantMind, an intelligent knowledge extraction and retrieval framework tailored to quantitative finance. QuantMind adopts a two-stage architecture: (i) a knowledge extraction stage that transforms heterogeneous documents into structured knowledge through multi-modal parsing of text, tables, and formulas, adaptive summarization for scalability, and domain-specific tagging for fine-grained indexing; and (ii) an intelligent retrieval stage that integrates semantic search with flexible strategies, multi-hop reasoning across sources, and knowledge-aware generation for auditable outputs. A controlled user study demonstrates that QuantMind improves both factual accuracy and user experience compared to unaided reading and generic AI assistance, underscoring the value of structured, domain-specific context engineering for finance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI for Sustainable Future Foods</title>
<link>https://arxiv.org/abs/2509.21556</link>
<guid>https://arxiv.org/abs/2509.21556</guid>
<content:encoded><![CDATA[
<div> AI for Food, transformative, molecular composition, functional performance, sustainability, innovation<br />
<br />
Summary: AI for Food is an emerging discipline that aims to revolutionize global food systems by leveraging artificial intelligence to enhance ingredient design, formulation development, sensory analysis, and more. While early successes show promise in predicting protein performance and mapping molecules to flavor, challenges such as lack of standardization, limited data, and cultural diversity still exist. To unlock the potential of AI in food innovation, three priorities are proposed: treating food as a programmable biomaterial, creating self-driving laboratories for automated discovery, and developing deep reasoning models that integrate sustainability and human health. By responsibly incorporating AI into the food innovation cycle, the transition to sustainable protein systems can be accelerated, leading to a predictive, design-driven science of food that benefits both human health and the planet.<br /><br /> <div>
arXiv:2509.21556v1 Announce Type: new 
Abstract: Global food systems must deliver nutritious and sustainable foods while sharply reducing environmental impact. Yet, food innovation remains slow, empirical, and fragmented. Artificial intelligence (AI) now offers a transformative path with the potential to link molecular composition to functional performance, bridge chemical structure to sensory outcomes, and accelerate cross-disciplinary innovation across the entire production pipeline. Here we outline AI for Food as an emerging discipline that integrates ingredient design, formulation development, fermentation and production, texture analysis, sensory properties, manufacturing, and recipe generation. Early successes demonstrate how AI can predict protein performance, map molecules to flavor, and tailor consumer experiences. But significant challenges remain: lack of standardization, scarce multimodal data, cultural and nutritional diversity, and low consumer confidence. We propose three priorities to unlock the field: treating food as a programmable biomaterial, building self-driving laboratories for automated discovery, and developing deep reasoning models that integrate sustainability and human health. By embedding AI responsibly into the food innovation cycle, we can accelerate the transition to sustainable protein systems and chart a predictive, design-driven science of food for our own health and the health of our planet.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Method of Moments and Generalized Scattering Matrix: Applications to Antennas in Radomes, Reflectors, and Implantable Media</title>
<link>https://arxiv.org/abs/2509.22000</link>
<guid>https://arxiv.org/abs/2509.22000</guid>
<content:encoded><![CDATA[
<div> Keywords: Antennas, Multiscale modeling, Method of moments, Generalized scattering matrix, Hybrid framework <br />
Summary: 
Antennas embedded in or interacting with large structures pose challenges due to their different scales. A hybrid method combining Method of Moments (MoM) and Generalized Scattering Matrix (GSM) has been developed to address these challenges. This framework allows for the separate treatment of fine-scale antenna details and large-scale environment characteristics while maintaining their full coupling. Antennas of any shape can be characterized and reused across various environments, or a single environment can accommodate multiple antenna designs. The framework is versatile and can be extended to include GSM-PO and GSM + T-matrix methods, creating a unified approach for multiscale antenna modeling. By representing the large structure in the formulation best suited to its scale and shape, the approach offers accuracy, efficiency, and adaptability. Numerical validations on different antenna systems show excellent agreement with full-wave solvers and significant computational cost reductions for design and optimization. <br /><br /> <div>
arXiv:2509.22000v1 Announce Type: new 
Abstract: Electromagnetic analysis of antennas embedded in or interacting with large surrounding structures poses inherent multiscale challenges: the antenna is electrically small yet geometrically detailed, while the environment is electrically large but comparatively smooth. To address this, we present a hybrid method of moments (MoM) and generalized scattering matrix (GSM) framework that achieves a clean separation between fine-scale and large-scale complexities while preserving their full mutual coupling. Antennas of arbitrary geometry can be characterized once and reused across different environments, or conversely, a given environment can be modeled once to accommodate multiple antenna designs. The framework is inherently versatile, encompassing GSM-PO and GSM + T-matrix extensions, and thus provides a unified paradigm for multiscale antenna modeling. With the large body always represented by the formulation best suited to its scale and shape, the approach combines accuracy, efficiency, and adaptability. Numerical validations on implantable antennas, radome-protected arrays, and reflector systems confirm excellent agreement with full-wave solvers while demonstrating dramatic reductions in computational cost for design and optimization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orochi: Versatile Biomedical Image Processor</title>
<link>https://arxiv.org/abs/2509.22583</link>
<guid>https://arxiv.org/abs/2509.22583</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, biomedical images, Orochi, computational efficiency, fine-tuning framework

Summary: 
Orochi is introduced as an application-oriented image processor focusing on biomedical image processing. The tool is pre-trained on a wide range of publicly available studies using a Random Multi-scale Sampling strategy. Additionally, the Task-related Joint-embedding Pre-Training (TJP) method is proposed for self-supervision, offering an alternative to traditional Masked Image Modelling (MIM) and enhancing performance in tasks like registration. The computational efficiency of Orochi is improved through the use of Mamba's linear complexity and Multi-head Hierarchy Mamba. The tool offers a three-tier fine-tuning framework (Full, Normal, Light) for flexibility and parameter efficiency. Orochi demonstrates comparable or superior performance to existing specialist models, providing biologists with a versatile and efficient tool to streamline their workflow by eliminating the need to choose among multiple models.

<br /><br />Summary: <div>
arXiv:2509.22583v1 Announce Type: new 
Abstract: Deep learning has emerged as a pivotal tool for accelerating research in the life sciences, with the low-level processing of biomedical images (e.g., registration, fusion, restoration, super-resolution) being one of its most critical applications. Platforms such as ImageJ (Fiji) and napari have enabled the development of customized plugins for various models. However, these plugins are typically based on models that are limited to specific tasks and datasets, making them less practical for biologists. To address this challenge, we introduce Orochi, the first application-oriented, efficient, and versatile image processor designed to overcome these limitations. Orochi is pre-trained on patches/volumes extracted from the raw data of over 100 publicly available studies using our Random Multi-scale Sampling strategy. We further propose Task-related Joint-embedding Pre-Training (TJP), which employs biomedical task-related degradation for self-supervision rather than relying on Masked Image Modelling (MIM), which performs poorly in downstream tasks such as registration. To ensure computational efficiency, we leverage Mamba's linear computational complexity and construct Multi-head Hierarchy Mamba. Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and Light) and demonstrate that Orochi achieves comparable or superior performance to current state-of-the-art specialist models, even with lightweight parameter-efficient options. We hope that our study contributes to the development of an all-in-one workflow, thereby relieving biologists from the overwhelming task of selecting among numerous models.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Risk Sharing on Networks</title>
<link>https://arxiv.org/abs/2509.21411</link>
<guid>https://arxiv.org/abs/2509.21411</guid>
<content:encoded><![CDATA[
<div> Linear risk sharing, network structures, stochastic matrices, risk redistribution, peer-to-peer insurance<br />
<br />Summary: The article presents a framework for linear risk sharing (LRS) in alternative insurance and banking systems that rely on network structures. It explores how random losses can be reallocated through nonnegative linear operators in various network topologies, ensuring constraints such as budget balance, fairness, and diversification. The analysis uses stochastic and doubly stochastic matrices to compare different risk allocations rigorously, emphasizing variance reduction and majorization through doubly stochastic mixing. The study extends to network-based risk sharing in different graph types, showing how network topology influences risk outcomes. Introducing randomness in the sharing matrix via Erd\H{o}s--R\'enyi and preferential-attachment networks links risk-sharing properties to degree distributions. The research also examines the trade-off between self-retention and diversification in peer-to-peer insurance and network-based risk pooling, offering design principles for fair and efficient risk redistribution. <div>
arXiv:2509.21411v1 Announce Type: cross 
Abstract: Over the past decade alternatives to traditional insurance and banking have grown in popularity. The desire to encourage local participation has lead products such as peer-to-peer insurance, reciprocal contracts, and decentralized finance platforms to increasingly rely on network structures to redistribute risk among participants. In this paper, we develop a comprehensive framework for linear risk sharing (LRS), where random losses are reallocated through nonnegative linear operators which can accommodate a wide range of networks. Building on the theory of stochastic and doubly stochastic matrices, we establish conditions under which constraints such as budget balance, fairness, and diversification are guaranteed. The convex order framework allows us to compare different allocations rigorously, highlighting variance reduction and majorization as natural consequences of doubly stochastic mixing. We then extend the analysis to network-based sharing, showing how their topology shapes risk outcomes in complete, star, ring, random, and scale-free graphs. A second layer of randomness, where the sharing matrix itself is random, is introduced via Erd\H{o}s--R\'enyi and preferential-attachment networks, connecting risk-sharing properties to degree distributions. Finally, we study convex combinations of identity and network-induced operators, capturing the trade-off between self-retention and diversification. Our results provide design principles for fair and efficient peer-to-peer insurance and network-based risk pooling, combining mathematical soundness with economic interpretability.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic</title>
<link>https://arxiv.org/abs/2509.21475</link>
<guid>https://arxiv.org/abs/2509.21475</guid>
<content:encoded><![CDATA[
<div> latency, decentralization, Ethereum, protocol design, geographical distribution
Summary:
The article explores how protocol design influences the geographical distribution of validators in decentralized systems like Ethereum. It compares two block-building paradigms, SSP and MSP, and finds that SSP concentrates around relay placement, while MSP centralizes faster due to location-dependent payoff dispersion. The article highlights the importance of source placement and consensus settings in shaping validator geography but notes that once validators are already clustered, source placement's impact on decentralization diminishes. North America consistently emerges as a focal hub in most scenarios, indicating a trend towards centralization in the region. The study suggests that protocol design can play a significant role in promoting geographical decentralization and offers insights into potential levers for achieving this goal. <div>
arXiv:2509.21475v1 Announce Type: cross 
Abstract: Decentralization has a geographic dimension that conventional metrics such as stake distribution overlook. Where validators run affects resilience to regional shocks (outages, disasters, government intervention) and fairness in reward access. Yet in permissionless systems, locations cannot be mandated, but they emerge from incentives. Today, Ethereum's validators cluster along the Atlantic (EU and U.S. East Coast), where latency is structurally favorable. This raises a key question: when some regions already enjoy latency advantages, how does protocol design shape validator incentives and the geography of (de)centralization? We develop a latency-calibrated agent-based model and compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP), akin to MEV-Boost, where proposers fetch full blocks from a relay that also propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate value from multiple sources and broadcast the block themselves. Simulations show that SSP concentrates around relay placement but more slowly, since proximity mainly affects propagation, and the marginal value of time is relatively uniform across regions. MSP centralizes faster: aggregating across sources makes marginal value location-dependent, amplifying payoff dispersion and migration toward latency minima. Source placement and consensus settings can dampen or intensify these effects, though once validators are already clustered, the impact of source placement on decentralization is marginal. In most cases, North America consistently emerges as the focal hub. These findings show that protocol design materially shapes validator geography and offer levers for promoting geographical decentralization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need</title>
<link>https://arxiv.org/abs/2509.21553</link>
<guid>https://arxiv.org/abs/2509.21553</guid>
<content:encoded><![CDATA[
<div> knowledge graph, AI agents, climate data science, data accessibility, scientific workflows

Summary:
This paper addresses the challenges faced in climate data science by integrating a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG acts as a unifying layer organizing datasets, tools, and workflows, while AI agents enhance natural language interaction, data access automation, and streamlined analysis using generative AI services. By leveraging cloud-ready API data portals, the system demonstrates that a knowledge graph can significantly reduce the technical barriers for engaging in climate data science, enabling non-specialists to identify and analyze relevant datasets. The open-source design allows for community contributions, supporting the evolution of the KG and associated tools as a shared commons. This approach aims to democratize access to climate data, establish a reproducible framework for scientific inquiry, and facilitate human-AI collaboration in research. 

<br /><br />Summary: <div>
arXiv:2509.21553v1 Announce Type: cross 
Abstract: Climate data science faces persistent barriers stemming from the fragmented nature of data sources, heterogeneous formats, and the steep technical expertise required to identify, acquire, and process datasets. These challenges limit participation, slow discovery, and reduce the reproducibility of scientific workflows. In this paper, we present a proof of concept for addressing these barriers through the integration of a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG provides a unifying layer that organizes datasets, tools, and workflows, while AI agents -- powered by generative AI services -- enable natural language interaction, automated data access, and streamlined analysis. Together, these components drastically lower the technical threshold for engaging in climate data science, enabling non-specialist users to identify and analyze relevant datasets. By leveraging existing cloud-ready API data portals, we demonstrate that "a knowledge graph is all you need" to unlock scalable and agentic workflows for scientific inquiry. The open-source design of our system further supports community contributions, ensuring that the KG and associated tools can evolve as a shared commons. Our results illustrate a pathway toward democratizing access to climate data and establishing a reproducible, extensible framework for human--AI collaboration in scientific research.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bacterial Gene Regulatory Neural Network as a Biocomputing Library of Mathematical Solvers</title>
<link>https://arxiv.org/abs/2509.21598</link>
<guid>https://arxiv.org/abs/2509.21598</guid>
<content:encoded><![CDATA[
<div> Keywords: biocomputing, gene expression dynamics, GRNN framework, mathematical solvers, reliability

Summary:
The article presents a novel approach to biocomputing by utilizing bacterial gene expression dynamics within a GRNN framework to create a library of mathematical solvers. A sub-GRNN search algorithm is introduced to customize functional subnetworks for specific mathematical tasks by analyzing gene expression patterns under different input conditions. Tasks such as identifying Fibonacci numbers, prime numbers, multiplication, and Collatz step counts are successfully achieved using this approach. The study assesses the identified sub-GRNNs for robustness and reliability through gene-wise and collective perturbation and Lyapunov-based stability analysis. The results highlight the potential of leveraging native transcriptional machinery for performing diverse mathematical calculations and classifications while ensuring computing stability and reliability. <div>
arXiv:2509.21598v1 Announce Type: cross 
Abstract: Current biocomputing approaches predominantly rely on engineered circuits with fixed logic, offering limited stability and reliability under diverse environmental conditions. Here, we use the GRNN framework introduced in our previous work to transform bacterial gene expression dynamics into a biocomputing library of mathematical solvers. We introduce a sub-GRNN search algorithm that identifies functional subnetworks tailored to specific mathematical calculation and classification tasks by evaluating gene expression patterns across chemically encoded input conditions. Tasks include identifying Fibonacci numbers, prime numbers, multiplication, and Collatz step counts. The identified problem-specific sub-GRNNs are then assessed using gene-wise and collective perturbation, as well as Lyapunov-based stability analysis, to evaluate robustness and reliability. Our results demonstrate that native transcriptional machinery can be harnessed to perform diverse mathematical calculation and classification tasks, while maintaining computing stability and reliability.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Assisted Sustainable Remanufacturing, Reusing and Recycling for Lithium-ion Batteries</title>
<link>https://arxiv.org/abs/2406.00276</link>
<guid>https://arxiv.org/abs/2406.00276</guid>
<content:encoded><![CDATA[
<div> Machine learning, lithium-ion batteries, sustainability, circular economy, carbon neutrality <br />
Summary: <br />
The dissertation proposes a machine learning framework to tackle data scarcity and heterogeneity in the lifecycle of lithium-ion batteries (LIBs). A physics-informed quality control model predicts long-term degradation, while a generative learning-based method evaluates retired batteries accurately. Federated learning ensures privacy-preserving and high-precision cathode material sorting for efficient recycling. A unified diagnostics and prognostics framework based on correlation alignment enhances adaptability across multiple tasks, including state of health estimation and remaining useful life prediction. These contributions integrate physics, data generation, privacy preservation, and adaptive learning to advance sustainable battery management, promoting circular economy and global carbon neutrality. <div>
arXiv:2406.00276v2 Announce Type: replace-cross 
Abstract: The sustainable utilization of lithium-ion batteries (LIBs) is crucial to the global energy transition and carbon neutrality, yet data scarcity and heterogeneity remain major barriers across remanufacturing, reusing, and recycling. This dissertation develops a machine learning assisted framework to address these challenges throughout the battery lifecycle. A physics informed quality control model predicts long-term degradation from limited early-cycle data, while a generative learning based residual value assessment method enables rapid and accurate evaluation of retired batteries under random conditions. A federated learning strategy achieves privacy preserving and high precision cathode material sorting, supporting efficient recycling. Furthermore, a unified diagnostics and prognostics framework based on correlation alignment enhances adaptability across tasks such as state of health estimation, state of charge estimation, and remaining useful life prediction under varied testing protocols. Collectively, these contributions advance sustainable battery management by integrating physics, data generation, privacy preserving collaboration, and adaptive learning, offering methodological innovations to promote circular economy and global carbon neutrality.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Domain-adaptive Post-training for Financial LLMs</title>
<link>https://arxiv.org/abs/2501.04961</link>
<guid>https://arxiv.org/abs/2501.04961</guid>
<content:encoded><![CDATA[
<div> Keywords: domain-adaptive, large language models, finance, post-training, evaluation

Summary:
The study introduces FINDAP, a systematic approach for domain-adaptive post-training of large language models (LLMs) in the finance domain. It includes FinCap, defining core domain capabilities, FinRec, an optimized training recipe incorporating data distillation, FinTrain, a set of curated training datasets, and FinEval, a comprehensive evaluation suite. The resulting Llama-Fin model achieves top performance in financial tasks, showcasing the effectiveness of each training stage in enhancing specific capabilities. The study provides insights into challenges and solutions for domain adaptation of LLMs, highlighting the importance of tailored training strategies and evaluation criteria in specialized domains like finance.<br /><br />Summary: <div>
arXiv:2501.04961v3 Announce Type: replace-cross 
Abstract: Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach consists of four key components: FinCap, which defines the core capabilities required for the target domain; FinRec, an effective training recipe that jointly optimizes continual pre-training and instruction-following, along with a novel preference data distillation method leveraging process signals from a generative reward model; FinTrain, a curated set of training datasets supporting FinRec; and FinEval, a comprehensive evaluation suite aligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Adaptive Diffusion Model for Flexible Protein-Protein Docking</title>
<link>https://arxiv.org/abs/2509.20542</link>
<guid>https://arxiv.org/abs/2509.20542</guid>
<content:encoded><![CDATA[
<div> Keywords: protein-protein interactions, structural prediction, hierarchical adaptive diffusion, conformational changes, DIPS-AF dataset <br />
Summary: 
The study introduces a hierarchical adaptive diffusion framework for predicting protein-protein interactions, particularly focusing on cases with significant conformational changes. The framework separates global rigid-body motions and local flexibility, incorporating noise schedules to mimic induced-fit effects. Adaptive scheduling based on predicted conformational changes allows for faster flexing in response to variations. By leveraging a dataset of 39,000 examples for pre-training, the model outperforms existing methods on a benchmark dataset in both rigid and flexible scenarios. Ablation studies highlight the significance of adaptive schedules, dynamics features, and pre-training. Despite improvements, gaps in sampling, scoring, and conformational resolution remain to be addressed. Case studies shed light on the practical implications of the proposed framework. This innovative approach holds promise for enhancing accuracy and efficiency in predicting complex protein-protein interactions. <br /> <div>
arXiv:2509.20542v1 Announce Type: new 
Abstract: Structural prediction of protein-protein interactions is important to understand the molecular basis of cellular interactions, but it still faces major challenges when significant conformational changes are present. We propose a generative framework of hierarchical adaptive diffusion to improve accuracy and efficiency in such cases. It is hierarchical in separating global inter-protein rigid-body motions and local intra-protein flexibility in diffusion processes, and the distinct local and global noise schedules are designed to mimic the induced-fit effect. It is adaptive in conditioning the local flexibility schedule on predicted levels of conformational change, allowing faster flexing for larger anticipated conformational changes. Furthermore, it couples the local and global diffusion processes through a common score and confidence network with sequence, evolution, structure, and dynamics features as inputs, and maintains rotational or translational invariance or equivariance in outputs. It builds on our newly curated DIPS-AF dataset of nearly 39,000 examples for pre-training. Experiments on the independent docking benchmark dataset DB5.5 show that our model outperforms an AlphaFold2-like iterative transformer (GeoDock) and a diffusion model (DiffDock-PP) in both rigid and flexible cases, with larger improvements in more flexible cases. Ablation studies prove the importance of adaptive schedules, dynamics features, and pre-training. Additional analyses and case studies reveal remaining gaps in sampling, scoring, and conformational resolution.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Difference-Guided Reasoning: A Temporal-Spatial Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2509.20713</link>
<guid>https://arxiv.org/abs/2509.20713</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, reasoning, differences, abnormal behavior detection, external information integration

Summary:
Large Language Models (LLMs) are powerful tools but often lack the ability to actively discover new questions, hindering human-like reasoning. To address this limitation, a difference-guided reasoning framework is proposed. This framework enables LLMs to identify and act upon changes over time and space by formalizing differences through feature extraction and prioritizing impactful changes. Additionally, mechanisms for abnormal behavior detection and the integration of external information are included to enhance the reliability of reasoning. Verification results demonstrate that prompting LLMs with differences enhances focus on critical issues, improving alignment with desired reasoning outcomes compared to direct prompting. The framework provides a structured approach for LLMs to reason more effectively and simulate human-like thinking. 

<br /><br />Summary: 
- Proposal of a difference-guided reasoning framework for Large Language Models (LLMs)
- LLMs can identify and act upon changes through feature extraction and prioritization
- Inclusion of mechanisms for abnormal behavior detection and external information integration
- Verification results show improved focus on critical issues and alignment with desired reasoning outcomes
- Framework enhances LLMs' ability to reason effectively and simulate human-like thinking. <div>
arXiv:2509.20713v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are important tools for reasoning and problem-solving, while they often operate passively, answering questions without actively discovering new ones. This limitation reduces their ability to simulate human-like thinking, where noticing differences is a key trigger for reasoning. Thus, in this paper we propose a difference-guided reasoning framework, which enables LLMs to identify and act upon changes across time and space. The model formalizes differences through feature extraction, prioritizes the most impactful and latest changes, and links them to appropriate actions. We further extend the framework with mechanisms for abnormal behavior detection and the integration of external information from users or sensors, ensuring more reliable and grounded reasoning. Verification results show that prompting LLMs with differences improves focus on critical issues, leading to higher alignment with desired reasoning outcomes compared to direct prompting.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extrapolating Phase-Field Simulations in Space and Time with Purely Convolutional Architectures</title>
<link>https://arxiv.org/abs/2509.20770</link>
<guid>https://arxiv.org/abs/2509.20770</guid>
<content:encoded><![CDATA[
<div> surrogate, convolutional self-attention, physics-aware padding, variable time-step skipping, alloy systems
Summary:
The article introduces a new approach for accelerating phase-field models of liquid metal dealloying (LMD) using a conditionally parameterized, fully convolutional U-Net surrogate. This surrogate model incorporates convolutional self-attention and physics-aware padding, allowing it to generalize beyond its training window in both space and time. By leveraging parameter conditioning, the surrogate can adapt to different alloy systems and skip variable time steps. Despite being trained on short simulations, the surrogate achieves high accuracy in reproducing LMD physics, with relative errors typically under 5%. It can accelerate computations by up to 16,000 times, significantly reducing simulation times from weeks to seconds. This method represents a promising advancement towards scalable and high-fidelity extrapolation of LMD phase-field models. <br /><br />Summary: <div>
arXiv:2509.20770v1 Announce Type: new 
Abstract: Phase-field models of liquid metal dealloying (LMD) can resolve rich microstructural dynamics but become intractable for large domains or long time horizons. We present a conditionally parameterized, fully convolutional U-Net surrogate that generalizes far beyond its training window in both space and time. The design integrates convolutional self-attention and physics-aware padding, while parameter conditioning enables variable time-step skipping and adaptation to diverse alloy systems. Although trained only on short, small-scale simulations, the surrogate exploits the translational invariance of convolutions to extend predictions to much longer horizons than traditional solvers. It accurately reproduces key LMD physics, with relative errors typically under 5% within the training regime and below 10% when extrapolating to larger domains and later times. The method accelerates computations by up to 16,000 times, cutting weeks of simulation down to seconds, and marks an early step toward scalable, high-fidelity extrapolation of LMD phase-field models.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh Interpolation Graph Network for Dynamic and Spatially Irregular Global Weather Forecasting</title>
<link>https://arxiv.org/abs/2509.20911</link>
<guid>https://arxiv.org/abs/2509.20911</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, weather forecasting, global, irregularly distributed, generalization<br />
<br />
Summary: 
The study focuses on global weather forecasting, accounting for the irregular distribution of weather stations and the dynamic nature of weather patterns. The proposed Mesh Interpolation Graph Network (MIGN) addresses the challenges by utilizing a regular mesh interpolation network to handle spatially irregular data and incorporating parametric spherical harmonics location embedding for enhanced spatial generalization. Experimental results demonstrate that MIGN outperforms existing data-driven models, showcasing its spatial generalization ability and capability to predict weather at unseen locations. <div>
arXiv:2509.20911v1 Announce Type: new 
Abstract: Graph neural networks have shown promising results in weather forecasting, which is critical for human activity such as agriculture planning and extreme weather preparation. However, most studies focus on finite and local areas for training, overlooking the influence of broader areas and limiting their ability to generalize effectively. Thus, in this work, we study global weather forecasting that is irregularly distributed and dynamically varying in practice, requiring the model to generalize to unobserved locations. To address such challenges, we propose a general Mesh Interpolation Graph Network (MIGN) that models the irregular weather station forecasting, consisting of two key designs: (1) learning spatially irregular data with regular mesh interpolation network to align the data; (2) leveraging parametric spherical harmonics location embedding to further enhance spatial generalization ability. Extensive experiments on an up-to-date observation dataset show that MIGN significantly outperforms existing data-driven models. Besides, we show that MIGN has spatial generalization ability, and is capable of generalizing to previous unseen stations.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extensions of a Line-Graph-Based Method for Token Routing in Decentralized Exchanges</title>
<link>https://arxiv.org/abs/2509.21152</link>
<guid>https://arxiv.org/abs/2509.21152</guid>
<content:encoded><![CDATA[
<div> Decentralized exchanges, DEXs, decentralized finance, DeFi, token trades<br />
Summary:<br />
Decentralized exchanges (DEXs) play a crucial role in the decentralized finance (DeFi) ecosystem by processing billions of dollars in token trades daily. However, a significant amount of these trades are inefficient, missing out on potential profits. This study introduces three key extensions to improve routing methods in DEXs. First, the implementation of a breadth-first search (BFS) link iteration rule reduces computational cost and execution time while maintaining profitability. Second, a route-splitting strategy divides large trades to minimize price slippage and increase trader profits, at the expense of higher computational overhead. Third, the method is expanded to a multi-DEX aggregator setting to reflect real-world trading environments. Empirical data from Uniswap V2 and Sushiswap V2 shows significant enhancements in both computational efficiency and profitability, paving the way for future routing improvements.<br /> <div>
arXiv:2509.21152v1 Announce Type: new 
Abstract: Decentralized exchanges (DEXs) form a cornerstone of the decentralized finance (DeFi) ecosystem, processing token trades worth billions of dollars daily. Yet, a significant fraction of these trades are suboptimal: alternative routing paths could yield more target tokens. Addressing this inefficiency is both practically urgent and theoretically compelling. Building on the linear line-graph-based routing method of Zhang et al. (2025), we propose three key extensions that better capture real-world trading complexity. First, we introduce a breadth-first search (BFS) link iteration rule that reduces computational cost and average execution time without sacrificing profitability. Second, we design a route-splitting strategy that divides large trades into smaller ones, alleviating price slippage and increasing average trader profits, albeit at the cost of higher computational overhead. Third, we generalize the method beyond a single DEX to a multi-DEX aggregator setting, reflecting actual trading environments. Using empirical data from Uniswap V2 and Sushiswap V2, we demonstrate that these extensions substantially improve both computational efficiency and profitability, establishing a foundation for future routing enhancements.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models</title>
<link>https://arxiv.org/abs/2509.20570</link>
<guid>https://arxiv.org/abs/2509.20570</guid>
<content:encoded><![CDATA[
<div> physics-informed generation, reward optimization, diffusion models, value function, PIRF <br />
Summary: 
The article introduces a new approach to physics-informed generation by treating adherence to physical constraints as a reward signal in a sparse reward optimization framework. The proposed method, Physics-Informed Reward Fine-tuning (PIRF), bypasses the use of diffusion posterior sampling-style value function approximations, which are prone to errors and training instability. PIRF computes trajectory-level rewards and backpropagates their gradients directly, enhancing efficiency and data fidelity. Two key strategies, layer-wise truncated backpropagation and weight-based regularization, improve sample efficiency and physical enforcement in scientific generative modeling. Across multiple benchmarks, PIRF consistently outperforms traditional methods in enforcing physical constraints while maintaining efficient sampling regimes. This highlights the potential of reward fine-tuning for advancing generative modeling in scientific domains. <br /> <div>
arXiv:2509.20570v1 Announce Type: cross 
Abstract: Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLIP Arena: Advancing Fairness and Transparency in Machine Learning Interatomic Potentials via an Open, Accessible Benchmark Platform</title>
<link>https://arxiv.org/abs/2509.20630</link>
<guid>https://arxiv.org/abs/2509.20630</guid>
<content:encoded><![CDATA[
<div> benchmark, machine learning interatomic potentials, physics awareness, chemical reactivity, predictive capabilities
Summary:
Machine learning interatomic potentials (MLIPs) have transformed molecular and materials modeling, but existing benchmarks face challenges like data leakage and limited transferability. The new MLIP Arena platform assesses force field performance based on physics awareness, chemical reactivity, and stability in extreme conditions. It evaluates predictive capabilities for thermodynamic properties and physical phenomena beyond error-based metrics tied to specific density functional theory references. By highlighting failure modes of current MLIPs in real-world scenarios, MLIP Arena guides the development of more accurate and efficient MLIPs while ensuring physical consistency. The Python package and online leaderboard for MLIP Arena can be accessed at https://github.com/atomind-ai/mlip-arena. 
<br /><br />Summary: <div>
arXiv:2509.20630v1 Announce Type: cross 
Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized molecular and materials modeling, but existing benchmarks suffer from data leakage, limited transferability, and an over-reliance on error-based metrics tied to specific density functional theory (DFT) references. We introduce MLIP Arena, a benchmark platform that evaluates force field performance based on physics awareness, chemical reactivity, stability under extreme conditions, and predictive capabilities for thermodynamic properties and physical phenomena. By moving beyond static DFT references and revealing the important failure modes of current foundation MLIPs in real-world settings, MLIP Arena provides a reproducible framework to guide the next-generation MLIP development toward improved predictive accuracy and runtime efficiency while maintaining physical consistency. The Python package and online leaderboard are available at https://github.com/atomind-ai/mlip-arena.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations</title>
<link>https://arxiv.org/abs/2509.20667</link>
<guid>https://arxiv.org/abs/2509.20667</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, chemistry computations, supercomputer, prediction, resource optimization

Summary:
Machine learning models were developed to predict the resources required for massively parallel chemistry computations, such as coupled-cluster methods, on supercomputers. The models aim to guide users in selecting optimal runtime parameter values to minimize execution time and resource usage. Two key questions addressed were the shortest-time question, determining parameter configurations for the shortest execution time, and the cheapest-run question, minimizing resource usage. Evaluation of ML models on CCSD application runtime parameter values showed a Gradient Boosting model achieving low Mean Absolute Percentage Errors on DOE Frontier and Aurora supercomputers. Active learning demonstrated effectiveness in achieving accurate predictions with limited experimental data points. This work provides valuable insights for application users to make informed decisions before running costly experiments on supercomputers.<br /><br />Summary: Machine learning models were developed to predict resources for chemistry computations on supercomputers, guiding users in optimizing runtime parameters for efficiency. Evaluation on CCSD application data showed Gradient Boosting model accuracy, with active learning efficiently obtaining predictions with limited data points. <div>
arXiv:2509.20667v1 Announce Type: cross 
Abstract: In this work, we develop machine learning (ML) based strategies to predict resources (costs) required for massively parallel chemistry computations, such as coupled-cluster methods, to guide application users before they commit to running expensive experiments on a supercomputer. By predicting application execution time, we determine the optimal runtime parameter values such as number of nodes and tile sizes. Two key questions of interest to users are addressed. The first is the shortest-time question, where the user is interested in knowing the parameter configurations (number of nodes and tile sizes) to achieve the shortest execution time for a given problem size and a target supercomputer. The second is the cheapest-run question in which the user is interested in minimizing resource usage, i.e., finding the number of nodes and tile size that minimizes the number of node-hours for a given problem size.
  We evaluate a rich family of ML models and strategies, developed based on the collections of runtime parameter values for the CCSD (Coupled Cluster with Singles and Doubles) application executed on the Department of Energy (DOE) Frontier and Aurora supercomputers. Our experiments show that when predicting the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora and Frontier, respectively. In the case where it is expensive to run experiments just to collect data points, we show that active learning can achieve a MAPE of about 0.2 with just around 450 experiments collected from Aurora and Frontier.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Semantic Representations of Social Interactions from Moving Shapes</title>
<link>https://arxiv.org/abs/2509.20673</link>
<guid>https://arxiv.org/abs/2509.20673</guid>
<content:encoded><![CDATA[
<div> Keywords: social interactions, visual features, semantic representations, human similarity judgments, verb-based embeddings

Summary:
Humans can recognize social interactions from simple moving shapes, using both visual features and semantic representations. Study 1 showed that human responses to labeled animations were varied. Study 2 analyzed the geometry of 27 social interactions through human similarity judgments and compared them with model predictions. Semantic models, particularly verb-based embeddings from descriptions, offered additional insights beyond visual features in explaining human judgments. The results indicate that social perception in animations incorporates both visual and abstract representations, with verb-based embeddings providing the most accurate reflection of human similarity judgments. These findings highlight the importance of considering semantic structures in understanding how humans perceive and interpret social interactions in visual displays.<br /><br />Summary: <div>
arXiv:2509.20673v1 Announce Type: cross 
Abstract: Humans are social creatures who readily recognize various social interactions from simple display of moving shapes. While previous research has often focused on visual features, we examine what semantic representations that humans employ to complement visual features. In Study 1, we directly asked human participants to label the animations based on their impression of moving shapes. We found that human responses were distributed. In Study 2, we measured the representational geometry of 27 social interactions through human similarity judgments and compared it with model predictions based on visual features, labels, and semantic embeddings from animation descriptions. We found that semantic models provided complementary information to visual features in explaining human judgments. Among the semantic models, verb-based embeddings extracted from descriptions account for human similarity judgments the best. These results suggest that social perception in simple displays reflects the semantic structure of social interactions, bridging visual and abstract representations.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting</title>
<link>https://arxiv.org/abs/2509.20852</link>
<guid>https://arxiv.org/abs/2509.20852</guid>
<content:encoded><![CDATA[
<div> Keywords: newborns, artificial intelligence, fetal heart rate monitoring, missing data, autoencoder <br />
Summary: <br />
Approximately 10% of newborns require assistance with breathing at birth, with fetal heart rate monitoring being crucial in assessing fetal well-being. Wearable FHR monitors have enabled continuous monitoring but face challenges due to signal dropouts from sensor displacement. Traditional methods like interpolation fail to preserve signal characteristics. This study proposes a masked transformer-based autoencoder to reconstruct missing FHR signals, capturing spatial and frequency components for signal inpainting and forecasting. The method shows robustness across varying durations of missing data and offers potential for AI-based risk algorithms. Integration into wearable monitors could enhance early and reliable risk detection. <div>
arXiv:2509.20852v1 Announce Type: cross 
Abstract: Approximately 10\% of newborns require assistance to initiate breathing at birth, and around 5\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem</title>
<link>https://arxiv.org/abs/2509.21039</link>
<guid>https://arxiv.org/abs/2509.21039</guid>
<content:encoded><![CDATA[
<div> MLIR, Mojo, GPU programming, scientific computing, Python interoperability <br />
Summary:
The article explores the performance and portability of Mojo, a novel language for scientific computing on GPUs. Mojo, based on LLVM's MLIR compiler infrastructure, aims to bridge gaps in performance and productivity by combining Python interoperability with CUDA-like syntax for GPU programming. Four scientific workloads were targeted for evaluation against vendor baselines on NVIDIA and AMD GPUs. Mojo's performance was competitive with CUDA and HIP for memory-bound kernels on both GPU platforms. However, discrepancies were observed on AMD GPUs for atomic operations and fast-math compute-bound kernels on both AMD and NVIDIA GPUs. Despite low-level programming requirements and learning curve, Mojo shows promise in bridging gaps in the Python ecosystem for scientific computing and AI convergence. <div>
arXiv:2509.21039v1 Announce Type: cross 
Abstract: We explore the performance and portability of the novel Mojo language for scientific computing workloads on GPUs. As the first language based on the LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure, Mojo aims to close performance and productivity gaps by combining Python's interoperability and CUDA-like syntax for compile-time portable GPU programming. We target four scientific workloads: a seven-point stencil (memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and Hartree-Fock (compute-bound with atomic operations); and compare their performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We show that Mojo's performance is competitive with CUDA and HIP for memory-bound kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve and programming requirements are still fairly low-level, Mojo can close significant gaps in the fragmented Python ecosystem in the convergence of scientific computing and AI.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust AI-ECG for Predicting Left Ventricular Systolic Dysfunction in Pediatric Congenital Heart Disease</title>
<link>https://arxiv.org/abs/2509.19564</link>
<guid>https://arxiv.org/abs/2509.19564</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, electrocardiogram, pediatric, left ventricular systolic dysfunction, low-resource<br />
Summary:<br />
- The study focuses on enhancing AI-ECG performance for detecting left ventricular systolic dysfunction in pediatric patients with congenital heart disease.
- Current AI-ECG methods rely on large datasets, limiting their practical application in hospitals with limited pediatric ECG data.
- The proposed training framework introduces an on-manifold adversarial perturbation strategy to generate synthetic noise samples that mimic real-world signal variations in pediatric ECGs.
- An uncertainty-aware adversarial training algorithm, independent of architecture, is developed to improve model robustness.
- Evaluation on a real-world pediatric dataset demonstrates the framework's effectiveness in enabling cost-effective and reliable detection of left ventricular systolic dysfunction, making it suitable for deployment in resource-limited clinical settings.<br /> 
Summary: <div>
arXiv:2509.19564v1 Announce Type: new 
Abstract: Artificial intelligence-enhanced electrocardiogram (AI-ECG) has shown promise as an inexpensive, ubiquitous, and non-invasive screening tool to detect left ventricular systolic dysfunction in pediatric congenital heart disease. However, current approaches rely heavily on large-scale labeled datasets, which poses a major obstacle to the democratization of AI in hospitals where only limited pediatric ECG data are available. In this work, we propose a robust training framework to improve AI-ECG performance under low-resource conditions. Specifically, we introduce an on-manifold adversarial perturbation strategy for pediatric ECGs to generate synthetic noise samples that better reflect real-world signal variations. Building on this, we develop an uncertainty-aware adversarial training algorithm that is architecture-agnostic and enhances model robustness. Evaluation on the real-world pediatric dataset demonstrates that our method enables low-cost and reliable detection of left ventricular systolic dysfunction, highlighting its potential for deployment in resource-limited clinical settings.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Language Models with Modality-Specific Experts for Financial Forecasting from Interleaved Sequences of Text and Time Series</title>
<link>https://arxiv.org/abs/2509.19628</link>
<guid>https://arxiv.org/abs/2509.19628</guid>
<content:encoded><![CDATA[
<div> Keywords: financial forecasting, neural architecture, multimodal understanding, cross-modal alignment, interpretability<br />
<br />
Summary: 
The article proposes a neural architecture that integrates text and time series data for financial forecasting. The model utilizes modality-specific experts to capture unique patterns in each modality while allowing for joint reasoning. It also introduces a cross-modal alignment framework with a token weighting mechanism to align representations across modalities. The approach achieves state-of-the-art performance compared to baselines and incorporates interpretability methods to understand the value of different modalities. Economic gains in investment simulations validate the effectiveness of the proposed method. <div>
arXiv:2509.19628v1 Announce Type: new 
Abstract: Text and time series data offer complementary views of financial markets: news articles provide narrative context about company events, while stock prices reflect how markets react to those events. However, despite their complementary nature, effectively integrating these interleaved modalities for improved forecasting remains challenging. In this work, we propose a unified neural architecture that models these interleaved sequences using modality-specific experts, allowing the model to learn unique time series patterns, while still enabling joint reasoning across modalities and preserving pretrained language understanding capabilities. To further improve multimodal understanding, we introduce a cross-modal alignment framework with a salient token weighting mechanism that learns to align representations across modalities with a focus on the most informative tokens. We demonstrate the effectiveness of our approach on a large-scale financial forecasting task, achieving state-of-the-art performance across a wide variety of strong unimodal and multimodal baselines. We develop an interpretability method that reveals insights into the value of time series-context and reinforces the design of our cross-modal alignment objective. Finally, we demonstrate that these improvements translate to meaningful economic gains in investment simulations.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing failure morphologies in fiber-reinforced composites via k-means clustering based multiscale framework</title>
<link>https://arxiv.org/abs/2509.20011</link>
<guid>https://arxiv.org/abs/2509.20011</guid>
<content:encoded><![CDATA[
<div> homogenization, composite materials, damage analysis, clustering methods, failure prediction 
Summary: 
A novel homogenization methodology is introduced for analyzing fiber-reinforced composite material failures. This method incorporates elastic and eigen influence tensors in a damage-informed transformation field analysis (D-TFA) framework, allowing for efficient and realistic simulations of damage under uniform stress and strain conditions. The use of a reduced-order modeling strategy improves computational efficiency, while clustering methods help partition the microscale domain for more accurate predictions. By simulating the response of a representative volume element (RVE) and comparing clustering schemes, the model's performance is evaluated. Damage morphologies are accurately captured and directional strengths predicted, with higher cluster counts proving essential for complex microstructures. The effectiveness of the proposed framework is demonstrated in open-hole specimen tests, accurately predicting failure paths for domains with different fiber layups. <div>
arXiv:2509.20011v1 Announce Type: new 
Abstract: A novel homogenization methodology is proposed for analyzing the failure of fiber-reinforced composite materials, utilizing elastic and eigen influence tensors within a damage informed transformation field analysis (D-TFA) framework. This approach includes a technique for calculating macroscopic damage under uniform stress and strain conditions, offering more realistic simulations. Computational efficiency is enhanced through a reduced-order modeling strategy, while elastic and eigen strain distribution driven k-means clustering methods are employed to partition the microscale domain. The model's performance is assessed by simulating the response of a representative volume element (RVE) treated as a homogenized continuum. Subsequently, a comparative assessment is carried out to check the efficacy of two clustering schemes. Damage morphologies are calculated using proposed framework and compared with predictions obtained using finite element method. Furthermore, open-hole specimen tests are simulated and failure paths are predicted for the domains with different fiber layups. Ultimately, we show that D-TFA can accurately capture damage patterns and directional strengths, providing improved predictions of the mechanical behavior of composite materials. It has been demonstrated that higher cluster counts are crucial for capturing a more accurate stress-strain response, especially for complex microstructures.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multi-Objective Constrained Bayesian Optimization of Bridge Girder</title>
<link>https://arxiv.org/abs/2509.20161</link>
<guid>https://arxiv.org/abs/2509.20161</guid>
<content:encoded><![CDATA[
<div> Keywords: greenhouse gas emissions, structural design optimization, Bayesian optimization, constrained optimization, finite element simulations

Summary: 
The article discusses the significant contribution of the buildings and construction sector to greenhouse gas emissions, with a focus on reducing emissions through optimized structural design. It introduces a novel approach using multi-objective constrained Bayesian optimization to achieve this goal efficiently. By incorporating proper orthogonal decomposition and Kriging partial least squares, the method aims to reduce the computational expenses associated with finite element simulations in structural design. The constrained expected improvement function is used for Bayesian optimization, demonstrated through a case study of a concrete bridge girder design. The results show potential cost savings of approximately 10% to 15% in financial costs and 20% in environmental costs while ensuring structural integrity. This approach showcases the effectiveness of optimization techniques in achieving both economic and environmental benefits in the construction industry. 

<br /><br />Summary: <div>
arXiv:2509.20161v1 Announce Type: new 
Abstract: The buildings and construction sector is a significant source of greenhouse gas emissions, with cement production alone contributing 7~\% of global emissions and the industry as a whole accounting for approximately 37~\%. Reducing emissions by optimizing structural design can achieve significant global benefits. This article introduces an efficient multi-objective constrained Bayesian optimization approach to address this challenge. Rather than attempting to determine the full set of non-dominated solutions with arbitrary trade-offs, the approach searches for a solution matching a specified trade-off. Structural design is typically conducted using computationally expensive finite element simulations, whereas Bayesian optimization offers an efficient approach for optimizing problems that involve such high-cost simulations. The proposed method integrates proper orthogonal decomposition for dimensionality reduction of simulation results with Kriging partial least squares to enhance efficiency. Constrained expected improvement is used as an acquisition function for Bayesian optimization. The approach is demonstrated through a case study of a two-lane, three-span post-tensioned concrete bridge girder, incorporating fifteen design variables and nine constraints. A comparison with conventional design methods demonstrates the potential of this optimization approach to achieve substantial cost reductions, with savings of approximately 10\% to 15\% in financial costs and about 20\% in environmental costs for the case study, while ensuring structural integrity.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Overview of Meshfree Collocation Methods</title>
<link>https://arxiv.org/abs/2509.20056</link>
<guid>https://arxiv.org/abs/2509.20056</guid>
<content:encoded><![CDATA[
<div> Keywords: meshfree collocation methods, differential operators, unstructured point clouds, numerical approximation, computational grid. 

Summary: 
This article provides a comprehensive overview of meshfree collocation methods used in numerically approximating differential operators on unstructured point clouds. The methods discussed in the literature do not rely on a computational grid, instead approximating functions and derivatives at irregularly distributed collocation points. The historical development of key concepts is traced, and a classification of methods based on their principle of derivation is proposed. While some methods are similar, subtle differences exist and are highlighted. A unifying formulation of meshfree collocation methods is presented to elucidate these differences, showing how each method can be derived from this formulation. Additionally, a generalized derivation for future meshfree collocation methods is suggested. <div>
arXiv:2509.20056v1 Announce Type: cross 
Abstract: We provide a comprehensive overview of meshfree collocation methods for numerically approximating differential operators on continuously labeled unstructured point clouds. Meshfree collocation methods do not require a computational grid or mesh. Instead, they approximate smooth functions and their derivatives at potentially irregularly distributed collocation points, often called particles, to a desired order of consistency. We review several meshfree collocation methods from the literature, trace the historical development of key concepts, and propose a classification of methods according to their principle of derivation. Although some of the methods reviewed are similar or identical, there are subtle yet important differences between many, which we highlight and discuss. We present a unifying formulation of meshfree collocation methods that renders these differences apparent and show how each method can be derived from this formulation. Finally, we propose a generalized derivation for meshfree collocation methods going forward.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Multi-Species Bird Classification on Low-Power Bioacoustic Loggers</title>
<link>https://arxiv.org/abs/2509.20103</link>
<guid>https://arxiv.org/abs/2509.20103</guid>
<content:encoded><![CDATA[
<div> Neural network, bird audio classification, microcontrollers, biodiversity monitoring, energy-efficient <br />
<br />
Summary: 
This paper introduces WrenNet, a neural network designed for real-time multi-species bird audio classification on low-power microcontrollers to facilitate scalable biodiversity monitoring. WrenNet features a semi-learnable spectral feature extractor that outperforms traditional alternatives. Testing on a 70-species dataset, WrenNet achieves high accuracy rates, particularly for acoustically distinct species. It displays energy efficiency, consuming only 77mJ per inference on an AudioMoth device with limited RAM. In comparison to Birdnet, WrenNet proves over 16 times more energy-efficient when running on a Raspberry Pi 3B+. This research presents a practical solution for continuous, multi-species acoustic monitoring on low-power edge devices, offering promising applications for biodiversity conservation efforts. <br /> <div>
arXiv:2509.20103v1 Announce Type: cross 
Abstract: This paper introduces WrenNet, an efficient neural network enabling real-time multi-species bird audio classification on low-power microcontrollers for scalable biodiversity monitoring. We propose a semi-learnable spectral feature extractor that adapts to avian vocalizations, outperforming standard mel-scale and fully-learnable alternatives. On an expert-curated 70-species dataset, WrenNet achieves up to 90.8\% accuracy on acoustically distinctive species and 70.1\% on the full task. When deployed on an AudioMoth device ($\leq$1MB RAM), it consumes only 77mJ per inference. Moreover, the proposed model is over 16x more energy-efficient compared to Birdnet when running on a Raspberry Pi 3B+. This work demonstrates the first practical framework for continuous, multi-species acoustic monitoring on low-power edge devices.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Fusion for Full-Range Response Reconstruction via Diffusion Models</title>
<link>https://arxiv.org/abs/2502.00795</link>
<guid>https://arxiv.org/abs/2502.00795</guid>
<content:encoded><![CDATA[
<div> Diffusion models, structural health monitoring, data fusion framework, sparse sensor measurements, probabilistic constraints<br />
Summary:<br />
Accurate monitoring of structures is essential for safety, but limited sensor deployment can be a challenge. This paper introduces a novel data fusion framework that utilizes diffusion models to reconstruct full-range structural responses from sparse sensor measurements. The framework incorporates Diffusion Posterior Sampling (DPS) to guide the reconstruction process using sensor measurements as probabilistic constraints. Three forward models are developed to adapt to different sensor placement scenarios and reconstruction targets. Validation on a steel plate shear wall shows promising results, with Weighted Mean Absolute Percentage Errors ranging from 1.62% to 3.49%. Sensitivity analyses demonstrate robust performance under various conditions. This framework presents a new approach for probabilistic modeling in structural health monitoring, offering a data fusion solution for comprehensive structural monitoring. <br /> <div>
arXiv:2502.00795v2 Announce Type: replace 
Abstract: Accurately capturing the full-range response of structures is crucial in structural health monitoring (SHM) for ensuring safety and operational integrity. However, limited sensor deployment due to cost, accessibility, or scale often hinders comprehensive monitoring. This paper presents a generative data fusion framework utilizing diffusion models, to reconstruct the full-range structural response from sparse and heterogeneous sensor measurements. We incorporate Diffusion Posterior Sampling (DPS) into the reconstruction framework, using sensor measurements as probabilistic constraints to guide the sampling process. Three forward models are designed: Direct Observation Mapping (DOM), Channel-based Observation Mapping (COM), and Neural Network Forward Model (NNFM), enabling flexible adaptation to different sensor placement conditions and reconstruction targets. The proposed framework is validated on a steel plate shear wall exhibiting nonlinear responses. By simultaneously sampling 100 realizations and averaging them as the ensemble prediction result, the three forward models achieve Weighted Mean Absolute Percentage Errors of 1.62% (DOM), 3.27% (COM), and 3.49% (NNFM). Sensitivity analyses further demonstrate robust performance under varying hyperparameters, sensor configurations, and noise levels. The proposed framework shows new possibilities for probabilistic modeling and decision-making in SHM by harnessing the capabilities of diffusion models, offering a novel data fusion approach for full-range monitoring of structures.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2D implementation of Kinetic-diffusion Monte Carlo in Eiron</title>
<link>https://arxiv.org/abs/2509.19140</link>
<guid>https://arxiv.org/abs/2509.19140</guid>
<content:encoded><![CDATA[
<div> Particle-based kinetic Monte Carlo simulations, neutral particles, computational bottlenecks, tokamak scrape-off layer simulations, high-collisional regimes<br />
Summary: 
The article introduces the Kinetic-diffusion Monte Carlo scheme for simulating neutral particles in high-collisional regimes, focusing on tokamak scrape-off layer simulations. By approximating high-collisional kinetic dynamics with diffusion in these regimes, computational costs are significantly reduced. The scheme's extension to the two-dimensional setting and implementation in the Eiron particle code are discussed. The results demonstrate a notable speedup in high-collisional cases compared to standard kinetic simulations. This advancement addresses the computational challenges associated with resolving individual collision events and showcases the potential of asymptotic-preserving schemes in improving the efficiency of particle-based simulations in tokamak environments. <div>
arXiv:2509.19140v1 Announce Type: new 
Abstract: Particle-based kinetic Monte Carlo simulations of neutral particles is one of the major computational bottlenecks in tokamak scrape-off layer simulations. This computational cost comes from the need to resolve individual collision events in high-collisional regimes. However, in such regimes, one can approximate the high-collisional kinetic dynamics with computationally cheaper diffusion. Asymptotic-preserving schemes make use of this limit to perform simulations in these regimes, without a blow-up in computational cost as incurred by standard kinetic approaches. One such scheme is Kinetic-diffusion Monte Carlo. In this paper, we present a first extension of this scheme to the two-dimensional setting and its implementation in the Eiron particle code. We then demonstrate that this implementation produces a significant speedup over kinetic simulations in high-collisional cases.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlloyInter: Visualising Alloy Mixture Interpolations in t-SNE Representations</title>
<link>https://arxiv.org/abs/2509.19202</link>
<guid>https://arxiv.org/abs/2509.19202</guid>
<content:encoded><![CDATA[
<div> keywords: AlloyInter, input mixtures, output parameters space, eXplainable Artificial Intelligence, interpolation

Summary: 
AlloyInter is a novel system proposed for joint exploration of input mixtures and output parameters space in the SciVis Contest 2025. The system utilizes an interpolation approach guided by eXplainable Artificial Intelligence (XAI) to allow users to discover input mixture ratios while specifying output parameter goals. The system leverages a learned model ensemble to iteratively adjust and improve towards a set goal. By incorporating robust XAI techniques and combining manifold learning with interpolation approaches, AlloyInter strengthens its capabilities for efficient exploration. The system aims to enhance user interaction and understanding by providing a platform for discovering optimal input mixtures based on desired output parameters. With a focus on improving usability and interpretability, AlloyInter offers a promising solution for navigating complex input-output relationships in scientific visualization tasks. <br /><br />Summary: <div>
arXiv:2509.19202v1 Announce Type: new 
Abstract: This entry description proposes AlloyInter, a novel system to enable joint exploration of input mixtures and output parameters space in the context of the SciVis Contest 2025. We propose an interpolation approach, guided by eXplainable Artificial Intelligence (XAI) based on a learned model ensemble that allows users to discover input mixture ratios by specifying output parameter goals that can be iteratively adjusted and improved towards a goal. We strengthen the capabilities of our system by building upon prior research within the robustness of XAI, as well as combining well-established techniques like manifold learning with interpolation approaches.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning</title>
<link>https://arxiv.org/abs/2509.18120</link>
<guid>https://arxiv.org/abs/2509.18120</guid>
<content:encoded><![CDATA[
<div> CoCross-silo federated learning, data privacy, economic competition, CoCoGen framework, generative AI, potential game theory  
Summary:  
CoCoGen is a framework designed to address the challenges of cross-silo federated learning in the presence of economic competition and statistical heterogeneity. It utilizes generative AI and potential game theory to optimize collaborative training among organizations while maximizing social welfare. The framework models competition and heterogeneity through learning performance and utility-based formulations, treating each training round as a weighted potential game. Experimental results on the Fashion-MNIST dataset demonstrate that CoCoGen outperforms baseline methods, showing how varying levels of heterogeneity and competition impact organizational behavior. By leveraging generative AI and game theory, CoCoGen enables organizations to engage in coopetitive-compatible data generation, fostering collaboration in a competitive landscape. <br /><br />Summary: <div>
arXiv:2509.18120v1 Announce Type: cross 
Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or banks) to collaboratively train artificial intelligence (AI) models while preserving data privacy by keeping data local. While prior work has primarily addressed statistical heterogeneity across organizations, a critical challenge arises from economic competition, where organizations may act as market rivals, making them hesitant to participate in joint training due to potential utility loss (i.e., reduced net benefit). Furthermore, the combined effects of statistical heterogeneity and inter-organizational competition on organizational behavior and system-wide social welfare remain underexplored. In this paper, we propose CoCoGen, a coopetitive-compatible data generation framework, leveraging generative AI (GenAI) and potential game theory to model, analyze, and optimize collaborative learning under heterogeneous and competitive settings. Specifically, CoCoGen characterizes competition and statistical heterogeneity through learning performance and utility-based formulations and models each training round as a weighted potential game. We then derive GenAI-based data generation strategies that maximize social welfare. Experimental results on the Fashion-MNIST dataset reveal how varying heterogeneity and competition levels affect organizational behavior and demonstrate that CoCoGen consistently outperforms baseline methods.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning</title>
<link>https://arxiv.org/abs/2509.18169</link>
<guid>https://arxiv.org/abs/2509.18169</guid>
<content:encoded><![CDATA[
<div> training, inference, neural networks, computation, reasoning

Summary:
- The article introduces the PiMoE (Physically-isolated Mixture of Experts) architecture, which integrates computation and reasoning capabilities into neural networks.
- PiMoE endogenously incorporates computational expertise within the network, allowing for iterative alternation within a single chain of thought.
- Evaluations against LLM finetuning and multi-agent system approaches show that PiMoE outperforms in accuracy and exhibits improvements in response latency, token usage, and GPU energy consumption.
- This architecture offers an efficient, interpretable, and scalable solution for next-generation intelligent systems in scientific or industrial applications. 

<br /><br />Summary: <div>
arXiv:2509.18169v1 Announce Type: cross 
Abstract: Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM</title>
<link>https://arxiv.org/abs/2509.18178</link>
<guid>https://arxiv.org/abs/2509.18178</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational Fluid Dynamics, Foam-Agent, automation, multi-agent framework, simulation

Summary:
Foam-Agent is a multi-agent framework designed to automate the entire OpenFOAM workflow, reducing the steep learning curve and manual setup challenges in Computational Fluid Dynamics (CFD) simulations. It offers end-to-end simulation automation, managing tasks from pre-processing to post-simulation visualization. The framework features a composable service architecture that allows for flexible integration with other systems, enhancing exploratory workflows. Foam-Agent ensures high-fidelity configuration generation through precise context retrieval and dependency-aware processes. Evaluation results show a high success rate in handling simulation tasks, outperforming existing frameworks. By democratizing complex scientific computing, Foam-Agent lowers the expertise barrier for CFD, making it more accessible to a wider range of users. The code for Foam-Agent is publicly available on GitHub for use and further development. 

<br /><br />Summary: <div>
arXiv:2509.18178v1 Announce Type: cross 
Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in engineering, yet its steep learning curve and complex manual setup create significant barriers. To address these challenges, we introduce Foam-Agent, a multi-agent framework that automates the entire end-to-end OpenFOAM workflow from a single natural language prompt. Our key innovations address critical gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation: Foam-Agent is the first system to manage the full simulation pipeline, including advanced pre-processing with a versatile Meshing Agent capable of handling external mesh files and generating new geometries via Gmsh, automatic generation of HPC submission scripts, and post-simulation visualization via ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent, the framework uses Model Context Protocol (MCP) to expose its core functions as discrete, callable tools. This allows for flexible integration and use by other agentic systems, such as Claude-code, for more exploratory workflows. 3. High-Fidelity Configuration Generation: We achieve superior accuracy through a Hierarchical Multi-Index RAG for precise context retrieval and a dependency-aware generation process that ensures configuration consistency. Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2% success rate with Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the expertise barrier for CFD, demonstrating how specialized multi-agent systems can democratize complex scientific computing. The code is public at https://github.com/csml-rpi/Foam-Agent.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market</title>
<link>https://arxiv.org/abs/2509.18820</link>
<guid>https://arxiv.org/abs/2509.18820</guid>
<content:encoded><![CDATA[
<div> cryptocurrency, correlation structures, fluctuation amplitudes, network structures, portfolio construction <br />
<br />
Summary: 
This study introduces a methodology for analyzing evolving correlation structures in complex systems based on cryptocurrency market dynamics. By using the $q$-dependent detrended cross-correlation coefficient $\rho(q,s)$, correlations at varying fluctuation amplitudes and time scales are captured. The approach utilizes $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Analysis of minute-by-minute exchange rate data for 140 cryptocurrencies on Binance from Jan 2021 to Oct 2024 reveals significant shifts in $q$MSTs, particularly during the Terra/Luna crash in April 2022. The network initially centered around Bitcoin (BTC) but later decentralized, with Ethereum (ETH) and other assets gaining prominence. Spectral analysis indicates decreasing dominance of BTC and increased diversification among assets. Medium-scale fluctuations exhibit stronger correlations than large-scale ones, influencing portfolio construction strategies. During crashes, major disruptions amplify correlation differences, leading to fully decentralized network structures. These findings demonstrate the effectiveness of $q$MSTs in uncovering fluctuation-dependent correlations and have potential applications in various complex systems beyond finance such as biology and social systems. <div>
arXiv:2509.18820v1 Announce Type: cross 
Abstract: Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A failure mode dependent continuum damage model for laminated composites with optimized model parameters : Application to curved beams</title>
<link>https://arxiv.org/abs/2509.19051</link>
<guid>https://arxiv.org/abs/2509.19051</guid>
<content:encoded><![CDATA[
<div> damage model, continuum damage modeling, laminated composite panels, optimization algorithm, finite element method

Summary: 
A new continuum damage model for laminated composite panels is proposed, utilizing polynomial-based damage hardening functions and characterized parameters based on experimental stress-strain curves. The model is optimized using a steepest descent algorithm to minimize differences between predicted and experimental data. Damage evolution equations are derived and applied to a curved beam model, showing non-linear behavior in load vs deflection curves and successfully capturing stiffness degradation and damage differences in tension and compression. The model is compared to existing models and found effective in representing material properties and non-linearity in composite behavior. The study showcases the model's ability to predict damage accurately and demonstrate thermodynamically consistent continuum damage modeling for laminated composite structures. <br /><br /> <div>
arXiv:2509.19051v1 Announce Type: cross 
Abstract: In this article, a failure mode dependent and thermodynamically consistent continuum damage model with polynomial-based damage hardening functions is proposed for continuum damage modeling of laminated composite panels. The damage model parameters are characterized based on all uniaxial/shear experimental stress-strain curves. Steepest descent optimization algorithm is used to minimize the difference between model predicted and experimental stress-strain curves to get the optimzed model parameters. The fully characterized damage evolution equations are used for damage prediction of a moderately thick laminated composite curved beam modeled using first-order shear deformation theory. Finite element method with load control is used to get the non-linear algebraic equations which are solved using Newton Raphson method. The developed model is compared with the existing failure mode dependent and failure mode independent damage models. The results depict the efficacy of the proposed model to capture non-linearity in the load vs deflection curve due to stiffness degradation and different damage in tension andcompression consistent with uniaxial/shear stress-strain response and strength properties of the material, respectively.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving Linear and Angular Momentum for Dynamical Systems</title>
<link>https://arxiv.org/abs/2501.07373</link>
<guid>https://arxiv.org/abs/2501.07373</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Multi-body Dynamical Systems, Physics-Informed, Interpretable, Real-time <br />
<br />
Summary: <br />
The paper introduces Dynami-CAL GraphNet, a Physics-Informed Graph Neural Network designed for accurate and interpretable modeling of multi-body dynamical systems. By integrating physics-based inductive biases with the learning capabilities of GNNs, the model enforces pairwise conservation of linear and angular momentum for interacting nodes using edge-local reference frames. This design ensures physically consistent predictions of node dynamics and offers interpretable edge-wise linear and angular impulses resulting from pairwise interactions. Evaluated on a 3D granular system, Dynami-CAL GraphNet demonstrates stable error accumulation over extended rollouts, effective extrapolations to unseen configurations, and robust handling of heterogeneous interactions and external forces. The model offers significant advantages in various fields like robotics, aerospace engineering, and materials science by providing scalable, physically consistent predictions that adhere to fundamental conservation laws, enabling the inference of forces and moments while efficiently handling diverse interactions and external forces. <div>
arXiv:2501.07373v2 Announce Type: replace-cross 
Abstract: Accurate, interpretable, and real-time modeling of multi-body dynamical systems is essential for predicting behaviors and inferring physical properties in natural and engineered environments. Traditional physics-based models face scalability challenges and are computationally demanding, while data-driven approaches like Graph Neural Networks (GNNs) often lack physical consistency, interpretability, and generalization. In this paper, we propose Dynami-CAL GraphNet, a Physics-Informed Graph Neural Network that integrates the learning capabilities of GNNs with physics-based inductive biases to address these limitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and angular momentum for interacting nodes using edge-local reference frames that are equivariant to rotational symmetries, invariant to translations, and equivariant to node permutations. This design ensures physically consistent predictions of node dynamics while offering interpretable, edge-wise linear and angular impulses resulting from pairwise interactions. Evaluated on a 3D granular system with inelastic collisions, Dynami-CAL GraphNet demonstrates stable error accumulation over extended rollouts, effective extrapolations to unseen configurations, and robust handling of heterogeneous interactions and external forces. Dynami-CAL GraphNet offers significant advantages in fields requiring accurate, interpretable, and real-time modeling of complex multi-body dynamical systems, such as robotics, aerospace engineering, and materials science. By providing physically consistent and scalable predictions that adhere to fundamental conservation laws, it enables the inference of forces and moments while efficiently handling heterogeneous interactions and external forces.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Engineering AGI: Benchmarking the Engineering Design Capabilities of LLMs</title>
<link>https://arxiv.org/abs/2509.16204</link>
<guid>https://arxiv.org/abs/2509.16204</guid>
<content:encoded><![CDATA[
<div> Engineering Design Benchmark, Large Language Models, Real-world tasks, Domain Knowledge, Simulation-based Evaluation<br />
<br />Summary:
Industry pioneers aim to develop general-purpose AI engineers for ambitious projects. Large language models face unique challenges in engineering design, requiring synthesis of domain knowledge, trade-off navigation, and process management. The ENGDESIGN benchmark evaluates LLMs across nine engineering domains, emphasizing design synthesis and objective-oriented solutions. Tasks simulate real-world engineering challenges with detailed descriptions of goals, constraints, and requirements. A simulation-based evaluation approach tests LLM-generated designs using domain-specific simulations. This benchmark fills a gap in existing benchmarks by focusing on design tasks rather than factual recall or question answering. Industry's vision of AI engineers shaping grand projects may become a reality with advancements in LLM capabilities through benchmarks like ENGDESIGN. <br /> <div>
arXiv:2509.16204v1 Announce Type: new 
Abstract: Today, industry pioneers dream of developing general-purpose AI engineers capable of designing and building humanity's most ambitious projects--from starships that will carry us to distant worlds to Dyson spheres that harness stellar energy. Yet engineering design represents a fundamentally different challenge for large language models (LLMs) compared to traditional textbook-style problem solving or factual question answering. Real-world engineering design demands the synthesis of domain knowledge, navigation of complex trade-offs, and management of the tedious processes that consume much of practicing engineers' time. Despite these shared challenges across engineering disciplines, no benchmark currently captures the unique demands of engineering design work. In this work, we introduce ENGDESIGN, an Engineering Design benchmark that evaluates LLMs' abilities to perform practical design tasks across nine engineering domains: Operating System Design, Computer Architecture Design, Control System Design, Mechanical Systems, Structural Design, Digital Hardware Design, Analog Integrated Circuit Design, Robotics, and Signal Processing. Unlike existing benchmarks that focus on factual recall or question answering, ENGDESIGN uniquely emphasizes LLMs' ability to synthesize domain knowledge, reason under constraints, and generate functional, objective-oriented designs. Each task in ENGDESIGN represents a real-world engineering design problem, accompanied by a detailed task description specifying design goals, constraints, and performance requirements. We pioneer a simulation-based evaluation paradigm where LLM-generated designs undergo rigorous testing through executable, domain-specific simulations-from circuit SPICE simulations to structural finite element analysis, from control system validation to robotic motion planning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning in Factor Investment</title>
<link>https://arxiv.org/abs/2509.16206</link>
<guid>https://arxiv.org/abs/2509.16206</guid>
<content:encoded><![CDATA[
<div> Factor Portfolio Construction, Deep Reinforcement Learning, Conditional Auto-encoded Factor-based Portfolio Optimisation, Performance Analysis, U.S. Equity Data

Summary: 
Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO) addresses the challenge of high-dimensional, unbalanced state space in factor portfolio construction by compressing stock-level returns into latent factors. Utilizing deep reinforcement learning (DRL) with PPO and DDPG, CAFPO outperforms traditional methods and other DRL approaches on 20 years of U.S. equity data. It achieves a 24.6% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis highlights the economically intuitive factor attributions. The study demonstrates that factor-aware representation learning enhances DRL for institutional, low-turnover portfolio management. <div>
arXiv:2509.16206v1 Announce Type: new 
Abstract: Deep reinforcement learning has shown promise in trade execution, yet its use in low-frequency factor portfolio construction remains under-explored. A key obstacle is the high-dimensional, unbalanced state space created by stocks that enter and exit the investable universe. We introduce Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO), which compresses stock-level returns into a small set of latent factors conditioned on 94 firm-specific characteristics. The factors feed a DRL agent implemented with both PPO and DDPG to generate continuous long-short weights. On 20 years of U.S. equity data (2000--2020), CAFPO outperforms equal-weight, value-weight, Markowitz, vanilla DRL, and Fama--French-driven DRL, delivering a 24.6\% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis further reveals economically intuitive factor attributions. Our results demonstrate that factor-aware representation learning can make DRL practical for institutional, low-turnover portfolio management.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Machine Learning to Increase the Throughput of Container Yards</title>
<link>https://arxiv.org/abs/2509.16207</link>
<guid>https://arxiv.org/abs/2509.16207</guid>
<content:encoded><![CDATA[
<div> Keywords: shipping container terminals, throughput rates, automation, Intelligent Planning System, Pareto Optimization 

Summary:
The study aims to enhance throughput rates for shipping container terminals, crucial for the U.S. economy. Despite the potential of automation, U.S. ports face limitations due to legal constraints on human labor. To address this, the paper presents an Intelligent Planning System (IPS) using Pareto Optimization and MILP. The IPS improves terminal throughput and processing times, offering recommendations for container positioning and truck pickup appointments. By optimizing container yard layout and flow, the IPS reduces real-time congestion and predicts future congestion. This innovative approach combines efficiency with cooperative agreement terms, providing a solution for U.S. ports seeking automation level efficiencies. The proposed IPS offers a valuable tool for enhancing operational efficiency and ensuring smoother container processing at shipping ports. 

<br /><br />Summary: <div>
arXiv:2509.16207v1 Announce Type: new 
Abstract: This study seeks to improve the throughput rates for shipping container terminals. In the United States, shipping ports link the domestic economy to global markets and are vital to sustain supply chain flow and economic stability. Maritime shipping accounts for nearly half of the U.S.'s annual international trade, two thirds of which are represented by container shipping. Previous studies highlighted the capability of automation in enhancing container processing; however, unlike in European and East Asian ports, full automation is limited in U.S. ports due to legal protections for human labor. Consequently, there is a need for alternative methods that deliver automation level efficiencies while maintaining the terms of cooperative agreements. This paper proposes an Intelligent Planning System (IPS) that applies the concept of Pareto Optimization to container yards through a mixed integer linear programming (MILP) based recursive appointment system. The results show an improvement from baseline for both daily terminal throughput volumes and processing times. The generated IPS can be employed to provide recommendations for container positioning and truck pickup appointments to optimize container yard layout and flow resulting in reduced realtime congestion and predictively mitigated future congestion.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesis of Service Life Prediction for Bridges in Texas</title>
<link>https://arxiv.org/abs/2509.16208</link>
<guid>https://arxiv.org/abs/2509.16208</guid>
<content:encoded><![CDATA[
<div> Bridge, design-build, service life, durability, prediction

Summary:
This research highlights the lack of standardized methods for achieving and verifying long-term service life requirements in design-build bridge contracts. It emphasizes the importance of accurately estimating remaining service life to prioritize repair and rehabilitation needs in aging bridges with limited financial resources. The study reviews current practices and recent advancements in bridge service life prediction, providing practical guidance for evaluating and extending the lifespan of both existing and new structures. By incorporating quantitative validation of durability practices, this research aims to support more efficient use of maintenance funds, enhance understanding of deterioration models and inspection methods, and inform strategies for ensuring long-term structural performance.<br /><br />Summary: <div>
arXiv:2509.16208v1 Announce Type: new 
Abstract: Design-build bridge contracts often include long-term service life requirements, but there are no clear technical guidelines or standardized methods to achieve or verify these goals. While durability practices are commonly applied, they lack quantitative validation. With many aging bridges and limited financial resources, accurately estimating remaining service life is essential for prioritizing repair and rehabilitation needs. This research reviews current practices and recent advancements in bridge service life prediction, providing practical guidance for evaluating and extending the lifespan of both existing and new structures. The findings support more efficient use of maintenance funds, better understanding of deterioration models and inspection methods, and informed strategies to ensure long-term structural performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Digital Twin Models</title>
<link>https://arxiv.org/abs/2509.16209</link>
<guid>https://arxiv.org/abs/2509.16209</guid>
<content:encoded><![CDATA[
<div> scale, digital twin, machine learning, dimensional analysis, calibration

Summary: 
The paper introduces a novel methodology and computational tool for scaling digital twin models using machine learning and dimensional analysis. This framework aims to address the complexity and barriers faced in developing accurate digital twin models for large-scale systems. By applying scaling techniques, the need for repetitive physical calibration of models in industries with varying product sizes can be eliminated. The methodology allows for transferring calibration data from smaller units to larger units within the same product line, without the need for additional data collection or experimentation. The paper also discusses challenges related to dimensional analysis for scaling and presents a framework for proper scaling of digital twin models. The results of the study demonstrate successful scaling between an industrial-size wheel loader vehicle and a miniaturized system in a laboratory setting. <div>
arXiv:2509.16209v1 Announce Type: new 
Abstract: In many industries, the scale and complexity of systems can present significant barriers to the development of accurate digital twin models. This paper introduces a novel methodology and a modular computational tool utilizing machine learning and dimensional analysis to establish a framework for scaling digital twin models. Scaling techniques have not yet been applied to digital twin technology, but they can eliminate the need for repetitive physical calibration of such models in industries where product lines include a variety of sizes of the same or similar products. In many cases, it may be easier or more cost-effective to perform physical calibration of the digital twin model on smaller units of a product line. Scaling techniques can then allow adapting the calibration data from the smaller units to other sizes of the product line without the need for additional data collection and experimentation for calibration. Conventional application of dimensional analysis for scaling in this context introduces several challenges due to distortion of scaling factors. This paper addresses these challenges and introduces a framework for proper scaling of digital twin models. The results are applied to scaling the models between an industrial-size wheel loader vehicle used in construction to a miniaturized system instrumented in a laboratory setting.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strain localization in reduced order asymptotic homogenization</title>
<link>https://arxiv.org/abs/2509.16210</link>
<guid>https://arxiv.org/abs/2509.16210</guid>
<content:encoded><![CDATA[
<div> damage, inelastic effects, multiscale technique, homogenization, composite materials

Summary:
- A new multiscale technique is proposed for capturing damage and inelastic effects in composite materials.
- The technique combines two-scale homogenization with eigen strain representation to model inelastic response. 
- Computational efficiency is improved through reduced order techniques.
- Macroscale stress is determined using influence tensors from representative volume element analysis. 
- Microscale damage is modeled using continuum damage mechanics with a method to prevent strain localization.
- Strategies are implemented to address spurious post-failure artificial stiffness at the macroscale.
- Verification studies show that the formulation accurately predicts macroscale response while capturing damage and inelastic strains. 

<br /><br />Summary: <div>
arXiv:2509.16210v1 Announce Type: new 
Abstract: A reduced order asymptotic homogenization based multiscale technique which can capture damage and inelastic effects in composite materials is proposed. This technique is based on two scale homogenization procedure where eigen strain representation accounts for the inelastic response and the computational efforts are alleviated by reduction of order technique. Macroscale stress is derived by calculating the influence tensors from the analysis of representative volume element (RVE). At microscale, the damage in the material is modeled using continuum damage mechanics (CDM) based framework. To solve the problem of strain localization a method of the alteration of stress-strain relation of micro con- stituents based on the dissipated fracture energy in a crack band is implemented. The issue of spurious post failure artificial stiffness at macroscale is discussed and effect of increasing the order to alleviate this problem is checked. Verification studies demonstrated the proposed formulation predicts the macroscale response and also captures the damage and plasticity induced inelastic strains.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E$^2$-TFA based multiscale analysis of failure in elasto-plastic composites</title>
<link>https://arxiv.org/abs/2509.16211</link>
<guid>https://arxiv.org/abs/2509.16211</guid>
<content:encoded><![CDATA[
<div> homogenization, elastoplastic composite materials, eigenstrain field, transformation field analysis, computational efficiency
Summary: 
The paper presents a novel homogenization methodology, $\mathtt{E}^2$-TFA, for analyzing the failure of elastoplastic composite materials. This technique incorporates the microscopic eigenstrain field to account for intra-phase damage and inelastic strains, overcoming the post-damage stiffness response limitation of traditional TFA-based methods. By utilizing elastic and eigen transformation functions, the model achieves computational efficiency and employs a reduced order modeling approach with a piecewise constant eigenstrain field. The performance of the model is evaluated through simulations on representative volume elements and various composites under complex load histories. The model accurately predicts the nonlinear shear stress-strain response of a glass fiber composite and compared well with experimental data on fracture initiation parameters, failure plane orientation, and strain histories. Overall, $\mathtt{E}^2$-TFA is shown to effectively capture damage and inelastic deformations, providing a more accurate estimation of the mechanical response in composite materials. <div>
arXiv:2509.16211v1 Announce Type: new 
Abstract: This paper describes a novel homogenization methodology for analyzing the failure of elastoplastic composite materials based on elastic and eigen influence tensors-driven transformation field analysis ($\mathtt{E}^2$-TFA). The proposed technique considers the microscopic eigenstrain field accounting for intra-phase damage and inelastic strains. This results in realistic computations by alleviating the post-damage stiffness response, which is a drawback of TFA-based methods. We attain computational efficiency by identifying the preprocessing data solely from the elastic and eigen transformation functions and adopting a reduced order modelling technique with a piecewise constant eigenstrain field throughout the subdomains. The performance of the model is assessed by simulating the response for (a) the representative volume element (RVE) as a homogenized continuum and (b) the various composites under complex load histories with intricate macroscale morphologies. Furthermore, the nonlinear shear stress-strain response of a glass fiber composite is calculated and compared to experimentally measured fracture initiation parameters, failure plane orientation, and strain histories. Finally, we show that $\mathtt{E}^2$-TFA can accurately and efficiently capture damage and inelastic deformations in order to estimate the mechanical response of composite materials in a better way.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An efficient framework for computing sensitivity of modal-related structural dynamic characteristics with multi-parameters</title>
<link>https://arxiv.org/abs/2509.16214</link>
<guid>https://arxiv.org/abs/2509.16214</guid>
<content:encoded><![CDATA[
<div> Keywords: structural dynamics, eigenmode sensitivity, sensitivity analysis, iterative method, computational efficiency 

Summary:
In this paper, a novel strategy is presented for computing the sensitivity of structural dynamic characteristics related to eigenmodes with multiple parameters. The method involves an algebraic approach to simplify the computation of eigenvector sensitivity, followed by the development of a framework for sensitivity analysis. By incorporating a preconditioning iterative method, the computational efficiency is enhanced, reducing the CPU computational time. The framework is user-friendly and minimizes the "Fill-in" operations of sparse matrices. Three numerical examples demonstrate the effectiveness of the algorithm in reducing computational time. This novel strategy provides a valuable tool for engineers in various fields to analyze and optimize structural dynamic characteristics related to eigenmodes efficiently. 

<br /><br />Summary: <div>
arXiv:2509.16214v1 Announce Type: new 
Abstract: The sensitivity of structural dynamic characteristics related to eigenmode (such as modal assurance criteria, modal flexibility, and modal mass etc.) has become a crucial and widely applied tool across various engineering fields. In this paper, a novel strategy is proposed for solving the sensitivity of structural dynamic characteristics related to eigenmode with respect to multiple variables. First, an algebraic method for computing the sensitivity of eigenvectors is developed to simplify the expression for sensitivity calculations. Subsequently, based on this new expression for eigenmode sensitivity, a framework for sensitivity analysis of structural dynamic characteristics related to eigenmodes with multiple parameters is established. With the incorporation of a preconditioning iterative method, the new computational framework effectively enhances the computational efficiency of sensitivity analysis for structural characteristics related to eigenmodes with multiple parameters. This framework is easy to operate and effectively reduces the "Fill-in" operations of sparse matrices. Three numerical examples are given to illustrate the effectiveness of the algorithm. The result shows that the novel strategy can significantly reduce central processing unit (CPU) computational time.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Detection of Internal Defects in Structured Media</title>
<link>https://arxiv.org/abs/2509.16216</link>
<guid>https://arxiv.org/abs/2509.16216</guid>
<content:encoded><![CDATA[
<div> Keywords: structural integrity, defects, imaging strategy, Laplace transform, optimization<br />
Summary:<br />
- Engineers face challenges in detecting internal fractures in structures using traditional methods like visual and audible aids.<br />
- This research proposes a strategy to image defects in structures using minimal, non-invasive measurements based on a one-dimensional wave equation model.<br />
- The study focuses on a homogeneous bar with a defect in Young's modulus, treating the problem as a spring-mass vibrational system.<br />
- The proposed imaging strategy utilizes MATLAB to collect synthetic data for various scenarios with defects, optimizing a residual function to identify defect location and stiffness.<br />
- By establishing an analytic map between defect characteristics and measurement data, engineers can effectively detect and assess the severity of fractures in structures. <br /> <div>
arXiv:2509.16216v1 Announce Type: new 
Abstract: A critical issue that affects engineers trying to assess the structural integrity of various infrastructures, such as metal rods or acoustic ducts, is the challenge of detecting internal fractures (defects). Traditionally, engineers depend on audible and visual aids to identify these fractures, as they do not physically dissect the object in question into multiple pieces to check for inconsistencies. This research introduces ideas towards the development of a robust strategy to image such defects using only a small set of minimal, non-invasive measurements.
  Assuming a one dimensional model (e.g. longitudinal waves in long and thin rods/acoustic ducts or transverse vibrations of strings), we make use of the continuous one-dimensional wave equation to model these physical phenomena and then employ specialized mathematical analysis tools (the Laplace transform and optimization) to introduce our defect imaging ideas. In particular, we will focus on the case of a long bar which is homogeneous throughout except in a small area where a defect in its Young's modulus is present. We will first demonstrate how the problem is equivalent to a spring-mass vibrational system, and then show how our imaging strategy makes use of the Laplace domain analytic map between the characteristics of the respective defect and the measurement data.
  More explicitly, we will utilize MATLAB (a platform for numerical computations) to collect synthetic data (computational alternative to real world measurements) for several scenarios with one defect of arbitrary location and stiffness. Subsequently, we will use this data along with our analytically developed map (between defect characteristics and measurements) to construct a residual function which, once optimized, will reveal the location and magnitude of the stiffness defect.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Transient Nonlinear Circuit Simulator Using Exponential Integration and Block-Jacobi Precondition</title>
<link>https://arxiv.org/abs/2509.16219</link>
<guid>https://arxiv.org/abs/2509.16219</guid>
<content:encoded><![CDATA[
<div> regularization, exponential integrators, matrix exponential vector products, structured block-Jacobi preconditioner, Additive Schwarz strategy <br />
Summary:
The article introduces a novel method for transient simulation of linear and nonlinear circuits in EDA tools. By applying a generalized row-echelon regularization approach, the method extends the use of exponential integrators to a wider range of differential algebraic equations, enabling larger time step sizes while maintaining accuracy and stability. To improve efficiency, a structured block-Jacobi preconditioner is designed for linear systems, and an Additive Schwarz strategy is employed for locally coupled circuits. Numerical experiments demonstrate significant speedup in computation time and reduced time steps compared to traditional methods, showcasing the potential for scalable and efficient nonlinear circuit simulation. <br /><br /> <div>
arXiv:2509.16219v1 Announce Type: new 
Abstract: Transient simulation of linear and nonlinear circuits remains an important task in modern EDA tools. At present, SPICE-like simulators face challenges in parallelization, nonlinear convergence and linear efficiency, especially when applied to large-scale circuits. To address the limitations of simulators in handling various nonlinear circuits, we adopt a generalized row-echelon regularization approach, which extends the applicability of exponential integrators to a broader class of differential algebraic equations. The proposed method employs matrix exponential vector products to integrate the regularized system, allowing for a larger time step size while preserving accuracy and stability. Furthermore, in order to accelerate GMRES-based solvers within Newton-Raphson iterations, a structured block-Jacobi preconditioner is designed for linear systems. For locally coupled circuits, Additive Schwarz overlapping strategy is adopted to enhance the solution performance. Numerical experiments of various nonlinear circuit models show that under same hardware environment, our method achieves a speedup of 1.95$\times$-- 3.27$\times$ in total computation time compared to Backward Euler with Inexact Newton iterations, and time steps have decreased by an average of 60.70\% (up to 74.59\%). Compared with EI-NK method, total computing time of our method has a speedup of 1.08$\times$-- 1.79$\times$. These results highlight the potential of proposed method for scalable and nonlinear circuit simulation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Open Dataset for Temperature Modelling in Machine Tools</title>
<link>https://arxiv.org/abs/2509.16222</link>
<guid>https://arxiv.org/abs/2509.16222</guid>
<content:encoded><![CDATA[
<div> dataset, transient thermal simulations, machine learning, deep learning, mechanical systems

Summary:
This article introduces a structured dataset of transient thermal simulations for a vertical axis of a machine tool test rig. The dataset includes temperature and heat flux values recorded at 29 probe locations over 1800 time steps. The simulations were derived from a fractional factorial design and aim to support machine learning and deep learning applications in thermal modelling. The dataset provides detailed information on material, mesh, and boundary conditions, as well as summary statistics, thermal evolution plots, and correlation matrix analyses. A reproducible Jupyter notebook is also included to support research and model development. The data is designed to help predict, correct, and compensate for thermally induced deviations in mechanical systems, making it valuable for researchers without finite element expertise. <div>
arXiv:2509.16222v1 Announce Type: new 
Abstract: This data set descriptor introduces a structured, high-resolution dataset of transient thermal simulations for a vertical axis of a machine tool test rig. The data set includes temperature and heat flux values recorded at 29 probe locations at 1800 time steps, sampled every second over a 30-minute range, across 17 simulation runs derived from a fractional factorial design. First, a computer-aided design model was de-featured, segmented, and optimized, followed by finite element (FE) modelling. Detailed information on material, mesh, and boundary conditions is included. To support research and model development, the dataset provides summary statistics, thermal evolution plots, correlation matrix analyses, and a reproducible Jupyter notebook. The data set is designed to support machine learning and deep learning applications in thermal modelling for prediction, correction, and compensation of thermally induced deviations in mechanical systems, and aims to support researchers without FE expertise by providing ready-to-use simulation data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Increasing Inter-Fiber Contact in the Altendorf-Jeulin Model</title>
<link>https://arxiv.org/abs/2509.16225</link>
<guid>https://arxiv.org/abs/2509.16225</guid>
<content:encoded><![CDATA[
<div> fiber materials, simulations, digital twins, inter-fiber contacts, parametric models

Summary:
- Fiber materials are crucial in fields like material design and biomedicine.
- Fiber simulations, known as digital twins, help in testing material behavior digitally.
- Inter-fiber contacts can impact the thermal and mechanical behavior of fiber systems.
- Existing parametric fiber models do not allow explicit modeling of the number of inter-fiber contacts.
- The proposed extension of the force-biased fiber packing model by Altendorf & Jeulin includes explicit modeling of inter-fiber contacts and an additional force to increase contacts.
- The packing is validated for parameter accuracy and shown to increase the number of contacts, potentially enhancing the accuracy of physical simulations. 

<br /><br />Summary: <div>
arXiv:2509.16225v1 Announce Type: new 
Abstract: In fields such as material design or biomedicine, fiber materials play an important role. Fiber simulations, also called digital twins, provide a basis for testing and optimizing the material's physical behavior digitally. Inter-fiber contacts can influence the thermal and mechanical behavior of a fiber system; to our knowledge, however, there exist no parametric fiber models allowing for explicit modeling of the number of inter-fiber contacts. Therefore, this paper proposes an extension of the iterative force-biased fiber packing by Altendorf \& Jeulin. In this extension, we model the inter-fiber contacts explicitly and add another force to the force-biased packing to increase the number of contacts. We successfully validate the packing with respect to its parameter accuracy. Moreover, we show that the extension indeed increases the number of contacts, even exceeding theoretical values. Hence, this packing scheme has the potential to achieve higher accuracy in physical simulations.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Rank Risky Investors: A Case Study of Predicting Retail Traders' Behaviour and Profitability</title>
<link>https://arxiv.org/abs/2509.16616</link>
<guid>https://arxiv.org/abs/2509.16616</guid>
<content:encoded><![CDATA[
<div> algorithm, financial markets, risk management, trader identification, profit-aware

Summary:
The article introduces a profit-aware risk ranker (PA-RiskRanker) for identifying risky traders in financial markets. Traditional methods often struggle to capture the complexity and dynamism of individual trader behaviors. PA-RiskRanker reframes the problem as a ranking task using Learning-to-Rank algorithms, incorporating profit and loss considerations through a Profit-Aware binary cross entropy loss function. It utilizes a transformer-based ranker with self-cross-trader attention to capture intra- and inter-trader relationships. The research highlights the limitations of existing deep learning-based algorithms in trading risk management and emphasizes the importance of profit and loss in financial scenarios. PA-RiskRanker outperforms state-of-the-art ranking models like Rankformer, showing an 8.4% increase in F1 score and a 10%-17% increase in average profit compared to benchmark models. <div>
arXiv:2509.16616v1 Announce Type: new 
Abstract: Identifying risky traders with high profits in financial markets is crucial for market makers, such as trading exchanges, to ensure effective risk management through real-time decisions on regulation compliance and hedging. However, capturing the complex and dynamic behaviours of individual traders poses significant challenges. Traditional classification and anomaly detection methods often establish a fixed risk boundary, failing to account for this complexity and dynamism. To tackle this issue, we propose a profit-aware risk ranker (PA-RiskRanker) that reframes the problem of identifying risky traders as a ranking task using Learning-to-Rank (LETOR) algorithms. Our approach features a Profit-Aware binary cross entropy (PA-BCE) loss function and a transformer-based ranker enhanced with a self-cross-trader attention pipeline. These components effectively integrate profit and loss (P&amp;L) considerations into the training process while capturing intra- and inter-trader relationships. Our research critically examines the limitations of existing deep learning-based LETOR algorithms in trading risk management, which often overlook the importance of P&amp;L in financial scenarios. By prioritising P&amp;L, our method improves risky trader identification, achieving an 8.4% increase in F1 score compared to state-of-the-art (SOTA) ranking models like Rankformer. Additionally, it demonstrates a 10%-17% increase in average profit compared to all benchmark models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rational Multi-Modal Transformers for TCR-pMHC Prediction</title>
<link>https://arxiv.org/abs/2509.17305</link>
<guid>https://arxiv.org/abs/2509.17305</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, TCR-pMHC interactions, explainability method, encoder-decoder, deep learning <br />
<br />Summary: 
This article introduces a novel approach using a transformer-based model to predict T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes. By employing a new explainability method, the model optimizes cross-attention strategies, incorporates additional training objectives, and introduces an early-stopping criterion based on explanation quality. This framework achieves improved predictive performance while enhancing explainability, robustness, and generalization. The approach establishes a principled, explanation-driven strategy for modeling TCR-pMHC binding and offers insights into sequence-level binding behavior through deep learning techniques. <div>
arXiv:2509.17305v1 Announce Type: new 
Abstract: T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is fundamental to adaptive immunity and central to the development of T cell-based immunotherapies. While transformer-based models have shown promise in predicting TCR-pMHC interactions, most lack a systematic and explainable approach to architecture design. We present an approach that uses a new post-hoc explainability method to inform the construction of a novel encoder-decoder transformer model. By identifying the most informative combinations of TCR and epitope sequence inputs, we optimize cross-attention strategies, incorporate auxiliary training objectives, and introduce a novel early-stopping criterion based on explanation quality. Our framework achieves state-of-the-art predictive performance while simultaneously improving explainability, robustness, and generalization. This work establishes a principled, explanation-driven strategy for modeling TCR-pMHC binding and offers mechanistic insights into sequence-level binding behavior through the lens of deep learning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$i$MIND: Insightful Multi-subject Invariant Neural Decoding</title>
<link>https://arxiv.org/abs/2509.17313</link>
<guid>https://arxiv.org/abs/2509.17313</guid>
<content:encoded><![CDATA[
<div> Keywords: visual signals, neural decoding, brain activity, semantic decoding, neural interpretation

Summary: 
The study introduces the iMIND model, focusing on decoding visual signals to understand brain cognition and perception. It utilizes a dual-decoding framework that combines biometric and semantic decoding to offer neural interpretability and deepen insights into visual processing mechanisms. The iMIND model consists of three key steps: creating a shared neural representation space across subjects, disentangling neural features into subject-specific and object-specific components, and performing dual decoding for biometric and semantic tasks. Experimental results show that iMIND achieves top decoding performance with minimal scalability constraints. It generates voxel-object activation fingerprints that uncover object-specific neural patterns and allow for the exploration of subject-specific attention variations to the same stimuli. This work lays the groundwork for more interpretable and generalizable subject-invariant neural decoding, enhancing knowledge of voxel semantic selectivity and neural vision processing dynamics. 

<br /><br />Summary: <div>
arXiv:2509.17313v1 Announce Type: new 
Abstract: Decoding visual signals holds the tantalizing potential to unravel the complexities of cognition and perception. While recent studies have focused on reconstructing visual stimuli from neural recordings to bridge brain activity with visual imagery, existing methods offer limited insights into the underlying mechanisms of visual processing in the brain. To mitigate this gap, we present an \textit{i}nsightful \textbf{M}ulti-subject \textbf{I}nvariant \textbf{N}eural \textbf{D}ecoding ($i$MIND) model, which employs a novel dual-decoding framework--both biometric and semantic decoding--to offer neural interpretability in a data-driven manner and deepen our understanding of brain-based visual functionalities. Our $i$MIND model operates through three core steps: establishing a shared neural representation space across subjects using a ViT-based masked autoencoder, disentangling neural features into complementary subject-specific and object-specific components, and performing dual decoding to support both biometric and semantic classification tasks. Experimental results demonstrate that $i$MIND achieves state-of-the-art decoding performance with minimal scalability limitations. Furthermore, $i$MIND empirically generates voxel-object activation fingerprints that reveal object-specific neural patterns and enable investigation of subject-specific variations in attention to identical stimuli. These findings provide a foundation for more interpretable and generalizable subject-invariant neural decoding, advancing our understanding of the voxel semantic selectivity as well as the neural vision processing dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction</title>
<link>https://arxiv.org/abs/2509.16369</link>
<guid>https://arxiv.org/abs/2509.16369</guid>
<content:encoded><![CDATA[
<div> Keywords: financial, retrieval, AI, Multi-HyDE, accuracy

Summary:
The article introduces a framework for financial Retrieval Augmented Generation (RAG) that utilizes agentic AI and the Multi-HyDE system to enhance knowledge retrieval in financial question-answering. Traditional systems are insufficient for handling the complexity of financial data sources, which require sophisticated approaches. The RAG framework focuses on token efficiency and multi-step financial reasoning to improve accuracy by 11.2% and reduce hallucinations by 15%. By integrating domain-specific retrieval mechanisms like Multi-HyDE and robust toolsets, including keyword and table-based retrieval, the framework significantly enhances the accuracy and reliability of answers in financial QA benchmarks. The research emphasizes the importance of structured agent workflows and multi-perspective retrieval for the trustworthy deployment of AI in high-stakes financial applications.

<br /><br />Summary: <div>
arXiv:2509.16369v1 Announce Type: cross 
Abstract: Accurate and reliable knowledge retrieval is vital for financial question-answering, where continually updated data sources and complex, high-stakes contexts demand precision. Traditional retrieval systems rely on a single database and retriever, but financial applications require more sophisticated approaches to handle intricate regulatory filings, market analyses, and extensive multi-year reports. We introduce a framework for financial Retrieval Augmented Generation (RAG) that leverages agentic AI and the Multi-HyDE system, an approach that generates multiple, nonequivalent queries to boost the effectiveness and coverage of retrieval from large, structured financial corpora. Our pipeline is optimized for token efficiency and multi-step financial reasoning, and we demonstrate that their combination improves accuracy by 11.2% and reduces hallucinations by 15%. Our method is evaluated on standard financial QA benchmarks, showing that integrating domain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets, including keyword and table-based retrieval, significantly enhances both the accuracy and reliability of answers. This research not only delivers a modular, adaptable retrieval framework for finance but also highlights the importance of structured agent workflows and multi-perspective retrieval for trustworthy deployment of AI in high-stakes financial applications.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction from PPG</title>
<link>https://arxiv.org/abs/2509.16491</link>
<guid>https://arxiv.org/abs/2509.16491</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, physiological data, fine-tuning, demographic fairness, heart rate prediction

Summary:
Fine-tuning foundation models pretrained on physiological data, such as photoplethysmography signals, can significantly improve heart rate prediction accuracy but may widen demographic fairness gaps, especially in different deployment domains. The study introduces FairTune, a bias-aware fine-tuning framework that includes mitigation strategies like class weighting, Group Distributionally Robust Optimization, and adversarial debiasing. These strategies effectively reduce fairness gaps without compromising prediction accuracy, with results varying based on the deployment domain. Representation analyses show that mitigation techniques reshape internal embeddings to reduce demographic clustering. The findings emphasize the necessity of explicit mitigation for equitable deployment of physiological foundation models.<br /><br />Summary: Fine-tuning can enhance HR prediction accuracy but may widen demographic fairness gaps, FairTune framework includes effective mitigation strategies, such as class weighting and GroupDRO, which reshape internal embeddings to reduce demographic clustering. <div>
arXiv:2509.16491v1 Announce Type: cross 
Abstract: Foundation models pretrained on physiological data such as photoplethysmography (PPG) signals are increasingly used to improve heart rate (HR) prediction across diverse settings. Fine-tuning these models for local deployment is often seen as a practical and scalable strategy. However, its impact on demographic fairness particularly under domain shifts remains underexplored. We fine-tune PPG-GPT a transformer-based foundation model pretrained on intensive care unit (ICU) data across three heterogeneous datasets (ICU, wearable, smartphone) and systematically evaluate the effects on HR prediction accuracy and gender fairness. While fine-tuning substantially reduces mean absolute error (up to 80%), it can simultaneously widen fairness gaps, especially in larger models and under significant distributional characteristics shifts. To address this, we introduce FairTune, a bias-aware fine-tuning framework in which we benchmark three mitigation strategies: class weighting based on inverse group frequency (IF), Group Distributionally Robust Optimization (GroupDRO), and adversarial debiasing (ADV). We find that IF and GroupDRO significantly reduce fairness gaps without compromising accuracy, with effectiveness varying by deployment domain. Representation analyses further reveal that mitigation techniques reshape internal embeddings to reduce demographic clustering. Our findings highlight that fairness does not emerge as a natural byproduct of fine-tuning and that explicit mitigation is essential for equitable deployment of physiological foundation models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cost-Effective ZK-Rollups: Modeling and Optimization of Proving Infrastructure</title>
<link>https://arxiv.org/abs/2509.16581</link>
<guid>https://arxiv.org/abs/2509.16581</guid>
<content:encoded><![CDATA[
<div> Zero-knowledge rollups, Halo2-based proving systems, cost model, Z3 SMT solver, simulator <br />
Summary:
Zero-knowledge rollups face challenges with costly hardware requirements, finality constraints, and rising transaction throughput. This study focuses on cost drivers such as transactions per second, gas usage, and finality time. A parametric cost model is proposed to optimize configurations and ensure provers can handle transaction loads efficiently. The model is formulated as a constraint system and solved using the Z3 SMT solver. A simulator is implemented to detect delays and estimate operational costs, showing a potential cost reduction of up to 70%. <div>
arXiv:2509.16581v1 Announce Type: cross 
Abstract: Zero-knowledge rollups rely on provers to generate multi-step state transition proofs under strict finality and availability constraints. These steps require expensive hardware (e.g., GPUs), and finality is reached only once all stages complete and results are posted on-chain. As rollups scale, staying economically viable becomes increasingly difficult due to rising throughput, fast finality demands, volatile gas prices, and dynamic resource needs. We base our study on Halo2-based proving systems and identify transactions per second (TPS), average gas usage, and finality time as key cost drivers. To address this, we propose a parametric cost model that captures rollup-specific constraints and ensures provers can keep up with incoming transaction load. We formulate this model as a constraint system and solve it using the Z3 SMT solver to find cost-optimal configurations. To validate our approach, we implement a simulator that detects lag and estimates operational costs. Our method shows a potential cost reduction of up to 70\%.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KANO: Kolmogorov-Arnold Neural Operator</title>
<link>https://arxiv.org/abs/2509.16825</link>
<guid>https://arxiv.org/abs/2509.16825</guid>
<content:encoded><![CDATA[
<div> Kolmogorov--Arnold Neural Operator, dual-domain neural operator, spectral bases, spatial bases, symbolic interpretability <br />
Summary:<br />
The Kolmogorov--Arnold Neural Operator (KANO) is introduced as a neural operator that combines spectral and spatial bases, providing intrinsic symbolic interpretability. KANO addresses the limitations of the Fourier Neural Operator (FNO) by remaining expressive over position-dependent dynamics, unlike FNO, which is only practical for spectrally sparse operators. Empirical verification shows that KANO generalizes robustly in position-dependent differential operators, while FNO does not. In a quantum Hamiltonian learning benchmark, KANO accurately reconstructs ground-truth Hamiltonians with closed-form symbolic representations and low state infidelity from projective measurement data. KANO significantly outperforms FNO in terms of state infidelity, highlighting its superior performance in capturing dynamics accurately. <br /> <div>
arXiv:2509.16825v1 Announce Type: cross 
Abstract: We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over generic position-dependent dynamics for any physical input, whereas FNO stays practical only for spectrally sparse operators and strictly imposes a fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\approx 6\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\approx 1.5\times10^{-2}$, by orders of magnitude.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability matters: The effect of liability rules on the healthcare sector</title>
<link>https://arxiv.org/abs/2509.17334</link>
<guid>https://arxiv.org/abs/2509.17334</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainability, Artificial Intelligence, Healthcare, Liability, Legal Framework

Summary: 
Explainability of artificial intelligence systems (AIS) is crucial in critical sectors like healthcare. This perspective analyzes the impact of explainability on liability determinations in healthcare settings. Contrasting the extreme cases of an "Oracle" AIS without explainability and an "AI Colleague" AIS with explainability, the discussion explores how automation and explainability affect liability assignment between medical practitioners, healthcare facilities, and AIS manufacturers. The article argues that explainability is essential in establishing a responsibility framework in healthcare, shaping behavior, and reducing the risk of defensive medicine practices. Ultimately, the level of explainability in AIS is vital for legal considerations and improving overall outcomes in healthcare. 

<br /><br />Summary: <div>
arXiv:2509.17334v1 Announce Type: cross 
Abstract: Explainability, the capability of an artificial intelligence system (AIS) to explain its outcomes in a manner that is comprehensible to human beings at an acceptable level, has been deemed essential for critical sectors, such as healthcare. Is it really the case? In this perspective, we consider two extreme cases, ``Oracle'' (without explainability) versus ``AI Colleague'' (with explainability) for a thorough analysis. We discuss how the level of automation and explainability of AIS can affect the determination of liability among the medical practitioner/facility and manufacturer of AIS. We argue that explainability plays a crucial role in setting a responsibility framework in healthcare, from a legal standpoint, to shape the behavior of all involved parties and mitigate the risk of potential defensive medicine practices.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures</title>
<link>https://arxiv.org/abs/2509.17734</link>
<guid>https://arxiv.org/abs/2509.17734</guid>
<content:encoded><![CDATA[
<div> Keywords: forecasting, deep learning, maximum daily temperature, climatology, AutoGluonTS <br />
Summary: <br />
In the field of meteorological forecasting, deep learning has shown significant advancements in predicting the daily average temperature over a ten-day horizon. However, accurately forecasting maximum daily temperatures over short horizons remains a challenge. This study focuses on predicting maximum daily temperatures over medium-term periods of 90 days, approaching the issue from a climatological perspective. Utilizing a large historical dataset from South America and incorporating information from various ocean basins, the AutoGluonTS platform was employed to address the forecasting problem. By framing the problem as a temporal classification task with classes of "above normal," "normal," or "below normal" temperatures, competitive forecasting performance was achieved. AutoGluonTS offers efficient forecasting capabilities with relatively low computational costs compared to other operational platforms, making it a promising tool for addressing complex climatological forecasting challenges. <br /> <div>
arXiv:2509.17734v1 Announce Type: cross 
Abstract: In recent years, great progress has been made in the field of forecasting meteorological variables. Recently, deep learning architectures have made a major breakthrough in forecasting the daily average temperature over a ten-day horizon. However, advances in forecasting events related to the maximum temperature over short horizons remain a challenge for the community. A problem that is even more complex consists in making predictions of the maximum daily temperatures in the short, medium, and long term. In this work, we focus on forecasting events related to the maximum daily temperature over medium-term periods (90 days). Therefore, instead of addressing the problem from a meteorological point of view, this article tackles it from a climatological point of view. Due to the complexity of this problem, a common approach is to frame the study as a temporal classification problem with the classes: maximum temperature "above normal", "normal" or "below normal". From a practical point of view, we created a large historical dataset (from 1981 to 2018) collecting information from weather stations located in South America. In addition, we also integrated exogenous information from the Pacific, Atlantic, and Indian Ocean basins. We applied the AutoGluonTS platform to solve the above-mentioned problem. This AutoML tool shows competitive forecasting performance with respect to large operational platforms dedicated to tackling this climatological problem; but with a "relatively" low computational cost in terms of time and resources.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRFT: Mining High-Frequency Risk Factor Collections End-to-End via Transformer</title>
<link>https://arxiv.org/abs/2408.01271</link>
<guid>https://arxiv.org/abs/2408.01271</guid>
<content:encoded><![CDATA[
<div> Keywords: quantitative trading, neural networks, risk factors, symbolic mathematics, transformer model 

Summary: 
Quantitative trading relies on transforming historical stock data into interpretable risk factors to identify market volatility and risk effectively. While neural networks have advanced in extracting latent risk factors, they lack explicit, formulaic designs. This paper introduces the Intraday Risk Factor Transformer (IRFT) methodology, which treats the mining of risk factors as a language modeling problem using symbolic mathematics. The IRFT model generates complete formulaic risk factors, including constants, without a predefined skeleton of operators. By training on high frequency trading datasets, IRFT determines the general form of stock volatility laws, refining predicted constants using the Broyden Fletcher Goldfarb Shanno (BFGS) algorithm. In comparative analysis, IRFT outperforms existing methods in HF risk factor mining tasks, achieving a 30% higher investment return on datasets like HS300 and SP500 while significantly reducing inference times. <br /><br />Summary: <div>
arXiv:2408.01271v5 Announce Type: replace 
Abstract: In quantitative trading, transforming historical stock data into interpretable, formulaic risk factors enhances the identification of market volatility and risk. Despite recent advancements in neural networks for extracting latent risk factors, these models remain limited to feature extraction and lack explicit, formulaic risk factor designs. By viewing symbolic mathematics as a language where valid mathematical expressions serve as meaningful "sentences" we propose framing the task of mining formulaic risk factors as a language modeling problem. In this paper, we introduce an end to end methodology, Intraday Risk Factor Transformer (IRFT), to directly generate complete formulaic risk factors, including constants. We use a hybrid symbolic numeric vocabulary where symbolic tokens represent operators and stock features, and numeric tokens represent constants. We train a Transformer model on high frequency trading (HFT) datasets to generate risk factors without relying on a predefined skeleton of operators. It determines the general form of the stock volatility law, including constants. We refine the predicted constants using the Broyden Fletcher Goldfarb Shanno (BFGS) algorithm to mitigate non linear issues. Compared to the ten approaches in SRBench, an active benchmark for symbolic regression (SR), IRFT achieves a 30% higher investment return on the HS300 and SP500 datasets, while achieving inference times that are orders of magnitude faster than existing methods in HF risk factor mining tasks.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StockGenChaR: A Study on the Evaluation of Large Vision-Language Models on Stock Chart Captioning</title>
<link>https://arxiv.org/abs/2412.04041</link>
<guid>https://arxiv.org/abs/2412.04041</guid>
<content:encoded><![CDATA[
<div> Keywords: Technical analysis, finance, AI tools, image captioning, stock charts

Summary:<br /><br />
The article focuses on using AI tools to assist non-expert investors in analyzing stock charts for better decision-making. It introduces a new dataset, StockGenChaR, to evaluate large vision-language models in image captioning with stock charts. The main goal is to generate informative descriptions of stock charts that can help investors in understanding market sentiment towards specific stocks. By utilizing AI technology, investors can gain insights into past market data and forecast potential price movements in the future. This can be particularly useful for those who may not have expertise in technical analysis in finance. Overall, the proposed task aims to provide valuable information through image captioning that can assist investors in making informed decisions in the stock market. <div>
arXiv:2412.04041v2 Announce Type: replace 
Abstract: Technical analysis in finance, which aims at forecasting price movements in the future by analyzing past market data, relies on the in- sights that can be gained from the interpretation of stock charts; therefore, non-expert investors could greatly benefit from AI tools that can assist with the captioning of such charts. In our work, we introduce a new dataset StockGenChaR to evaluate large vision-language models in image captioning with stock charts. The purpose of the proposed task is to generate informative descriptions of the depicted charts and help to read the sentiment of the market regarding specific stocks, thus providing useful information for investors
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPH-Net: A Co-Attention Hybrid Model for Accurate Stock Price Prediction</title>
<link>https://arxiv.org/abs/2509.15414</link>
<guid>https://arxiv.org/abs/2509.15414</guid>
<content:encoded><![CDATA[
<div> SPH-Net, Stock Price Prediction, Hybrid Neural Network, Deep Learning, Time Series Forecasting
<br />
Summary: 
SPH-Net is a new deep learning framework designed for stock price prediction that incorporates a co-attention mechanism. It utilizes a Vision Transformer for processing temporal patterns and an attention mechanism for feature extraction, capturing global and local dependencies in market data. The model is evaluated on eight stock datasets using six fundamental market indicators. Results show that SPH-Net outperforms existing models in stock prediction accuracy. Its success lies in effectively capturing complex temporal patterns and maintaining robustness against market noise. This improved accuracy can provide valuable decision-support for investors and financial analysts, improving investment strategies and risk assessment in volatile market conditions. 
<br /> <div>
arXiv:2509.15414v1 Announce Type: new 
Abstract: Prediction of stock price movements presents a formidable challenge in financial analytics due to the inherent volatility, non-stationarity, and nonlinear characteristics of market data. This paper introduces SPH-Net (Stock Price Prediction Hybrid Neural Network), an innovative deep learning framework designed to enhance the accuracy of time series forecasting in financial markets. The proposed architecture employs a novel co-attention mechanism that initially processes temporal patterns through a Vision Transformer, followed by refined feature extraction via an attention mechanism, thereby capturing both global and local dependencies in market data. To rigorously evaluate the model's performance, we conduct comprehensive experiments on eight diverse stock datasets: AMD, Ebay, Facebook, FirstService Corp, Tesla, Google, Mondi ADR, and Matador Resources. Each dataset is standardized using six fundamental market indicators: Open, High, Low, Close, Adjusted Close, and Volume, representing a complete set of features for comprehensive market analysis. Experimental results demonstrate that SPH-Net consistently outperforms existing stock prediction models across all evaluation metrics. The model's superior performance stems from its ability to effectively capture complex temporal patterns while maintaining robustness against market noise. By significantly improving prediction accuracy in financial time series analysis, SPH-Net provides valuable decision-support capabilities for investors and financial analysts, potentially enabling more informed investment strategies and risk assessment in volatile market conditions.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Memory Efficient Adjoint Method to Enable Billion Parameter Optimization on a Single GPU in Dynamic Problems</title>
<link>https://arxiv.org/abs/2509.15744</link>
<guid>https://arxiv.org/abs/2509.15744</guid>
<content:encoded><![CDATA[
<div> sensitivity computations, dynamic optimization, adjoint method, superposition principle, CUDA implementation <br />
Summary: <br />
Dynamic optimization faces limitations due to memory requirements for sensitivity computations relying on full forward and adjoint wave fields. A new approach based on the adjoint method and superposition principle is introduced to approximate sensitivity computations for self-adjoint problems, enabling iterative computation and reducing memory burden to the number of degrees of freedom. This allows for sensitivity computations on GPUs with limited memory capacity, such as the A100 from NVIDIA. The approach is demonstrated on full waveform inversion and transient acoustic topology optimization, utilizing a highly efficient finite difference forward solver implemented in CUDA. However, the technique is limited to self-adjoint problems and does not account for phenomena like damping. <br /> <div>
arXiv:2509.15744v1 Announce Type: new 
Abstract: Dynamic optimization is currently limited by sensitivity computations that require information from full forward and adjoint wave fields. Since the forward and adjoint solutions are computed in opposing time directions, the forward solution must be stored. This requires a substantial amount of memory for large-scale problems even when using check pointing or data compression techniques. As a result, the problem size is memory bound rather than bound by wall clock time, when working with modern GPU-based implementations that have limited memory capacity. To overcome this limitation, we introduce a new approach for approximate sensitivity computation based on the adjoint method (for self-adjoint problems) that relies on the principle of superposition. The approximation allows an iterative computation of the sensitivity, reducing the memory burden to that of the solution at a small number of time steps, i.e., to the number of degrees of freedom. This enables sensitivity computations for problems with billions of degrees of freedom on current GPUs, such as the A100 from NVIDIA (from 2020). We demonstrate the approach on full waveform inversion and transient acoustic topology optimization problems, relying on a highly efficient finite difference forward solver implemented in CUDA. Phenomena such as damping cannot be considered, as the approximation technique is limited to self-adjoint problems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A CARLA-based Simulation of Electrically Driven Forklifts</title>
<link>https://arxiv.org/abs/2509.15909</link>
<guid>https://arxiv.org/abs/2509.15909</guid>
<content:encoded><![CDATA[
<div> Keywords: electric forklift fleet, simulation, intralogistics, CARLA, traffic detection

Summary: 
This paper discusses the simulation of an electric forklift fleet within an intralogistics scenario using the open-source simulation tool CARLA. The study involves generating and visualizing a 3D outdoor warehouse scenario with moving forklifts, simulating intralogistics transport tasks, and playing back localization data from a real forklift fleet. The simulation also includes modeling the energy consumption of forklift trucks using a physical battery model. Two use cases are explored: detecting regions with high traffic densities and optimizing the placement of charging stations for the forklift trucks. Both scenarios are analyzed in an exemplary warehouse model to demonstrate the versatility of the CARLA simulation platform. <div>
arXiv:2509.15909v1 Announce Type: new 
Abstract: This paper presents the simulation of the operation of an electric forklift fleet within an intralogistics scenario. For this purpose, the open source simulation tool CARLA is used; according to our knowledge this is a novel approach in the context of logistics simulation. First, CARLA is used to generate and visualize a realistic 3D outdoor warehouse scenario, incorporating a number of randomly moving forklifts. In a next step, intralogistics transport tasks, such as pick-and-place, are simulated for the forklift fleet, including shortest-path finding. Furthermore, the capability to play back localization data, previously recorded from a ''real'' forklift fleet, is demonstrated.This play back is done in the original recreated environment, thereby enabling the visualization of the forklifts movements. Finally, the energy consumption of the forklift trucks is simulated by integrating a physical battery model that generates the state of charge (SOC) of each truck as a function of load and activity. To demonstrate the wide range of possible applications for the CARLA simulation platform, we describe two use cases. The first deals with the problem of detecting regions with critically high traffic densities, the second with optimal placement of charging stations for the forklift trucks. Both use cases are calculated for an exemplary warehouse model.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Experimental Design of a Moving Sensor for Linear Bayesian Inverse Problems</title>
<link>https://arxiv.org/abs/2509.15961</link>
<guid>https://arxiv.org/abs/2509.15961</guid>
<content:encoded><![CDATA[
<div> optimize, mobile sensor, Bayesian inverse problem, partial differential equation, uncertainty

Summary:
The article focuses on optimizing the path of a mobile sensor to reduce posterior uncertainty in a Bayesian inverse problem. Measurements are taken continuously along the path to estimate the state, modeled as a solution of a partial differential equation with uncertain parameters. The posterior covariance matrix of the model parameters is derived in closed-form for linear PDEs, enabling the formulation of optimal experimental design to minimize uncertainty. A discretization approach is used to maintain cost function consistency under temporal refinement, while constraints ensure obstacle avoidance and path interpretability. The constrained optimization problem is solved using an interior-point method. Computational results for a convection-diffusion equation with unknown initial conditions are presented. <div>
arXiv:2509.15961v1 Announce Type: new 
Abstract: We optimize the path of a mobile sensor to minimize the posterior uncertainty of a Bayesian inverse problem. Along its path, the sensor continuously takes measurements of the state, which is a physical quantity modeled as the solution of a partial differential equation (PDE) with uncertain parameters. Considering linear PDEs specifically, we derive the closed-form expression of the posterior covariance matrix of the model parameters as a function of the path, and formulate the optimal experimental design problem for minimizing the posterior's uncertainty. We discretize the problem such that the cost function remains consistent under temporal refinement. Additional constraints ensure that the path avoids obstacles and remains physically interpretable through a control parameterization. The constrained optimization problem is solved using an interior-point method. We present computational results for a convection-diffusion equation with unknown initial condition.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria</title>
<link>https://arxiv.org/abs/2509.16040</link>
<guid>https://arxiv.org/abs/2509.16040</guid>
<content:encoded><![CDATA[
<div> Sparse regression, constitutive model discovery, automated framework, hyperelasticity, model selection criteria  
Summary:  
- An automated framework for constitutive model discovery is presented, utilizing three sparse regression algorithms and three model selection criteria.  
- The framework pairs algorithms like LASSO, LARS, and OMP with selection criteria including $K$-fold cross-validation, AIC, and BIC.  
- Results show that all nine algorithm-criterion combinations perform well for discovering isotropic and anisotropic materials.  
- LARS efficiently solves the $\ell_1$-constrained problem, while OMP serves as a heuristic for $\ell_0$-regularized selection.  
- The study broadens the range of viable discovery algorithms beyond typical $\ell_1$-based approaches like LASSO.  

<br /><br />Summary: <div>
arXiv:2509.16040v1 Announce Type: cross 
Abstract: The automated discovery of constitutive models from data has recently emerged as a promising alternative to the traditional model calibration paradigm. In this work, we present a fully automated framework for constitutive model discovery that systematically pairs three sparse regression algorithms (Least Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression (LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). This pairing yields nine distinct algorithms for model discovery and enables a systematic exploration of the trade-off between sparsity, predictive performance, and computational cost. While LARS serves as an efficient path-based solver for the $\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for $\ell_0$-regularized selection. The framework is applied to both isotropic and anisotropic hyperelasticity, utilizing both synthetic and experimental datasets. Results reveal that all nine algorithm-criterion combinations perform consistently well for the discovery of isotropic and anisotropic materials, yielding highly accurate constitutive models. These findings broaden the range of viable discovery algorithms beyond $\ell_1$-based approaches such as LASSO.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Loss Balancing in Physics-Informed Neural Networks for Fluid Flow Applications</title>
<link>https://arxiv.org/abs/2509.14437</link>
<guid>https://arxiv.org/abs/2509.14437</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, Partial Differential Equations, Loss Balancing, Navier-Stokes<br />
Summary:<br />
Physics-Informed Neural Networks (PINNs) are used to solve partial differential equations (PDEs) by balancing multiple competing loss terms. Challenges arise in weighting physics residuals, boundary conditions, and initial conditions. Existing loss balancing schemes have been limited to fixed activation functions in neural network architectures. This study introduces trainable activation functions within neural networks and evaluates their effectiveness on complex fluid flow problems represented by Navier-Stokes equations. The proposed solution shows significant root mean square error (RMSE) improvements, ranging from 7.4% to 95.2%, across various scenarios. The results highlight the importance of considering the interplay between activation function selection and balancing algorithms when designing effective loss balancing strategies.<br /> 
Summary: <div>
arXiv:2509.14437v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising machine learning approach for solving partial differential equations (PDEs). However, PINNs face significant challenges in balancing multi-objective losses, as multiple competing loss terms such as physics residuals, boundary conditions, and initial conditions must be appropriately weighted. While various loss balancing schemes have been proposed, they have been implemented within neural network architectures with fixed activation functions, and their effectiveness has been assessed using simpler PDEs. We hypothesize that the effectiveness of loss balancing schemes depends not only on the balancing strategy itself, but also on the neural network's inherent function approximation capabilities, which are influenced by the choice of activation function. In this paper, we extend existing solutions by incorporating trainable activation functions within the neural network architecture and evaluate the proposed approach on complex fluid flow applications modeled by the Navier-Stokes equations. Our evaluation across diverse Navier-Stokes problems demonstrates that this proposed solution achieves root mean square error (RMSE) improvements ranging from 7.4\% to 95.2\% across different scenarios. These findings underscore the importance of carefully considering the interaction between activation function selection and balancing algorithms when designing loss balancing strategies.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lagrangian-Eulerian Multiscale Data Assimilation in Physical Domain based on Conditional Gaussian Nonlinear System</title>
<link>https://arxiv.org/abs/2509.14586</link>
<guid>https://arxiv.org/abs/2509.14586</guid>
<content:encoded><![CDATA[
<div> Keywords: Lagrangian-Eulerian Multiscale Data Assimilation, physical domain, sea ice floe trajectories, two-layer Quasi geostrophic model, Conditional Gaussian Nonlinear System

Summary: 
This research explores Lagrangian-Eulerian Multiscale Data Assimilation (LEMDA) by transitioning from Fourier space to the physical domain. Focusing on sea ice floe trajectories in the Arctic, a two-layer Quasi geostrophic model is used to recover ocean eddies. The model employs Conditional Gaussian Nonlinear System (CGNS) to handle non-linearity effectively. Performance evaluation using normalised root mean square error (RMSE) and pattern correlation (Corr) supports the efficacy of the physical domain approach. Future enhancements, like integrating neural networks (NN) to speed up localized particle recovery in Lagrangian DA, are discussed. Overall, the study demonstrates the benefits of utilizing the two-layer QG model in the physical domain for accurate data assimilation in non-periodic systems. 

<br /><br />Summary: <div>
arXiv:2509.14586v1 Announce Type: new 
Abstract: This research aims to further investigate the process of Lagrangian-Eulerian Multiscale Data Assimilation (LEMDA) by replacing the Fourier space with the physical domain. Such change in the perspective of domain introduces the advantages of being able to deal in non-periodic system and more intuitive representation of localised phenomena or time-dependent problems. The context of the domain for this paper was set as sea ice floe trajectories to recover the ocean eddies in the Arctic regions, which led the model to be derived from two-layer Quasi geostrophic (QG) model. The numerical solution to this model utilises the Conditional Gaussian Nonlinear System (CGNS) to accommodate the inherent non-linearity in analytical and continuous manner. The normalised root mean square error (RMSE) and pattern correlation (Corr) are used to evaluate the performance of the posterior mean of the model. The results corroborate the effectiveness of exploiting the two-layer QG model in physical domain. Nonetheless, the paper still discusses opportunities of improvement, such as deploying neural network (NN) to accelerate the recovery of local particle of Lagrangian DA for the fine scale.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of conductive nonmagnetic objects in presence of the Lenz effect</title>
<link>https://arxiv.org/abs/2509.14976</link>
<guid>https://arxiv.org/abs/2509.14976</guid>
<content:encoded><![CDATA[
<div> dynamics, MRI room, Lenz effect, numerical procedure, experimental data <br />
Summary:<br />
The study aims to model and predict the behavior of conductive nonmagnetic objects in an MRI room influenced by the Lenz effect. By formulating an ordinary differential equation and neglecting the skin effect, the Lenz effect's impact on object dynamics is separated into position and velocity dependencies, enabling the development of a straightforward numerical procedure applicable to objects of any shape. The model's accuracy was confirmed through experiments involving the rotation and translation of an aluminum plate within a 1.5 T MRI scanner. The findings highlight that precise predictions of motion in the presence of the Lenz effect can be achieved by accurately determining induced electric currents in the metal objects during motion steps without considering the skin effect. <div>
arXiv:2509.14976v1 Announce Type: new 
Abstract: Purpose: To model and predict the dynamics of conductive nonmagnetic objects within the MRI room under the influence of Lenz effect. Methods: The dynamics are described by an ordinary differential equation and the Lenz effect approximated by recognizing that the skin effect is negligible. This separated Lenz effect dependency on the object position and velocity, leading to a simple numerical procedure for objects of any shape. Results: The model and numerical procedure were validated with experimental data recording the rotation of an aluminum plate falling inside a 1.5 T MRI scanner. The model was also applied for studying the translation of an aluminum plate pushed with constant force towards the MRI bore through the fringe field. Conclusion: The collected results showed that it is possible to obtain accurate predictions of motion in the presence of Lenz effect by neglecting the skin effect while determining the electric currents induced in the metallic object during each infinitesimal motion step.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Warp Quantification Analysis: A Framework For Path-based Signal Alignment Metrics</title>
<link>https://arxiv.org/abs/2509.14994</link>
<guid>https://arxiv.org/abs/2509.14994</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic time warping, warp quantification analysis, fMRI, schizophrenia, alignment descriptors

Summary: 
Dynamic Time Warping (DTW) is commonly used to align time series with different timescales, but traditional applications only focus on a single distance metric. This study introduces Warp Quantification Analysis (WQA), a novel framework that extracts geometric and structural descriptors from DTW paths. Simulations demonstrate that each metric in WQA accurately tracks specific characteristics without interference from others. When applied to large-scale fMRI data, WQA reveals distinct network signatures and their varied correlations with schizophrenia negative symptom severity. By expanding DTW into a family of alignment descriptors, WQA offers a more comprehensive approach for analyzing temporal coupling in domains requiring nonlinear normalization. WQA enables rich characterization beyond traditional DTW distance measurements, enhancing interpretations and insights in various research fields. 

<br /><br />Summary: <div>
arXiv:2509.14994v1 Announce Type: new 
Abstract: Dynamic time warping (DTW) is widely used to align time series evolving on mismatched timescales, yet most applications reduce alignment to a scalar distance. We introduce warp quantification analysis (WQA), a framework that derives interpretable geometric and structural descriptors from DTW paths. Controlled simulations showed that each metric selectively tracked its intended driver with minimal crosstalk. Applied to large-scale fMRI, WQA revealed distinct network signatures and complementary associations with schizophrenia negative symptom severity, capturing clinically meaningful variability beyond DTW distance. WQA transforms DTW from a single-score method into a family of alignment descriptors, offering a principled and generalizable extension for richer characterization of temporal coupling across domains where nonlinear normalization is essential.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A cell centered Galerkin method for miscible displacement in heterogeneous porous media</title>
<link>https://arxiv.org/abs/2509.14864</link>
<guid>https://arxiv.org/abs/2509.14864</guid>
<content:encoded><![CDATA[
<div> CCG method, miscible displacement, heterogeneous porous media, finite volume, discontinuous Galerkin<br />
<br />
Summary: The paper introduces the cell centered Galerkin (CCG) method for solving miscible displacement problems in porous media. This approach combines finite volume and discontinuous Galerkin methods to achieve an efficient lowest-order approximation, with only one unknown per cell. By utilizing classical DG weak formulations, the CCG method shows comparable accuracy and improved efficiency compared to traditional higher-order interior penalty DG methods. The study also proves that the CCG method produces an inverse-positive matrix for a model Poisson problem in 1D. Computational experiments in 2D and 3D highlight the effectiveness of CCG for highly heterogeneous flow and transport problems, with comparisons to classical DG methods demonstrating its advantages. <br /><br /> <div>
arXiv:2509.14864v1 Announce Type: cross 
Abstract: In this paper we present a cell centered Galerkin (CCG) method applied to miscible displacement problems in heterogeneous porous media. The CCG approach combines concepts from finite volume and discontinuous Galerkin (DG) methods to arrive at an efficient lowest-order approximation (one unknown per cell). We demonstrate that the CCG method can be defined using classical DG weak formulations, only requires one unknown per cell, and is able to deliver comparable accuracy and improved efficiency over traditional higher-order interior penalty DG methods. In addition, we prove that the CCG method for a model Poisson problem gives rise to a inverse-positive matrix in 1D. A plethora of computational experiments in 2D and 3D showcase the effectiveness of the CCG method for highly heterogeneous flow and transport problems in porous media. Comparisons between CCG and classical DG methods are included.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed GCN-LSTM Framework for Long-Term Forecasting of 2D and 3D Microstructure Evolution</title>
<link>https://arxiv.org/abs/2509.15029</link>
<guid>https://arxiv.org/abs/2509.15029</guid>
<content:encoded><![CDATA[
<div> framework, graph convolutional networks, long short-term memory, microstructure evolution, composition-aware <br />
Summary:
This paper introduces a novel framework that combines graph convolutional networks (GCN) and long short-term memory (LSTM) to predict microstructure evolution in 2D and 3D systems effectively. The framework considers the composition of the datasets and operates in latent graph space to efficiently capture composition and morphological changes. By compressing phase-field simulation data with convolutional autoencoders, the model can accurately forecast microstructural evolution in different compositions and dimensions over long time spans. The framework successfully captures spatial and temporal patterns in evolving microstructures, allowing for efficient long-term forecasting post-training. The integration of GCN and LSTM enables robust performance across various evaluation metrics, making it a promising tool for studying microstructure evolution. <br /><br /> <div>
arXiv:2509.15029v1 Announce Type: cross 
Abstract: This paper presents a physics-informed framework that integrates graph convolutional networks (GCN) with long short-term memory (LSTM) architecture to forecast microstructure evolution over long time horizons in both 2D and 3D with remarkable performance across varied metrics. The proposed framework is composition-aware, trained jointly on datasets with different compositions, and operates in latent graph space, which enables the model to capture compositions and morphological dynamics while remaining computationally efficient. Compressing and encoding phase-field simulation data with convolutional autoencoders and operating in Latent graph space facilitates efficient modeling of microstructural evolution across composition, dimensions, and long-term horizons. The framework captures the spatial and temporal patterns of evolving microstructures while enabling long-range forecasting at reduced computational cost after training.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying companies and financial actors exposed to marine tipping points</title>
<link>https://arxiv.org/abs/2411.10307</link>
<guid>https://arxiv.org/abs/2411.10307</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, marine ecosystems, tipping points, fisheries, investors<br />
<br />
Summary: 
Climate change and other anthropogenic pressures are likely to induce tipping points in marine ecosystems, potentially leading to declines in primary productivity and fisheries. Despite increasing attention to nature-related financial risks and opportunities within the ocean economy, the extent to which these tipping points could affect investors has remained largely unexplored. Tracking fishing vessels in areas prone to marine regime shifts revealed key countries, companies, and shareholders exposed to tipping risk. Data gaps were acknowledged, but potential challenges and opportunities for these actors if marine ecosystems shift to less productive states were outlined. <div>
arXiv:2411.10307v2 Announce Type: replace 
Abstract: Climate change and other anthropogenic pressures are likely to induce tipping points in marine ecosystems, potentially leading to declines in primary productivity and fisheries. Despite increasing attention to nature-related financial risks and opportunities within the ocean economy, the extent to which these tipping points could affect investors has remained largely unexplored. Here we used satellite data to track fishing vessels operating in areas prone to marine regime shifts, as identified by their loss of resilience and vulnerability to marine heatwaves, and uncovered their corporate beneficial owners and shareholders. Despite some data gaps, we identified key countries, companies, and shareholders exposed to tipping risk. We also outline the potential challenges and opportunities that these actors may face if marine ecosystems shift to less productive states.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-based deep kernel learning for parameter estimation in high dimensional PDEs</title>
<link>https://arxiv.org/abs/2509.14054</link>
<guid>https://arxiv.org/abs/2509.14054</guid>
<content:encoded><![CDATA[
<div> train, deep kernel learning, Hamiltonian Monte Carlo, parameter inference, uncertainty quantification

Summary: 
This paper proposes a novel two-stage Bayesian framework for inferring parameters of high-dimensional partial differential equations (PDEs). It combines physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to accurately estimate unknown PDE parameters and quantify their uncertainties from sparse observations. In the first stage, a surrogate model is trained using DKL to optimize a neural network feature extractor and provide initial parameter estimates. The second stage uses fixed neural network weights and HMC to sample the joint posterior distribution of kernel hyperparameters and PDE parameters efficiently. Numerical experiments show that the framework accurately estimates parameters, offers reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity. This approach provides a robust and scalable tool for a wide range of scientific and engineering applications.<br /><br />Summary: <div>
arXiv:2509.14054v1 Announce Type: new 
Abstract: Inferring parameters of high-dimensional partial differential equations (PDEs) poses significant computational and inferential challenges, primarily due to the curse of dimensionality and the inherent limitations of traditional numerical methods. This paper introduces a novel two-stage Bayesian framework that synergistically integrates training, physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE parameters and quantify their uncertainties from sparse, exact observations. The first stage leverages physics-based DKL to train a surrogate model, which jointly yields an optimized neural network feature extractor and robust initial estimates for the PDE parameters. In the second stage, with the neural network weights fixed, HMC is employed within a full Bayesian framework to efficiently sample the joint posterior distribution of the kernel hyperparameters and the PDE parameters. Numerical experiments on canonical and high-dimensional inverse PDE problems demonstrate that our framework accurately estimates parameters, provides reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity, offering a robust and scalable tool for diverse scientific and engineering applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Cognitive Bias in Social Agents</title>
<link>https://arxiv.org/abs/2509.13588</link>
<guid>https://arxiv.org/abs/2509.13588</guid>
<content:encoded><![CDATA[
<div> cognitive bias, social simulation, agent behavior, CoBRA, HCI toolkit

Summary:
CoBRA is a new toolkit for specifying agent behavior in LLM-based social simulations. Traditional methods using natural language descriptions often result in inconsistent behaviors that do not accurately represent the intended nuances. CoBRA introduces a novel approach by explicitly programming agents' cognitive biases based on social science experiments. It includes a Cognitive Bias Index that quantifies agents' reactions in validated experiments and a Behavioral Regulation Engine to align behaviors with controlled cognitive bias. Evaluation as an HCI toolkit demonstrated CoBRA's ability to precisely program cognitive bias in social agents across different models. This approach enables accurate representation of cognitive biases in social simulations, enhancing the realism and effectiveness of agent behavior programming. <div>
arXiv:2509.13588v1 Announce Type: cross 
Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying agent behavior in LLM-based social simulation. We found that conventional approaches that specify agent behaviors through implicit natural language descriptions cannot yield consistent behaviors across models, and the produced agent behaviors do not capture the nuances of the descriptions. In contrast, CoBRA presents a new approach to program agents' cognitive biases explicitly, by grounding agents' expected behaviors using classic social science experiments. CoBRA has two components: (1) Cognitive Bias Index that measures the cognitive bias of a social agent, by quantifying the agent's reactions in a set of validated classical social science experiments; (2) Behavioral Regulation Engine that aligns the agent's behavior to demonstrate controlled cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and technical benchmarks. Our results suggest that CoBRA can precisely program the cognitive bias demonstrated in a social agent in a model-agnostic manner.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Meshing Framework for Digital Twins for Extrusion based Additive Manufacturing</title>
<link>https://arxiv.org/abs/2509.12436</link>
<guid>https://arxiv.org/abs/2509.12436</guid>
<content:encoded><![CDATA[
<div> Keywords: additive manufacturing, computational meshes, finite element analysis, toolpath infill, design optimization<br />
Summary:<br />
Additive manufacturing (AM) allows for the production of complex 3D geometries with unique internal microstructures that influence mechanical properties. A new framework is proposed for creating computational meshes suitable for finite element analysis (FEA) of fine-scale features generated by AM tool paths. This framework enables in-depth numerical simulations to assess the impact of internal microstructures on component properties without the need for physical testing. By analyzing toolpath infill, the framework facilitates design optimization for components such as soft elastomeric lattices. This approach reduces time and resource waste typically associated with trial-and-error design cycles and enables the exploration of unconventional design spaces. Overall, the framework enhances the process-structure-property-performance linkage in AM components, opening up possibilities for innovative design solutions. <br /><br />Summary: <div>
arXiv:2509.12436v1 Announce Type: new 
Abstract: Additive manufacturing (AM) allows for manufacturing of complex three-dimensional geometries not typically realizable with standard subtractive manufacturing practices. The internal microstructure of a 3D printed component can have a significant impact on its mechanical, vibrational, and shock properties and allows for a richer design space when this is controllable. Due to the complex interactions of the internal geometry of an extrusion-based AM component, it is common practice to assume a homogeneous behavior or to perform characterization testing on the specific toolpath configurations. To avoid unnecessary testing or material waste, it is necessary to develop an accurate and consistent numerical simulation framework with relevant boundary value problems that can handle the complicated geometry of internal material microstructure present in AM components. Herein, a framework is proposed to directly create computational meshes suitable for finite element analysis (FEA) of the fine-scale features generated from extrusion-based AM tool paths to maintain a strong process-structure-property-performance linkage. This mesh can be manually or automatically analyzed using standard FEA simulations such as quasi-static preloading, modal analysis, or thermal analysis. The framework allows an in-silico assessment of a target AM geometry where fine-scale features may greatly impact quantities of design interest such as in soft elastomeric lattices where toolpath infill can greatly influence the self contact of a structure in compression, which we will use as a motivating exemplar. This approach greatly reduces the waste of both time and resources consumed through traditional build and test design cycles for non-intuitive design spaces. It also further allows for the exploration of toolpath infill to optimize component properties beyond simple linear properties such as density and stiffness.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Language Models for Forecasting Market Impact from Sequences of Financial News</title>
<link>https://arxiv.org/abs/2509.12519</link>
<guid>https://arxiv.org/abs/2509.12519</guid>
<content:encoded><![CDATA[
<div> financial news, stock prices, information diffusion, historical context, large language models

Summary:
- Financial news is a key driver of stock prices and plays a crucial role in information dissemination in financial markets.
- Each news article may require broader historical context for accurate interpretation, posing challenges in identifying relevant information.
- Historical context significantly improves performance in understanding the market impact of financial news across methods and time horizons.
- An efficient method is proposed that uses large language models to process the main article and small models to encode historical context for improved performance.
- Qualitative and quantitative tests reveal insights into the value of contextualization in model behavior and predictions, leading to substantial improvements in simulated investment performance. <div>
arXiv:2509.12519v1 Announce Type: new 
Abstract: Financial news plays a critical role in the information diffusion process in financial markets and is a known driver of stock prices. However, the information in each news article is not necessarily self-contained, often requiring a broader understanding of the historical news coverage for accurate interpretation. Further, identifying and incorporating the most relevant contextual information presents significant challenges. In this work, we explore the value of historical context in the ability of large language models to understand the market impact of financial news. We find that historical context provides a consistent and significant improvement in performance across methods and time horizons. To this end, we propose an efficient and effective contextualization method that uses a large LM to process the main article, while a small LM encodes the historical context into concise summary embeddings that are then aligned with the large model's representation space. We explore the behavior of the model through multiple qualitative and quantitative interpretability tests and reveal insights into the value of contextualization. Finally, we demonstrate that the value of historical context in model predictions has real-world applications, translating to substantial improvements in simulated investment performance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Geometric Uncertainty on the Computation of Abdominal Aortic Aneurysm Wall Strain</title>
<link>https://arxiv.org/abs/2509.12550</link>
<guid>https://arxiv.org/abs/2509.12550</guid>
<content:encoded><![CDATA[
<div> geometry, abdominal aortic aneurysm, wall strain, wall stress, computed tomography angiography<br />
<br />
Summary:<br />
Abdominal aortic aneurysm (AAA) is a life-threatening condition characterized by the permanent enlargement of the aorta. Current management relies on aneurysm diameter and growth rate, which may not accurately predict rupture risk. This study examined the impact of geometric uncertainty on AAA wall strain calculated from 4D-CTA. Results showed that uncertainties in wall geometry reduce the accuracy of computed strain, with inward bias causing greater deviations than outward bias. Peak strain is more sensitive but less robust, while the 99th percentile strain remains stable. To ensure accurate strain estimation, geometric uncertainty should remain within one wall thickness. This research emphasizes the importance of considering geometric uncertainty in AAA risk assessments and highlights the need for precise image-derived geometry in computational analyses. <div>
arXiv:2509.12550v1 Announce Type: new 
Abstract: Abdominal aortic aneurysm (AAA) is a life-threatening condition characterized by permanent enlargement of the aorta, often detected incidentally during imaging for unrelated conditions. Current management relies primarily on aneurysm diameter and growth rate, which may not reliably predict patient-specific rupture risk. Computation of AAA wall stress and strain has the potential to improve individualized risk assessment, but these analyses depend on image-derived geometry, which is subject to segmentation uncertainty and lacks a definitive ground truth for the wall boundary. While the effect of geometric uncertainty on wall stress has been studied, its influence on wall strain remains unclear. In this study, we assessed the impact of geometric uncertainty on AAA wall strain computed using deformable image registration of time-resolved 3D computed tomography angiography (4D-CTA). Controlled perturbations were applied to the wall geometry along the surface normal, parameterized by the standard deviation for random variation and the mean for systematic inward or outward bias, both scaled relative to wall thickness. Results show that uncertainties in AAA wall geometry reduce the accuracy of computed strain, with inward bias (toward the blood lumen and intraluminal thrombus) consistently causing greater deviations than outward bias (toward regions external to the aortic wall). Peak strain is more sensitive but less robust, whereas the 99th percentile strain remains more stable under perturbations. We concluded that, for sufficiently accurate strain estimation, geometric uncertainty should remain within one wall thickness (typically 1.5 mm).
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSentLLM: Multi-LLM and Structured Semantic Signals for Enhanced Financial Sentiment Forecasting</title>
<link>https://arxiv.org/abs/2509.12638</link>
<guid>https://arxiv.org/abs/2509.12638</guid>
<content:encoded><![CDATA[
<div> Sentiment Analysis, Large Language Models, Financial Signals, Stock Markets, Forecasting <br />
Summary: <br />
The study introduces FinSentLLM, a framework that combines multiple large language models and financial signals for financial sentiment analysis. It outperforms existing models on accuracy and F1-score without the need for extensive retraining. The framework shows a 3-6% improvement over baseline models using the Financial PhraseBank dataset. Additionally, econometric analysis using the DCC-GARCH and Johansen cointegration test demonstrates a significant long-term relationship between financial sentiment and stock market movements. This suggests that sentiment signals can effectively predict equity market dynamics in the long run. <div>
arXiv:2509.12638v1 Announce Type: new 
Abstract: Financial sentiment analysis (FSA) has attracted significant attention, and recent studies increasingly explore large language models (LLMs) for this field. Yet most work evaluates only classification metrics, leaving unclear whether sentiment signals align with market behavior. We propose FinSentLLM, a lightweight multi-LLM framework that integrates an expert panel of sentiment forecasting LLMs, and structured semantic financial signals via a compact meta-classifier. This design captures expert complementarity, semantic reasoning signal, and agreement/divergence patterns without costly retraining, yielding consistent 3-6% gains over strong baselines in accuracy and F1-score on the Financial PhraseBank dataset. In addition, we also provide econometric evidence that financial sentiment and stock markets exhibit statistically significant long-run comovement, applying Dynamic Conditional Correlation GARCH (DCC-GARCH) and the Johansen cointegration test to daily sentiment scores computed from the FNSPID dataset and major stock indices. Together, these results demonstrate that FinSentLLM delivers superior forecasting accuracy for financial sentiment and further establish that sentiment signals are robustly linked to long-run equity market dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cost-Optimization Model for EV Charging Stations Utilizing Solar Energy and Variable Pricing</title>
<link>https://arxiv.org/abs/2509.12214</link>
<guid>https://arxiv.org/abs/2509.12214</guid>
<content:encoded><![CDATA[
<div> framework, electric vehicle charging stations, cost optimization, photovoltaic generation, electricity price uncertainty
Summary:
The paper introduces a cost optimization framework for electric vehicle (EV) charging stations that integrates on-site photovoltaic (PV) generation and considers electricity price uncertainty. The model, formulated as a linear program, ensures the satisfaction of vehicle energy demands, adherence to charging and grid capacity constraints, and minimization of procurement cost. Evaluations based on actual charging data demonstrate average savings of approximately 12% compared to a traditional first-come-first-served approach, with potential peak monthly reductions reaching 19.2%. A sensitivity analysis suggests that a slight increase in nominal cost can significantly mitigate worst-case exposure by 14%. Computational tests confirm the practicality and scalability of the proposed solution by successfully solving instances with up to 50 concurrent EVs in under 5 seconds on a standard laptop. This innovative method offers a grid-friendly and efficient approach for future EV charging operations. 
<br /><br />Summary: <div>
arXiv:2509.12214v1 Announce Type: cross 
Abstract: This paper presents a cost optimization framework for electric vehicle (EV) charging stations that leverages on-site photovoltaic (PV) generation and explicitly accounts for electricity price uncertainty through a Bertsimas--Sim robust formulation. The model is formulated as a linear program that satisfies vehicle energy demands, respects charging and grid capacity constraints, and minimizes procurement cost. Evaluations on real charging data from the Caltech ACN dataset show average savings of about 12\% compared to a first-come--first-served baseline, with peak monthly reductions up to 19.2\%. A lightweight sensitivity analysis indicates that a modest $\sim$5\% increase in nominal cost can reduce worst-case exposure by 14\%. Computational tests confirm real-time feasibility, with instances of up to 50 concurrent EVs solved in under 5 seconds on a standard laptop. The proposed method provides a practical, grid-friendly, and scalable solution for future EV charging operations.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End to End Edge to Cloud Data and Analytics Strategy</title>
<link>https://arxiv.org/abs/2509.12296</link>
<guid>https://arxiv.org/abs/2509.12296</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, cloud, edge, data strategy, reference architectures

Summary:
Internet of Things (IoT) devices are rapidly increasing, leading to the need for real-time data for critical decision-making. Enterprises are quickly embracing cloud technology, creating a demand for secure and efficient strategies to maximize cloud and edge capabilities. This paper presents an end-to-end secure edge-to-cloud data and analytics strategy. It includes reference architectures for the device layer, edge layer, and cloud layer, facilitating practical implementation. By addressing the exponential growth of IoT devices, leveraging cloud technology, and ensuring secure data transmission, this strategy aims to optimize the utilization of cloud and edge assets for real-time decision-making. The proposed approach not only enhances data security but also enables efficient data analysis, ultimately contributing to the successful implementation of IoT applications in various industries. 

Summary: <div>
arXiv:2509.12296v1 Announce Type: cross 
Abstract: There is an exponential growth of connected Internet of Things (IoT) devices. These have given rise to applications that rely on real time data to make critical decisions quickly. Enterprises today are adopting cloud at a rapid pace. There is a critical need to develop secure and efficient strategy and architectures to best leverage capabilities of cloud and edge assets. This paper provides an end to end secure edge to cloud data and analytics strategy. To enable real life implementation, the paper provides reference architectures for device layer, edge layer and cloud layer.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparison of pipelines for the translation of a low resource language based on transformers</title>
<link>https://arxiv.org/abs/2509.12514</link>
<guid>https://arxiv.org/abs/2509.12514</guid>
<content:encoded><![CDATA[
<div> transformer-based neural networks, machine translation, Bambara language, low-resource translation, language distillation

Summary:
This work compares three pipelines for training transformer-based neural networks to produce machine translators for the Bambara language. The first pipeline focuses on translating French into Bambara using a simple transformer model. The second pipeline fine-tunes instructor models for French-to-Bambara translation, achieving varying results based on dataset specificity. The third pipeline utilizes language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model for language-agnostic embeddings, followed by a BERT extension for translation. Results show the first pipeline achieves the best accuracy overall, particularly on low-resource datasets. The instructor-based models perform better on individual datasets compared to aggregated collections, indicating their efficacy in capturing dataset-specific patterns. <div>
arXiv:2509.12514v1 Announce Type: cross 
Abstract: This work compares three pipelines for training transformer-based neural networks to produce machine translators for Bambara, a Mand\`e language spoken in Africa by about 14,188,850 people. The first pipeline trains a simple transformer to translate sentences from French into Bambara. The second fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures for French-to-Bambara translation. Models from the first two pipelines were trained with different hyperparameter combinations to improve BLEU and chrF scores, evaluated on both test sentences and official Bambara benchmarks. The third pipeline uses language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model, which provides language-agnostic embeddings. A BERT extension is then applied to LaBSE to generate translations. All pipelines were tested on Dokotoro (medical) and Bayelemagaba (mixed domains). Results show that the first pipeline, although simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on Bayelemagaba), consistent with low-resource translation results. On the Yiri dataset, created for this work, it achieves 33.81% BLEU and 41% chrF. Instructor-based models perform better on single datasets than on aggregated collections, suggesting they capture dataset-specific patterns more effectively.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Pipeline for Patient-Specific Modeling of Thoracic Aortic Aneurysm: From Medical Image to Finite Element Analysis</title>
<link>https://arxiv.org/abs/2509.12596</link>
<guid>https://arxiv.org/abs/2509.12596</guid>
<content:encoded><![CDATA[
<div> Keywords: aorta, thoracic aortic aneurysm, finite element analysis, deep learning, patient-specific modeling

Summary: 
The article discusses the significance of thoracic aortic aneurysms (TAAs) as a leading cause of mortality, emphasizing the importance of accurate diagnosis for treatment. It highlights the use of three-dimensional computed tomography (3D CT) imaging for precise evaluation of aortic geometry and stresses on the aortic wall. Deep learning-based image segmentation is recognized as a reliable method for extracting anatomical structures from medical images. The conversion of segmentation masks into structured mesh representation enables accurate finite element analysis (FEA) simulations, with hexahedral meshes commonly used for efficiency and accuracy in aortic biomechanics. Patient-specific modeling allows for detailed assessment of individual anatomical and biomechanical characteristics, supporting personalized treatment strategies. Developing accurate FE models is crucial for establishing a biomechanically based framework to predict the risk of TAA. <div>
arXiv:2509.12596v1 Announce Type: cross 
Abstract: The aorta is the body's largest arterial vessel, serving as the primary pathway for oxygenated blood within the systemic circulation. Aortic aneurysms consistently rank among the top twenty causes of mortality in the United States. Thoracic aortic aneurysm (TAA) arises from abnormal dilation of the thoracic aorta and remains a clinically significant disease, ranking as one of the leading causes of death in adults. A thoracic aortic aneurysm ruptures when the integrity of all aortic wall layers is compromised due to elevated blood pressure. Currently, three-dimensional computed tomography (3D CT) is considered the gold standard for diagnosing TAA. The geometric characteristics of the aorta, which can be quantified from medical imaging, and stresses on the aortic wall, which can be obtained by finite element analysis (FEA), are critical in evaluating the risk of rupture and dissection. Deep learning based image segmentation has emerged as a reliable method for extracting anatomical regions of interest from medical images. Voxel based segmentation masks of anatomical structures are typically converted into structured mesh representation to enable accurate simulation. Hexahedral meshes are commonly used in finite element simulations of the aorta due to their computational efficiency and superior simulation accuracy. Due to anatomical variability, patient specific modeling enables detailed assessment of individual anatomical and biomechanics behaviors, supporting precise simulations, accurate diagnoses, and personalized treatment strategies. Finite element (FE) simulations provide valuable insights into the biomechanical behaviors of tissues and organs in clinical studies. Developing accurate FE models represents a crucial initial step in establishing a patient-specific, biomechanically based framework for predicting the risk of TAA.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry</title>
<link>https://arxiv.org/abs/2509.12679</link>
<guid>https://arxiv.org/abs/2509.12679</guid>
<content:encoded><![CDATA[
<div> transformer-based NQS, neural quantum states, scaling laws, language models, second-quantized quantum chemistry<br />
Summary: <br />
The study investigates scaling laws for neural quantum states (NQS) incorporating language model components. It aims to understand NQS scalability and optimal performance trade-offs. By analyzing transformer-based NQS in second-quantized quantum chemistry applications, the research identifies scaling laws predicting performance based on problem size. The study delves into the relationship between model size, training time, and performance metrics, such as absolute error and V-score. Unlike language models, the relationship between model size and training time in NQS is highly dependent on the loss metric and ansatz used, challenging the linear relationship observed in traditional language models. This research sheds light on the scalability of NQS ansatze and provides insights into performance-resource trade-offs in quantum chemistry applications. <br /> <div>
arXiv:2509.12679v1 Announce Type: cross 
Abstract: Scaling laws have been used to describe how large language model (LLM) performance scales with model size, training data size, or amount of computational resources. Motivated by the fact that neural quantum states (NQS) has increasingly adopted LLM-based components, we seek to understand NQS scaling laws, thereby shedding light on the scalability and optimal performance--resource trade-offs of NQS ansatze. In particular, we identify scaling laws that predict the performance, as measured by absolute error and V-score, for transformer-based NQS as a function of problem size in second-quantized quantum chemistry applications. By performing analogous compute-constrained optimization of the obtained parametric curves, we find that the relationship between model size and training time is highly dependent on loss metric and ansatz, and does not follow the approximately linear relationship found for language models.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs</title>
<link>https://arxiv.org/abs/2509.12730</link>
<guid>https://arxiv.org/abs/2509.12730</guid>
<content:encoded><![CDATA[
<div> Keywords: digital ecosystems, financial sector, graph machine learning, network analysis, financial crime

Summary:
The article discusses the challenges faced by traditional rule-based systems in detecting sophisticated criminal behaviors in digital ecosystems within the financial sector. It proposes an approach that integrates graph machine learning and network analysis to improve the detection of topological patterns in transactional graphs. The key challenge lies in the sparse and unlabeled information in traditional financial datasets, prompting the development of a four-step preprocessing framework to generate weak ground-truth labels for analysis. Graph Autoencoders are then implemented to distinguish among topological patterns, with three variants compared in the analysis. Preliminary results suggest that this method is effective in detecting complex financial crime schemes, presenting a promising alternative to rule-based detection systems.<br /><br />Summary: <div>
arXiv:2509.12730v1 Announce Type: cross 
Abstract: The rise of digital ecosystems has exposed the financial sector to evolving abuse and criminal tactics that share operational knowledge and techniques both within and across different environments (fiat-based, crypto-assets, etc.). Traditional rule-based systems lack the adaptability needed to detect sophisticated or coordinated criminal behaviors (patterns), highlighting the need for strategies that analyze actors' interactions to uncover suspicious activities and extract their modus operandi. For this reason, in this work, we propose an approach that integrates graph machine learning and network analysis to improve the detection of well-known topological patterns within transactional graphs. However, a key challenge lies in the limitations of traditional financial datasets, which often provide sparse, unlabeled information that is difficult to use for graph-based pattern analysis. Therefore, we firstly propose a four-step preprocessing framework that involves (i) extracting graph structures, (ii) considering data temporality to manage large node sets, (iii) detecting communities within, and (iv) applying automatic labeling strategies to generate weak ground-truth labels. Then, once the data is processed, Graph Autoencoders are implemented to distinguish among the well-known topological patterns. Specifically, three different GAE variants are implemented and compared in this analysis. Preliminary results show that this pattern-focused, topology-driven method is effective for detecting complex financial crime schemes, offering a promising alternative to conventional rule-based detection systems.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanics-Informed Machine Learning for Geospatial Modeling of Soil Liquefaction: Global and National Surrogate Models for Simulation and Near-Real-Time Response</title>
<link>https://arxiv.org/abs/2509.10962</link>
<guid>https://arxiv.org/abs/2509.10962</guid>
<content:encoded><![CDATA[
<div> machine learning, soil liquefaction, geospatial information, high performance computing, regional-scale modeling

Summary:<br />
Using machine learning and geospatial information, surrogate models are developed to predict soil liquefaction at regional scales. Global and New Zealand-specific models are trained to mimic geotechnical models, anchored to mechanics and driven by ML for more predictive information. The models are geostatistically updated by subsurface data and precomputed globally for all earthquake scenarios, making them easy to execute and encouraging user adoption. Test applications show the proposed models outperform others significantly, with geostatistical updating further improving performance. Region-specific models may not offer significant advantages over global datasets. These models are ideal for regional-scale liquefaction hazard simulation and near-real-time response, with accompanying variance products indicating the influence of local geotechnical data on predicted liquefaction response. <br />Summary: <div>
arXiv:2509.10962v1 Announce Type: new 
Abstract: Using machine learning (ML), high performance computing, and a large body of geospatial information, we develop surrogate models to predict soil liquefaction across regional scales. Two sets of models - one global and one specific to New Zealand - are trained by learning to mimic geotechnical models at the sites of in-situ tests. Our geospatial approach has conceptual advantages in that predictions: (i) are anchored to mechanics, which encourages more sensible response and scaling across the domains of soil, site, and loading characteristics; (ii) are driven by ML, which allows more predictive information to be used, with greater potential for it to be exploited; (iii) are geostatistically updated by subsurface data, which anchors the predictions to known conditions; and (iv) are precomputed everywhere on earth for all conceivable earthquakes, which allows the models to be executed very easily, thus encouraging user adoption and evaluation. Test applications suggest that: (i) the proposed models outperform others to a statistically significant degree; (ii) the geostatistical updating further improves performance; and (iii) the anticipated advantages of region-specific models may largely be negated by the benefits of learning from larger global datasets. These models are best suited for regional-scale liquefaction hazard simulation and near-real-time response and are accompanied by variance products that convey where, and to what degree, the ML-predicted liquefaction response is influenced by local geotechnical data.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial AI for Liquefaction Hazard and Impact Forecasting: A Demonstrative Study in the U.S. Pacific Northwest</title>
<link>https://arxiv.org/abs/2509.10965</link>
<guid>https://arxiv.org/abs/2509.10965</guid>
<content:encoded><![CDATA[
<div> machine learning, liquefaction hazard, geospatial model, regional-scale, earthquake

Summary:
- The study focuses on predicting liquefaction hazards in the Pacific Northwest region, specifically in Washington and Oregon, using a geospatial model driven by machine learning.
- The model is able to predict the probability of damaging ground deformation for 85 scenario earthquakes, providing high-resolution forecasts across regional scales.
- The model improves upon prior approaches by incorporating mechanics-based predictions, utilizing machine learning for more geospatial information, and anchoring predictions to known subsurface conditions.
- The resulting liquefaction hazard forecasts are made available in a GIS-ready, public repository for various regional-scale applications such as disaster simulations, evacuation route planning, and infrastructure vulnerability assessments.
- These predictions can be utilized for land-use planning, insurance loss modeling, hazard communication, and public investment prioritization to better prepare for and mitigate the consequences of potential large-magnitude earthquakes in the region.

<br /><br />Summary: <div>
arXiv:2509.10965v1 Announce Type: new 
Abstract: Recent large-magnitude earthquakes have demonstrated the damaging consequences of soil liquefaction and reinforced the need to understand and plan for liquefaction hazards at a regional scale. In the United States, the Pacific Northwest is uniquely vulnerable to such consequences given the potential for crustal, intraslab, and subduction zone earthquakes. In this study, the liquefaction hazard is predicted geospatially at high resolution and across regional scales for 85 scenario earthquakes in the states of Washington and Oregon. This is accomplished using an emergent geospatial model that is driven by machine learning, and which predicts the probability of damaging ground deformation by surrogating state-of-practice geotechnical models. The adopted model shows improved performance and has conceptual advantages over prior regional-scale modeling approaches in that predictions (i) are informed by mechanics, (ii) employ more geospatial information using machine learning, and (iii) are geostatistically anchored to known subsurface conditions. The utility of the resulting predictions for the 85 scenarios is then demonstrated via asset and network infrastructure vulnerability assessments. The liquefaction hazard forecasts are published in a GIS-ready, public repository and are suitable for disaster simulations, evacuation route planning, network vulnerability analysis, land-use planning, insurance loss modeling, hazard communication, public investment prioritization, and other regional-scale applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why "AI" Models for Predicting Soil Liquefaction have been Ignored, Plus Some that Shouldn't Be</title>
<link>https://arxiv.org/abs/2509.10966</link>
<guid>https://arxiv.org/abs/2509.10966</guid>
<content:encoded><![CDATA[
<div> AI, soil liquefaction, prediction models, model development, state-of-practice models

Summary:
- AI liquefaction models are increasingly used but often lack comparison to state-of-practice models, deviate from best practices, may not be used effectively, are presented in a complex manner, and are not provided for use.
- Understanding and addressing these issues can improve the direction and perception of AI in liquefaction research.
- While not all prior efforts are without merit, recognizing recurring shortcomings can guide future research.
- Highlighted papers show applications where AI can add value by enabling new modeling approaches and improving predictions of liquefaction phenomena. 

<br /><br />Summary: <div>
arXiv:2509.10966v1 Announce Type: new 
Abstract: Soil liquefaction remains an important and interesting problem that has attracted the development of enumerable prediction models. Increasingly, these models are utilizing algorithmic learning, or "artificial intelligence" (AI). The rapid growth of AI in the liquefaction literature is unsurprising, given its ease of implementation and potential advantages over traditional statistical methods. However, AI liquefaction models have been widely ignored by practitioners and researchers alike; the objective of this paper is to investigate "why?" Through a sample review of 75 publications, we identify several good reasons. Namely, these models frequently: (i) are not compared to state-of-practice models, making it unclear why they should be adopted; (ii) depart from best practices in model development; (iii) use AI in ways that may not be useful; (iv) are presented in ways that overstate their complexity and make them unapproachable; and (v) are discussed but not actually provided, meaning that no one can use the models even if they wanted to. These prevailing problems must be understood, identified, and remedied, but this does not mean that AI itself is problematic, or that all prior efforts have been without merit or utility. Instead, understanding these recurrent shortcomings can help improve the direction and perceptions of this growing body of work. Towards this end, we highlight papers that are generally free from these shortcomings, and which demonstrate applications where AI is more likely to provide value in the near term: permitting new modeling approaches and potentially improving predictions of liquefaction phenomena.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language model-empowered next-generation computer-aided engineering</title>
<link>https://arxiv.org/abs/2509.11447</link>
<guid>https://arxiv.org/abs/2509.11447</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, computer-aided engineering, model order reduction, autonomous collaborators, parametric analysis<br />
<br />Summary: <br />
The article introduces the concept of using large language models (LLMs) as autonomous collaborators in computer-aided engineering (CAE) to automate and optimize workflows. Specifically, it focuses on data-free model order reduction (MOR) for ultra-fast large-scale parametric analysis. By leveraging LLMs, intrusive MOR, a powerful but underused approach, can be made more accessible and practical through automation of derivations, code restructuring, and implementation. An LLM-empowered CAE agent for solving ultra-large-scale space-parameter-time (S-P-T) physical problems is presented, demonstrating the capability to translate natural language prompts into efficient solver implementations and generate novel MOR solvers for unseen cases. This showcases the potential of LLMs in establishing next-generation CAE systems. <div>
arXiv:2509.11447v1 Announce Type: new 
Abstract: Software development has entered a new era where large language models (LLMs) now serve as general-purpose reasoning engines, enabling natural language interaction and transformative applications across diverse domains. This paradigm is now extending into computer-aided engineering (CAE). Recent applications of LLMs in CAE have successfully automated routine tasks, including CAD model generation and FEM simulations. Nevertheless, these contributions, which primarily serve to reduce manual labor, are often insufficient for addressing the significant computational challenges posed by large-scale, high-dimensional systems. To this aim, we first introduce the concept of LLM-empowered CAE agent, where LLMs act as autonomous collaborators that plan, execute, and adapt CAE workflows. Then, we propose an LLM-empowered CAE agent for data-free model order reduction (MOR), a powerful yet underused approach for ultra-fast large-scale parametric analysis due to the intrusive nature and labor-intensive redevelopment of solvers. LLMs can alleviate this barrier by automating derivations, code restructuring, and implementation, making intrusive MOR both practical and broadly accessible. To demonstrate feasibility, we present an LLM-empowered CAE agent for solving ultra-large-scale space-parameter-time (S-P-T) physical problems using Tensor-decomposition-based A Priori Surrogates (TAPS). Our results show that natural language prompts describing parametric partial differential equations (PDEs) can be translated into efficient solver implementations, substantially reducing human effort while producing high-fidelity reduced-order models. Moreover, LLMs can synthesize novel MOR solvers for unseen cases such as nonlinear and high-dimensional parametric problems based on their internal knowledge base. This highlights the potential of LLMs to establish the foundation for next-generation CAE systems.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward lean industry 5.0: a human-centered model for integrating lean and industry 4.0 in an automotive supplier</title>
<link>https://arxiv.org/abs/2509.11658</link>
<guid>https://arxiv.org/abs/2509.11658</guid>
<content:encoded><![CDATA[
<div> lean, Industry 4.0, human-centered, automotive, case study 

Summary: 
This paper presents a human-centered conceptual model that integrates lean principles and Industry 4.0. It fills a gap in research by offering theoretical insights and practical findings through a case study at an advanced automotive supplier. The study emphasizes the importance of a human-centered approach, identifying key enablers and barriers in lean Industry 4.0 implementation. Through a five-phase multi-method approach, the study examines operational, social, and technological perspectives at both group and model site levels. It illustrates effective implementation strategies and showcases how advanced lean tools can be digitized. The case study identifies 26 positive aspects and 10 negative aspects, showcasing their causal relationships. Successful implementation is shown to benefit organizations and employees when supported by appropriate technological knowledge and people skills, paving the way for lean Industry 5.0. <br /><br /> <div>
arXiv:2509.11658v1 Announce Type: new 
Abstract: This paper proposes a human-centered conceptual model integrating lean and Industry 4.0 based on the literature review and validated it through a case study in the context of an advanced automotive first-tier supplier. Addressing a significant gap in existing research on lean Industry 4.0 implementations, the study provides both theoretical insights and practical findings. It emphasizes the importance of a human-centered approach, identifies key enablers and barriers. In the implementation process of the case study, it is considered at group level and model site level through operational, social and technological perspectives in a five-phase multi-method approach. It shows what effective human-centered lean Industry 4.0 implementation look like and how advanced lean tools can be digitized. It highlights 26 positive and 10 negative aspects of the case and their causal relation. With the appropriate internal and external technological knowhow and people skills, it shows how successful implementation can benefit the organization and employees based on the conceptual model that serves as a first step toward lean Industry 5.0.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Very-low-field MRI scanners: from the ideal to the real permanent magnet array</title>
<link>https://arxiv.org/abs/2509.11762</link>
<guid>https://arxiv.org/abs/2509.11762</guid>
<content:encoded><![CDATA[
<div> MRI, very-low-field, permanent magnets, spatial homogeneity, numerical model<br />
Summary:<br />
Very-low-field MRI technology, utilizing permanent magnets for B0 generation, is gaining popularity due to its portable and cost-effective nature. This article explores the importance of magnet performance in achieving spatial homogeneity of the magnetic field. It investigates factors affecting homogeneity and discrepancies between numerical predictions and actual measurements on fabricated magnets. The study also evaluates the impact of different numerical model approximations on results, highlighting the trade-offs between computational efficiency and result reliability. By providing insights into magnet characterization and model assumptions, this research contributes to enhancing the design and performance of low-cost MRI scanners. <div>
arXiv:2509.11762v1 Announce Type: new 
Abstract: Very-low-field MRIs are becoming increasingly popular due to their portability and adaptability to different environments. They are being successfully used for various clinical applications, leading to a paradigm shift in the way imaging care is typically performed. The development of low-cost MRI scanner prototypes began a few years ago, with some interesting and promising open-source projects emerging in both hardware and software design. Using permanent magnets (PMs) to generate the static magnetic field B0 can substantially reduce the manufacturing cost of low-field scanners while achieving satisfactory homogeneity. This article focuses on characterizing magnet performance in terms of B0 spatial homogeneity. Specifically, it investigates its sensitivity to various factors and explores the reasons for discrepancies between numerical expectations and actual measurements on fabricated magnets. The analysis also examines the consequences of using different numerical model approximations, revisiting concepts most frequently used in other design contexts. While these assumptions simplify the numerical model and may improve its performance in terms of computational time, this paper demonstrates that they also impact the reliability of the obtained results.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hetero-EUCLID: Interpretable model discovery for heterogeneous hyperelastic materials using stress-unsupervised learning</title>
<link>https://arxiv.org/abs/2509.11784</link>
<guid>https://arxiv.org/abs/2509.11784</guid>
<content:encoded><![CDATA[
<div> segmentation, parameter identification, hyperelastic behavior, heterogeneous material, Bayesian-EUCLID

Summary:
The computational framework Hetero-EUCLID is proposed for the segmentation and parameter identification of heterogeneous materials' hyperelastic behavior. Leveraging the Bayesian-EUCLID framework, the approach efficiently solves the heterogenized formulation through model selection with sparsity-promoting priors and Monte Carlo Markov Chain sampling. By utilizing experimentally observable data from non-equi-biaxial tension tests, the framework involves residual force-based segmentation and constitutive parameter identification. Validation shows its capability to segment domains and characterize constituent materials on thin square heterogeneous domains, even with noise in displacement data and non-native mesh discretizations. The framework's potential applications include Digital Image/Volume Correlation-based experimental scenarios like aerospace composites and medical conditions such as fibroatheroma, atherosclerosis, or cancer, offering rapid and interpretable model discovery from a single experiment. <br /><br />Summary: <div>
arXiv:2509.11784v1 Announce Type: new 
Abstract: We propose a computational framework, Hetero-EUCLID, for segmentation and parameter identification to characterize the full hyperelastic behavior of all constituents of a heterogeneous material. In this work, we leverage the Bayesian-EUCLID (Efficient Unsupervised Constitutive Law Identification and Discovery) framework to efficiently solve the heterogenized formulation through parsimonious model selection using sparsity-promoting priors and Monte Carlo Markov Chain sampling. We utilize experimentally observable 3D surface displacement and boundary-averaged force data generated from Finite Element simulations of non-equi-biaxial tension tests on heterogeneous specimens. The framework broadly consists of two steps -- residual force-based segmentation, and constitutive parameter identification. We validate and demonstrate the ability of the proposed framework to segment the domain, and characterize the constituent materials on various types of thin square heterogeneous domains. We validate of the framework's ability to segment and characterize materials with various levels of displacement noises and non-native mesh discretizations, i.e, using different meshes for the forward FE simulations and the inverse EUCLID problem. This demonstrates Hetero-EUCLID framework's applicability in Digital Image/Volume Correlation-based experimental scenarios. Furthermore, the proposed framework performs successful segmentation and material characterizations based on data from a single experiment, thereby making it viable for rapid, interpretable model discovery in domains such as aerospace and defense composites and for characterization of selective tissue stiffening in medical conditions such as fibroatheroma, atherosclerosis, or cancer.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical analysis of fluid estimation for source terms in neutral particles simulation</title>
<link>https://arxiv.org/abs/2509.11883</link>
<guid>https://arxiv.org/abs/2509.11883</guid>
<content:encoded><![CDATA[
<div> Keywords: plasma edge simulations, kinetic Monte Carlo, asymptotic-preserving method, convergence analysis, numerical analysis

Summary:
In this study, the authors investigate the efficiency of a kinetic-diffusion Monte Carlo (KDMC) simulation method coupled with a fluid estimation technique in plasma edge simulations. The aim is to address the computational cost associated with high particle collision rates in large-sized reactors like ITER and DEMO. Through numerical analysis, the researchers compare the accuracy of the proposed algorithm with an approximate fluid method and traditional kinetic Monte Carlo method as a reference. Results show that KDMC with the fluid estimation exhibits significantly lower errors than the fluid method in both high- and low-collisional regimes. Additionally, the KDMC method demonstrates a clear speed-up compared to the kinetic Monte Carlo approach. Overall, the study confirms the effectiveness of the KDMC algorithm in improving computational efficiency and accuracy in plasma edge simulations. <br /><br />Summary: <div>
arXiv:2509.11883v1 Announce Type: new 
Abstract: In plasma edge simulations, kinetic Monte Carlo (MC) is often used to simulate neutral particles and estimate source terms. For large-sized reactors, like ITER and DEMO, high particle collision rates lead to a substantial computational cost for such schemes. To address this challenge, an asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation method and a corresponding fluid estimation technique have been proposed in the literature. In this work, we perform numerical analysis on the convergence of KDMC with the fluid estimation. To do so, we compare the accuracy of the analyzed algorithm with the accuracy of an approximate fluid method using the kinetic MC method as a reference. In a one-dimensional test case, KDMC with the fluid estimation achieves at least one order of magnitude lower errors than the fluid method for both high- and low-collisional regimes. Moreover, KDMC with the fluid estimation outperforms the kinetic MC method with a clear speed-up. Overall, our analysis confirms the effectiveness of the discussed algorithm.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval</title>
<link>https://arxiv.org/abs/2509.12042</link>
<guid>https://arxiv.org/abs/2509.12042</guid>
<content:encoded><![CDATA[
<div> keywords: financial disclosures, retrieval framework, FinGEAR, FLAM, 10-K filings

Summary:
FinGEAR is a retrieval framework designed specifically for financial documents, addressing challenges such as complex regulatory language and document structure. It incorporates the finance lexicon for Item-level guidance (FLAM), dual hierarchical indices for within-Item search, and a two-stage cross-encoder reranker. This framework aligns retrieval with the structure and terminology of financial disclosures, resulting in improved precision, recall, F1 score, and relevancy compared to existing models. FinGEAR outperforms flat RAG models by up to 56.7%, graph-based RAGs by 12.5%, and prior tree-based systems by 217.6%. It also enhances downstream answer accuracy when used with a fixed reader. By combining section hierarchy and domain lexicon signals, FinGEAR enhances retrieval fidelity, making it a valuable tool for high-stakes financial analysis.

<br /><br />Summary: <div>
arXiv:2509.12042v1 Announce Type: new 
Abstract: Financial disclosures such as 10-K filings present challenging retrieval problems due to their length, regulatory section hierarchy, and domain-specific language, which standard retrieval-augmented generation (RAG) models underuse. We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a retrieval framework tailored to financial documents. FinGEAR combines a finance lexicon for Item-level guidance (FLAM), dual hierarchical indices for within-Item search (Summary Tree and Question Tree), and a two-stage cross-encoder reranker. This design aligns retrieval with disclosure structure and terminology, enabling fine-grained, query-aware context selection. Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR delivers consistent gains in precision, recall, F1, and relevancy, improving F1 by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over prior tree-based systems, while also increasing downstream answer accuracy with a fixed reader. By jointly modeling section hierarchy and domain lexicon signals, FinGEAR improves retrieval fidelity and provides a practical foundation for high-stakes financial analysis.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing MacPherson Suspension Architectures using Bayesian Optimization</title>
<link>https://arxiv.org/abs/2206.09022</link>
<guid>https://arxiv.org/abs/2206.09022</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, engineering design, compliance, target specifications, discipline model
Summary: 
The article introduces a Bayesian optimization system for enhancing the traditional manual engineering design process. By optimizing compliance with target specifications directly and without requiring gradient information, the system aims to automate and expedite the design process. The method focuses on computing a generalized inverse of a high-dimensional non-linear function, enabling efficient optimization in a scalable manner. Additionally, the proposed two-tier convergence criterion ensures either convergence to an optimal solution satisfying all specified design criteria or convergence to a minimum-norm solution. The system's effectiveness is demonstrated through a vehicle chassis design problem in an industry context, utilizing a commercial discipline model. The results highlight the system's general applicability, scalability, and efficiency, showcasing the straightforward implementation of novel convergence criteria within popular Bayesian optimization software packages. 
<br /><br />Summary: <div>
arXiv:2206.09022v1 Announce Type: cross 
Abstract: Engineering design is traditionally performed by hand: an expert makes design proposals based on past experience, and these proposals are then tested for compliance with certain target specifications. Testing for compliance is performed first by computer simulation using what is called a discipline model. Such a model can be implemented by a finite element analysis, multibody systems approach, etc. Designs passing this simulation are then considered for physical prototyping. The overall process may take months, and is a significant cost in practice. We have developed a Bayesian optimization system for partially automating this process by directly optimizing compliance with the target specification with respect to the design parameters. The proposed method is a general framework for computing a generalized inverse of a high-dimensional non-linear function that does not require e.g. gradient information, which is often unavailable from discipline models. We furthermore develop a two-tier convergence criterion based on (i) convergence to a solution optimally satisfying all specified design criteria, or (ii) convergence to a minimum-norm solution. We demonstrate the proposed approach on a vehicle chassis design problem motivated by an industry setting using a state-of-the-art commercial discipline model. We show that the proposed approach is general, scalable, and efficient, and that the novel convergence criteria can be implemented straightforwardly based on existing concepts and subroutines in popular Bayesian optimization software packages.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs</title>
<link>https://arxiv.org/abs/2509.10504</link>
<guid>https://arxiv.org/abs/2509.10504</guid>
<content:encoded><![CDATA[
<div> Reframe retrosynthesis planning, worst-path optimization, tree-structured Markov Decision Processes, Interactive Retrosynthesis Planning, self-imitation learning <br />
Summary: 
This paper proposes a new approach to retrosynthesis planning by treating it as a worst-path optimization problem within tree-structured Markov Decision Processes, ensuring a unique optimal solution and offering improvement guarantees. The Interactive Retrosynthesis Planning (InterRetro) method is introduced, which interacts with the tree MDP, learns a value function for worst-path outcomes, and improves its policy through self-imitation learning. Empirical results show that InterRetro achieves state-of-the-art performance by solving 100% of targets on the Retro*-190 benchmark, shortening synthetic routes by 4.9%, and exhibiting promising performance with only 10% of the training data. This approach represents a significant advancement in computational retrosynthesis planning. <br /><br />Summary: <div>
arXiv:2509.10504v1 Announce Type: cross 
Abstract: Retrosynthesis planning aims to decompose target molecules into available building blocks, forming a synthesis tree where each internal node represents an intermediate compound and each leaf ideally corresponds to a purchasable reactant. However, this tree becomes invalid if any leaf node is not a valid building block, making the planning process vulnerable to the "weakest link" in the synthetic route. Existing methods often optimise for average performance across branches, failing to account for this worst-case sensitivity. In this paper, we reframe retrosynthesis as a worst-path optimisation problem within tree-structured Markov Decision Processes (MDPs). We prove that this formulation admits a unique optimal solution and offers monotonic improvement guarantees. Building on this insight, we introduce Interactive Retrosynthesis Planning (InterRetro), a method that interacts with the tree MDP, learns a value function for worst-path outcomes, and improves its policy through self-imitation, preferentially reinforcing past decisions with high estimated advantage. Empirically, InterRetro achieves state-of-the-art results, solving 100% of targets on the Retro*-190 benchmark, shortening synthetic routes by 4.9%, and achieving promising performance using only 10% of the training data - representing a significant advance in computational retrosynthesis planning.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective</title>
<link>https://arxiv.org/abs/2509.10506</link>
<guid>https://arxiv.org/abs/2509.10506</guid>
<content:encoded><![CDATA[
<div> boosting, retail demand forecasting, interpretable learning, feature attention, supply chain management <br />
Summary: <br />
The article discusses the challenges faced in forecasting product demand in retail supply chains and introduces a new framework called AttnBoost. AttnBoost integrates feature-level attention into the boosting process to improve predictive accuracy and explainability in the presence of noisy and heterogeneous data. By dynamically adjusting feature importance through an attention mechanism, AttnBoost can focus on high-impact variables like promotions, pricing, and seasonal trends. The model outperforms traditional machine learning and deep tabular models on a large-scale retail sales dataset, providing valuable insights for supply chain managers. An ablation study confirms the effectiveness of the attention module in reducing overfitting and enhancing interpretability. The results highlight the potential of attention-guided boosting for scalable and interpretable AI in real-world forecasting applications. <div>
arXiv:2509.10506v1 Announce Type: cross 
Abstract: Forecasting product demand in retail supply chains presents a complex challenge due to noisy, heterogeneous features and rapidly shifting consumer behavior. While traditional gradient boosting decision trees (GBDT) offer strong predictive performance on structured data, they often lack adaptive mechanisms to identify and emphasize the most relevant features under changing conditions. In this work, we propose AttnBoost, an interpretable learning framework that integrates feature-level attention into the boosting process to enhance both predictive accuracy and explainability. Specifically, the model dynamically adjusts feature importance during each boosting round via a lightweight attention mechanism, allowing it to focus on high-impact variables such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a large-scale retail sales dataset and demonstrate that it outperforms standard machine learning and deep tabular models, while also providing actionable insights for supply chain managers. An ablation study confirms the utility of the attention module in mitigating overfitting and improving interpretability. Our results suggest that attention-guided boosting represents a promising direction for interpretable and scalable AI in real-world forecasting applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Temporal Fusion Transformers for Cryptocurrency Price Prediction</title>
<link>https://arxiv.org/abs/2509.10542</link>
<guid>https://arxiv.org/abs/2509.10542</guid>
<content:encoded><![CDATA[
<div> Transformers, short-term prediction, cryptocurrency market, adaptive modeling, price forecasting

Summary: 
This paper introduces an adaptive Temporal Fusion Transformer (TFT) approach for precise short-term price prediction in the cryptocurrency market. By leveraging dynamic subseries lengths and pattern-based categorization, the model addresses the market's non-stationary nature and extreme volatility. A novel segmentation method is proposed, where subseries end at relative maxima, capturing significant upward movements and filtering noise. The fixed-length pattern ending each subseries determines the category assigned to the subsequent variable-length subseries, allowing for specialized prediction models for each category. Experimental results on ETH-USDT 10-minute data demonstrate that the adaptive approach outperforms baseline models in prediction accuracy and simulated trading profitability. The combination of adaptive segmentation and pattern-conditioned forecasting enhances robust and responsive cryptocurrency price prediction. 

<br /><br />Summary: <div>
arXiv:2509.10542v1 Announce Type: cross 
Abstract: Precise short-term price prediction in the highly volatile cryptocurrency market is critical for informed trading strategies. Although Temporal Fusion Transformers (TFTs) have shown potential, their direct use often struggles in the face of the market's non-stationary nature and extreme volatility. This paper introduces an adaptive TFT modeling approach leveraging dynamic subseries lengths and pattern-based categorization to enhance short-term forecasting. We propose a novel segmentation method where subseries end at relative maxima, identified when the price increase from the preceding minimum surpasses a threshold, thus capturing significant upward movements, which act as key markers for the end of a growth phase, while potentially filtering the noise. Crucially, the fixed-length pattern ending each subseries determines the category assigned to the subsequent variable-length subseries, grouping typical market responses that follow similar preceding conditions. A distinct TFT model trained for each category is specialized in predicting the evolution of these subsequent subseries based on their initial steps after the preceding peak. Experimental results on ETH-USDT 10-minute data over a two-month test period demonstrate that our adaptive approach significantly outperforms baseline fixed-length TFT and LSTM models in prediction accuracy and simulated trading profitability. Our combination of adaptive segmentation and pattern-conditioned forecasting enables more robust and responsive cryptocurrency price prediction.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COVID-BLUeS -- A Prospective Study on the Value of AI in Lung Ultrasound Analysis</title>
<link>https://arxiv.org/abs/2509.10556</link>
<guid>https://arxiv.org/abs/2509.10556</guid>
<content:encoded><![CDATA[
<div> Keywords: lung ultrasound, Artificial intelligence, COVID-19, severity, multi-modal models<br />
Summary:<br />
In a study at Maastricht University Medical Centre, lung ultrasound (LUS) data from COVID-19 suspects were analyzed using Artificial intelligence (AI) models for detecting and assessing pulmonary infections. The severity of lung involvement in COVID-19 positive and negative patients was similar according to human annotators. AI models showed 65% accuracy in detecting COVID-19 without specific training, improving to 79% with targeted training. Multi-modal models combining images and CBC data outperformed image-only models. However, the performance of AI models was limited due to heterogeneous LUS datasets, frame-based processing ignoring video-level information, and a lack of focus on multi-modal models. The dataset used in the study has been made publicly available to aid future research efforts. <div>
arXiv:2509.10556v1 Announce Type: cross 
Abstract: As a lightweight and non-invasive imaging technique, lung ultrasound (LUS) has gained importance for assessing lung pathologies. The use of Artificial intelligence (AI) in medical decision support systems is promising due to the time- and expertise-intensive interpretation, however, due to the poor quality of existing data used for training AI models, their usability for real-world applications remains unclear. In a prospective study, we analyze data from 63 COVID-19 suspects (33 positive) collected at Maastricht University Medical Centre. Ultrasound recordings at six body locations were acquired following the BLUE protocol and manually labeled for severity of lung involvement. Several AI models were applied and trained for detection and severity of pulmonary infection. The severity of the lung infection, as assigned by human annotators based on the LUS videos, is not significantly different between COVID-19 positive and negative patients (p = 0.89). Nevertheless, the predictions of image-based AI models identify a COVID-19 infection with 65% accuracy when applied zero-shot (i.e., trained on other datasets), and up to 79% with targeted training, whereas the accuracy based on human annotations is at most 65%. Multi-modal models combining images and CBC improve significantly over image-only models. Although our analysis generally supports the value of AI in LUS assessment, the evaluated models fall short of the performance expected from previous work. We find this is due to 1) the heterogeneity of LUS datasets, limiting the generalization ability to new data, 2) the frame-based processing of AI models ignoring video-level information, and 3) lack of work on multi-modal models that can extract the most relevant information from video-, image- and variable-based inputs. To aid future research, we publish the dataset at: https://github.com/NinaWie/COVID-BLUES.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations</title>
<link>https://arxiv.org/abs/2509.10659</link>
<guid>https://arxiv.org/abs/2509.10659</guid>
<content:encoded><![CDATA[
<div> segmentation, hierarchical network, mesh-based, graph neural networks, PDE simulations

Summary:
M4GN is a hierarchical network for mesh-based graph neural networks, addressing challenges of high cost and over-smoothing on large meshes. It utilizes a hybrid segmentation strategy to create contiguous segments of nodes that respect mesh topology and geometry. The segments are encoded by a permutation-invariant aggregator to capture local dynamics efficiently. M4GN incorporates a micro-level GNN and a macro-level transformer to balance accuracy and efficiency. Results show an improvement in prediction accuracy of up to 56% with up to 22% faster inference compared to state-of-the-art baselines. <div>
arXiv:2509.10659v1 Announce Type: cross 
Abstract: Mesh-based graph neural networks (GNNs) have become effective surrogates for PDE simulations, yet their deep message passing incurs high cost and over-smoothing on large, long-range meshes; hierarchical GNNs shorten propagation paths but still face two key obstacles: (i) building coarse graphs that respect mesh topology, geometry, and physical discontinuities, and (ii) maintaining fine-scale accuracy without sacrificing the speed gained from coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric hierarchical network. M4GN begins with a hybrid segmentation strategy that pairs a fast graph partitioner with a superpixel-style refinement guided by modal-decomposition features, producing contiguous segments of dynamically consistent nodes. These segments are encoded by a permutation-invariant aggregator, avoiding the order sensitivity and quadratic cost of aggregation approaches used in prior works. The resulting information bridges a micro-level GNN, which captures local dynamics, and a macro-level transformer that reasons efficiently across segments, achieving a principled balance between accuracy and efficiency. Evaluated on multiple representative benchmark datasets, M4GN improves prediction accuracy by up to 56% while achieving up to 22% faster inference than state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data</title>
<link>https://arxiv.org/abs/2509.11053</link>
<guid>https://arxiv.org/abs/2509.11053</guid>
<content:encoded><![CDATA[
<div> conditional consistent latent representation, generative adversarial network, contrastive learning, 1D fourier convolution neural network, bearing fault diagnosis

Summary:<br />
- The article proposes a novel framework, DAC-FCF, for bearing fault diagnosis using limited data by addressing data scarcity and model limitations.
- It introduces a Conditional Consistent Latent Representation Generative Adversarial Network (CCLR-GAN) to generate diverse data and a contrastive learning mechanism for better modeling relationships between training samples.
- The framework utilizes a 1D Fourier Convolution Neural Network (1D-FCNN) to extract global features from complex vibration signals.
- Experimental results on the CWRU dataset and a self-collected test bench show that DAC-FCF outperforms baselines by up to 32% and 10% respectively.
- Ablation experiments confirm the effectiveness of the proposed components, demonstrating the promising potential of DAC-FCF for bearing fault diagnosis under limited data.<br /><br />Summary: <div>
arXiv:2509.11053v1 Announce Type: cross 
Abstract: In the area of bearing fault diagnosis, deep learning (DL) methods have been widely used recently. However, due to the high cost or privacy concerns, high-quality labeled data are scarce in real world scenarios. While few-shot learning has shown promise in addressing data scarcity, existing methods still face significant limitations in this domain. Traditional data augmentation techniques often suffer from mode collapse and generate low-quality samples that fail to capture the diversity of bearing fault patterns. Moreover, conventional convolutional neural networks (CNNs) with local receptive fields makes them inadequate for extracting global features from complex vibration signals. Additionally, existing methods fail to model the intricate relationships between limited training samples. To solve these problems, we propose an advanced data augmentation and contrastive fourier convolution framework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, a novel conditional consistent latent representation and reconstruction generative adversarial network (CCLR-GAN) is proposed to generate more diverse data. Secondly, a contrastive learning based joint optimization mechanism is utilized to better model the relations between the available training data. Finally, we propose a 1D fourier convolution neural network (1D-FCNN) to achieve a global-aware of the input data. Experiments demonstrate that DAC-FCF achieves significant improvements, outperforming baselines by up to 32\% on case western reserve university (CWRU) dataset and 10\% on a self-collected test bench. Extensive ablation experiments prove the effectiveness of the proposed components. Thus, the proposed DAC-FCF offers a promising solution for bearing fault diagnosis under limited data.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations</title>
<link>https://arxiv.org/abs/2509.11376</link>
<guid>https://arxiv.org/abs/2509.11376</guid>
<content:encoded><![CDATA[
<div> framework, multimodal data fusion, reservoir analysis, AI models, operational efficiency

Summary:
The study presents an integrated framework utilizing cutting-edge AI models and advanced prompt engineering techniques for reservoir management in the petroleum industry. The framework combines large language models with multimodal data fusion to analyze seismic interpretations, well logs, and production data for real-time decision support. Field validation across 15 reservoir environments shows high accuracy in reservoir characterization, production forecasting, and well placement optimization. The system achieves rapid response times and high safety reliability with significant cost reductions compared to traditional methods. Few-shot learning and prompt optimization improve field adaptation time and reasoning quality. Real-time data processing includes anomaly detection and reduces environmental incidents. The research highlights the practical integration of AI technologies with domain expertise to enhance operational efficiency, safety, and economic performance. 

<br /><br />Summary: <div>
arXiv:2509.11376v1 Announce Type: cross 
Abstract: The petroleum industry faces unprecedented challenges in reservoir management, requiring rapid integration of complex multimodal datasets for real-time decision support. This study presents a novel integrated framework combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data fusion for comprehensive reservoir analysis. The framework implements domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum engineering documents, chain-of-thought reasoning, and few-shot learning for rapid field adaptation. Multimodal integration processes seismic interpretations, well logs, and production data through specialized AI models with vision transformers. Field validation across 15 diverse reservoir environments demonstrates exceptional performance: 94.2% reservoir characterization accuracy, 87.6% production forecasting precision, and 91.4% well placement optimization success rate. The system achieves sub-second response times while maintaining 96.2% safety reliability with no high-risk incidents during evaluation. Economic analysis reveals 62-78% cost reductions (mean 72%) relative to traditional methods with 8-month payback period. Few-shot learning reduces field adaptation time by 72%, while automated prompt optimization achieves 89% improvement in reasoning quality. The framework processed real-time data streams with 96.2% anomaly detection accuracy and reduced environmental incidents by 45%. We provide detailed experimental protocols, baseline comparisons, ablation studies, and statistical significance testing to ensure reproducibility. This research demonstrates practical integration of cutting-edge AI technologies with petroleum domain expertise for enhanced operational efficiency, safety, and economic performance.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.11420</link>
<guid>https://arxiv.org/abs/2509.11420</guid>
<content:encoded><![CDATA[
<div> Trading-R1, reasoning LLMs, financial decision making, risk-sensitive, supervised fine-tuning<br />
<br />
Summary: 
Trading-R1 is a financially-aware model designed to improve reasoning and decision-making in finance, addressing the need for interpretable and trustworthy AI in the market. It incorporates strategic thinking and planning, aligned with trading principles, through a comprehensive thesis composition and volatility-adjusted decision-making process. The model is trained on a diverse dataset and shows improved risk-adjusted returns and lower drawdowns compared to other models. By generating evidence-based investment theses, Trading-R1 supports structured and interpretable trading decisions. The system's approach combines supervised fine-tuning and reinforcement learning, following a three-stage curriculum. This innovative model fills a gap in applying reasoning LLMs to risk-sensitive financial tasks, providing a valuable tool for traders and analysts. Trading-R1 Terminal is available for access on GitHub, offering a practical application for financial professionals. <br /><br /> <div>
arXiv:2509.11420v1 Announce Type: cross 
Abstract: Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. We present Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. Trading-R1 Terminal will be released at https://github.com/TauricResearch/Trading-R1.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Requirements for Early Quantum Advantage and Quantum Utility in the Capacitated Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2509.11469</link>
<guid>https://arxiv.org/abs/2509.11469</guid>
<content:encoded><![CDATA[
<div> framework, Capacitated Vehicle Routing Problem, early quantum advantage, NISQ hardware, encoding <br />
Summary: 
This study introduces a framework for assessing the potential for early quantum advantage in solving the Capacitated Vehicle Routing Problem (CVRP). The analysis suggests that achieving quantum advantage on noisy intermediate scale quantum (NISQ) hardware is unlikely, even with the most qubit-efficient encoding methods. Through closed-form resource calculations and device benchmarks, key figures of merit such as the quantum feasibility point, qubit-feasibility line, and gate-feasibility line are identified to evaluate the feasibility of solving CVRP on quantum devices. A comparison of direct QUBO mapping and a space-efficient HOBO encoding reveals significant differences in resource requirements. The framework highlights that CVRP instances might need innovative problem decomposition techniques to leverage quantum devices effectively. Additionally, benchmarking on early-advantage instances like Golden-5 shows that HOBO circuits outperform QUBO encodings in terms of qubit utilization. <div>
arXiv:2509.11469v1 Announce Type: cross 
Abstract: We introduce a transparent, encoding-agnostic framework for determining when the Capacitated Vehicle Routing Problem (CVRP) can achieve early quantum advantage. Our analysis shows this is unlikely on noisy intermediate scale quantum (NISQ) hardware even in best case scenarios that use the most qubit-efficient direct encodings. Closed-form resource counts, combined with recent device benchmarks, yield three decisive go/no-go figures of merit: the quantum feasibility point and the qubit- and gate-feasibility lines, which place any CVRP instance on a single decision diagram. Contrasting a direct QUBO mapping with a space-efficient higher-order (HOBO) encoding reveals a large gap. Applied to early-advantage benchmarks such as Golden-5, our diagram shows that HOBO circuits require only 7,685 qubits, whereas comparable QUBO encodings still exceed 200,000 qubits. In addition to identifying candidate instances for early quantum advantage in CVRP, the framework provides a unifying go/no-go metric that ingests any CVRP encoding together with any hardware profile and highlights when quantum devices could challenge classical heuristics. Quantum advantage in CVRP would likely require innovative problem decomposition techniques.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions</title>
<link>https://arxiv.org/abs/2509.11595</link>
<guid>https://arxiv.org/abs/2509.11595</guid>
<content:encoded><![CDATA[
<div> transaction datasets, AML research, AMLNet, synthetic transactions, detection ensemble<br />
<br />
Summary: A new framework called AMLNet has been introduced to address the lack of publicly shareable transaction datasets for anti-money laundering (AML) research. AMLNet consists of a regulation-aware transaction generator producing over 1 million synthetic transactions covering various money laundering phases and typologies. The generated transactions have a high regulatory alignment and technical fidelity score. The detection ensemble within AMLNet achieves high performance on internal test partitions and shows adaptability to external datasets. The framework enables multi-dimensional evaluation and the release of the dataset for reproducible and regulation-conscious AML experimentation. <div>
arXiv:2509.11595v1 Announce Type: cross 
Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly shareable, regulation-aligned transaction datasets. We present AMLNet, a knowledge-based multi-agent framework with two coordinated units: a regulation-aware transaction generator and an ensemble detection pipeline. The generator produces 1,090,173 synthetic transactions (approximately 0.16\% laundering-positive) spanning core laundering phases (placement, layering, integration) and advanced typologies (e.g., structuring, adaptive threshold behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage (Section 4.2), while a composite technical fidelity score of 0.75 summarizes temporal, structural, and behavioral realism components (Section 4.4). The detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the internal test partitions of AMLNet and adapts to the external SynthAML dataset, indicating architectural generalizability across different synthetic generation paradigms. We provide multi-dimensional evaluation (regulatory, temporal, network, behavioral) and release the dataset (Version 1.0, https://doi.org/10.5281/zenodo.16736515), to advance reproducible and regulation-conscious AML experimentation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing</title>
<link>https://arxiv.org/abs/2402.16445</link>
<guid>https://arxiv.org/abs/2402.16445</guid>
<content:encoded><![CDATA[
<div> Keywords: Protein Language Models, Protein Engineering, ProLLaMA, Evolutionary Protein Generation Framework, Multitask Model

Summary:
ProLLaMA is a multitask protein language model that addresses the limitations of current Protein Language Models in both Protein Language Understanding and Generation. It is enhanced by the Evolutionary Protein Generation Framework (EPGF) and trained on a comprehensive dataset with superfamily annotations. ProLLaMA excels in both unconditional and controllable protein generation tasks, displaying superior structural quality metrics. It also demonstrates strong understanding capabilities with a high exact match rate in superfamily prediction. The EPGF significantly improves the biological viability of generated sequences, as shown by enhanced biophysical scores and structural metrics. This model bridges the gap between PLU and PLG, boosting progress in protein engineering. The project code is available on GitHub for further exploration and development. <br /><br />Summary: <div>
arXiv:2402.16445v3 Announce Type: replace 
Abstract: Recent advances in Protein Language Models (PLMs) have transformed protein engineering, yet unlike their counterparts in Natural Language Processing (NLP), current PLMs exhibit a fundamental limitation: they excel in either Protein Language Understanding (PLU) or Protein Language Generation (PLG), but rarely both. This fragmentation hinders progress in protein engineering. To bridge this gap, we introduce ProLLaMA, a multitask protein language model enhanced by the Evolutionary Protein Generation Framework (EPGF). We construct a comprehensive instruction dataset containing approximately 13 million samples with over 11,000 superfamily annotations to facilitate better modeling of sequence-function landscapes. We leverage a two-stage training approach to develop ProLLaMA, a multitask LLM with protein domain expertise. Our EPGF addresses the mismatch between statistic language modeling and biological constraints through three innovations: a multi-dimensional interpretable scorer, hierarchical efficient decoding, and a probabilistic-biophysical joint selection mechanism. Extensive experiments demonstrate that ProLLaMA excels in both unconditional and controllable protein generation tasks, achieving superior structural quality metrics compared to existing PLMs. Additionally, ProLLaMA demonstrates strong understanding capabilities with a 67.1% exact match rate in superfamily prediction. EPGF significantly enhances the biological viability of generated sequences, as evidenced by improved biophysical scores (+4.3%) and structural metrics (+14.5%). The project is available at https://github.com/PKU-YuanGroup/ProLLaMA.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Machine Learning Models for Predicting the Next Targets of Activist Funds</title>
<link>https://arxiv.org/abs/2404.16169</link>
<guid>https://arxiv.org/abs/2404.16169</guid>
<content:encoded><![CDATA[
<div> activist investment, predictive model, machine learning, Shapley value, corporate governance<br />
<br />
Summary: This research introduces a predictive model to identify potential targets of activist investment funds, essential for companies to reduce intervention risks, activist funds to make optimal investments, and investors to capitalize on stock price gains. Evaluating 123 model configurations using data from the Russell 3000 index, the best model achieved an AUC-ROC of 0.782, showcasing its ability to predict activist fund targets effectively. The Shapley value method was employed to determine key factors influencing a company's likelihood of being targeted, shedding light on the dynamic mechanisms behind activist fund target selection. These findings provide valuable insights for proactive corporate governance and informed investment strategies, contributing to a deeper understanding of the factors driving activist investment decisions. <br /><br /> <div>
arXiv:2404.16169v3 Announce Type: replace 
Abstract: This research presents a predictive model to identify potential targets of activist investment funds--entities that acquire significant corporate stakes to influence strategic and operational decisions, ultimately enhancing shareholder value. Predicting such targets is crucial for companies aiming to mitigate intervention risks, activist funds seeking optimal investments, and investors looking to leverage potential stock price gains. Using data from the Russell 3000 index from 2016 to 2022, we evaluated 123 model configurations incorporating diverse imputation, oversampling, and machine learning techniques. Our best model achieved an AUC-ROC of 0.782, demonstrating its capability to effectively predict activist fund targets. To enhance interpretability, we employed the Shapley value method to identify key factors influencing a company's likelihood of being targeted, highlighting the dynamic mechanisms underlying activist fund target selection. These insights offer a powerful tool for proactive corporate governance and informed investment strategies, advancing understanding of the mechanisms driving activist investment decisions.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plastic Arbor: a modern simulation framework for synaptic plasticity -- from single synapses to networks of morphological neurons</title>
<link>https://arxiv.org/abs/2411.16445</link>
<guid>https://arxiv.org/abs/2411.16445</guid>
<content:encoded><![CDATA[
<div> Keywords: Arbor, neuronal networks, synaptic plasticity, computational modeling, dendritic structures 

Summary: 
Arbor is a software library for simulating large-scale networks of biological neurons with detailed morphological structures. It supports multi-core CPU and GPU systems, combining customizable neuronal and synaptic mechanisms for efficient simulation. The library has been extended to model a variety of spike-driven plasticity paradigms, from single-synapse dynamics to large recurrent networks. By comparing with other simulators, it is shown that Arbor allows simulating plastic networks of multi-compartment neurons at minimal runtime cost. The extension also demonstrates high efficiency in terms of runtime and memory usage. Using this framework, the impact of dendritic structures on network dynamics over hours is investigated, revealing a relationship between dendritic tree length and information storage efficiency. This extended Arbor framework provides a valuable tool for future studies on the influence of synaptic plasticity in large networks, particularly in conjunction with neuronal morphology.<br /><br />Summary: <div>
arXiv:2411.16445v3 Announce Type: replace 
Abstract: Arbor is a software library designed for efficient simulation of large-scale networks of biological neurons with detailed morphological structures. It combines customizable neuronal and synaptic mechanisms with high-performance computing, supporting multi-core CPU and GPU systems. In humans and other animals, synaptic plasticity processes play a vital role in cognitive functions, including learning and memory. Recent studies have shown that intracellular molecular processes in dendrites significantly influence single-neuron dynamics. However, for understanding how the complex interplay between dendrites and synaptic processes influences network dynamics, computational modeling is required.
  To enable the modeling of large-scale networks of morphologically detailed neurons with diverse plasticity processes, we have extended the Arbor library to support simulations of a large variety of spike-driven plasticity paradigms. To showcase the features of the extended framework, we present examples of computational models, beginning with single-synapse dynamics, progressing to multi-synapse rules, and finally scaling up to large recurrent networks. While cross-validating our implementations by comparison with other simulators, we show that Arbor allows simulating plastic networks of multi-compartment neurons at nearly no additional cost in runtime compared to point-neuron simulations. In addition, we demonstrate that Arbor is highly efficient in terms of runtime and memory use as compared to other simulators. Using the extended framework, as an example, we investigate the impact of dendritic structures on network dynamics across a timescale of several hours, finding a relation between the length of dendritic trees and the ability of the network to efficiently store information. By our extension of Arbor, we aim to provide a valuable tool that will support future studies on the impact of synaptic plasticity, especially, in conjunction with neuronal morphology, in large networks.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superstructure Optimization with Embedded Neural Networks for Sustainable Aviation Fuel Production</title>
<link>https://arxiv.org/abs/2509.09796</link>
<guid>https://arxiv.org/abs/2509.09796</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-objective optimization, sustainable aviation fuel, artificial neural networks, Fischer-Tropsch kerosene production, carbon emissions constraints

Summary: 
This study introduces a novel framework for sustainable aviation fuel (SAF) production that combines artificial neural networks (ANNs) with mathematical optimization techniques. By integrating ANNs into a mixed-integer quadratically constrained programming (MIQCP) formulation, the framework allows for simultaneous optimization of discrete process choices and continuous operating parameters. The application of this framework to Fischer-Tropsch kerosene production reveals that configurations minimizing costs under unconstrained CO2 emissions favor fossil-based autothermal reforming (ATR). However, imposing carbon emission constraints leads to the integration of biomass gasification and direct air capture coupled with carbon sequestration (DAC-CS) for reduced net emissions at higher production costs. Hybrid configurations with flexible process parameters, enabled by embedded ANNs, outperform fixed setups and achieve cost savings of up to 20%. Sensitivity analyses show the impact of process conditions on economic and environmental performance, emphasizing the importance of process adaptability in SAF production. 

<br /><br />Summary: <div>
arXiv:2509.09796v1 Announce Type: new 
Abstract: This study presents a multi-objective optimization framework for sustainable aviation fuel (SAF) production, integrating artificial neural networks (ANNs) within a mixed-integer quadratically constrained programming (MIQCP) formulation. By embedding data-driven surrogate models into the mathematical optimization structure, the proposed methodology addresses key limitations of conventional superstructure-based approaches, enabling simultaneous optimization of discrete process choices and continuous operating parameters. The framework captures variable input and output stream compositions, facilitating the joint optimization of target product composition and system design. Application to Fischer-Tropsch (FT) kerosene production demonstrates that cost-minimizing configurations under unconstrained CO2 emissions are dominated by the fossil-based autothermal reforming (ATR) route. Imposing carbon emission constraints necessitates the integration of biomass gasification and direct air capture coupled with carbon sequestration (DAC-CS), resulting in substantially reduced net emissions but higher production costs. At the zero-emission limit, hybrid configurations combining ATR and biomass gasification achieve the lowest costs (~2.38 \$/kg-kerosene), followed closely by biomass gasification-only (~2.43 \$/kg), both of which outperform the ATR-only pathway with DAC-CS (~2.65 \$/kg). In contrast, DAC-only systems relying exclusively on atmospheric CO2 and water electrolysis are prohibitively expensive (~10.8 \$/kg). The results highlight the critical role of process adaptability: configurations exploiting flexible process parameters, facilitated by embedded ANNs, consistently outperform fixed setups, achieving up to 20% cost savings. Sensitivity analyses elucidate the influence of process conditions, such as FT reactor pressure and gasification temperature, on economic and environmental performance.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fraud detection and risk assessment of online payment transactions on e-commerce platforms based on LLM and GCN frameworks</title>
<link>https://arxiv.org/abs/2509.09928</link>
<guid>https://arxiv.org/abs/2509.09928</guid>
<content:encoded><![CDATA[
<div> Keywords: e-commerce, fraud detection, Large Language Models, Graph Convolutional Networks, online payment

Summary: 
This study presents a novel fraud detection framework for e-commerce online payment transactions that combines Large Language Models (LLM) with Graph Convolutional Networks (GCN). A dataset of 2,840,000 transactions involving consumers and merchants was used to train the model, addressing the imbalanced nature of fraud instances in the data. The model represents consumers and merchants as nodes and transactions as edges in a heterogeneous graph, enabling the GCN to learn complex behavioral patterns. By integrating semantic features from GPT-4o and Tabformer with structural features, the model achieves an accuracy of 0.98 in fraud detection, balancing precision and sensitivity effectively. This framework offers a scalable, real-time solution for securing online payment environments and demonstrates the potential of graph-based deep learning in financial fraud prevention.

<br /><br />Summary: <div>
arXiv:2509.09928v1 Announce Type: new 
Abstract: With the rapid growth of e-commerce, online payment fraud has become increasingly complex, posing serious threats to financial security and consumer trust. Traditional detection methods often struggle to capture the intricate relational structures inherent in transactional data. This study presents a novel fraud detection framework that combines Large Language Models (LLM) with Graph Convolutional Networks (GCN) to effectively identify fraudulent activities in e-commerce online payment transactions. A dataset of 2,840,000 transactions was collected over 14 days from major platforms such as Amazon, involving approximately 2,000 U.S.-based consumers and 30 merchants. With fewer than 6000 fraudulent instances, the dataset represents a highly imbalanced scenario. Consumers and merchants were modeled as nodes and transactions as edges to form a heterogeneous graph, upon which a GCN was applied to learn complex behavioral patterns. Semantic features extracted via GPT-4o and Tabformer were integrated with structural features to enhance detection performance. Experimental results demonstrate that the proposed model achieves an accuracy of 0.98, effectively balancing precision and sensitivity in fraud detection. This framework offers a scalable and real-time solution for securing online payment environments and provides a promising direction for applying graph-based deep learning in financial fraud prevention.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading</title>
<link>https://arxiv.org/abs/2509.09995</link>
<guid>https://arxiv.org/abs/2509.09995</guid>
<content:encoded><![CDATA[
<div> Language Models, Financial Reasoning, High-Frequency Trading, Multi-Agent Framework, Algorithmic Trading 
<br />
Summary:
QuantAgent is a novel multi-agent Large Language Model (LLM) framework designed specifically for high-frequency algorithmic trading. It consists of four specialized agents - Indicator, Pattern, Trend, and Risk - each equipped with tools and reasoning capabilities tailored for short-term market dynamics. In evaluations across various financial instruments, QuantAgent outperformed neural and rule-based baselines in terms of predictive accuracy and cumulative return over 4-hour trading intervals. By combining structured financial priors with language-native reasoning, QuantAgent demonstrates the potential for creating real-time decision systems in high-frequency financial markets. <div>
arXiv:2509.09995v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated impressive capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks, leveraging fundamental and sentiment-based inputs for strategic decision-making. However, such systems are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT requires rapid, risk-aware decisions based on structured, short-horizon signals, including technical indicators, chart patterns, and trend-based features, distinct from the long-term semantic reasoning typical of traditional financial LLM applications. To this end, we introduce QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents, Indicator, Pattern, Trend, and Risk, each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. In zero-shot evaluations across ten financial instruments, including Bitcoin and Nasdaq futures, QuantAgent demonstrates superior performance in both predictive accuracy and cumulative return over 4-hour trading intervals, outperforming strong neural and rule-based baselines. Our findings suggest that combining structured financial priors with language-native reasoning unlocks new potential for traceable, real-time decision systems in high-frequency financial markets.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Freight Rail Electrification: A Framework for Charge Station Selection and Battery Charge/Swap Scheduling</title>
<link>https://arxiv.org/abs/2509.10157</link>
<guid>https://arxiv.org/abs/2509.10157</guid>
<content:encoded><![CDATA[
<div> Keywords: battery electric freight trains, charging infrastructure, charge scheduling, optimization, algorithms <br />
Summary: <br />
Battery electric freight trains play a vital role in achieving decarbonization goals through zero-emission transportation options. The study focuses on developing an efficient strategy for the adoption of battery electric freight trains, involving the optimal design of charging infrastructure and charge scheduling for each train. The model allows for flexibility by enabling batteries to be either charged or swapped at deployed stations, with each train able to carry multiple batteries. The problem is formulated as a mixed integer linear programming model. Three algorithms are proposed to solve the optimization problem, including a Rectangle Piecewise Linear Approximation technique, a Fixed Algorithm heuristic, and a Benders Decomposition algorithm. Computational experiments show that the Benders Decomposition algorithm outperforms the other two algorithms in terms of objective function value, with the Rectangle Piecewise Linear Approximation technique closely following. The Fixed Algorithm provides the least optimal solution. <div>
arXiv:2509.10157v1 Announce Type: new 
Abstract: Battery electric freight trains are crucial for decarbonization by providing zero-emission transportation alternatives. The proper adoption of battery electric freight trains depends on an efficient battery electrification strategy, involving both infrastructure setup and charge scheduling. The study presents a comprehensive model for the optimal design of charging infrastructure and charge scheduling for each train. To provide more refueling flexibility, we allow batteries to be either charged or swapped in a deployed station, and each train can carry multiple batteries. This problem is formulated as a mixed integer linear programming model. To obtain real-time solutions for a large scale network, we develop three algorithms to solve the optimization problem: (1) a Rectangle Piecewise Linear Approximation technique, (2) a Fixed Algorithm heuristic, and (3) Benders Decomposition algorithm. In computational experiments, we use the three proposed algorithms to solve instances with up to 25 stations. Statistical analysis verifies that Benders Decomposition outperforms the other two algorithms with respect to the objective function value, closely followed by the Rectangle Piecewise Linear Approximation technique, and the Fixed Algorithm provides the least optimal solution.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Flow Separation Control Strategies in 3D Wings via Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.10185</link>
<guid>https://arxiv.org/abs/2509.10185</guid>
<content:encoded><![CDATA[
<div> keywords: deep reinforcement learning, active flow control, SD7003 wing, aerodynamic efficiency, turbulence <br />
Summary:
In this study, deep reinforcement learning (DRL) is utilized to optimize active flow control (AFC) on a three-dimensional SD7003 wing at specific aerodynamic conditions. The uncontrolled baseline case exhibits significant flow separation and a turbulent wake. Through the use of a GPU-accelerated CFD solver and multi-agent training, DRL successfully identifies control strategies that significantly enhance lift (79%), reduce drag (65%), and improve aerodynamic efficiency (408%). Flow visualizations validate the reattachment of the separated shear layer, highlighting the potential of DRL in tackling complex and turbulent flows. This research showcases the effectiveness of DRL in improving aerodynamic performance by discovering optimized control strategies in challenging flow conditions. <br /><br />Summary: <div>
arXiv:2509.10185v1 Announce Type: new 
Abstract: In this work, deep reinforcement learning (DRL) is applied to active flow control (AFC) over a threedimensional SD7003 wing at a Reynolds number of Re = 60,000 and angle of attack of AoA = 14 degrees. In the uncontrolled baseline case, the flow exhibits massive separation and a fully turbulent wake. Using a GPU-accelerated CFD solver and multi-agent training, DRL discovers control strategies that enhance lift (79%), reduce drag (65%), and improve aerodynamic efficiency (408%). Flow visualizations confirm reattachment of the separated shear layer, demonstrating the potential of DRL for complex and turbulent flows.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Active Flow Control around a Three-Dimensional Flow-Separated Wing at Re = 1,000</title>
<link>https://arxiv.org/abs/2509.10195</link>
<guid>https://arxiv.org/abs/2509.10195</guid>
<content:encoded><![CDATA[
<div> Keywords: deep reinforcement learning, active flow control, NACA0012 wing, computational fluid dynamics, aerodynamic performance

Summary:<br /><br />
This study investigates the application of deep reinforcement learning (DRL) for active flow control (AFC) to mitigate flow separation on wings at high angles of attack. The DRL agent autonomously adjusts the flow over a three-dimensional NACA0012 wing section at Re = 1,000 and AoA = 20 degrees by analyzing real-time flow data and utilizing a reward function focused on enhancing aerodynamic performance. By integrating the GPU-accelerated computational fluid dynamics (CFD) solver SOD2D with the TF-Agents DRL library through a Redis in-memory database, the framework enables efficient training. This research showcases the potential of DRL in addressing intricate aerodynamic challenges and pushing the boundaries of conventional AFC techniques. The study underscores the effectiveness of DRL in optimizing control actions to improve aerodynamic performance on wing sections, thereby advancing the field of flow control in aerodynamics. <div>
arXiv:2509.10195v1 Announce Type: new 
Abstract: This study explores the use of deep reinforcement learning (DRL) for active flow control (AFC) to reduce flow separation on wings at high angles of attack. Concretely, here the DRL agent controls the flow over the three-dimensional NACA0012 wing section at the Reynolds number Re = 1,000 and angle of attack AoA = 20 degrees, autonomously identifying optimal control actions through real-time flow data and a reward function focused on improving aerodynamic performance. The framework integrates the GPU-accelerated computational fluid dynamics (CFD) solver SOD2D with the TF-Agents DRL library via a Redis in-memory database, enabling rapid training. This work builds on previous DRL flow-control studies, demonstrating DRL potential to address complex aerodynamic challenges and push the boundaries of traditional AFC methods.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TubeBEND: A Real-World Dataset for Geometry Prediction in Rotary Draw Bending</title>
<link>https://arxiv.org/abs/2509.10272</link>
<guid>https://arxiv.org/abs/2509.10272</guid>
<content:encoded><![CDATA[
<div> Dataset, TubeBEND, rotary tube bending processes, machine learning, signal analysis<br />
<br />
Summary: <br />
This paper introduces TubeBEND, a dataset consisting of 318 rotary tube bending processes curated by experts. The dataset aims to solve the industrial challenge of predicting the geometry of a first-stage bend to optimize machine clamping molds for the second-stage bend in two-stage rotary draw bending. It includes criteria such as final bent angle and cross-sectional deformation of the tube. The dataset enables the development and testing of machine learning models to predict tube geometry, aiding machine operators in optimizing springback and deformation. By recording process parameters like tool movements and forces, the dataset provides detailed information on their effects on tube geometry. The goal is to explore solutions that leverage experimental process variables in machine learning algorithms to replace traditional trial-and-error or simulation-based methods. The dataset is publicly available for further research and improvement of data-driven approaches in the domain. <div>
arXiv:2509.10272v1 Announce Type: new 
Abstract: This paper presents TubeBEND, a real-world dataset comprising 318 rotary tube bending processes, which were collected and sorted by experts from various fields to evaluate machine learning and signal analysis methods. The dataset addresses the industrial challenge of predicting the geometry of a first-stage bend, which can be beneficial for designing machine clamping molds for the second-stage bend in two-stage rotary draw bending. Some geometry criteria, such as the tube's final bent angle (or springback) and its cross-sectional deformation, are being recorded in this dataset. This dataset gives us the possibility to build and test machine learning models that can predict the geometry and help the machine operators with a better machine setup to optimize the tube's springback and deformation. Moreover, by recording some process parameters, such as tool movements and forces or torques applied to them, we deliver detailed information about their impacts on the final tube geometry. The focus of our work is to discover solutions that can replace traditional methods, such as trial-and-error or simulation-based predictions, by including experimental process variables in ML algorithms. Our dataset is publicly available at https://github.com/zeyneddinoz/tubebend and https://zenodo.org/records/16614082 as a benchmark to improve data-driven methods in this field.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs</title>
<link>https://arxiv.org/abs/2509.09727</link>
<guid>https://arxiv.org/abs/2509.09727</guid>
<content:encoded><![CDATA[
<div> framework, financial education, question answering, multi-agent, domain-specific 

Summary: 
The article introduces a multi-agent framework for financial question answering in education. Existing large language models often struggle with the nuanced reasoning required in finance. The proposed framework includes a Base Generator, Evidence Retriever, and Expert Reviewer agent to enhance domain-specific QA. By leveraging retrieval-augmented generation and prompting strategies, the framework improves answer accuracy by 6.6-8.3% over baselines. The Gemini-2.0-Flash model shows the highest performance. Additionally, the method enables GPT-4o-mini to achieve performance comparable to FinGPT-mt_Llama3-8B_LoRA. The results suggest a cost-effective approach to enhancing financial QA and provide insights for future research in multi-agent financial language model systems. 

<br /><br />Summary: <div>
arXiv:2509.09727v1 Announce Type: cross 
Abstract: Question answering (QA) plays a central role in financial education, yet existing large language model (LLM) approaches often fail to capture the nuanced and specialized reasoning required for financial problem-solving. The financial domain demands multistep quantitative reasoning, familiarity with domain-specific terminology, and comprehension of real-world scenarios. We present a multi-agent framework that leverages role-based prompting to enhance performance on domain-specific QA. Our framework comprises a Base Generator, an Evidence Retriever, and an Expert Reviewer agent that work in a single-pass iteration to produce a refined answer. We evaluated our framework on a set of 3,532 expert-designed finance education questions from Study.com, an online learning platform. We leverage retrieval-augmented generation (RAG) for contextual evidence from 6 finance textbooks and prompting strategies for a domain-expert reviewer. Our experiments indicate that critique-based refinement improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines, with the highest performance from Gemini-2.0-Flash. Furthermore, our method enables GPT-4o-mini to achieve performance comparable to the finance-tuned FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to enhancing financial QA and offer insights for further research in multi-agent financial LLM systems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing understanding and clinical applications of cerebral autoregulation: A novel integrated numerical framework</title>
<link>https://arxiv.org/abs/2509.10295</link>
<guid>https://arxiv.org/abs/2509.10295</guid>
<content:encoded><![CDATA[
<div> cerebral autoregulation, numerical algorithm, cerebral blood flow, hemodynamic parameters, personalized model 
Summary: 
This study introduces a novel numerical algorithm that incorporates key factors driving cerebral autoregulation (CA) to regulate cerebral blood flow (CBF). The algorithm utilizes partial and ordinary differential equations to capture spatial and temporal distributions of arterial pressure, oxygen, and carbon dioxide levels in the cerebral vasculature. Validation with two datasets confirms its reliability in simulating the regulatory effects of CA on CBF under various physiological conditions. By integrating with a personalized multi-dimensional model, this framework enhances our understanding of CA and offers potential for developing hemodynamic-based therapeutic strategies for cerebrovascular disorders. <div>
arXiv:2509.10295v1 Announce Type: cross 
Abstract: Cerebral autoregulation (CA) is a fundamental mechanism that modulates cerebrovascular resistance, primarily by regulating the diameter of small cerebral vessels to maintain stable cerebral blood flow (CBF) in response to fluctuations in systemic arterial pressure. However, the clinical understanding of CA remains limited due to the intricate structure of the cerebral vasculature and the challenges in accurately quantifying the hemodynamic and physiological parameters that govern this autoregulatory process. Method: In this study, we introduced a novel numerical algorithm that employs three partial differential equations and one ordinary differential equation to capture both the spatial and temporal distributions of key CA-driving factors, including the arterial pressure (P) and the partial pressures of oxygen (PO_2) and carbon dioxide (PCO_2) within the cerebral vasculature, together with a Windkessel model in turn to regulate the CBF based on the calculated P, PO_2, and PCO_2. This algorithm was sequentially integrated with our previously developed personalized 0D-1D multi-dimensional model to account for the patient-specific effects. Results: The integrated framework was rigorously validated using two independent datasets, demonstrating its high reliability and accuracy in capturing the regulatory effects of CA on CBF across a range of physiological conditions. Conclusion: This work significantly advances our understanding of CA and provides a promising foundation for developing hemodynamic-based therapeutic strategies aimed at improving clinical outcomes in patients with cerebrovascular disorders.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetries in stochastic homogenization and acclimatizations for the RVE method</title>
<link>https://arxiv.org/abs/2509.08977</link>
<guid>https://arxiv.org/abs/2509.08977</guid>
<content:encoded><![CDATA[
<div> symmetry, microstructure, effective tensor, thermal conductivity, RVE method
<br />
Summary:
The study explores the impact of symmetry in a random microstructure on effective tensor and fluctuations in thermal conductivity. It investigates methods to enforce symmetries in postprocessing using orthogonal projectors. In the context of the Representative Volume Element (RVE) method, invariance conditions for effective tensor and fluctuations under different microstructure symmetry groups are established. It is found that the symmetry of the RVE cell type can disrupt ensemble symmetry, affecting effective property approximation. Strategies are introduced to enforce expected symmetries, reducing errors and improving accuracy. Theoretical arguments support the use of projections for unbiased variance reduction and exact symmetry enforcement. Large-scale simulations confirm the effectiveness of symmetry-projection techniques, especially in fiber-reinforced composites of industrial size. <div>
arXiv:2509.08977v1 Announce Type: new 
Abstract: We investigate the implications of a given symmetry of a random microstructure on the obtained effective tensor and its fluctuation in the context of thermal conductivity, and study strategies for enforcing these symmetries in postprocessing via orthogonal projectors. Within the framework of the representative volume element (RVE) method, we establish the invariance conditions for the effective tensor and its fluctuation under different symmetry groups of the microstructure. Interestingly, the symmetry of the considered cell type in the RVE method may break the ensemble symmetry and compromise the approximation of the effective properties. To rectify this issue, we introduce dedicated techniques which permit to enforce the expected symmetries in postprocessing and study the implications on the bounds for the effective properties as well as the total, the random and the systematic errors. We provide theoretical arguments that suitable projections lead to unbiased variance-reduction strategies which furthermore enforce the expected symmetries exactly. Through large-scale FFT-based homogenization simulations, we study the symmetry structure of the estimated effective conductivities and their fluctuations. Moreover, we demonstrate the power of the symmetry-projection techniques for fiber-reinforced composite microstructures of industrial scale.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Isogeometric Topology Optimization Based on Topological Derivatives</title>
<link>https://arxiv.org/abs/2509.09236</link>
<guid>https://arxiv.org/abs/2509.09236</guid>
<content:encoded><![CDATA[
<div> Topology optimization, isogeometric approach, topological derivatives, level-set method, immersed isogeometric framework <br />
<br />
Summary: In this work, an isogeometric approach to topology optimization driven by topological derivatives is proposed. This approach allows for seamless geometry updates without the need for remeshing. The combination of a level-set method and an immersed isogeometric framework enables topological modifications without the requirement of defining initial holes. The influence of higher-degree basis functions in both the level-set representation and solution approximation is investigated, showing that using higher-degree basis functions for the solution improves accuracy, while linear basis functions are sufficient for the level-set function representation. Two numerical examples are presented to demonstrate the effectiveness of the proposed approach. <div>
arXiv:2509.09236v1 Announce Type: cross 
Abstract: Topology optimization is a valuable tool in engineering, facilitating the design of optimized structures. However, topological changes often require a remeshing step, which can become challenging. In this work, we propose an isogeometric approach to topology optimization driven by topological derivatives. The combination of a level-set method together with an immersed isogeometric framework allows seamless geometry updates without the necessity of remeshing. At the same time, topological derivatives provide topological modifications without the need to define initial holes [7]. We investigate the influence of higher-degree basis functions in both the level-set representation and the approximation of the solution. Two numerical examples demonstrate the proposed approach, showing that employing higher-degree basis functions for approximating the solution improves accuracy, while linear basis functions remain sufficient for the level-set function representation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified RIME algorithm with covariance learning and diversity enhancement for numerical optimization</title>
<link>https://arxiv.org/abs/2509.09529</link>
<guid>https://arxiv.org/abs/2509.09529</guid>
<content:encoded><![CDATA[
<div> Covariance learning, diversity enhancement, metaheuristic algorithm, optimization, RIME.<br />
Summary:<br />
The modified RIME algorithm with covariance learning and diversity enhancement (MRIME-CD) addresses the shortcomings of the RIME algorithm by introducing three key strategies. Firstly, a covariance learning strategy increases population diversity and balances exploitation and exploration abilities. Secondly, an average bootstrapping strategy guides population search in the early stage for better global search abilities. Lastly, a new stagnation indicator and stochastic covariance learning enhance the ability to escape local optima. Validation on test sets shows that MRIME-CD improves solution accuracy, convergence speed, and stability compared to basic RIME. The algorithm outperforms in terms of performance and demonstrates its effectiveness in various experiments. <br />Summary: <div>
arXiv:2509.09529v1 Announce Type: cross 
Abstract: Metaheuristics are widely applied for their ability to provide more efficient solutions. The RIME algorithm is a recently proposed physical-based metaheuristic algorithm with certain advantages. However, it suffers from rapid loss of population diversity during optimization and is prone to fall into local optima, leading to unbalanced exploitation and exploration. To address the shortcomings of RIME, this paper proposes a modified RIME with covariance learning and diversity enhancement (MRIME-CD). The algorithm applies three strategies to improve the optimization capability. First, a covariance learning strategy is introduced in the soft-rime search stage to increase the population diversity and balance the over-exploitation ability of RIME through the bootstrapping effect of dominant populations. Second, in order to moderate the tendency of RIME population to approach the optimal individual in the early search stage, an average bootstrapping strategy is introduced into the hard-rime puncture mechanism, which guides the population search through the weighted position of the dominant populations, thus enhancing the global search ability of RIME in the early stage. Finally, a new stagnation indicator is proposed, and a stochastic covariance learning strategy is used to update the stagnant individuals in the population when the algorithm gets stagnant, thus enhancing the ability to jump out of the local optimal solution. The proposed MRIME-CD algorithm is subjected to a series of validations on the CEC2017 test set, the CEC2022 test set, and the experimental results are analyzed using the Friedman test, the Wilcoxon rank sum test, and the Kruskal Wallis test. The results show that MRIME-CD can effectively improve the performance of basic RIME and has obvious superiorities in terms of solution accuracy, convergence speed and stability.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An improved educational competition optimizer with multi-covariance learning operators for global optimization problems</title>
<link>https://arxiv.org/abs/2509.09552</link>
<guid>https://arxiv.org/abs/2509.09552</guid>
<content:encoded><![CDATA[
<div> Keywords: educational competition optimizer, metaheuristic algorithm, multi-covariance learning operators, optimization problems, constrained optimization

Summary:
The study introduces an enhanced version of the educational competition optimizer (IECO-MCO) that utilizes multi-covariance learning operators to improve performance in tackling complex optimization problems. Three distinct covariance learning operators are introduced in IECO to balance exploitation and exploration effectively, preventing premature convergence. Experimental results using benchmark functions from CEC 2017 and CEC 2022 test suites show that IECO-MCO outperforms basic ECO and other algorithms in terms of convergence speed, stability, and avoiding local optima. Statistical analyses support the superiority of IECO-MCO. The algorithm demonstrates practical applicability in solving constrained optimization problems, showcasing its robustness and effectiveness in real-world scenarios.<br /><br />Summary: <div>
arXiv:2509.09552v1 Announce Type: cross 
Abstract: The educational competition optimizer is a recently introduced metaheuristic algorithm inspired by human behavior, originating from the dynamics of educational competition within society. Nonetheless, ECO faces constraints due to an imbalance between exploitation and exploration, rendering it susceptible to local optima and demonstrating restricted effectiveness in addressing complex optimization problems. To address these limitations, this study presents an enhanced educational competition optimizer (IECO-MCO) utilizing multi-covariance learning operators. In IECO, three distinct covariance learning operators are introduced to improve the performance of ECO. Each operator effectively balances exploitation and exploration while preventing premature convergence of the population. The effectiveness of IECO is assessed through benchmark functions derived from the CEC 2017 and CEC 2022 test suites, and its performance is compared with various basic and improved algorithms across different categories. The results demonstrate that IECO-MCO surpasses the basic ECO and other competing algorithms in convergence speed, stability, and the capability to avoid local optima. Furthermore, statistical analyses, including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test, are conducted to validate the superiority of IECO-MCO over the compared algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test suites. Additionally, the practical applicability of the proposed IECO-MCO algorithm is verified by solving constrained optimization problems. The experimental outcomes demonstrate the superior performance of IECO-MCO in tackling intricate optimization problems, underscoring its robustness and practical effectiveness in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Intraday Risk Factor Collections via Hierarchical Reinforcement Learning based on Transferred Options</title>
<link>https://arxiv.org/abs/2501.07274</link>
<guid>https://arxiv.org/abs/2501.07274</guid>
<content:encoded><![CDATA[
<div> Keywords: risk factors, stock return volatility, genetic programming, Hierarchical Proximal Policy Optimization, transfer learning

Summary:
The traditional risk factors used to measure and predict stock return volatility often lag behind market dynamics. Statistical models like PCA and factor analysis struggle to capture hidden nonlinear relationships. Genetic programming (GP) can identify nonlinear factors but lacks mechanisms for evaluating factor quality and results in complex formulas. In response to these challenges, the authors propose a Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor generation and evaluation. HPPO utilizes two PPO models: a high-level policy assigns weights to stock features, and a low-level policy identifies latent nonlinear relationships. The Pearson correlation between generated factors and return volatility serves as the reward signal. Transfer learning is employed to pre-train the high-level policy on historical data and fine-tune it with the latest data. Experimental results demonstrate that the HPPO-TO algorithm achieves a 25% excess return in HFT markets across China (CSI 300/800), India (Nifty 100), and the US (S&amp;P 500). <div>
arXiv:2501.07274v3 Announce Type: replace 
Abstract: Traditional risk factors like beta, size/value, and momentum often lag behind market dynamics in measuring and predicting stock return volatility. Statistical models like PCA and factor analysis fail to capture hidden nonlinear relationships. Genetic programming (GP) can identify nonlinear factors but often lacks mechanisms for evaluating factor quality, and the resulting formulas are complex. To address these challenges, we propose a Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor generation and evaluation. HPPO uses two PPO models: a high-level policy assigns weights to stock features, and a low-level policy identifies latent nonlinear relationships. The Pearson correlation between generated factors and return volatility serves as the reward signal. Transfer learning pre-trains the high-level policy on large-scale historical data, fine-tuning it with the latest data to adapt to new features and shifts. Experiments show the HPPO-TO algorithm achieves a 25\% excess return in HFT markets across China (CSI 300/800), India (Nifty 100), and the US (S\&amp;P 500). Code and data are available at https://github.com/wencyxu/HRL-HF_risk_factor_set.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Physics-Data Enrichments to Represent Uncertainty in Reduced Gas-Surface Chemistry Models for Hypersonic Flight</title>
<link>https://arxiv.org/abs/2509.08137</link>
<guid>https://arxiv.org/abs/2509.08137</guid>
<content:encoded><![CDATA[
<div> Reaction products, thermal protection system, ablation modeling, gas-surface chemistry, data-driven enrichments

Summary:
- During hypersonic flight, reactions with air deplete a re-entry vehicle's thermal protection system (TPS).
- Accurate ablation models are crucial for assessing TPS performance.
- New finite-rate gas-surface chemistry models are improving TPS ablation modeling.
- Model reductions may be necessary for computational tractability, but can lead to discrepancies in predicted carbon monoxide production.
- Hybrid physics-based and data-driven enrichments are developed to enhance predictive capability and quantify uncertainties in low-fidelity models.
- The enrichments significantly improve accuracy with the addition of only three reactions.<br /><br />Summary: <div>
arXiv:2509.08137v1 Announce Type: new 
Abstract: During hypersonic flight, air reacts with a planetary re-entry vehicle's thermal protection system (TPS), creating reaction products that deplete the TPS. Reliable assessment of TPS performance depends on accurate ablation models. New finite-rate gas-surface chemistry models are advancing state-of-the-art in TPS ablation modeling, but model reductions that omit chemical species and reactions may be necessary in some cases for computational tractability. This work develops hybrid physics-based and data-driven enrichments to improve the predictive capability and quantify uncertainties in such low-fidelity models while maintaining computational tractability. We focus on discrepancies in predicted carbon monoxide production that arise because the low-fidelity model tracks only a subset of reactions. To address this, we embed targeted enrichments into the low-fidelity model to capture the influence of omitted reactions. Numerical results show that the hybrid enrichments significantly improve predictive accuracy while requiring the addition of only three reactions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving contact problems using Fiber Monte Carlo</title>
<link>https://arxiv.org/abs/2509.08609</link>
<guid>https://arxiv.org/abs/2509.08609</guid>
<content:encoded><![CDATA[
<div> contact algorithm, computational modeling, contact forces, geometric descriptors, finite element method <br />
Summary: 
This work presents a novel contact algorithm that computes contact forces by calculating the gradient of an energy function with respect to geometric descriptors. The algorithm, inspired by the Fiber Monte Carlo method, accurately computes contact forces for bodies with complex geometries, independent of mesh conformity. It eliminates the need for master-slave identification and projection iterations, making it easy to incorporate into existing numerical solvers like the finite element method. Various numerical examples demonstrate the algorithm's efficiency in handling a wide range of contact scenarios, from small-deformation static contact to large-deformation dynamic contact with nonlinear material behavior. Examples include Hertzian contact for small-deformation verification, contact between different-shaped bodies, contact with hyperelastic materials, and dynamic collision cases to examine transient behavior. <div>
arXiv:2509.08609v1 Announce Type: new 
Abstract: Computational modeling of contact is fundamental to many engineering applications, yet accurately and efficiently solving complex contact problems remains challenging. In this work, we propose a new contact algorithm that computes contact forces by taking the gradient of an energy function of the contact volume (overlap) with respect to the geometry descriptors. While elegant in concept, evaluating this gradient is non-trivial due to the arbitrary geometry of the contact region. Inspired by the recently proposed Fiber Monte Carlo (FMC) method, we develop an algorithm that accurately computes contact forces based on the overlap volume between bodies with complex geometries. Our computational framework operates independently of mesh conformity, eliminating the need for master-slave identification and projection iterations, thus handling arbitrary discretizations. Moreover, by removing explicit complementarity constraints, the method retains a simple structure that can be easily incorporated into existing numerical solvers, such as the finite element method. In this paper, numerical examples cover a wide range of contact scenarios, from classical small-deformation static contact to complex large-deformation dynamic contact in both two- and three-dimensional settings with nonlinear material behavior. These cases include Hertzian contact for small-deformation verification; contact between wedge- and cone-shaped bodies to assess pressure and displacement predictions at non-smooth boundaries; contact involving Neo-Hookean hyperelastic materials for evaluating nonlinear responses under finite deformation; and dynamic collision cases to examine transient behavior.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying model prediction sensitivity to model-form uncertainty</title>
<link>https://arxiv.org/abs/2509.08708</link>
<guid>https://arxiv.org/abs/2509.08708</guid>
<content:encoded><![CDATA[
<div> Model-form uncertainty, Physics-based model, Uncertainty quantification, Sensitivity analysis, Model assumptions<br />
<br />
Summary: 
Model-form uncertainty (MFU) in physics-based model development is a significant source of uncertainty. A novel method is proposed to quantify the importance of uncertainties associated with model assumptions. By using parameterized modifications to assumptions (MFU representations) and grouped variance-based sensitivity analysis, the importance of assumptions can be measured. This approach can be applied even without calibration data. However, if calibration data is available, it can inform the MFU representation. The method is shown to be effective even when there is dependence between parameters, which often occurs during calibration. This method can help prioritize resources and efforts to reduce error in model predictions by understanding the importance of model assumptions relative to other sources of uncertainty. <div>
arXiv:2509.08708v1 Announce Type: new 
Abstract: Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration).
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.08207</link>
<guid>https://arxiv.org/abs/2509.08207</guid>
<content:encoded><![CDATA[
<div> Keywords: Aurora, Exascale supercomputer, Intel Xeon Data Center GPU Max Series, High Bandwidth Memory, DAOS <br />
Summary: <br />
Aurora, the Exascale supercomputer from Argonne National Laboratory, showcases advanced technologies like Intel Xeon Data Center GPU Max Series and High Bandwidth Memory. It also features the Distributed Asynchronous Object Storage (DAOS) and utilizes Intel's oneAPI programming environment. The node architecture, HPE Slingshot interconnect, and software ecosystem of Aurora are explored in detail in this paper. Standard benchmark performance and applications readiness are highlighted through the Early Science Program and the Exascale Computing Project. The integration of Intel's Data Center GPU Max Series on each compute node provides enhanced computational power, while the innovative DAOS storage solution offers high-speed data access. This comprehensive analysis demonstrates Aurora's potential to revolutionize scientific discovery through cutting-edge technologies and infrastructure. <br /> <div>
arXiv:2509.08207v1 Announce Type: cross 
Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer, designed to accelerate scientific discovery with cutting-edge architectural innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named Ponte Vecchio) on each compute node. Aurora also integrates the Distributed Asynchronous Object Storage (DAOS), a novel exascale storage solution, and leverages Intel's oneAPI programming environment. This paper presents an in-depth exploration of Aurora's node architecture, the HPE Slingshot interconnect, the supporting software ecosystem, and DAOS. We provide insights into standard benchmark performance and applications readiness efforts via Aurora's Early Science Program and the Exascale Computing Project.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic AI Workflow to Simplify Parameter Estimation of Complex Differential Equation Systems</title>
<link>https://arxiv.org/abs/2509.07283</link>
<guid>https://arxiv.org/abs/2509.07283</guid>
<content:encoded><![CDATA[
<div> Parameter identification, Ordinary Differential Equation models, AI workflow, calibration pipeline, optimization. 
<br /> 
Summary: 
An AI workflow is introduced to streamline the parameter identification process for mechanistic Ordinary Differential Equation models. The system translates a human-readable specification into a parallel and differentiable calibration pipeline. Users input an XML description and fill in a Python code skeleton, with the AI agent handling validation and remediation of common issues. Python callables are converted to JAX functions for efficient compilation and parallelization. The workflow includes global exploration of the parameter space followed by gradient-based refinement. This approach provides a reproducible workflow that simplifies advanced calibration while maintaining expert involvement. The open-source implementation allows for quick progression from problem statement to fitted models with minimal boilerplate. <div>
arXiv:2509.07283v1 Announce Type: new 
Abstract: Parameter identification for mechanistic Ordinary Differential Equation (ODE) models underpins prediction and control in several applications, yet remains a labor-intensive and brittle process: datasets are noisy and partial, models can be stiff or misspecified, and differentiable implementations demand framework expertise. An agentic AI workflow is presented that converts a lightweight, human-readable specification into a compiled, parallel, and differentiable calibration pipeline. Users supply an XML description of the problem and fill in a Python code skeleton; the agent automatically validates consistency between spec and code, and auto-remediates common pathologies. It transforms Python callables into pure JAX functions for efficient just-in-time compilation and parallelization. The system then orchestrates a two-stage search comprising global exploration of the parameter space followed by gradient-based refinement. The result is an AD-native, reproducible workflow that lowers the barrier to advanced calibration while preserving expert control. An open-source implementation with a documented API and examples is released, enabling rapid movement from problem statement to fitted, auditable models with minimal boilerplate.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Data-Driven Framework for Efficient Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.07303</link>
<guid>https://arxiv.org/abs/2509.07303</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific discovery, data-driven, formula discovery, dimensional constraints, symbolic regression

Summary:
Scientific discovery is crucial for progress across disciplines, but identifying physical laws from datasets can be challenging and resource-intensive. This study introduces a novel approach, FIND, which utilizes the Buckingham $\Pi$ theorem and Taylor's theorem to create a unified representation of formulas. FIND focuses on determining the structure of latent formulas first, then streamlines parameter identification by adhering to dimensional constraints and simplicity in formulas. Strategic optimization techniques are employed to minimize search iterations, with complex outcomes refined using symbolic regression. Validation across 11 datasets showcases FIND's effectiveness in discovering physical laws, dimensionless numbers, partial differential equations, and critical system parameters in fields like astronomy, physics, chemistry, and electronics. The results position FIND as a versatile and powerful tool for advancing data-driven scientific discovery in diverse domains. 

<br /><br />Summary: <div>
arXiv:2509.07303v1 Announce Type: new 
Abstract: Scientific discovery drives progress across disciplines, from fundamental physics to industrial applications. However, identifying physical laws automatically from gathered datasets requires identifying the structure and parameters of the formula underlying the data, which involves navigating a vast search space and consuming substantial computational resources. To address these issues, we build on the Buckingham $\Pi$ theorem and Taylor's theorem to create a unified representation of diverse formulas, which introduces latent variables to form a two-stage structure. To minimize the search space, we initially focus on determining the structure of the latent formula, including the relevant contributing inputs, the count of latent variables, and their interconnections. Following this, the process of parameter identification is expedited by enforcing dimensional constraints for physical relevance, favoring simplicity in the formulas, and employing strategic optimization techniques. Any overly complex outcomes are refined using symbolic regression for a compact form. These general strategic techniques drastically reduce search iterations from hundreds of millions to just tens, significantly enhancing the efficiency of data-driven formula discovery. We performed comprehensive validation to demonstrate FIND's effectiveness in discovering physical laws, dimensionless numbers, partial differential equations, and uniform critical system parameters across various fields, including astronomy, physics, chemistry, and electronics. The excellent performances across 11 distinct datasets position FIND as a powerful and versatile tool for advancing data-driven scientific discovery in multiple domains.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Driven Hierarchical Sampling for Unbalanced Continual Malware Detection with Time-Series Update-Based Retrieval</title>
<link>https://arxiv.org/abs/2509.07532</link>
<guid>https://arxiv.org/abs/2509.07532</guid>
<content:encoded><![CDATA[
<div> Hierarchical balanced sampler, uncertainty-guided continual learning, dynamic class balancing, vector retrieval mechanism, Android malware detection <br />
Summary: <br />
- The study addresses challenges in Android malware detection due to concept drift and class imbalance.
- Existing replay-based methods have bias issues as they prioritize the benign class, leading to overfitting.
- A novel uncertainty-guided continual learning framework is proposed to balance benign and malicious samples and select high-information instances.
- The framework incorporates a vector retrieval mechanism using historical malware embeddings to identify evolved variants.
- Experimental results show significant performance improvement over state-of-the-art methods, achieving high true positive rate and mean accuracy, making it effective for sustainable Android malware detection. <div>
arXiv:2509.07532v1 Announce Type: new 
Abstract: Android malware detection continues to face persistent challenges stemming from long-term concept drift and class imbalance, as evolving malicious behaviors and shifting usage patterns dynamically reshape feature distributions. Although continual learning (CL) mitigates drift, existing replay-based methods suffer from inherent bias. Specifically, their reliance on classifier uncertainty for sample selection disproportionately prioritizes the dominant benign class, causing overfitting and reduced generalization to evolving malware. To address these limitations, we propose a novel uncertainty-guided CL framework. First, we introduce a hierarchical balanced sampler that employs a dual-phase uncertainty strategy to dynamically balance benign and malicious samples while simultaneously selecting high-information, high-uncertainty instances within each class. This mechanism ensures class equilibrium across both replay and incremental data, thereby enhancing adaptability to emerging threats. Second, we augment the framework with a vector retrieval mechanism that exploits historical malware embeddings to identify evolved variants via similarity-based retrieval, thereby complementing classifier updates. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods under strict low-label conditions (50 labels per phase). It achieves a true positive rate (TPR) of 92.95\% and a mean accuracy (mACC) of 94.26\%, which validates its efficacy for sustainable Android malware detection.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
<link>https://arxiv.org/abs/2509.07627</link>
<guid>https://arxiv.org/abs/2509.07627</guid>
<content:encoded><![CDATA[
<div> Framework, TCR design, epitope-specific, gene-aware Transformer, multi-architecture, LSMTCR <br />
Summary: <br />
The article introduces LSMTCR, a multi-architecture framework for designing full-length, epitope-specific TCRs. It separates specificity from constraint learning to generate paired TCRs conditioned on epitopes. By utilizing a diffusion-enhanced BERT encoder and conditional GPT decoders, LSMTCR can generate chain-specific CDR3 sequences with high predicted binding and diversity. The gene-aware Transformer ensures immunogenetic fidelity by predicting V/J usage for complete TCR assembly. LSMTCR outperforms baselines in predicted binding, grammar fidelity, and diversity on various datasets. Transfer learning improves predicted binding, length realism, and diversity for TCR generation. Full-length TCR assembly from known or de novo CDR3s maintains k-mer spectra and achieves high pTM/ipTM scores in paired co-modelling with epitopes. LSMTCR enables the generation of diverse, gene-contextualized TCR designs solely from epitope input, facilitating high-throughput screening and iterative optimization. <br /> <div>
arXiv:2509.07627v1 Announce Type: new 
Abstract: Designing full-length, epitope-specific TCR {\alpha}\b{eta} remains challenging due to vast sequence space, data biases and incomplete modeling of immunogenetic constraints. We present LSMTCR, a scalable multi-architecture framework that separates specificity from constraint learning to enable de novo, epitope-conditioned generation of paired, full-length TCRs. A diffusion-enhanced BERT encoder learns time-conditioned epitope representations; conditional GPT decoders, pretrained on CDR3\b{eta} and transferred to CDR3{\alpha}, generate chain-specific CDR3s under cross-modal conditioning with temperature-controlled diversity; and a gene-aware Transformer assembles complete {\alpha}/\b{eta} sequences by predicting V/J usage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and our curated dataset, LSMTCR achieves higher predicted binding than baselines on most datasets, more faithfully recovers positional and length grammars, and delivers superior, temperature-tunable diversity. For {\alpha}-chain generation, transfer learning improves predicted binding, length realism and diversity over representative methods. Full-length assembly from known or de novo CDR3s preserves k-mer spectra, yields low edit distances to references, and, in paired {\alpha}/\b{eta} co-modelling with epitope, attains higher pTM/ipTM than single-chain settings. LSMTCR outputs diverse, gene-contextualized, full-length TCR designs from epitope input alone, enabling high-throughput screening and iterative optimization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized eigenvalue stabilization for immersed explicit dynamics</title>
<link>https://arxiv.org/abs/2509.07632</link>
<guid>https://arxiv.org/abs/2509.07632</guid>
<content:encoded><![CDATA[
<div> Stabilization, Immersed finite element discretizations, Generalized eigenvalue stabilization, Spectral basis functions, Finite cell method <br />
Summary: <br />
This article proposes a Generalized Eigenvalue Stabilization (GEVS) strategy for element mass matrices of cut elements in explicit time integration for immersed finite element discretizations. The use of spectral basis functions and the Finite Cell Method (FCM) ensures high-order convergence and definiteness of system matrices. The GEVS approach can be applied to various immersed boundary finite element methods. Numerical experiments show that the stabilization strategy achieves optimal convergence rates and restores critical time step sizes of boundary-conforming discretizations. It is effective even with weakly enforced Dirichlet boundary conditions using Nitsche's method or penalty formulations. <div>
arXiv:2509.07632v1 Announce Type: new 
Abstract: Explicit time integration for immersed finite element discretizations severely suffers from the influence of poorly cut elements. In this contribution, we propose a generalized eigenvalue stabilization (GEVS) strategy for the element mass matrices of cut elements to cure their adverse impact on the critical time step size of the global system. We use spectral basis functions, specifically $C^0$ continuous Lagrangian interpolation polynomials defined on Gauss-Lobatto-Legendre (GLL) points, which, in combination with its associated GLL quadrature rule, yield high-order convergent diagonal mass matrices for uncut elements. Moreover, considering cut elements, we combine the proposed GEVS approach with the finite cell method (FCM) to guarantee definiteness of the system matrices. However, the proposed GEVS stabilization can directly be applied to other immersed boundary finite element methods. Numerical experiments demonstrate that the stabilization strategy achieves optimal convergence rates and recovers critical time step sizes of equivalent boundary-conforming discretizations. This also holds in the presence of weakly enforced Dirichlet boundary conditions using either Nitsche's method or penalty formulations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards automatizing detection and quantification of intestinal metaplasia: a multi-expert comparative study</title>
<link>https://arxiv.org/abs/2509.06991</link>
<guid>https://arxiv.org/abs/2509.06991</guid>
<content:encoded><![CDATA[
<div> deep learning, gastric cancer, intestinal metaplasia, risk assessment, pathologists <br />
Summary: 
- The study introduces an automated method utilizing deep learning models to detect and quantify intestinal metaplasia in gastric samples for gastric cancer risk assessment.
- Deep learning models achieved high performance in classifying intestinal metaplasia, outperforming experienced pathologists.
- The best-performing model demonstrated F1-Score of 0.80 and AUC of 0.91 in classifying intestinal metaplasia.
- Pathologists' inter-observer agreement ranged from 0.61 to 0.75, while agreement between pathologists and the deep learning model ranged from 0.37 to 0.54.
- Deep learning models offer potential for more precise and reproducible detection and quantification of intestinal metaplasia, highlighting the variability in risk assessment when visually estimating intestinal metaplasia percentage. <br /> 
Summary: <div>
arXiv:2509.06991v1 Announce Type: cross 
Abstract: Current gastric cancer risk systems are prone to errors since they evaluate a visual estimation of intestinal metaplasia percentages to assign a risk. This study presents an automated method to detect and quantify intestinal metaplasia using deep learning models as well as a comparative analysis with visual estimations of three experienced pathologists. Gastric samples were collected from two different cohorts: 149 asymptomatic volunteers from a region with a high prevalence of GCa in Colombia and 56 patients from a third-level hospital. Deep learning models were selected and trained to classify intestinal metaplasia, and predictions were used to estimate the percentage of intestinal metaplasia and assign the risk score. Results were compared with independent blinded assessments performed by three experienced pathologists. The best-performing deep learning architecture classified intestinal metaplasia with F1-Score of 0.80 +- 0.01 and AUC of 0.91 +- 0.01. Among pathologists, inter-observer agreement by a Fleiss's Kappa score ranged from 0.61 to 0.75. In comparison, agreement between the pathologists and the best-performing model ranged from 0.37 to 0.54. Deep learning models show potential to detect and quantify the percentage of intestinal metaplasia with greater precision and reproducibility than experienced pathologists. Likewise, estimated risk shows high inter-observer variability when visually assigning the intestinal metaplasia percentage.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-strategy improved gazelle optimization algorithm for solving numerical optimization and engineering applications</title>
<link>https://arxiv.org/abs/2509.07211</link>
<guid>https://arxiv.org/abs/2509.07211</guid>
<content:encoded><![CDATA[
<div> iteration-based updating framework, adaptive parameter tuning, dominant population-based restart strategy, exploration, exploitation

Summary:
The article introduces a multi-strategy improved gazelle optimization algorithm (MSIGOA) to address the limitations of the basic GOA. The proposed algorithm utilizes an iteration-based updating framework to balance exploration and exploitation, enhancing convergence speed. Adaptive parameter tuning strategies improve applicability, while a dominant population-based restart strategy helps escape local optima. Evaluation on benchmark test sets shows MSIGOA outperforms basic GOA and other advanced algorithms, with a significant improvement in exploration and exploitation capabilities. Results indicate that MSIGOA performs well on various functions, demonstrating superiority over other methods. The algorithm's effectiveness is further confirmed through engineering design optimization problems, highlighting its extensibility for practical applications. The study showcases the potential of MSIGOA in enhancing optimization processes and solving complex problems efficiently and effectively. 

<br /><br />Summary: <div>
arXiv:2509.07211v1 Announce Type: cross 
Abstract: Aiming at the shortcomings of the gazelle optimization algorithm, such as the imbalance between exploration and exploitation and the insufficient information exchange within the population, this paper proposes a multi-strategy improved gazelle optimization algorithm (MSIGOA). To address these issues, MSIGOA proposes an iteration-based updating framework that switches between exploitation and exploration according to the optimization process, which effectively enhances the balance between local exploitation and global exploration in the optimization process and improves the convergence speed. Two adaptive parameter tuning strategies improve the applicability of the algorithm and promote a smoother optimization process. The dominant population-based restart strategy enhances the algorithms ability to escape from local optima and avoid its premature convergence. These enhancements significantly improve the exploration and exploitation capabilities of MSIGOA, bringing superior convergence and efficiency in dealing with complex problems. In this paper, the parameter sensitivity, strategy effectiveness, convergence and stability of the proposed method are evaluated on two benchmark test sets including CEC2017 and CEC2022. Test results and statistical tests show that MSIGOA outperforms basic GOA and other advanced algorithms. On the CEC2017 and CEC2022 test sets, the proportion of functions where MSIGOA is not worse than GOA is 92.2% and 83.3%, respectively, and the proportion of functions where MSIGOA is not worse than other algorithms is 88.57% and 87.5%, respectively. Finally, the extensibility of MSIGAO is further verified by several engineering design optimization problems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyHexTop: a compact Python code for topology optimization using hexagonal elements</title>
<link>https://arxiv.org/abs/2310.01968</link>
<guid>https://arxiv.org/abs/2310.01968</guid>
<content:encoded><![CDATA[
<div> Topology Optimization, Python, Hexagonal Elements, Educational, Compliance Minimization

Summary:
Python-based code "PyHexTop" is introduced as an alternative to MATLAB for topology optimization, featuring hexagonal elements for design domains without checkerboard issues. Developed from the MATLAB code "HoneyTop90," it utilizes NumPy and SciPy libraries, making it easy to understand for beginners in the field. The code focuses on compliance minimization with volume constraints, demonstrated with the Messerschmitt-Bolkow-Blohm beam problem and other variations. "PyHexTop" is a valuable educational tool shared openly for learning and exploration in topology optimization. <div>
arXiv:2310.01968v4 Announce Type: replace 
Abstract: Python serves as an open-source and cost-effective alternative to the MATLAB programming language. This paper introduces a concise topology optimization Python code, named ``\texttt{PyHexTop}," primarily intended for educational purposes. Code employs hexagonal elements to parameterize design domains as such elements provide checkerboard-free optimized design naturally. \texttt{PyHexTop} is developed based on the ``\texttt{HoneyTop90}" MATLAB code~\cite{kumar2023honeytop90} and uses the \texttt{NumPy} and \texttt{SciPy} libraries. Code is straightforward and easily comprehensible, proving a helpful tool that can help people new in the topology optimization field to learn and explore. \texttt{PyHexTop} is specifically tailored to address compliance minimization with specified volume constraints. The paper provides a detailed explanation of the code for solving the Messerschmitt-Bolkow-Blohm beam and extensions to solve problems different problems. The code is publicly shared at: https://github.com/PrabhatIn/PyHexTop
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOaCNN: Adaptive Convolutional Neural Network for Multidisciplinary Topology Optimization</title>
<link>https://arxiv.org/abs/2310.02069</link>
<guid>https://arxiv.org/abs/2310.02069</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive convolutional neural network, topology optimization, encoder-decoder networks, dense layers, compliance minimization problems

Summary:<br />
This paper introduces an adaptive convolutional neural network (CNN) architecture designed to automate a variety of topology optimization (TO) problems with different physical constraints. The network utilizes encoder-decoder networks with dense layers, incorporating an adaptive layer to capture complex geometric features. Trained on datasets from three open-source TO codes with varying physics, the CNN demonstrates robustness and success in compliance minimization tasks involving constant and design-dependent loads, as well as material bulk modulus optimization. Able to generate optimized designs quickly based on user input of volume fraction, the architecture closely matches results obtained from existing TO codes with minimal errors in performance and volume fraction. <div>
arXiv:2310.02069v2 Announce Type: replace 
Abstract: This paper presents an adaptive convolutional neural network (CNN) architecture that can automate diverse topology optimization (TO) problems having different underlying physics. The architecture uses the encoder-decoder networks with dense layers in the middle which includes an additional adaptive layer to capture complex geometrical features. The network is trained using the dataset obtained from the three open-source TO codes involving different physics. The robustness and success of the presented adaptive CNN are demonstrated on compliance minimization problems with constant and design-dependent loads and material bulk modulus optimization. The architecture takes the user's input of the volume fraction. It instantly generates optimized designs resembling their counterparts obtained via open-source TO codes with negligible performance and volume fraction error.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expanding Hardware-Efficiently Manipulable Hilbert Space via Hamiltonian Embedding</title>
<link>https://arxiv.org/abs/2401.08550</link>
<guid>https://arxiv.org/abs/2401.08550</guid>
<content:encoded><![CDATA[
<div> Quantum, Sparse Hamiltonian, Simulation, Hamiltonian Embedding, Quantum Applications <br />
Summary: 
The paper introduces the concept of Hamiltonian embedding as a technique for efficient quantum simulation of sparse Hamiltonians. This approach involves embedding the sparse Hamiltonian into a larger and more structured quantum system to enable more efficient simulation using hardware-efficient operations. Through a systematic study, the researchers demonstrate significant savings in computational resources, making it possible to implement quantum walks on complex graphs, quantum spatial search, and simulate real-space Schrdinger equations on current quantum platforms. The technique enhances the implementability of quantum advantages in the NISQ era by expanding the possibilities for quantum algorithms design. This advancement paves the way for practical implementation of quantum applications that depend on efficient sparse Hamiltonian simulation, offering a promising outlook for near-term quantum computing technology. <br /><br /> <div>
arXiv:2401.08550v2 Announce Type: replace-cross 
Abstract: Many promising quantum applications depend on the efficient quantum simulation of an exponentially large sparse Hamiltonian, a task known as sparse Hamiltonian simulation, which is fundamentally important in quantum computation. Although several theoretically appealing quantum algorithms have been proposed for this task, they typically require a black-box query model of the sparse Hamiltonian, rendering them impractical for near-term implementation on quantum devices.
  In this paper, we propose a technique named Hamiltonian embedding. This technique simulates a desired sparse Hamiltonian by embedding it into the evolution of a larger and more structured quantum system, allowing for more efficient simulation through hardware-efficient operations. We conduct a systematic study of this new technique and demonstrate significant savings in computational resources for implementing prominent quantum applications. As a result, we can now experimentally realize quantum walks on complicated graphs (e.g., binary trees, glued-tree graphs), quantum spatial search, and the simulation of real-space Schr\"odinger equations on current trapped-ion and neutral-atom platforms. Given the fundamental role of Hamiltonian evolution in the design of quantum algorithms, our technique markedly expands the horizon of implementable quantum advantages in the NISQ era.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Newton to Einstein: Axiom-Based Discovery via Game Design</title>
<link>https://arxiv.org/abs/2509.05448</link>
<guid>https://arxiv.org/abs/2509.05448</guid>
<content:encoded><![CDATA[
<div> machine learning, scientific discovery, axiom-based reasoning, rule-evolving system, interpretability

Summary: 
This position paper advocates for a shift in machine learning for scientific discovery from inductive pattern recognition to axiom-based reasoning. The proposed framework treats scientific inquiry as a rule-evolving system, where agents operate within environments governed by axioms and modify them to explain outlier observations. Unlike traditional machine learning approaches, this method allows for the discovery of new theoretical structures through systematic rule adaptation. Preliminary experiments in logic-based games demonstrate the feasibility of this approach by showing that agents can evolve axioms to solve previously unsolvable problems. This framework provides a basis for developing machine learning systems capable of creative, interpretable, and theory-driven discovery. <div>
arXiv:2509.05448v1 Announce Type: new 
Abstract: This position paper argues that machine learning for scientific discovery should shift from inductive pattern recognition to axiom-based reasoning. We propose a game design framework in which scientific inquiry is recast as a rule-evolving system: agents operate within environments governed by axioms and modify them to explain outlier observations. Unlike conventional ML approaches that operate within fixed assumptions, our method enables the discovery of new theoretical structures through systematic rule adaptation. We demonstrate the feasibility of this approach through preliminary experiments in logic-based games, showing that agents can evolve axioms that solve previously unsolvable problems. This framework offers a foundation for building machine learning systems capable of creative, interpretable, and theory-driven discovery.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUIDe: Generative and Uncertainty-Informed Inverse Design for On-Demand Nonlinear Functional Responses</title>
<link>https://arxiv.org/abs/2509.05641</link>
<guid>https://arxiv.org/abs/2509.05641</guid>
<content:encoded><![CDATA[
<div> Machine learning, generative models, inverse design, nonlinear systems, probabilistic modeling <br />
Summary: The article discusses the challenges of inverse design problems in engineering, particularly when dealing with nonlinear system responses. Traditional methods such as deep generative models and optimization-based approaches may yield unreliable solutions or incomplete coverage of the solution space. To overcome this, the Generative and Uncertainty-informed Inverse Design (GUIDe) framework is proposed, leveraging probabilistic machine learning and statistical inference. Unlike traditional inverse models, GUIDe uses a design-to-response strategy to generate designs with targeted nonlinear behaviors. By predicting each design's nonlinear functional response and evaluating the confidence that a design will meet the target, GUIDe enables the discovery of diverse feasible solutions, even for out-of-distribution targets. The method is validated through the design of interface properties for nacre-inspired composites to achieve target stress-strain responses. <div>
arXiv:2509.05641v1 Announce Type: new 
Abstract: Inverse design problems are pervasive in engineering, particularly when dealing with nonlinear system responses, such as in mechanical behavior or spectral analysis. The inherent intractability, non-existence, or non-uniqueness of their solutions, and the need for swift exploration of the solution space necessitate the adoption of machine learning and data-driven approaches, such as deep generative models. Here, we show that both deep generative model-based and optimization-based methods can yield unreliable solutions or incomplete coverage of the solution space. To address this, we propose the Generative and Uncertainty-informed Inverse Design (GUIDe) framework, leveraging probabilistic machine learning, statistical inference, and Markov chain Monte Carlo sampling to generate designs with targeted nonlinear behaviors. Unlike inverse models that directly map response to design, i.e., response $\mapsto$ design, we employ a design $\mapsto$ response strategy: a forward model that predicts each design's nonlinear functional response allows GUIDe to evaluate the confidence that a design will meet the target, conditioned on a target response with a user-specified tolerance level. Then, solutions are generated by sampling the solution space based on the confidence. We validate the method by designing the interface properties for nacre-inspired composites to achieve target stress-strain responses. Results show that GUIDe enables the discovery of diverse feasible solutions, including those outside the training data range, even for out-of-distribution targets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-based Topology Optimization</title>
<link>https://arxiv.org/abs/2509.05800</link>
<guid>https://arxiv.org/abs/2509.05800</guid>
<content:encoded><![CDATA[
<div> Transformer-based machine learning model, topology optimization, transfer learning, FFT encoding, auxiliary loss functions

Summary:
The study introduces a transformer-based machine learning model for topology optimization that incorporates critical boundary and loading conditions using a class token mechanism. Transfer learning and FFT encoding are utilized to enhance performance on dynamic datasets. The model includes auxiliary loss functions to improve the realism and manufacturability of generated designs. Performance evaluation shows that the model approaches the fidelity of diffusion-based models while eliminating the need for iterations. It achieves low compliance error, volume fraction error, floating material percentage, and load discrepancy error, demonstrating its potential for real-time, high-fidelity topology generation. <div>
arXiv:2509.05800v1 Announce Type: new 
Abstract: Topology optimization enables the design of highly efficient and complex structures, but conventional iterative methods, such as SIMP-based approaches, often suffer from high computational costs and sensitivity to initial conditions. Although machine learning methods have recently shown promise for accelerating topology generation, existing models either remain iterative or struggle to match ground-truth performance. In this work, we propose a transformer-based machine learning model for topology optimization that embeds critical boundary and loading conditions directly into the tokenized domain representation via a class token mechanism. We implement this model on static and dynamic datasets, using transfer learning and FFT encoding of dynamic loads to improve our performance on the dynamic dataset. Auxiliary loss functions are introduced to promote the realism and manufacturability of the generated designs. We conduct a comprehensive evaluation of the model's performance, including compliance error, volume fraction error, floating material percentage, and load discrepancy error, and benchmark it against state-of-the-art non-iterative and iterative generative models. Our results demonstrate that the proposed model approaches the fidelity of diffusion-based models while remaining iteration-free, offering a significant step toward real-time, high-fidelity topology generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distortion Minimization in Reverse Engineering for Additive Manufacturing: An Integrated 3D Scanning and Simulation Framework</title>
<link>https://arxiv.org/abs/2509.05857</link>
<guid>https://arxiv.org/abs/2509.05857</guid>
<content:encoded><![CDATA[
<div> Keywords: reverse engineering, additive manufacturing, 3D scanning, process simulation, geometric distortions

Summary:
This paper introduces a new framework for reverse engineering additively manufactured components, focusing on minimizing distortions. The framework combines 3D scanning with process simulation to predict geometric distortions and minimize errors between predicted and measured dimensions. The approach was demonstrated on Inconel-718 components produced using laser powder bed fusion additive manufacturing. The framework generates compensated STL and parametric CAD models, eliminating the need for experimental adjustments. The CAD-based method showed better accuracy, with an average absolute percent error of 0.087% between predicted and measured dimensions. This integrated approach offers a promising solution for reverse engineering and additive manufacturing processes, particularly for parts with complex geometries and high process-induced distortions. <div>
arXiv:2509.05857v1 Announce Type: new 
Abstract: Reverse engineering can be used to derive a 3D model of an existing physical part when such a model is not readily available. For parts that will be fabricated with subtractive and formative manufacturing processes, existing reverse engineering techniques can be readily applied, but parts produced with additive manufacturing can present new challenges due to the high level of process-induced distortions and unique part attributes. This paper introduces an integrated 3D scanning and process simulation data-driven framework to minimize distortions of reverse-engineered additively manufactured components. This framework employs iterative finite element simulations to predict geometric distortions to minimize errors between the predicted and measured geometrical deviations of the key dimensional characteristics of the part. The effectiveness of this approach is then demonstrated by reverse engineering two Inconel-718 components manufactured using laser powder bed fusion additive manufacturing. This paper presents a remanufacturing process that combines reverse engineering and additive manufacturing, leveraging geometric feature-based part compensation through process simulation. Our approach can generate both compensated STL and parametric CAD models, eliminating laborious experimentation during reverse engineering. We evaluate the merits of STL-based and CAD-based approaches by quantifying the errors induced at the different steps of the proposed approach and analyzing the influence of varying part geometries. Using the proposed CAD-based method, the average absolute percent error between simulation-predicted distorted dimensions and actual measured dimensions of the manufactured parts was 0.087%, with better accuracy than the STL-based method.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anticipating AMOC transitions via deep learning</title>
<link>https://arxiv.org/abs/2509.06450</link>
<guid>https://arxiv.org/abs/2509.06450</guid>
<content:encoded><![CDATA[
<div> AMOC, abrupt transitions, Earth system, convolutional neural network, early warning indicators
Summary: Key components of the Earth system, such as the Atlantic Meridional Overturning Circulation (AMOC), can undergo abrupt and potentially irreversible transitions when external forcing exceeds critical thresholds. This study explores the challenges of predicting such transitions, which can be induced by bifurcations, critical forcing rates, and noise. Traditional early warning indicators based on critical slowing down are unreliable in the stochastic regime of these transitions. To address this limitation, a convolutional neural network (CNN)-based approach is developed to identify statistical differences between transitioning and non-transitioning trajectories within ensemble simulations. This CNN-based indicator allows for real-time prediction of transition probabilities for individual trajectories prior to tipping points. The results demonstrate the effectiveness of this approach in providing early warnings for abrupt transitions of Earth system components, highlighting the importance of identifying safe operating spaces and early warning indicators under uncertainty. 
<br /><br />Summary: <div>
arXiv:2509.06450v1 Announce Type: new 
Abstract: Key components of the Earth system can undergo abrupt and potentially irreversible transitions when the magnitude or rate of external forcing exceeds critical thresholds. In this study, we use the example of the Atlantic Meridional Overturning Circulation (AMOC) to demonstrate the challenges associated with anticipating such transitions when the system is susceptible to bifurcation-induced, rate-induced, and noise-induced tipping. Using a calibrated AMOC box model, we conduct large ensemble simulations and show that transition behavior is inherently probabilistic: under identical freshwater forcing scenarios, some ensemble members exhibit transitions while others do not. In this stochastic regime, traditional early warning indicators based on critical slowing down are unreliable in predicting impending transitions. To address this limitation, we develop a convolutional neural network (CNN)-based approach that identifies higher-order statistical differences between transitioning and non-transitioning trajectories within the ensemble realizations. This method enables the real-time prediction of transition probabilities for individual trajectories prior to the onset of tipping. Our results show that the CNN-based indicator provides effective early warnings in a system where transitions can be induced by bifurcations, critical forcing rates, and noise. These findings underscore the potential in identifying safe operating spaces and early warning indicators for abrupt transitions of Earth system components under uncertainty.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reusable Surrogate Models for Distillation Columns</title>
<link>https://arxiv.org/abs/2509.06638</link>
<guid>https://arxiv.org/abs/2509.06638</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, chemical process engineering, distillation columns, ML-fueled modelfluid representation, entrainer distillation<br />
Summary:<br />
This article introduces a novel approach to surrogate modeling in chemical process engineering, aiming to create reusable models for distillation columns. By implementing a new ML-fueled modelfluid representation, the researchers were able to generate a vast dataset of over $1,000,000 samples, allowing the surrogate model to generalize across various column specifications and chemical compositions. The model's accuracy was validated, and it was successfully applied in a case study on entrainer distillation, where it efficiently screened and ranked candidate entrainers. This approach has the potential to significantly reduce computational efforts in optimization tasks compared to traditional flowsheet simulators, marking a paradigm shift towards more versatile and efficient surrogate models in chemical engineering.<br /> 
Summary: <div>
arXiv:2509.06638v1 Announce Type: new 
Abstract: Surrogate modeling is a powerful methodology in chemical process engineering, frequently employed to accelerate optimization tasks where traditional flowsheet simulators are computationally prohibitive. However, the state-of-the-art is dominated by surrogate models trained for a narrow range of fixed chemical systems and operating conditions, limiting their reusability. This work introduces a paradigm shift towards reusable surrogates by developing a single model for distillation columns that generalizes across a vast design space. The key enabler is a novel ML-fueled modelfluid representation which allows for the generation of datasets of more than $1,000,000$ samples. This allows the surrogate to generalize not only over column specifications but also over the entire chemical space of homogeneous ternary vapor-liquid mixtures. We validate the model's accuracy and demonstrate its practical utility in a case study on entrainer distillation, where it successfully screens and ranks candidate entrainers, significantly reducing the computational effort compared to rigorous optimization.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing</title>
<link>https://arxiv.org/abs/2509.05794</link>
<guid>https://arxiv.org/abs/2509.05794</guid>
<content:encoded><![CDATA[
<div> microservices, stateful services, migration, Kubernetes, optimization
Summary:<br />
- The paper addresses the challenge of migrating stateful microservices in Kubernetes, proposing an optimized scheme that integrates the MS2M framework with Kubernetes' FCC feature.
- Key enhancements include support for migrating StatefulSet-managed Pods and a Threshold-Based Cutoff Mechanism to manage high message rates.
- Evaluation results show that MS2M for individual Pods significantly reduces downtime by 96.986% compared to cold migration methods.
- The StatefulSet approach offers greater flexibility in managing stateful services within Kubernetes.
- The insights provided offer practical strategies for optimizing stateful microservice migration in cloud-native environments.<br />Summary: <div>
arXiv:2509.05794v1 Announce Type: cross 
Abstract: The widespread adoption of microservices architecture in modern software systems has emphasized the need for efficient management of distributed services. While stateless microservices enable straightforward migration, stateful microservices introduce added complexity due to the need to preserve in-memory state during migration. However, most container orchestrators, including Kubernetes, lack native support for live stateful service migration. This paper proposes an optimized migration scheme for stateful services in Kubernetes by integrating the Message-based Stateful Microservice Migration (MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC) feature. Key enhancements include support for migrating StatefulSet-managed Pods and the introduction of a Threshold-Based Cutoff Mechanism to handle high incoming message rates. Evaluation results demonstrate that MS2M for individual Pods reduces downtime by 96.986% compared to cold migration methods, while the StatefulSet approach provides greater flexibility in managing stateful services. These insights provide practical strategies for optimizing stateful microservice migration in cloud-native environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection</title>
<link>https://arxiv.org/abs/2509.06035</link>
<guid>https://arxiv.org/abs/2509.06035</guid>
<content:encoded><![CDATA[
<div> detection, transmission lines, UAVs, small defects, power grids  
TinyDef-DETR is a framework for small-defect detection in transmission lines using UAVs. It addresses challenges such as detail loss, weak boundary sensitivity, and lack of global context integration. The method includes a stride-free space-to-depth module for downsampling, edge-enhanced convolution for boundary awareness, and dual-domain multi-scale attention for global and local information capture. It also uses a Focaler-Wise-SIoU regression loss for improved localization of small objects. TinyDef-DETR outperforms competitive baselines in precision and recall, especially for small objects, with minimal computational overhead. Validation on the VisDrone benchmark confirms the approach's generalization capability for power grid inspections. Overall, the integration of detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression offers an efficient solution for UAV-based small-defect inspection in power grids.<br /><br />Summary: <div>
arXiv:2509.06035v1 Announce Type: cross 
Abstract: Automated inspection of transmission lines using UAVs is hindered by the difficulty of detecting small and ambiguous defects against complex backgrounds. Conventional detectors often suffer from detail loss due to strided downsampling, weak boundary sensitivity in lightweight backbones, and insufficient integration of global context with local cues. To address these challenges, we propose TinyDef-DETR, a DETR-based framework designed for small-defect detection. The method introduces a stride-free space-to-depth module for lossless downsampling, an edge-enhanced convolution for boundary-aware feature extraction, a cross-stage dual-domain multi-scale attention module to jointly capture global and local information, and a Focaler-Wise-SIoU regression loss to improve localization of small objects. Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR achieves substantial improvements in both precision and recall compared to competitive baselines, with particularly notable gains on small-object subsets, while incurring only modest computational overhead. Further validation on the VisDrone benchmark confirms the generalization capability of the proposed approach. Overall, the results indicate that integrating detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression provides a practical and efficient solution for UAV-based small-defect inspection in power grids.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation of CNN Ensemble Results for Enhanced Long-Term Prediction of the ENSO Phenomenon</title>
<link>https://arxiv.org/abs/2509.06227</link>
<guid>https://arxiv.org/abs/2509.06227</guid>
<content:encoded><![CDATA[
<div> Skill, ENSO forecasting, Deep learning, Ensemble members, Climate science  
Summary:  
- The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO) remains a challenge in climate science.  
- Operational systems often use the mean of all ensemble members assuming equal skill, but a subset of members show higher skill levels.  
- Study using a state-of-the-art ENSO forecast system found Top-5 subsets with significantly higher correlation and lower RMSE compared to the mean.  
- Improvement in skill is most pronounced at extreme lead times, crucial transition periods like SON and DJF, and season-dependent months like JJA and MJJ.  
- Identification of high-quality ensemble members could enhance forecasting skill and provide clues for future investigations.  

Summary: <div>
arXiv:2509.06227v1 Announce Type: cross 
Abstract: The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO) is still one of the biggest challenges in climate science. While it is true that short-to medium-range performance has been improved significantly using the advances in deep learning, statistical dynamical hybrids, most operational systems still use the simple mean of all ensemble members, implicitly assuming equal skill across members. In this study, we demonstrate, through a strictly a-posteriori evaluation , for any large enough ensemble of ENSO forecasts, there is a subset of members whose skill is substantially higher than that of the ensemble mean. Using a state-of-the-art ENSO forecast system cross-validated against the 1986-2017 observed Nino3.4 index, we identify two Top-5 subsets one ranked on lowest Root Mean Square Error (RMSE) and another on highest Pearson correlation. Generally across all leads, these outstanding members show higher correlation and lower RMSE, with the advantage rising enormously with lead time. Whereas at short leads (1 month) raises the mean correlation by about +0.02 (+1.7%) and lowers the RMSE by around 0.14 {\deg}C or by 23.3% compared to the All-40 mean, at extreme leads (23 months) the correlation is raised by +0.43 (+172%) and RMSE by 0.18 {\deg}C or by 22.5% decrease. The enhancements are largest during crucial ENSO transition periods such as SON and DJF, when accurate amplitude and phase forecasting is of greatest socio-economic benefit, and furthermore season-dependent e.g., mid-year months such as JJA and MJJ have incredibly large RMSE reductions. This study provides a solid foundation for further investigations to identify reliable clues for detecting high-quality ensemble members, thereby enhancing forecasting skill.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
<link>https://arxiv.org/abs/2509.06465</link>
<guid>https://arxiv.org/abs/2509.06465</guid>
<content:encoded><![CDATA[
<div> Keywords: Antibody binding site prediction, CAME-AB, multimodal representation, cross-modal reasoning, contrastive learning.

Summary: 
Antibody binding site prediction is crucial for computational immunology and antibody design. Existing methods often lack in representation and fail to identify antibody-specific binding sites. In this study, a novel framework called CAME-AB is introduced, which integrates multiple biologically grounded modalities to create a robust multimodal representation for antibody binding site prediction. The framework includes raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs. An adaptive modality fusion module dynamically weighs each modality for cross-modal reasoning, while a Transformer encoder with a Mixture-of-Experts module enhances feature specialization. Supervised contrastive learning shapes the latent space geometry for improved performance. Experimental results on antibody-antigen datasets show that CAME-AB outperforms existing baselines on various metrics. Ablation studies confirm the effectiveness of each architectural component and the benefits of multimodal feature integration. <div>
arXiv:2509.06465v1 Announce Type: cross 
Abstract: Antibody binding site prediction plays a pivotal role in computational immunology and therapeutic antibody design. Existing sequence or structure methods rely on single-view features and fail to identify antibody-specific binding sites on the antigens-a dual limitation in representation and prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding site prediction. CAME-AB integrates five biologically grounded modalities, including raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs-into a unified multimodal representation. To enhance adaptive cross-modal reasoning, we propose an adaptive modality fusion module that learns to dynamically weight each modality based on its global relevance and input-specific contribution. A Transformer encoder combined with an MoE module further promotes feature specialization and capacity expansion. We additionally incorporate a supervised contrastive learning objective to explicitly shape the latent space geometry, encouraging intra-class compactness and inter-class separability. To improve optimization stability and generalization, we apply stochastic weight averaging during training. Extensive experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB consistently outperforms strong baselines on multiple metrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further validate the effectiveness of each architectural component and the benefit of multimodal feature integration. The model implementation details and the codes are available on https://anonymous.4open.science/r/CAME-AB-C525
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A machine-learned expression for the excess Gibbs energy</title>
<link>https://arxiv.org/abs/2509.06484</link>
<guid>https://arxiv.org/abs/2509.06484</guid>
<content:encoded><![CDATA[
<div> neural network, Gibbs energy, thermodynamic properties, liquid mixtures, multi-component mixtures <br />
<br />Summary: 
HANNA, a neural network model, was developed to predict the excess Gibbs energy of multi-component liquid mixtures based on molecular structures. Physical laws were integrated as constraints to ensure thermodynamic consistency in predictions. The model was trained on a comprehensive experimental dataset and included a novel solver to incorporate liquid-liquid equilibrium data. A geometric projection method enabled accurate extrapolations to multi-component mixtures without the need for additional parameters. HANNA significantly outperformed existing methods in terms of accuracy and scope. The trained model and code are publicly available, with an interactive interface provided on the MLPROP website. <div>
arXiv:2509.06484v1 Announce Type: cross 
Abstract: The excess Gibbs energy plays a central role in chemical engineering and chemistry, providing a basis for modeling the thermodynamic properties of liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures solely from the molecular structures of their components is a long-standing challenge. In this work, we address this challenge by integrating physical laws as hard constraints within a flexible neural network. The resulting model, HANNA, was trained end-to-end on an extensive experimental dataset for binary mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent predictions. A novel surrogate solver developed in this work enabled the inclusion of liquid-liquid equilibrium data in the training process. Furthermore, a geometric projection method was applied to enable robust extrapolations to multi-component mixtures, without requiring additional parameters. We demonstrate that HANNA delivers excellent predictions, clearly outperforming state-of-the-art benchmark methods in accuracy and scope. The trained model and corresponding code are openly available, and an interactive interface is provided on our website, MLPROP.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Parallel Solver with Multiphysics Finite Element Method for Poroelasticity Coupled with Elasticity Model</title>
<link>https://arxiv.org/abs/2509.06673</link>
<guid>https://arxiv.org/abs/2509.06673</guid>
<content:encoded><![CDATA[
<div> pressure, poroelasticity, elasticity, Lagrange multiplier, parallel solver

Summary: 
This paper presents a parallel solver for the quasi-static linear poroelasticity and linear elasticity model in the Lagrange multiplier framework. The model is reformulated as a coupling of nearly incompressible elasticity and unsteady advection-diffusion equations, with new variables introduced to ensure normal stress continuity. Variational formulations and finite element methods are employed for each subdomain, with a parallel solver using the FETI method and FETI preconditioner for efficiency. Numerical tests demonstrate computational efficiency and convergence error order, and the model is validated using Barry-Mercer's model as a benchmark. The results show no oscillations in computed pressure, affirming the effectiveness of the proposed parallel solver for solving poroelasticity and elasticity models. 

<br /><br />Summary: <div>
arXiv:2509.06673v1 Announce Type: cross 
Abstract: In this paper, we propose a parallel solver for solving the quasi-static linear poroelasticity coupled with linear elasticity model in the Lagrange multiplier framework. Firstly, we reformulate the model into a coupling of the nearly incompressible elasticity and an unsteady affection-diffusion equations by setting new variable ``elastic pressure" and ``volumetric fluid content". And we introduce a Lagrange multiplier to guarantee the normal stress continuity on the interface. Then, we give the variational formulations in each subdomain and choose the $\boldsymbol{P}_k$-$P_1$-$P_1$ mixed finite element tuple for poroelasticity subdomain, and $\boldsymbol{P}_k$-$P_1$ finite element pair ($k=1,2$) for elasticity subdomain and the backward Euler scheme for time. Also, we propose a parallel solver for solving the fully discrete scheme at each time step-- the FETI method with a classical FETI preconditioner for solving the Lagrange multiplier and calculating the subproblems in each subdomain in parallel. And we show several numerical tests to validate the computational efficiency and the convergence error order, and we consider Barry-Mercer's model as the benchmark test to show that there no oscillation in the computed pressure. Finally, we draw conclusions to summarize the main results of this paper.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A simple and efficient hybrid discretization approach to alleviate membrane locking in isogeometric thin shells</title>
<link>https://arxiv.org/abs/2312.16944</link>
<guid>https://arxiv.org/abs/2312.16944</guid>
<content:encoded><![CDATA[
<div> membrane locking, isogeometric finite element, Kirchhoff-Love shells, hybrid discretization, stress recovery  
Summary:  
This work introduces a hybrid discretization technique to address membrane locking in isogeometric finite element models for Kirchhoff-Love shells. The method, compatible with existing isogeometric finite element codes, combines isogeometric and Lagrange-based surface discretizations without increasing the tangent matrix bandwidth or requiring additional degrees of freedom or static condensation. It proves effective for both linear and nonlinear problems, with simplified stress recovery. By incorporating quadratic NURBS surfaces, the approach significantly improves accuracy in membrane stresses relative to traditional methods. Rigorous analysis on various benchmark problems confirms the efficacy of the proposed technique in alleviating or eliminating membrane locking, suggesting potential extensions to other discretization types and constraints. <div>
arXiv:2312.16944v2 Announce Type: replace 
Abstract: This work presents a new hybrid discretization approach to alleviate membrane locking in isogeometric finite element formulations for Kirchhoff-Love shells. The approach is simple, and requires no additional dofs and no static condensation. It does not increase the bandwidth of the tangent matrix and is effective for both linear and nonlinear problems. It combines isogeometric surface discretizations with classical Lagrange-based surface discretizations, and can thus be run with existing isogeometric finite element codes. Also, the stresses can be recovered straightforwardly. The effectiveness of the proposed approach in alleviating, if not eliminating, membrane locking is demonstrated through the rigorous study of the convergence behavior of several classical benchmark problems. Accuracy gains are particularly large in the membrane stresses. The approach is formulated here for quadratic NURBS, but an extension to other discretization types can be anticipated. The same applies to other constraints and associated locking phenomena.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection-based model-order reduction via graph autoencoders suited for unstructured meshes</title>
<link>https://arxiv.org/abs/2407.13669</link>
<guid>https://arxiv.org/abs/2407.13669</guid>
<content:encoded><![CDATA[
<div> Graph autoencoder, projection-based model-order reduction, nonlinear manifold least-squares Petrov-Galerkin projection scheme, advection-dominated flows, unstructured meshes<br />
<br />
Summary: <br />
This paper introduces a graph autoencoder architecture, GD-LSPG, for projection-based model-order reduction in advection-dominated flow simulations on unstructured meshes. The architecture combines reduced graph hierarchy generation and message passing operations to emulate the filtering process of CNNs, allowing for improved flexibility and interpretability. GD-LSPG's latent state variables offer interpretable mode shapes similar to proper orthogonal decomposition modes. The framework is demonstrated on a one-dimensional Burgers' model with a structured mesh and two test cases for two-dimensional Euler equations using an unstructured mesh, showcasing its flexibility and accuracy compared to traditional affine projections. This approach provides a significant enhancement in accuracy for low-dimensional latent spaces and outperforms CNN-based autoencoders, making it a promising tool for model reduction in computational fluid dynamics. <br /> <div>
arXiv:2407.13669v4 Announce Type: replace 
Abstract: This paper presents the development of a graph autoencoder architecture capable of performing projection-based model-order reduction (PMOR) using a nonlinear manifold least-squares Petrov-Galerkin (LSPG) projection scheme. The architecture is particularly useful for advection-dominated flows modeled by unstructured meshes, as it provides a robust nonlinear mapping that can be leveraged in a PMOR setting. The presented graph autoencoder is constructed with a two-part process that consists of (1) generating a hierarchy of reduced graphs to emulate the compressive abilities of convolutional neural networks (CNNs) and (2) training a message passing operation at each step in the hierarchy of reduced graphs to emulate the filtering process of a CNN. The resulting framework provides improved flexibility over traditional CNN-based autoencoders because it is readily extendable to unstructured meshes. We provide an analysis of the interpretability of the graph autoencoder's latent state variables, where we find that the Jacobian of the decoder for the proposed graph autoencoder provides interpretable mode shapes akin to traditional proper orthogonal decomposition modes. To highlight the capabilities of the proposed framework, which is named geometric deep least-squares Petrov-Galerkin (GD-LSPG), we benchmark the method on a one-dimensional Burgers' model with a structured mesh and demonstrate the flexibility of GD-LSPG by deploying it on two test cases for two-dimensional Euler equations that use an unstructured mesh. The proposed framework is more flexible than using a traditional CNN-based autoencoder and provides considerable improvement in accuracy for very low-dimensional latent spaces in comparison with traditional affine projections.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed-precision numerics in scientific applications: survey and perspectives</title>
<link>https://arxiv.org/abs/2412.19322</link>
<guid>https://arxiv.org/abs/2412.19322</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, mixed-precision, scientific applications, algorithmic innovations, computational science
<br />
Summary: 
This article discusses the utilization of mixed-precision computations in artificial intelligence (AI) workloads and scientific applications. It highlights the potential for significant performance improvements up to 8x compared to double-precision in compute-intensive tasks. The survey covers various scientific domains like fluid dynamics, weather and climate, quantum chemistry, and computational genomics that have started implementing mixed-precision strategies. State-of-the-art algorithmic techniques such as iterative refinement and adaptive precision solvers are examined for their implications on accuracy, performance, and resource utilization. The article also discusses the emerging software ecosystem supporting mixed-precision methods at scale. Overall, the survey emphasizes the transformative impact of mixed-precision numerics in computational science, emphasizing the importance of aligning algorithms with evolving hardware capabilities. 
<br /><br /> <div>
arXiv:2412.19322v3 Announce Type: replace 
Abstract: The explosive demand for artificial intelligence (AI) workloads has led to a significant increase in silicon area dedicated to lower-precision computations on recent high-performance computing hardware designs. However, mixed-precision capabilities, which can achieve performance improvements of 8x compared to double-precision in extreme compute-intensive workloads, remain largely untapped in most scientific applications. A growing number of efforts have shown that mixed-precision algorithmic innovations can deliver superior performance without sacrificing accuracy. These developments should prompt computational scientists to seriously consider whether their scientific modeling and simulation applications could benefit from the acceleration offered by new hardware and mixed-precision algorithms. In this survey, we (1) review progress across diverse scientific domains -- including fluid dynamics, weather and climate, quantum chemistry, and computational genomics -- that have begun adopting mixed-precision strategies; (2) examine state-of-the-art algorithmic techniques such as iterative refinement, splitting and emulation schemes, and adaptive precision solvers; (3) assess their implications for accuracy, performance, and resource utilization; and (4) survey the emerging software ecosystem that enables mixed-precision methods at scale. We conclude with perspectives and recommendations on cross-cutting opportunities, domain-specific challenges, and the role of co-design between application scientists, numerical analysts and computer scientists. Collectively, this survey underscores that mixed-precision numerics can reshape computational science by aligning algorithms with the evolving landscape of hardware capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical optimization of aviation decarbonization scenarios: balancing traffic and emissions with maturing energy carriers and aircraft technology</title>
<link>https://arxiv.org/abs/2503.22435</link>
<guid>https://arxiv.org/abs/2503.22435</guid>
<content:encoded><![CDATA[
<div> Aviation, emissions, decarbonization, transportation, mitigation
<br />
Summary:<br />
- The article focuses on the role of aviation emissions in long-term climate mitigation in transportation.
- Low-carbon energy carriers and new aircraft deployment are modeled as technology-centered decarbonization policies.
- Supply constraints in targeted market segments are considered as demand-side policies.
- Shared Socioeconomic Pathways (SSPs) are used to estimate trend traffic demand and limit sectoral consumption of electricity and biomass.
- Emissions peak by 2040 in all scenarios, but meeting Paris Agreement goals requires targeted demand management or additional low-carbon energy supply.
- Gradient-based optimization in a multidisciplinary framework efficiently addresses nonlinear, high-dimensional problems while reducing implementation effort. 
<br />Summary: <div>
arXiv:2503.22435v2 Announce Type: replace 
Abstract: Despite being considered a hard-to-abate sector, aviation's emissions will play an important role in long-term climate mitigation of transportation. The introduction of low-carbon energy carriers and the deployment of new aircraft in the current fleet are modeled as technology-centered decarbonization policies, while supply constraints in targeted market segments are modeled as demand-side policies. Shared Socioeconomic Pathways (SSPs) are used to estimate trend traffic demand and to limit the sectoral consumption of electricity and biomass. Mitigation scenarios are formulated as optimization problems, and three applications are demonstrated: no-policy baselines, single-policy optimization, and scenario-robust policies. Results show that the choice of energy carrier is highly dependent on assumptions regarding aircraft technology and the background energy system. Across all SSP-based scenarios, emissions peak by around 2040, but achieving alignment with the Paris Agreement requires either targeted demand management or additional low-carbon energy supply. The use of gradient-based optimization within a multidisciplinary framework enables the efficient resolution of these nonlinear, high-dimensional problems while reducing implementation effort.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust blue-green urban flood risk management optimised with a genetic algorithm for multiple rainstorm return periods</title>
<link>https://arxiv.org/abs/2502.12174</link>
<guid>https://arxiv.org/abs/2502.12174</guid>
<content:encoded><![CDATA[
<div> optimisation, Blue-Green Infrastructure, flood risk, return periods, multi-objective

Summary:
This study introduces a novel methodology for optimising Blue-Green Infrastructure (BGI) designs to enhance flood risk management. By incorporating multiple return periods (10, 20, 30, 50, and 100 years) into a multi-objective optimisation framework, the study aims to improve the robustness of BGI schemes. Utilising a Non-dominated Sorting Genetic Algorithm II (NSGA-II) with a hydrodynamic model, the design process considers direct damage cost (DDC) and expected annual damage (EAD) as risk objective functions. Results highlight that a BGI design optimised for a single 100-year return period may not perform well for other return periods, indicating the importance of considering various storm magnitudes. The study demonstrates that a composite return period approach leads to improved performance metrics across all return periods, enhancing resilience to future climate extremes. This paradigm shift towards multi-return period-based designs in flood risk management can enhance adaptability and resilience in the face of changing climate conditions.

<br /><br />Summary: <div>
arXiv:2502.12174v2 Announce Type: replace-cross 
Abstract: Flood risk managers seek to optimise Blue-Green Infrastructure (BGI) designs to maximise return on investment. Current systems often use optimisation algorithms and detailed flood models to maximise benefit-cost ratios for single rainstorm return periods. However, these schemes may lack robustness in mitigating flood risks across different storm magnitudes. For example, a BGI scheme optimised for a 100-year return period may differ from one optimised for a 10-year return period. This study introduces a novel methodology incorporating five return periods (T = 10, 20, 30, 50, and 100 years) into a multi-objective BGI optimisation framework. The framework combines a Non-dominated Sorting Genetic Algorithm II (NSGA-II) with a fully distributed hydrodynamic model to optimise the spatial placement and combined size of BGI features. For the first time, direct damage cost (DDC) and expected annual damage (EAD), calculated for various building types, are used as risk objective functions, transforming a many-objective problem into a multi-objective one. Performance metrics such as Median Risk Difference (MedRD), Maximum Risk Difference (MaxRD), and Area Under Pareto Front (AUPF) reveal that a 100-year optimised BGI design performs poorly when evaluated for other return periods, particularly shorter ones. In contrast, a BGI design optimised using composite return periods enhances performance metrics across all return periods, with the greatest improvements observed in MedRD (22%) and AUPF (73%) for the 20-year return period, and MaxRD (23%) for the 50-year return period. Furthermore, climate uplift stress testing confirms the robustness of the proposed design to future rainfall extremes. This study advocates a paradigm shift in flood risk management, moving from single maximum to multiple rainstorm return period-based designs to enhance resilience and adaptability to future climate extremes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phase-field and lip-field approaches for fracture with extreme mesh deformation (X-Mesh): a one-dimensional study</title>
<link>https://arxiv.org/abs/2509.04971</link>
<guid>https://arxiv.org/abs/2509.04971</guid>
<content:encoded><![CDATA[
<div> Keywords: fracture, phase-field, lip-field, variational mesh study, X-Mesh

Summary:
The study examines a one-dimensional fracture problem using the phase-field or lip-field approach, focusing on optimizing incremental potential in relation to displacement and damage fields, as well as nodal coordinates of the mesh. Through variational mesh analysis, the research shows that as damage increases, the most damaged element decreases in size until it reaches zero, providing an accurate representation of the bar breaking. The optimized solution proves to be more precise than fixed mesh solutions. This work contributes to exploring the possibilities of extreme meshes in computational mechanics within the X-Mesh framework. 

<br /><br />Summary: <div>
arXiv:2509.04971v1 Announce Type: new 
Abstract: We consider a one-dimensional fracture problem modelled using either the phase-field or lip-field approach. In both cases, we optimise the incremental potential with respect to the displacement and damage fields and the nodal coordinates of the mesh. This is thus a variational mesh study. We observe that, as the damage reaches its maximum value, the optimisation drives the most damaged element to zero size as the damage reaches its maximum value. This peculiar element provides a precise displacement jump representation as the bar breaks. The overall solution is also shown to be much more accurate than the fixed mesh solution. This work forms part of an exploration into the capabilities of extreme meshes in computational mechanics (X-Mesh).
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Inverse Rosenblatt Transport for Structural Reliability Analysis</title>
<link>https://arxiv.org/abs/2509.05061</link>
<guid>https://arxiv.org/abs/2509.05061</guid>
<content:encoded><![CDATA[
<div> probability estimation, reliability analysis, solid mechanics, deep learning, high-dimensional spaces

Summary:
The article investigates the Deep Inverse Rosenblatt Transport (DIRT) framework for reliability analysis in solid mechanics. DIRT combines a TT decomposition with an inverse Rosenblatt transformation to efficiently estimate the probability of failure in high-dimensional settings. The framework scales linearly in input dimension and provides a compact and reusable surrogate of the target distribution. Through experimentation on various analytical and numerical examples, DIRT demonstrates lower estimator variance and accurate estimation of rare event probabilities compared to established methods like Bayesian updating with Subset Simulation (BUS-SuS). This research addresses the challenge of accurate failure probability estimation in engineering systems, particularly in high-dimensional settings with rare events. <div>
arXiv:2509.05061v1 Announce Type: new 
Abstract: Accurately estimating the probability of failure in engineering systems under uncertainty is a fundamental challenge, particularly in high-dimensional settings and for rare events. Conventional reliability analysis methods often become computationally intractable or exhibit high estimator variance when applied to problems with hundreds of uncertain parameters or highly concentrated failure regions. In this work, we investigate the use of the recently proposed Deep Inverse Rosenblatt Transport (DIRT) framework for reliability analysis in solid mechanics. DIRT combines a TT decomposition with an inverse Rosenblatt transformation to construct a low-rank approximation of the posterior distribution, enabling efficient sampling and probability estimation in high-dimensional spaces. By representing the optimal importance density in the TT format, DIRT scales linearly in the input dimension while maintaining a compact, reusable surrogate of the target distribution. We demonstrate the effectiveness of the DIRT framework on three analytical reliability problems and one numerical example with dimensionality ranging from 2 to 250. Compared to established methods such as Bayesian updating with Subset Simulation (BUS-SuS), DIRT seems to lower the estimator variance while accurately capturing rare event probabilities for the benchmark problems of this study.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Evaluation of Derivatives of Green's Functions Using Recurrences</title>
<link>https://arxiv.org/abs/2509.03687</link>
<guid>https://arxiv.org/abs/2509.03687</guid>
<content:encoded><![CDATA[
<div> Green's functions, derivatives, symbolic-numerical procedures, quadrature by expansion, higher-order accuracy. 
Summary: This article presents hybrid symbolic-numerical methods for efficiently computing higher-order derivatives of Green's functions, crucial in fast multipole methods and Barnes-Hut algorithms. The proposed procedures achieve an O(n) cost for computing n derivatives, offering significant computational savings. These methods are applicable to radially symmetric Green's functions and are general, requiring only knowledge of the relevant PDE. The algorithm guarantees controlled error levels, enhancing reliability. Additionally, the article introduces a rotation-based approach for target-specific evaluation in the Cartesian setting within the method of quadrature by expansion, which significantly reduces computational expenses compared to traditional symbolic methods. Numerical experiments validate the accuracy and efficiency of the proposed techniques. <div>
arXiv:2509.03687v1 Announce Type: new 
Abstract: High-order derivatives of Green's functions are a key ingredient in Taylor-based fast multipole methods, Barnes-Hut $n$-body algorithms, and quadrature by expansion (QBX). In these settings, derivatives underpin either the formation, evaluation, and/or translation of Taylor expansions.
  In this article, we provide hybrid symbolic-numerical procedures that generate recurrences to attain an $O(n)$ cost for the the computation of $n$ derivatives (i.e. $O(1)$ per derivative) for arbitrary radially symmetric Green's functions. These procedures are general--only requiring knowledge of the PDE that the Green's function solves. We show that the algorithm has controlled, theoretically-understood error.
  We apply these methods to the method of quadrature by expansion, a method for the evaluation of singular layer potentials, which requires higher-order derivatives of Green's functions. In doing so, we contribute a new rotation-based method for target-specific QBX evaluation in the Cartesian setting that attains dramatically lower cost than existing symbolic approaches.
  Numerical experiments support our claims of accuracy and cost.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamic Energy-Based Hysteresis Model for Pulsed-Operated Fast-Ramping Magnets</title>
<link>https://arxiv.org/abs/2509.04115</link>
<guid>https://arxiv.org/abs/2509.04115</guid>
<content:encoded><![CDATA[
<div> Keywords: ferromagnetic yokes, fast-ramping magnets, hysteresis description, eddy-current model, normal-conducting bending magnet<br />
<br />
Summary: 
This paper presents a dynamic ferromagnetic model that combines energy-based hysteresis description and a thin-sheet eddy-current model in the time domain. The study addresses the challenges of accurately analyzing fast-ramping magnets due to their strongly nonlinear behavior. Existing approaches often oversimplify the analysis using anhysteretic material descriptions and after-the-fact loss formulae. By utilizing a more comprehensive model, the research aims to provide a more precise calculation of losses and magnetic fields. The model's effectiveness was demonstrated through its application in analyzing a normal-conducting bending magnet. This work sheds light on the importance of considering dynamic ferromagnetic behavior in the numerical analysis of magnets, offering a more accurate representation of their performance. <div>
arXiv:2509.04115v1 Announce Type: new 
Abstract: Due to the strongly nonlinear behavior of ferromagnetic yokes, the numerical analysis of fast-ramping magnets is highly cumbersome and, therefore, in practice overly simplified by means of anhysteretic material descriptions and a posteriori loss formulae. This paper establishes the use of a dynamic ferromagnetic model combining a preconditioned energy-based hysteresis description and a thin-sheet eddy-current model in time-domain. The model was successfully employed in the analysis of a normal-conducting bending magnet in order to precisely calculate losses and fields.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COBRA: Multimodal Sensing Deep Learning Framework for Remote Chronic Obesity Management via Wrist-Worn Activity Monitoring</title>
<link>https://arxiv.org/abs/2509.04210</link>
<guid>https://arxiv.org/abs/2509.04210</guid>
<content:encoded><![CDATA[
<div> Keywords: Chronic obesity management, Deep learning, Behavioral monitoring, Multimodal sensors, Digital health systems

Summary: 
COBRA (Chronic Obesity Behavioral Recognition Architecture) is a novel deep learning framework designed for objective monitoring of energy balance behaviors in individuals with chronic obesity. It utilizes wrist-worn multimodal sensors and a hybrid D-Net architecture that incorporates spatial modeling, self-attention mechanisms, and temporal processing to classify daily activities related to obesity. Validation on the WISDM-Smart dataset shows high accuracy and outperformance of state-of-the-art baselines. The framework's optimal preprocessing strategy includes spectral-temporal feature extraction, enabling robust generalizability with low demographic variance. COBRA's success in accurately categorizing activities such as Food Intake, Physical Activity, Sedentary Behavior, and Daily Living showcases its potential for scalable deployment in personalized obesity interventions and continuous lifestyle monitoring.<br /><br />Summary: <div>
arXiv:2509.04210v1 Announce Type: new 
Abstract: Chronic obesity management requires continuous monitoring of energy balance behaviors, yet traditional self-reported methods suffer from significant underreporting and recall bias, and difficulty in integration with modern digital health systems. This study presents COBRA (Chronic Obesity Behavioral Recognition Architecture), a novel deep learning framework for objective behavioral monitoring using wrist-worn multimodal sensors. COBRA integrates a hybrid D-Net architecture combining U-Net spatial modeling, multi-head self-attention mechanisms, and BiLSTM temporal processing to classify daily activities into four obesity-relevant categories: Food Intake, Physical Activity, Sedentary Behavior, and Daily Living. Validated on the WISDM-Smart dataset with 51 subjects performing 18 activities, COBRA's optimal preprocessing strategy combines spectral-temporal feature extraction, achieving high performance across multiple architectures. D-Net demonstrates 96.86% overall accuracy with category-specific F1-scores of 98.55% (Physical Activity), 95.53% (Food Intake), 94.63% (Sedentary Behavior), and 98.68% (Daily Living), outperforming state-of-the-art baselines by 1.18% in accuracy. The framework shows robust generalizability with low demographic variance (<3%), enabling scalable deployment for personalized obesity interventions and continuous lifestyle monitoring.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and scalable deep Maxwell solvers using multilevel iterative methods</title>
<link>https://arxiv.org/abs/2509.03622</link>
<guid>https://arxiv.org/abs/2509.03622</guid>
<content:encoded><![CDATA[
<div> surrogate PDE solvers, neural networks, subdomain neural operator model, iterative algorithms, multilevel domain decomposition <br /> 
Summary: 
This article explores the use of neural networks as surrogate solvers for partial differential equations, addressing challenges in accuracy and scalability. By combining neural network surrogates with iterative algorithms, the study demonstrates the accurate solution of PDE problems with varying scales, resolutions, and boundary conditions. A subdomain neural operator model is developed to handle arbitrary Robin-type boundary conditions, serving as a flexible preconditioner for solving subdomain problems iteratively. The model also supports the construction of global coarse spaces for efficient large-scale PDE problem solving through multilevel domain decomposition. Using two-dimensional Maxwell's equations as a test case, a single neural network is trained to simulate diverse problem scenarios with varying sizes, resolutions, wavelengths, and media distributions. The study showcases the platform's effectiveness in accurately designing multi-wavelength nanophotonic devices through inverse design techniques.  <br /><br />Summary: <div>
arXiv:2509.03622v1 Announce Type: cross 
Abstract: Neural networks have promise as surrogate partial differential equation (PDE) solvers, but it remains a challenge to use these concepts to solve problems with high accuracy and scalability. In this work, we show that neural network surrogates can combine with iterative algorithms to accurately solve PDE problems featuring different scales, resolutions, and boundary conditions. We develop a subdomain neural operator model that supports arbitrary Robin-type boundary condition inputs, and we show that it can be utilized as a flexible preconditioner to iteratively solve subdomain problems with bounded accuracy. We further show that our subdomain models can facilitate the construction of global coarse spaces to enable accelerated, large scale PDE problem solving based on iterative multilevel domain decomposition. With two-dimensional Maxwell's equations as a model system, we train a single network to simulate large scale problems with different sizes, resolutions, wavelengths, and dielectric media distribution. We further demonstrate the utility of our platform in performing the accurate inverse design of multi-wavelength nanophotonic devices. Our work presents a promising path to building accurate and scalable multi-physics surrogate solvers for large practical problems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Linear Time Quantum Algorithm for Pairwise Sequence Alignment</title>
<link>https://arxiv.org/abs/2307.04479</link>
<guid>https://arxiv.org/abs/2307.04479</guid>
<content:encoded><![CDATA[
<div> Sequence Alignment, Quantum Algorithm, DNA sequences, Grover's search algorithm, optimal alignment

Summary: 
The paper introduces a Quantum Algorithm for sequence alignment, specifically for DNA sequences. By mapping the problem into a path-searching 2D graph and using a proposed oracle for profit calculation, the algorithm is able to find the optimal alignment in linear time. This surpasses classical deterministic algorithms in efficiency. By utilizing Grover's search algorithm, the proposed approach provides quadratic speeding up for unstructured search problems, ensuring optimal solutions deterministically. This is a significant improvement over existing randomized algorithms that often produce sub-optimal alignments. The quantum algorithm not only aligns sequences accurately but also guarantees finding the optimal solution, making it a promising tool for bioinformatics research. <br /><br />Summary: <div>
arXiv:2307.04479v2 Announce Type: replace-cross 
Abstract: Sequence Alignment is the process of aligning biological sequences in order to identify similarities between multiple sequences. In this paper, a Quantum Algorithm for finding the optimal alignment between DNA sequences has been demonstrated which works by mapping the sequence alignment problem into a path-searching problem through a 2D graph. The transition, which converges to a fixed path on the graph, is based on a proposed oracle for profit calculation. By implementing Grover's search algorithm, our proposed approach is able to align a pair of sequences and figure out the optimal alignment within linear time, which hasn't been attained by any classical deterministic algorithm. In addition to that, the proposed algorithm is capable of quadratic speeding up to any unstructured search problem by finding out the optimal paths accurately in a deterministic manner, in contrast to existing randomized algorithms that frequently sort out the sub-optimal alignments, therefore, don't always guarantee of finding out the optimal solutions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Use of Physicochemical Modification Methods for Producing Traditional and Nanomodified Polymeric Composites with Improved Operational Properties</title>
<link>https://arxiv.org/abs/2509.02572</link>
<guid>https://arxiv.org/abs/2509.02572</guid>
<content:encoded><![CDATA[
<div> surface modification, thermoplastic composite materials, interfacial interaction, ultrasonic processing, nanocomposites <br />
Summary: Various physical and physicochemical methods for modifying components of thermoplastic composite materials were analyzed. Improving the surface properties of fillers and the interaction between components of the composite is crucial for enhancing the reliability of the composite. Research focused on modifying the surface of reinforcing fibrous fillers and liquid polymer binders to improve contact properties within the composite. The effectiveness of low-frequency ultrasonic processing in enhancing interfacial interaction was highlighted. Cluster formation and physicochemical modification of epoxy polymers filled with dispersed fillers were discussed, with emphasis on ultrasonic cavitation for deagglomeration and nanoparticle distribution in nanocomposites. Experimental results showed improved technological and physicomechanical properties of sonicated epoxy matrices. The article also briefly touched on biological modifications of polymer components for functional applications. <br /><br />Summary: <div>
arXiv:2509.02572v1 Announce Type: new 
Abstract: Various aspects of the methods of physical and physicochemical modification of components of filled thermoplastic composite materials are analyzed, aimed at improving the surface properties of the fillers and the technological properties of the polymer matrix during their interaction. It is noted that the improvement of the interfacial interaction of the components of polymer reactoplastic composites, including adhesive strength, is a key factor for improving the reliability of the cured filled composite. As a promising area of research, a modification of the surface of the reinforcing fibrous filler and the technological characteristics of the liquid polymer binder, aimed at increasing their contact properties in the composite, was chosen. The effectiveness of the physical method of modifying the components of composites in the form of low-frequency ultrasonic processing is described. The peculiarities of cluster formation and physicochemical modification of epoxy polymers filled with dispersed fillers are analyzed. Attention is focused on the effectiveness of ultrasonic processing in the cavitation mode for deagglomeration and uniform distribution of nanoparticles in a liquid medium during the creation of nanocomposites. Experimentally confirmed is the improvement of the technological properties of liquid epoxy polymers, modified by ultrasound, used for the impregnation of oriented fibrous fillers, as well as the improvement of the physicomechanical properties of the sonicated epoxy matrices. Some issues of biological modifications of components of polymers for functional application are briefly reviewed.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Use ADAS Data to Predict Near-Miss Events: A Group-Based Zero-Inflated Poisson Approach</title>
<link>https://arxiv.org/abs/2509.02614</link>
<guid>https://arxiv.org/abs/2509.02614</guid>
<content:encoded><![CDATA[
<div> telematics, driving behavior, risk evaluation, usage-based insurance, zero-inflated Poisson framework  
Summary:  
The article discusses the utilization of driving behavior big data and telematics to understand how people drive and its applications in risk evaluation and insurance pricing. Traditional statistical models struggle to accurately analyze sparse and zero-inflated near-miss events captured by telematics. The study proposes zero-inflated Poisson frameworks that learn latent behavior groups and fit offset-based count models to improve weekly risk predictions. Using a naturalistic dataset from a fleet of commercial drivers, the results show significant improvements over prior models, with better calibration and lower information criteria values. Sensitivity analyses on the EM-based grouping demonstrate robust and interpretable gains, supporting context-aware ratemaking and fairer premiums by recognizing heterogeneous driving styles.<br /><br />Summary: <div>
arXiv:2509.02614v1 Announce Type: cross 
Abstract: Driving behavior big data leverages multi-sensor telematics to understand how people drive and powers applications such as risk evaluation, insurance pricing, and targeted intervention. Usage-based insurance (UBI) built on these data has become mainstream. Telematics-captured near-miss events (NMEs) provide a timely alternative to claim-based risk, but weekly NMEs are sparse, highly zero-inflated, and behaviorally heterogeneous even after exposure normalization. Analyzing multi-sensor telematics and ADAS warnings, we show that the traditional statistical models underfit the dataset. We address these challenges by proposing a set of zero-inflated Poisson (ZIP) frameworks that learn latent behavior groups and fit offset-based count models via EM to yield calibrated, interpretable weekly risk predictions. Using a naturalistic dataset from a fleet of 354 commercial drivers over a year, during which the drivers completed 287,511 trips and logged 8,142,896 km in total, our results show consistent improvements over baselines and prior telematics models, with lower AIC/BIC values in-sample and better calibration out-of-sample. We also conducted sensitivity analyses on the EM-based grouping for the number of clusters, finding that the gains were robust and interpretable. Practically, this supports context-aware ratemaking on a weekly basis and fairer premiums by recognizing heterogeneous driving styles.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Differentiation of Agent-Based Models</title>
<link>https://arxiv.org/abs/2509.03303</link>
<guid>https://arxiv.org/abs/2509.03303</guid>
<content:encoded><![CDATA[
<div> Agent-based models, automatic differentiation, computational burden, parameter calibration, variational inference <br />
<br />
Summary: <br />
Agent-based models (ABMs) are used to simulate complex systems by modeling individual agents and their interactions. However, the large number of agents in such systems requires significant computational resources and calibration of numerous parameters, limiting their widespread adoption. This paper demonstrates that automatic differentiation (AD) techniques can help alleviate these computational challenges by providing gradients of the simulator, making tasks like calibration and sensitivity analysis more efficient. By applying variational inference (VI) techniques enabled by AD, the study shows improved performance and computational savings in calibrating three different ABMs: Axtell's firm model, Sugarscape, and the SIR epidemiological model. This approach enhances the practicality and scalability of ABMs for studying complex systems. <br /> <div>
arXiv:2509.03303v1 Announce Type: cross 
Abstract: Agent-based models (ABMs) simulate complex systems by capturing the bottom-up interactions of individual agents comprising the system. Many complex systems of interest, such as epidemics or financial markets, involve thousands or even millions of agents. Consequently, ABMs often become computationally demanding and rely on the calibration of numerous free parameters, which has significantly hindered their widespread adoption. In this paper, we demonstrate that automatic differentiation (AD) techniques can effectively alleviate these computational burdens. By applying AD to ABMs, the gradients of the simulator become readily available, greatly facilitating essential tasks such as calibration and sensitivity analysis. Specifically, we show how AD enables variational inference (VI) techniques for efficient parameter calibration. Our experiments demonstrate substantial performance improvements and computational savings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape; and the SIR epidemiological model. Our approach thus significantly enhances the practicality and scalability of ABMs for studying complex systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems</title>
<link>https://arxiv.org/abs/2509.03340</link>
<guid>https://arxiv.org/abs/2509.03340</guid>
<content:encoded><![CDATA[
<div> machine learning, bifurcation phenomena, symmetries, flow matching, multistability 

Summary: 
This study addresses the challenge of capturing multiple stable solutions in nonlinear dynamical systems due to symmetry breaking and bifurcation phenomena. The proposed generative framework based on flow matching allows modeling the full probability distribution over bifurcation outcomes, enabling direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. A symmetric matching strategy aligns predicted and target outputs under group actions, facilitating accurate learning in equivariant settings. The method is validated on various systems, demonstrating superior performance in capturing multimodal distributions and symmetry-breaking bifurcations compared to non-probabilistic and variational approaches. Overall, flow matching offers a principled and scalable solution for modeling multistability in high-dimensional systems. <div>
arXiv:2509.03340v1 Announce Type: cross 
Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple coexisting stable solutions, particularly in the presence of symmetry breaking. Deterministic machine learning models struggle to capture this multiplicity, averaging over solutions and failing to represent lower-symmetry outcomes. In this work, we propose a generative framework based on flow matching to model the full probability distribution over bifurcation outcomes. Our method enables direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. We introduce a symmetric matching strategy that aligns predicted and target outputs under group actions, allowing accurate learning in equivariant settings. We validate our approach on a range of systems, from toy models to complex physical problems such as buckling beams and the Allen-Cahn equation. Our results demonstrate that flow matching significantly outperforms non-probabilistic and variational methods in capturing multimodal distributions and symmetry-breaking bifurcations, offering a principled and scalable solution for modeling multistability in high-dimensional systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Differentiable Boundary Element Solver for Hydrodynamic Sensitivity Analysis of Wave-Structure Interactions</title>
<link>https://arxiv.org/abs/2501.06988</link>
<guid>https://arxiv.org/abs/2501.06988</guid>
<content:encoded><![CDATA[
<div> wave-structure interactions, marine structures, boundary element method, linear potential flow theory, automatic differentiation

Summary:
Accurately predicting wave-structure interactions in marine structures is crucial for effective design and analysis. Current solvers using the boundary element method (BEM) rely on linear potential flow theory but lack the ability to provide sensitivities for system-level applications like design optimization. To address this limitation, a fully differentiable BEM solver has been developed, capable of estimating sensitivities. This advancement allows for precise estimation of wave-structure interaction sensitivity, which is crucial for optimizing designs. By incorporating automatic differentiation (AD) into BEM solvers, a more comprehensive understanding of wave-structure interactions can be achieved, enabling better design and analysis of marine structures. <div>
arXiv:2501.06988v2 Announce Type: replace 
Abstract: Accurately predicting wave-structure interactions is critical for the effective design and analysis of marine structures. This is typically achieved using solvers that employ the boundary element method (BEM), which relies on linear potential flow theory. Precise estimation of the sensitivity of these interactions is equally important for system-level applications such as design optimization. Current BEM solvers are unable to provide these sensitivities as they do not support automatic differentiation (AD). To address these challenges, we have developed a fully differentiable BEM solver
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Data Encoding and Variational Algorithms: A Framework for Hybrid Quantum Classical Machine Learning</title>
<link>https://arxiv.org/abs/2502.11951</link>
<guid>https://arxiv.org/abs/2502.11951</guid>
<content:encoded><![CDATA[
<div> QML, Quantum Machine Learning, classical data pipelines, hybrid quantum-classical models, CQ paradigm
<br />
Summary:
The article discusses the development of Quantum Machine Learning (QML) and its integration of quantum mechanics with classical machine learning. It proposes a broad architecture connecting classical data pipelines with quantum algorithms, emphasizing hybrid quantum-classical models for scalable quantum benefits. The Classical-Quantum (CQ) paradigm is highlighted, using classical encoding strategies to compress information into Hilbert space representations. Variational quantum circuits are explored to overcome device constraints. Experimental comparisons show that small quantum circuits can approximate probabilistic inference with competitive accuracy and robustness to noisy data. The article provides a roadmap for implementing quantum kernels, variational algorithms, and hybrid feedback loops in practice for optimization, computer vision, and medical diagnostics. It emphasizes the importance of strong data encoding and adaptive error protection in moving QML from theory to practice. 
<br /> <div>
arXiv:2502.11951v2 Announce Type: replace 
Abstract: The development of quantum computers has been the stimulus that enables the realization of Quantum Machine Learning (QML), an area that integrates the calculational framework of quantum mechanics with the adaptive properties of classical machine learning. This article suggests a broad architecture that allows the connection between classical data pipelines and quantum algorithms, hybrid quantum-classical models emerge as a promising route to scalable and near-term quantum benefit. At the core of this paradigm lies the Classical-Quantum (CQ) paradigm, in which the qubit states of high-dimensional classical data are encoded using sophisticated classical encoding strategies which encode the data in terms of amplitude and angle of rotation, along with superposition mapping. These techniques allow compression of information exponentially into Hilbert space representations, which, together with reduced sample complexity, allows greater feature expressivity. We also examine variational quantum circuits, quantum gates expressed as trainable variables that run with classical optimizers to overcome decoherence, noise, and gate-depth constraints of the existing Noisy Intermediate-Scale Quantum (NISQ) devices. Experimental comparisons with a Quantum Naive Bayes classifier prove that even small quantum circuits can approximate probabilistic inference with competitive accuracy compared to classical benchmarks, and have much better robustness to noisy data distributionsThis model does not only explain the algorithmic and architectural design of QML, it also offers a roadmap to the implementation of quantum kernels, variational algorithms, and hybrid feedback loops into practice, including optimization, computer vision, and medical diagnostics. The results support the idea that hybrid architectures with strong data encoding and adaptive error protection are key to moving QML out of theory to practice.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Metrics to Meaning: Time to Rethink Evaluation in Human-AI Collaborative Design</title>
<link>https://arxiv.org/abs/2402.07911</link>
<guid>https://arxiv.org/abs/2402.07911</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, human-AI collaboration, interactive design, evaluation methods, user engagement

Summary:
The study explores the interaction between humans and AI systems in the context of creative design. By analyzing the engagement of participants in a co-creative system called The Genetic Car Designer, the research highlights the importance of evaluating human-AI collaborative systems in a multidimensional manner. The system, which employs an interactive evolutionary algorithm, showed that exposure to design suggestions generated by the AI system led to enhanced cognitive and behavioral engagement, resulting in higher-quality design outcomes. The findings suggest that conventional evaluation methods focused solely on behavioral and design metrics may not capture the full extent of user engagement. It is proposed that evaluating human-AI systems holistically, considering emotional, behavioral, and cognitive states of the designer, is crucial for a comprehensive understanding of the user experience and the role of intelligent systems in creative design processes. <div>
arXiv:2402.07911v2 Announce Type: replace-cross 
Abstract: As AI systems increasingly shape decision making in creative design contexts, understanding how humans engage with these tools has become a critical challenge for interactive intelligent systems research. This paper contributes a challenge to rethink how to evaluate human--AI collaborative systems, advocating for a more nuanced and multidimensional approach. Findings from one of the largest field studies to date (n = 808) of a human--AI co-creative system, The Genetic Car Designer, complemented by a controlled lab study (n = 12) are presented. The system is based on an interactive evolutionary algorithm where participants were tasked with designing a simple two dimensional representation of a car. Participants were exposed to galleries of design suggestions generated by an intelligent system, MAP--Elites, and a random control. Results indicate that exposure to galleries generated by MAP--Elites significantly enhanced both cognitive and behavioural engagement, leading to higher-quality design outcomes. Crucially for the wider community, the analysis reveals that conventional evaluation methods, which often focus on solely behavioural and design quality metrics, fail to capture the full spectrum of user engagement. By considering the human--AI design process as a changing emotional, behavioural and cognitive state of the designer, we propose evaluating human--AI systems holistically and considering intelligent systems as a core part of the user experience -- not simply a back end tool.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel manifolds: nonlinear-augmentation dimensionality reduction using reproducing kernel Hilbert spaces</title>
<link>https://arxiv.org/abs/2509.00224</link>
<guid>https://arxiv.org/abs/2509.00224</guid>
<content:encoded><![CDATA[
<div> Kernel methods, Nonlinear dimensionality reduction, Quadratic manifold, Feature map, Reproducing kernel Hilbert space <br />
<br />
Summary: 
This paper introduces a novel approach to dimensionality reduction called kernel methods-based nonlinear-augmentation dimensionality reduction as an extension of quadratic manifold (QM) dimensionality reduction. The method involves augmenting linear dimensionality reduction with a nonlinear correction term in the reconstruction map to improve accuracy. Unlike previous methods that use least-squares optimal polynomial correction terms, this approach learns an optimal nonlinear correction from a reproducing kernel Hilbert space defined by the user. It allows for the imposition of various nonlinear structures on the correction term, including polynomial and radial basis function-based structures. The method is computationally efficient and exhibits decreasing error as the latent space dimension increases. Comparisons with other dimensionality reduction techniques like proper orthogonal decomposition and existing QM approaches demonstrate the effectiveness of the proposed method on various datasets. <div>
arXiv:2509.00224v1 Announce Type: new 
Abstract: This paper generalizes recent advances on quadratic manifold (QM) dimensionality reduction by developing kernel methods-based nonlinear-augmentation dimensionality reduction. QMs, and more generally feature map-based nonlinear corrections, augment linear dimensionality reduction with a nonlinear correction term in the reconstruction map to overcome approximation accuracy limitations of purely linear approaches. While feature map-based approaches typically learn a least-squares optimal polynomial correction term, we generalize this approach by learning an optimal nonlinear correction from a user-defined reproducing kernel Hilbert space. Our approach allows one to impose arbitrary nonlinear structure on the correction term, including polynomial structure, and includes feature map and radial basis function-based corrections as special cases. Furthermore, our method has relatively low training cost and has monotonically decreasing error as the latent space dimension increases. We compare our approach to proper orthogonal decomposition and several recent QM approaches on data from several example problems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I-FENN with DeepONets: accelerating simulations in coupled multiphysics problems</title>
<link>https://arxiv.org/abs/2509.00604</link>
<guid>https://arxiv.org/abs/2509.00604</guid>
<content:encoded><![CDATA[
<div> Keywords: DeepONet, Finite Element Method, multiphysics simulations, thermoelasticity, poroelasticity

Summary: 
This article introduces a novel framework that integrates DeepONet with the Finite Element Method to address coupled thermoelasticity and poroelasticity problems efficiently. The framework, called I-FENN, employs neural networks as PDE solvers within FEM, resulting in a hybrid staggered solver. By decoupling multiphysics interactions, the framework reduces computational costs while maintaining flexibility across various scenarios. The modified DeepONet architecture allows for multiple inputs and an efficient strategy for enforcing boundary conditions on distinct boundaries. Numerical examples demonstrate the computational efficiency, accuracy, and generalization capabilities of the proposed work, even under unseen loading conditions. The computational savings increase with model complexity while maintaining high accuracy levels in challenging regions of the domain. Overall, the framework shows promise in tackling high-dimensional, large-scale coupled multiphysics simulations. 

Summary: <div>
arXiv:2509.00604v1 Announce Type: new 
Abstract: Coupled multiphysics simulations for high-dimensional, large-scale problems can be prohibitively expensive due to their computational demands. This article presents a novel framework integrating a deep operator network (DeepONet) with the Finite Element Method (FEM) to address coupled thermoelasticity and poroelasticity problems. This integration occurs within the context of I-FENN, a framework where neural networks are directly employed as PDE solvers within FEM, resulting in a hybrid staggered solver. In this setup, the mechanical field is computed using FEM, while the other coupled field is predicted using a neural network (NN). By decoupling multiphysics interactions, the hybrid framework reduces computational cost by simplifying calculations and reducing the FEM unknowns, while maintaining flexibility across unseen scenarios. The proposed work introduces a new I-FENN architecture with extended generalizability due to the DeepONets ability to efficiently address several combinations of natural boundary conditions and body loads. A modified DeepONet architecture is introduced to accommodate multiple inputs, along with a streamlined strategy for enforcing boundary conditions on distinct boundaries. We showcase the applicability and merits of the proposed work through numerical examples covering thermoelasticity and poroelasticity problems, demonstrating computational efficiency, accuracy, and generalization capabilities. In all examples, the test cases involve unseen loading conditions. The computational savings scale with the model complexity while preserving an accuracy of more than 95\% in the non-trivial regions of the domain.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new definition of peridynamic damage for thermo-mechanical fracture modeling</title>
<link>https://arxiv.org/abs/2509.01079</link>
<guid>https://arxiv.org/abs/2509.01079</guid>
<content:encoded><![CDATA[
<div> Thermo-mechanical fracture modeling, thermal failure, heat conduction, continuum mechanics, peridynamic model<br />
Summary: A novel thermo-mechanical fracture modeling approach is proposed, combining classical continuum mechanics and peridynamic models to address thermal failure issues. The model incorporates a CCM/PD alternating solution for accurate calculations using finite element discretization. A new definition of PD damage is introduced, considering both the number and distribution of broken bonds for capturing damage in various directions. Validation against analytical solutions and simulations of crack propagation demonstrate the model's effectiveness in simulating complex thermal fractures and understanding initiation and propagation mechanisms. <div>
arXiv:2509.01079v1 Announce Type: new 
Abstract: A thermo-mechanical fracture modeling is proposed to address thermal failure issues, where the temperature field is calculated by a heat conduction model based on classical continuum mechanics (CCM), while the deformation field with discontinuities is calculated by the peridynamic (PD) model. The model is calculated by a CCM/PD alternating solution based on the finite element discretization, which ensures the calculation accuracy and facilitates engineering applications. The original PD model defines damage solely based on the number of broken bonds in the vicinity of the material point, neglecting the distribution of these bonds. To address this limitation, a new definition of the PD damage accounting for both the number of broken bonds and their specific distribution is proposed. As a result, damage in various directions can be captured, enabling more realistic thermal fracture simulations based on a unified mesh discretization. The effectiveness of the proposed model is validated by comparing numerical examples with analytical solutions. Moreover, simulation results of quasi-static and dynamic crack propagation demonstrate the model's ability to aid in understanding the initiation and propagation mechanisms of complex thermal fractures.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAMS: Residual-based adversarial-gradient moving sample method for scientific machine learning in solving partial differential equations</title>
<link>https://arxiv.org/abs/2509.01234</link>
<guid>https://arxiv.org/abs/2509.01234</guid>
<content:encoded><![CDATA[
<div> Neural networks, PDEs, PINNs, SciML, RAMS 
Summary: 
Physics-informed neural networks (PINNs) and neural operators are powerful tools for solving PDEs. Increasing the training sample size enhances network performance but increases computational costs. To address this trade-off, the residual-based adversarial-gradient moving sample (RAMS) method is proposed. RAMS moves samples based on the adversarial gradient direction to maximize the PDE residual, improving sampling efficiency for high-dimensional problems. It can be integrated into existing sampling methods. Extensive experiments show RAMS's effectiveness in PINNs for high-dimensional PDEs and operator learning tasks, making it the first efficient adaptive sampling approach for operator learning in the SciML field. <br /><br />Summary: <div>
arXiv:2509.01234v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) and neural operators, two leading scientific machine learning (SciML) paradigms, have emerged as powerful tools for solving partial differential equations (PDEs). Although increasing the training sample size generally enhances network performance, it also increases computational costs for physics-informed or data-driven training. To address this trade-off, different sampling strategies have been developed to sample more points in regions with high PDE residuals. However, existing sampling methods are computationally demanding for high-dimensional problems, such as high-dimensional PDEs or operator learning tasks. Here, we propose a residual-based adversarial-gradient moving sample (RAMS) method, which moves samples according to the adversarial gradient direction to maximize the PDE residual via gradient-based optimization. RAMS can be easily integrated into existing sampling methods. Extensive experiments, ranging from PINN applied to high-dimensional PDEs to physics-informed and data-driven operator learning problems, have been conducted to demonstrate the effectiveness of RAMS. Notably, RAMS represents the first efficient adaptive sampling approach for operator learning, marking a significant advancement in the SciML field.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A continuum multi-species biofilm model with a novel interaction scheme</title>
<link>https://arxiv.org/abs/2509.01274</link>
<guid>https://arxiv.org/abs/2509.01274</guid>
<content:encoded><![CDATA[
<div> biofilms, microorganisms, mathematical modeling, biofilm interactions, antibiotic agents 

Summary:
This article introduces a comprehensive multi-species continuum-based biofilm model that aims to understand the interactions between different species of microorganisms within biofilms. The model, derived using Hamilton's principle of stationary action, can replicate various biofilm interactions with an arbitrary number of species and incorporates the effects of nutrient sources and antibiotic agents on biofilm behavior. By combining mathematical modeling with in vitro and in vivo experiments, researchers can gain more insights into biofilm dynamics while reducing costs. The model demonstrates good quantitative agreement with biofilm behavior, showcasing its potential utility for researchers looking to study biofilm systems. <div>
arXiv:2509.01274v1 Announce Type: new 
Abstract: Biofilms are complex structures which are inhabited by numerous amount of different species of microorganisms. Due to their ubiquity, they influence human life on an everyday basis. It is therefore important to understand the interactions between different biofilm components and reactions to outside conditions. For this purpose, mathematical models and in silico experiments have proven themselves to be fundamental. In combination with in vitro and in vivo experiments, they can give more insights and focus researchers' attention, reducing costs in the process. In this work, a comprehensive multi-species continuum-based biofilm model is presented. This model is capable of replicating a variety of different biofilm interactions with an arbitrary number of species, while still being comprehensive to encourage usage by researchers less familiar with mathematical modeling. In addition to a nutrient source, antibiotic agents and their effect on the biofilm can also be depicted. The model is derived using Hamilton's principle of stationary action, ensuring thermodynamic consistency automatically. The results show good quantitative agreement with biofilm behavior.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Alpha Weighting with PPO: Enhancing Prompt-Based LLM-Generated Alphas in Quant Trading</title>
<link>https://arxiv.org/abs/2509.01393</link>
<guid>https://arxiv.org/abs/2509.01393</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, proximal policy optimization, large language model, formulaic alphas, stock trading strategies

Summary:
This paper presents a novel approach using reinforcement learning with Proximal Policy Optimization (PPO) to dynamically optimize the weights of multiple large language model-generated formulaic alphas for stock trading strategies. The study leverages a deepseek-r1-distill-llama-70b model to generate fifty alphas for five major stocks and applies PPO to adjust their weights in real time. The experimental results show that the PPO-optimized strategy outperforms an equal-weighted alpha portfolio and traditional benchmarks like Nikkei 225, S&amp;P 500, and Hang Seng Index. This highlights the significance of using reinforcement learning in the allocation of alpha weights and demonstrates the potential of combining large language model-generated signals with adaptive optimization for robust financial forecasting and trading. <div>
arXiv:2509.01393v1 Announce Type: new 
Abstract: This paper proposes a reinforcement learning framework that employs Proximal Policy Optimization (PPO) to dynamically optimize the weights of multiple large language model (LLM)-generated formulaic alphas for stock trading strategies. Formulaic alphas are mathematically defined trading signals derived from price, volume, sentiment, and other data. Although recent studies have shown that LLMs can generate diverse and effective alphas, a critical challenge lies in how to adaptively integrate them under varying market conditions. To address this gap, we leverage the deepseek-r1-distill-llama-70b model to generate fifty alphas for five major stocks: Apple, HSBC, Pepsi, Toyota, and Tencent, and then use PPO to adjust their weights in real time. Experimental results demonstrate that the PPO-optimized strategy achieves strong returns and high Sharpe ratios across most stocks, outperforming both an equal-weighted alpha portfolio and traditional benchmarks such as the Nikkei 225, S&amp;P 500, and Hang Seng Index. The findings highlight the importance of reinforcement learning in the allocation of alpha weights and show the potential of combining LLM-generated signals with adaptive optimization for robust financial forecasting and trading.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisit of Two-dimensional CEM on Crack Branching: from Single Crack-tip Tracking to Multiple Crack-tips Tracking</title>
<link>https://arxiv.org/abs/2509.01827</link>
<guid>https://arxiv.org/abs/2509.01827</guid>
<content:encoded><![CDATA[
<div> algorithm, crack tracking, fracture energy release rate, crack branching, GPU acceleration
Summary:
The article introduces the Multiple Crack-tips Tracking algorithm in two-dimensional Crack Element Model (MCT-2D-CEM) for predicting complex crack patterns in dynamic fracturing problems. The algorithm is developed to model advancements like crack branching and fragmentation. It utilizes a fracture energy release rate formulation for split elementary topology and includes benchmark examples to demonstrate its efficiency. The MCT-2D-CEM can also handle single crack propagation while introducing extra micro-cracks. The use of GPU acceleration in two-dimensional simulations ensures high computational efficiency, consistency, and accuracy. Overall, the algorithm offers a comprehensive solution for tracking and predicting advanced crack patterns in dynamic fracturing scenarios. 
<br /><br />Summary: <div>
arXiv:2509.01827v1 Announce Type: new 
Abstract: In this work, a Multiple Crack-tips Tracking algorithm in two-dimensional Crack Element Model (MCT-2D-CEM) is developed, aiming at modeling and predicting advanced and complicated crack patterns in two-dimensional dynamic fracturing problems, such as crack branching and fragmentation. Based on the developed fracture energy release rate formulation of split elementary topology, the Multiple Crack-tips Tracking algorithm is proposed and a series of benchmark examples are provided to validate effectiveness and efficiency in modeling crack branching and fragmentation. Besides, the proposed MCR-2D-CEM can still model single crack propagation but extra micro-cracks are introduced. GPU acceleration is employed in all two-dimensional simulations, providing high computational efficiency, consistency, and accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Fluid Dynamics Optimization of F1 Front Wing using Physics Informed Neural Networks</title>
<link>https://arxiv.org/abs/2509.01963</link>
<guid>https://arxiv.org/abs/2509.01963</guid>
<content:encoded><![CDATA[
<div> Neural Network, Computational Fluid Dynamics, Formula 1, Aerodynamics, Physics-Informed<br />
<br />
Summary: 
A Physics-Informed Neural Network (PINN) is proposed for fast prediction of Formula 1 front wing aerodynamic coefficients in response to new FIA regulations. The hybrid loss function combines CFD data with fluid dynamics principles to ensure accurate predictions while reducing computational time. With high R-squared values for drag and lift coefficient prediction, the PINN model offers F1 teams an efficient tool for design space exploration within the budget and time constraints. The physics-informed framework maintains adherence to fundamental aerodynamic principles, making it a valuable asset for aerodynamic development in the F1 industry. <div>
arXiv:2509.01963v1 Announce Type: new 
Abstract: In response to recent FIA regulations reducing Formula 1 team wind tunnel hours (from 320 hours for last-place teams to 200 hours for championship leaders) and strict budget caps of 135 million USD per year, more efficient aerodynamic development tools are needed by teams. Conventional computational fluid dynamics (CFD) simulations, though offering high fidelity results, require large computational resources with typical simulation durations of 8-24 hours per configuration analysis. This article proposes a Physics-Informed Neural Network (PINN) for the fast prediction of Formula 1 front wing aerodynamic coefficients. The suggested methodology combines CFD simulation data from SimScale with first principles of fluid dynamics through a hybrid loss function that constrains both data fidelity and physical adherence based on Navier-Stokes equations. Training on force and moment data from 12 aerodynamic features, the PINN model records coefficient of determination (R-squared) values of 0.968 for drag coefficient and 0.981 for lift coefficient prediction while lowering computational time. The physics-informed framework guarantees that predictions remain adherent to fundamental aerodynamic principles, offering F1 teams an efficient tool for the fast exploration of design space within regulatory constraints.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoder-based non-intrusive model order reduction in continuum mechanics</title>
<link>https://arxiv.org/abs/2509.02237</link>
<guid>https://arxiv.org/abs/2509.02237</guid>
<content:encoded><![CDATA[
<div> Autoencoder, reduced-order modeling, continuum mechanics, deep learning, surrogate models

Summary:

This article introduces a non-intrusive framework for reduced-order modeling in continuum mechanics using Autoencoder-based techniques. The framework comprises three stages: compression of high-dimensional finite element solutions into a latent space, mapping problem parameters to latent codes through regression networks, and reconstructing full-field solutions from input parameters. Two key extensions, including a force-augmented variant and a multi-field architecture, enhance the framework's capabilities, allowing for accurate predictions in complex scenarios such as thermo-mechanical coupling. The proposed method is validated on various benchmark problems and demonstrates accurate reconstructions of high-fidelity solutions without intruding on the original model. By combining deep learning with dimensionality reduction, this approach offers an efficient and extensible solution for building surrogate models in continuum mechanics. The publicly available implementation offers a foundation for integrating data-driven model order reduction into various applications, including uncertainty quantification, optimization, and digital twins.<br /><br />Summary: <div>
arXiv:2509.02237v1 Announce Type: new 
Abstract: We propose a non-intrusive, Autoencoder-based framework for reduced-order modeling in continuum mechanics. Our method integrates three stages: (i) an unsupervised Autoencoder compresses high-dimensional finite element solutions into a compact latent space, (ii) a supervised regression network maps problem parameters to latent codes, and (iii) an end-to-end surrogate reconstructs full-field solutions directly from input parameters.
  To overcome limitations of existing approaches, we propose two key extensions: a force-augmented variant that jointly predicts displacement fields and reaction forces at Neumann boundaries, and a multi-field architecture that enables coupled field predictions, such as in thermo-mechanical systems. The framework is validated on nonlinear benchmark problems involving heterogeneous composites, anisotropic elasticity with geometric variation, and thermo-mechanical coupling. Across all cases, it achieves accurate reconstructions of high-fidelity solutions while remaining fully non-intrusive.
  These results highlight the potential of combining deep learning with dimensionality reduction to build efficient and extensible surrogate models. Our publicly available implementation provides a foundation for integrating data-driven model order reduction into uncertainty quantification, optimization, and digital twin applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning-Fueled Modelfluid for Flowsheet Optimization</title>
<link>https://arxiv.org/abs/2509.02242</link>
<guid>https://arxiv.org/abs/2509.02242</guid>
<content:encoded><![CDATA[
<div> machine learning, process optimization, thermodynamic data, distillation, entrainer selection

Summary:
This article discusses the use of machine learning techniques to predict thermodynamic mixture properties for process optimization in chemical engineering. The authors present a novel modelfluid representation that integrates machine learning predicted data directly into flowsheet optimization, specifically tailored for distillation processes. The approach is built on physically interpretable features derived from vapor-liquid equilibrium phenomena, ensuring compatibility with existing simulation tools and optimization methods. The study demonstrates the accuracy and efficiency of this machine learning-fueled modelfluid by applying it to the problem of entrainer selection for azeotropic separation. Results show that the framework successfully identifies optimal entrainers with high fidelity compared to traditional models. This work provides a practical way to incorporate large-scale property prediction into process design, overcoming limitations of traditional thermodynamic models and complex equations of state.

<br /><br />Summary: <div>
arXiv:2509.02242v1 Announce Type: new 
Abstract: Process optimization in chemical engineering may be hindered by the limited availability of reliable thermodynamic data for fluid mixtures.
  Remarkable progress is being made in predicting thermodynamic mixture properties by machine learning techniques. The vast information provided by these prediction methods enables new possibilities in process optimization.
  This work introduces a novel modelfluid representation that is designed to seamlessly integrate these ML-predicted data directly into flowsheet optimization. Tailored for distillation, our approach is built on physically interpretable and continuous features derived from core vapor liquid equilibrium phenomena. This ensures compatibility with existing simulation tools and gradient-based optimization. We demonstrate the power and accuracy of this ML-fueled modelfluid by applying it to the problem of entrainer selection for an azeotropic separation. The results show that our framework successfully identifies optimal, thermodynamically consistent entrainers with high fidelity compared to conventional models.
  Ultimately, this work provides a practical pathway to incorporate large-scale property prediction into efficient process design and optimization, overcoming the limitations of both traditional thermodynamic models and complex molecular-based equations of state.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Electromechanical computational model of the human stomach</title>
<link>https://arxiv.org/abs/2509.02486</link>
<guid>https://arxiv.org/abs/2509.02486</guid>
<content:encoded><![CDATA[
<div> Keywords: gastric electromechanics, computational framework, peristalsis, motility disorders, organ scale

Summary:
The article presents a new computational framework for modeling human gastric electromechanics to study digestion and related motility disorders. It addresses the limitations of existing approaches by incorporating spatial heterogeneity, anisotropic deformations, and active-strain dynamics. The framework combines a rotation-free shell formulation with a constrained mixture material model, allowing for realistic simulation of gastric peristalsis. It can reproduce key features of gastric motility such as slow-wave entrainment, conduction velocity gradients, and peristaltic contractions. This new tool enables robust simulations of the entire stomach at the organ scale, offering promise for in-depth studies of both normal physiology and pathological conditions affecting gastric motility. <div>
arXiv:2509.02486v1 Announce Type: new 
Abstract: The stomach plays a central role in digestion through coordinated muscle contractions, known as gastric peristalsis, driven by slow-wave electrophysiology. Understanding this process is critical for treating motility disorders such as gastroparesis, dyspepsia, and gastroesophageal reflux disease. Computer simulations can be a valuable tool to deepen our understanding of these disorders and help to develop new therapies. However, existing approaches often neglect spatial heterogeneity, fail to capture large anisotropic deformations, or rely on computationally expensive three-dimensional formulations. We present here a computational framework of human gastric electromechanics, that combines a nonlinear, rotation-free shell formulation with a constrained mixture material model. The formulation incorporates active-strain, constituent-specific prestress, and spatially non-uniform parameter fields. Numerical examples demonstrate that the framework can reproduce characteristic features of gastric motility, including slow-wave entrainment, conduction velocity gradients, and large peristaltic contractions with physiologically realistic amplitudes. The proposed framework enables robust electromechanical simulations of the whole stomach at the organ scale. It thus provides a promising basis for future in silico studies of both physiological function and pathological motility disorders.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized Renewable Energy Planning MDP for Socially-Equitable Electricity Coverage in the US</title>
<link>https://arxiv.org/abs/2509.00008</link>
<guid>https://arxiv.org/abs/2509.00008</guid>
<content:encoded><![CDATA[
<div> optimization, renewable energy, social equity, electricity distribution, clean energy 

Summary: 
The study introduces a Markov Decision Process framework to optimize renewable energy allocation in the electricity distribution system while addressing social equity concerns. By considering budget constraints, energy demand variability, and social vulnerability indicators across major U.S. cities, the model evaluates policy alternatives for achieving equitable clean energy transitions. Numerical experiments show that an equity-focused approach can increase renewable energy penetration by 32.9% and reduce underserved low-income populations by 55% compared to conventional methods. The expert policy performed the best, while the Monte Carlo Tree Search baseline showed competitive performance with lower budget utilization. This study highlights that fair distribution of clean energy resources is possible without compromising system performance, offering a pathway to integrate social equity considerations with climate goals and provide inclusive access to clean power infrastructure. 

<br /><br />Summary: <div>
arXiv:2509.00008v1 Announce Type: cross 
Abstract: Traditional power grid infrastructure presents significant barriers to renewable energy integration and perpetuates energy access inequities, with low-income communities experiencing disproportionately longer power outages. This study develops a Markov Decision Process (MDP) framework to optimize renewable energy allocation while explicitly addressing social equity concerns in electricity distribution. The model incorporates budget constraints, energy demand variability, and social vulnerability indicators across eight major U.S. cities to evaluate policy alternatives for equitable clean energy transitions. Numerical experiments compare the MDP-based approach against baseline policies including random allocation, greedy renewable expansion, and expert heuristics. Results demonstrate that equity-focused optimization can achieve 32.9% renewable energy penetration while reducing underserved low-income populations by 55% compared to conventional approaches. The expert policy achieved the highest reward, while the Monte Carlo Tree Search baseline provided competitive performance with significantly lower budget utilization, demonstrating that fair distribution of clean energy resources is achievable without sacrificing overall system performance and providing ways for integrating social equity considerations with climate goals and inclusive access to clean power infrastructure.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Parameter Fields in Multi-Physics PDEs from Scarce Measurements</title>
<link>https://arxiv.org/abs/2509.00203</link>
<guid>https://arxiv.org/abs/2509.00203</guid>
<content:encoded><![CDATA[
<div> parameterized partial differential equations, parameter estimation methods, sparse measurements, Neptune, parameter fields<br />
<br />
Summary: Neptune is a novel method for inferring parameter fields from sparse measurements of system responses in parameterized partial differential equations. It addresses challenges in accurately estimating parameters with nonlinear and spatiotemporal variations. Neptune outperforms existing methods like sparse identification and PINNs by reducing parameter estimation errors significantly and improving dynamic response prediction accuracy. It achieves reliable parameter inference from a small number of observations and exhibits superior extrapolation capabilities compared to PINNs. Neptune's performance in various physical and biomedical problems makes it a promising tool for applications in engineering and healthcare, offering data-efficient and robust parameter estimation for complex systems. <div>
arXiv:2509.00203v1 Announce Type: cross 
Abstract: Parameterized partial differential equations (PDEs) underpin the mathematical modeling of complex systems in diverse domains, including engineering, healthcare, and physics. A central challenge in using PDEs for real-world applications is to accurately infer the parameters, particularly when the parameters exhibit non-linear and spatiotemporal variations. Existing parameter estimation methods, such as sparse identification and physics-informed neural networks (PINNs), struggle in such cases, especially with nonlinear dynamics, multiphysics interactions, or limited observations of the system response. To address these challenges, we introduce Neptune, a general-purpose method capable of inferring parameter fields from sparse measurements of system responses. Neptune employs independent coordinate neural networks to continuously represent each parameter field in physical space or in state variables. Across various physical and biomedical problems, where direct parameter measurements are prohibitively expensive or unattainable, Neptune significantly outperforms existing methods, achieving robust parameter estimation from as few as 50 observations, reducing parameter estimation errors by two orders of magnitude and dynamic response prediction errors by a factor of ten compared to PINNs. Furthermore, Neptune exhibits superior extrapolation capabilities, enabling accurate predictions in regimes beyond training data where PINN fail. By facilitating reliable and data-efficient parameter inference, Neptune promises broad transformative impacts in engineering, healthcare, and beyond.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilization techniques for immersogeometric analysis of plate and shell problems in explicit dynamics</title>
<link>https://arxiv.org/abs/2509.00522</link>
<guid>https://arxiv.org/abs/2509.00522</guid>
<content:encoded><![CDATA[
<div> Finite element, plate, shell, immersed finite element, lumped mass matrix <br />
<br />
Summary: <br />
The article discusses the challenges faced in dynamic analyses of slender structures using finite element plate and shell formulations due to high order partial differential equations and badly cut elements in immersed finite element discretizations. The critical time step in explicit dynamics is constrained, and lumping the mass matrix, while increasing the critical time step, can lead to spurious oscillations. The authors extend their previous work to enable stable immersogeometric analysis of plate and shell problems with lumped mass matrices by using polynomial extensions. This technique restores accuracy comparable to boundary-fitted discretizations, providing a solution to the issues faced in dynamic analysis of slender structures. <div>
arXiv:2509.00522v1 Announce Type: cross 
Abstract: Finite element plate and shell formulations are ubiquitous in structural analysis for modeling all kinds of slender structures, both for static and dynamic analyses. The latter are particularly challenging as the high order nature of the underlying partial differential equations and the slenderness of the structures all impose a stringent constraint on the critical time step in explicit dynamics. Unfortunately, badly cut elements in immersed finite element discretizations further aggravate the issue. While lumping the mass matrix often increases the critical time step, it might also trigger spurious oscillations in the approximate solution thereby compromising the numerical solution. In this article, we extend our previous work in \cite{voet2025stabilization} to allow stable immersogeometric analysis of plate and shell problems with lumped mass matrices. This technique is based on polynomial extensions and restores a level of accuracy comparable to boundary-fitted discretizations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures</title>
<link>https://arxiv.org/abs/2509.00633</link>
<guid>https://arxiv.org/abs/2509.00633</guid>
<content:encoded><![CDATA[
<div> thermal vulnerabilities, memory wall, HBM architectures, performance degradation attacks, thermal wave<br />
<br />
Summary: 3D-stacked High Bandwidth Memory (HBM) architectures address the memory wall challenge but are vulnerable to thermal attacks due to vertical adjacency. Adversaries could exploit this by injecting intense heat pulses from adjacent memory banks, creating a convergent thermal wave that delays victim applications. These attacks do not access out-of-range memory, bypassing security tests and memory management policies. Detection is difficult as the attack mimics legitimate workloads. <div>
arXiv:2509.00633v1 Announce Type: cross 
Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance memory interactions to address the well-known performance challenge, namely the memory wall. However, these architectures are susceptible to thermal vulnerabilities due to the inherent vertical adjacency that occurs during the manufacturing process of HBM architectures. We anticipate that adversaries may exploit the intense vertical and lateral adjacency to design and develop thermal performance degradation attacks on the memory banks that host data/instructions from victim applications. In such attacks, the adversary manages to inject short and intense heat pulses from vertically and/or laterally adjacent memory banks, creating a convergent thermal wave that maximizes impact and delays the victim application from accessing its data/instructions. As the attacking application does not access any out-of-range memory locations, it can bypass both design-time security tests and the operating system's memory management policies. In other words, since the attack mimics legitimate workloads, it will be challenging to detect.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resting-state fMRI Analysis using Quantum Time-series Transformer</title>
<link>https://arxiv.org/abs/2509.00711</link>
<guid>https://arxiv.org/abs/2509.00711</guid>
<content:encoded><![CDATA[
<div> fMRI, Quantum Time-series Transformer, neural biomarkers, computational complexity, interpretability analysis <br />
Summary:<br />
The article introduces a Quantum Time-series Transformer, a novel quantum-enhanced transformer architecture for analyzing resting-state fMRI data. It addresses the limitations of classical transformer models by reducing computational complexity, parameter counts, and data requirements. The Quantum Time-series Transformer outperforms traditional models in predictive performance, especially in small-sample scenarios, and reliably identifies neural biomarkers related to ADHD. By leveraging quantum techniques like Linear Combination of Unitaries and Quantum Singular Value Transformation, this approach shows promise in efficiently modeling complex brain dynamics and enhancing clinical interpretability in computational neuroscience. <div>
arXiv:2509.00711v1 Announce Type: cross 
Abstract: Resting-state functional magnetic resonance imaging (fMRI) has emerged as a pivotal tool for revealing intrinsic brain network connectivity and identifying neural biomarkers of neuropsychiatric conditions. However, classical self-attention transformer models--despite their formidable representational power--struggle with quadratic complexity, large parameter counts, and substantial data requirements. To address these barriers, we introduce a Quantum Time-series Transformer, a novel quantum-enhanced transformer architecture leveraging Linear Combination of Unitaries and Quantum Singular Value Transformation. Unlike classical transformers, Quantum Time-series Transformer operates with polylogarithmic computational complexity, markedly reducing training overhead and enabling robust performance even with fewer parameters and limited sample sizes. Empirical evaluation on the largest-scale fMRI datasets from the Adolescent Brain Cognitive Development Study and the UK Biobank demonstrates that Quantum Time-series Transformer achieves comparable or superior predictive performance compared to state-of-the-art classical transformer models, with especially pronounced gains in small-sample scenarios. Interpretability analyses using SHapley Additive exPlanations further reveal that Quantum Time-series Transformer reliably identifies clinically meaningful neural biomarkers of attention-deficit/hyperactivity disorder (ADHD). These findings underscore the promise of quantum-enhanced transformers in advancing computational neuroscience by more efficiently modeling complex spatio-temporal dynamics and improving clinical interpretability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Modeling for Personalized Transcranial Electrical Stimulation: Theory, Tools, and Applications</title>
<link>https://arxiv.org/abs/2509.01192</link>
<guid>https://arxiv.org/abs/2509.01192</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized tES, computational modeling, individualized stimulation optimization, head modeling, optimization algorithms<br />
Summary:<br />
The review focuses on the advancements in personalized transcranial electrical stimulation (tES) through computational modeling. It emphasizes the importance of individualized stimulation optimization due to the significant variability in brain anatomy and physiology among individuals. The review systematically examines recent developments in forward and inverse modeling techniques to simulate personalized electric fields and optimize stimulation parameters. It discusses the progress in constructing subject-specific head conductor models, utilizing optimization algorithms, and integrating multimodal brain data. Recent advancements have led to dynamic and individualized stimulation planning, moving away from traditional trial-and-error approaches. The review highlights the challenges, opportunities, and future directions in achieving precision neuromodulation in research and clinical settings. <div>
arXiv:2509.01192v1 Announce Type: cross 
Abstract: Objective. Personalized transcranial electrical stimulation (tES) has gained growing attention due to the substantial inter-individual variability in brain anatomy and physiology. While previous reviews have discussed the physiological mechanisms and clinical applications of tES, there remains a critical gap in up-to-date syntheses focused on the computational modeling frameworks that enable individualized stimulation optimization. Approach. This review presents a comprehensive overview of recent advances in computational techniques supporting personalized tES. We systematically examine developments in forward modeling for simulating individualized electric fields, as well as inverse modeling approaches for optimizing stimulation parameters. We critically evaluate progress in head modeling pipelines, optimization algorithms, and the integration of multimodal brain data. Main results. Recent advances have substantially accelerated the construction of subject-specific head conductor models and expanded the landscape of optimization methods, including multi-objective optimization and brain network-informed optimization. These advances allow for dynamic and individualized stimulation planning, moving beyond empirical trial-and-error approaches.Significance. By integrating the latest developments in computational modeling for personalized tES, this review highlights current challenges, emerging opportunities, and future directions for achieving precision neuromodulation in both research and clinical contexts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Bifurcation Handling in Physics-Based Reduced-Order Vascular Hemodynamic Models</title>
<link>https://arxiv.org/abs/2508.21165</link>
<guid>https://arxiv.org/abs/2508.21165</guid>
<content:encoded><![CDATA[
<div> machine learning, cardiovascular flows, reduced-order models, bifurcation coefficients, numerical framework

Summary:
- The study presents a numerical framework that combines machine learning-predicted bifurcation coefficients with zero-dimensional (0D) hemodynamic reduced-order models (ROMs) to enhance accuracy while maintaining computational efficiency.
- A resistor-resistor-inductor (RRI) model utilizing neural networks is developed to predict pressure-flow relationships based on bifurcation geometry, incorporating linear and quadratic resistances and inductive effects.
- Non-dimensionalization is used to reduce training data requirements, and a priori flow split prediction is employed for better bifurcation characterization.
- The RRI model is integrated into a 0D model using an optimization-based solution strategy, demonstrating significant accuracy improvements, especially at high Reynolds numbers and in complex vascular networks.
- The enhanced 0D models enable real-time hemodynamic modeling for clinical decision support, uncertainty quantification, and digital twins in cardiovascular biomedical engineering.

<br /><br />Summary: <div>
arXiv:2508.21165v1 Announce Type: new 
Abstract: Three-dimensional (3D) finite-element simulations of cardiovascular flows provide high-fidelity predictions to support cardiovascular medicine, but their high computational cost limits clinical practicality. Reduced-order models (ROMs) offer computationally efficient alternatives but suffer reduced accuracy, particularly at vessel bifurcations where complex flow physics are inadequately captured by standard Poiseuille flow assumptions. We present an enhanced numerical framework that integrates machine learning-predicted bifurcation coefficients into zero-dimensional (0D) hemodynamic ROMs to improve accuracy while maintaining computational efficiency. We develop a resistor-resistor-inductor (RRI) model that uses neural networks to predict pressure-flow relationships from bifurcation geometry, incorporating linear and quadratic resistances along with inductive effects. The method employs non-dimensionalization to reduce training data requirements and apriori flow split prediction for improved bifurcation characterization. We incorporate the RRI model into a 0D model using an optimization-based solution strategy. We validate the approach in isolated bifurcations and vascular trees, across Reynolds numbers from 0 to 5,500, defining ROM accuracy by comparison to 3D finite element simulation. Results demonstrate substantial accuracy improvements: averaged across all trees and Reynolds numbers, the RRI method reduces inlet pressure errors from 54 mmHg (45%) for standard 0D models to 25 mmHg (17%), while a simplified resistor-inductor (RI) variant achieves 31 mmHg (26%) error. The enhanced 0D models show particular effectiveness at high Reynolds numbers and in extensive vascular networks. This hybrid numerical approach enables accurate, real-time hemodynamic modeling for clinical decision support, uncertainty quantification, and digital twins in cardiovascular biomedical engineering.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A hyperreduced manifold learning approach to nonlinear model order reduction for the homogenisation of hyperelastic RVEs</title>
<link>https://arxiv.org/abs/2508.21527</link>
<guid>https://arxiv.org/abs/2508.21527</guid>
<content:encoded><![CDATA[
<div> Keywords: graph-based manifold learning, nonlinear Galerkin-reduction, model order reduction, hyperreduction methods, online computational costs <br />
Summary: 
The article presents a graph-based manifold learning scheme for nonlinear Galerkin-reduction in quasi-static solid mechanical problems. This approach allows for the creation of nonlinear approximation spaces that closely represent nonlinear solution manifolds. By integrating hyperreduction methods, the scheme significantly reduces online computational costs while maintaining high accuracy. The algorithmic complexity is independent of the original system size, and improvements to the local online linearization scheme enhance performance and robustness. In an example problem, the model order reduction scheme accelerates computations by over two orders of magnitude with minimal training data and negligible loss of accuracy. The approach outperforms alternative methods in the accuracy-runtime trade-off, showcasing its efficiency and effectiveness in reducing computational costs in nonlinear solid mechanics problems. <br /><br />Summary: <div>
arXiv:2508.21527v1 Announce Type: new 
Abstract: In a recent work, we proposed a graph-based manifold learning scheme for the nonlinear Galerkin-reduction of quasi-static solid mechanical problems [1]. The resulting nonlinear approximation spaces can closely and flexibly represent nonlinear solution manifolds. The present work discusses how this nonlinear model order reduction (MOR) approach can be employed to reduce online computational costs by multiple orders of magnitude while retaining high levels of accuracy. We integrate two popular hyperreduction methods into the nonlinear MOR framework and discuss how we achieve an algorithmic complexity which is independent from the original system size. Furthermore, improvements are made to the local online linearisation scheme for the sake of performance and robustness. On an example RVE problem, the MOR scheme accelerates computations by more than two orders of magnitude with little training data and negligible loss of accuracy. Additionally, the algorithm Pareto-dominates alternative approaches in the trade-off between accuracy and runtime on the considered example.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIPPO: A Graph-based, Iterative, Printing-Path Optimization Slicer for Architected Lattices</title>
<link>https://arxiv.org/abs/2508.21694</link>
<guid>https://arxiv.org/abs/2508.21694</guid>
<content:encoded><![CDATA[
<div> Graph-based, Iterative, Printing-Path Optimization, lattice structures, 3D printing, mechanical properties, open-source slicing platform, fabrication

Summary:
GIPPO is an open-source slicing platform that optimizes printing trajectories for complex lattice designs using a modified version of Prim's algorithm. It improves shape fidelity, reduces local thickness deviations, eliminates missing struts, and minimizes excess material deposition. GIPPO outperforms conventional slicing software in fabricating architected lattice structures made of thermoplastic polyurethane through fused deposition modeling. The optimization of printing paths directly affects the mechanical responses of the structures under different loading conditions. GIPPO accommodates planar and non-planar printing geometries and allows for the fabrication of objects with varying infill patterns per layer. This platform addresses limitations in commercial slicing software and enables high-fidelity fabrication of intricate architected materials. 

<br /><br />Summary: <div>
arXiv:2508.21694v1 Announce Type: new 
Abstract: Architected materials of significant geometric complexity offer exceptional mechanical properties that often surpass those of their constituent materials. However, their fabrication through extrusion-based 3D printing remains hindered by suboptimal printing trajectories, which is inherent to commercial slicing software. They produce multiple non-continuous paths that compromise fabrication time, shape fidelity, and structural integrity, particularly for thin-walled lattice structures. To address this issue, we introduce GIPPO (Graph-based, Iterative, Printing-Path Optimization), an open-source slicing platform that transforms complex lattice designs into optimized printing trajectories. Lattices are converted to graph networks to derive the optimal printing trajectories through a modified version of Prim's algorithm. The resulting paths are translated back to Euclidean coordinates and exported as a ready-to-use G-code. We validated GIPPO's performance against conventional slicing software across six architected lattice geometries fabricated from thermoplastic polyurethane using fused deposition modeling. GIPPO-optimized constructs demonstrated superior shape fidelity with reduced local thickness deviations, no missing struts, and minimized excess material deposition compared to conventionally printed controls. Mechanical testing revealed that printing path optimization directly influences both uniaxial and out-of-plane mechanical responses, with different optimization strategies yielding distinct performance characteristics suited to specific loading conditions. Moreover, the platform accommodates both planar and non-planar printing geometries and enables fabrication of objects with varying infill patterns per layer. Our work addresses critical limitations in commercial slicing software and opens new opportunities for high-fidelity fabrication of complex architected materials.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Financial Brain Scan of the LLM</title>
<link>https://arxiv.org/abs/2508.21285</link>
<guid>https://arxiv.org/abs/2508.21285</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, economic forecasts, sentiment, risk-averse, biases

Summary:
Using advanced computer science techniques, researchers can analyze large language models (LLMs) to understand the concepts guiding their economic forecasts. This method allows for the identification of key factors such as sentiment, technical analysis, and timing without compromising performance. The approach also enables researchers to manipulate the models to be more or less risk-averse, optimistic, or pessimistic, providing opportunities to correct or simulate biases in the forecasts. Importantly, this method is transparent, easy to implement, and replicable, making it a valuable tool for empirical research in the social sciences.<br /><br />Summary: <div>
arXiv:2508.21285v1 Announce Type: cross 
Abstract: Emerging techniques in computer science make it possible to "brain scan" large language models (LLMs), identify the plain-English concepts that guide their reasoning, and steer them while holding other factors constant. We show that this approach can map LLM-generated economic forecasts to concepts such as sentiment, technical analysis, and timing, and compute their relative importance without reducing performance. We also show that models can be steered to be more or less risk-averse, optimistic, or pessimistic, which allows researchers to correct or simulate biases. The method is transparent, lightweight, and replicable for empirical research in the social sciences.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality</title>
<link>https://arxiv.org/abs/2508.21736</link>
<guid>https://arxiv.org/abs/2508.21736</guid>
<content:encoded><![CDATA[
<div> Keywords: Microbiomes, mathematical modeling, spatiotemporal data, virtual reality, data analysis

Summary: <br /><br />Microbiomes play a crucial role in the human body, but studying them experimentally is challenging, leading to more research in mathematical modeling. Current tools for simulating microbial communities lack interactive functionalities and are complex to use. To address these limitations, a user-friendly tool called MicroLabVR has been developed. It transfers spatial data into virtual reality (VR) and allows users to explore spatiotemporal simulation data interactively. Users can import datasets containing population growth, substance concentration development, and metabolic flux distribution data. This tool aims to improve data analysis by enabling the exploration of microbiome data in their spatial context. <div>
arXiv:2508.21736v1 Announce Type: cross 
Abstract: Microbiomes are a vital part of the human body, engaging in tasks like food digestion and immune defense. Their structure and function must be understood in order to promote host health and facilitate swift recovery during disease. Due to the difficulties in experimentally studying these systems in situ, more research is being conducted in the field of mathematical modeling. Visualizing spatiotemporal data is challenging, and current tools that simulate microbial communities' spatial and temporal development often only provide limited functionalities, often requiring expert knowledge to generate useful results. To overcome these limitations, we provide a user-friendly tool to interactively explore spatiotemporal simulation data, called MicroLabVR, which transfers spatial data into virtual reality (VR) while following guidelines to enhance user experience (UX). With MicroLabVR, users can import CSV datasets containing population growth, substance concentration development, and metabolic flux distribution data. The implemented visualization methods allow users to evaluate the dataset in a VR environment interactively. MicroLabVR aims to improve data analysis for the user by allowing the exploration of microbiome data in their spatial context.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Hypergraph Diffusion for Crystal Structure Prediction</title>
<link>https://arxiv.org/abs/2501.18850</link>
<guid>https://arxiv.org/abs/2501.18850</guid>
<content:encoded><![CDATA[
<div> Keywords: Crystal Structure Prediction, Generative Models, Hypergraphs, EH-Diff, Symmetry-preserving Properties

Summary: 
Crystal Structure Prediction (CSP) is a challenging task crucial for developing new materials. Traditional graph-based models struggle to capture complex high-order interactions in crystal structures. This study introduces a novel approach using hypergraphs to represent crystal structures, allowing for the modeling of multi-way atomic interactions. The Equivariant Hypergraph Diffusion Model (EH-Diff) is proposed as a generative model that leverages the symmetry-preserving properties of hypergraphs to accurately predict crystal structures. Experimental results on benchmark datasets show that EH-Diff outperforms existing CSP methods with just one sample. This approach offers an efficient and accurate method for crystal structure prediction, emphasizing the importance of symmetry and high-order relationships in accurately characterizing crystal structures.<br /><br />Summary: <div>
arXiv:2501.18850v2 Announce Type: replace 
Abstract: Crystal Structure Prediction (CSP) remains a fundamental challenge with significant implications for the development of new materials and the advancement of various scientific disciplines. Recent developments have shown that generative models, particularly diffusion models, hold great promise for CSP. However, traditional graph-based representations, where atomic bonds are modeled as pairwise graph edges, fail to fully capture the intricate high-order interactions essential for accurately representing crystal structures. In this work, we propose a novel approach that utilizes hypergraphs to represent crystal structures, providing a more expressive abstraction for modeling multi-way atomic interactions. By adopting hypergraphs, we can effectively capture complex high-order relationships and symmetries, such as permutation and periodic translation invariance, which are crucial for characterizing crystal structures. In this work, we propose the \textbf{E}quivariant \textbf{H}ypergraph \textbf{Diff}usion Model (\textbf{EH-Diff}), a generative model designed to take advantage of the symmetry-preserving properties of hypergraphs. EH-Diff exploits these features to offer an efficient and accurate method for predicting crystal structures with a strong theoretical justification to preserve invariance properties. Empirically, we conduct extensive experiments on four benchmark datasets, and the results demonstrate that EH-Diff outperforms state-of-the-art CSP methods with only one sample.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mass conservation analysis of extrusion-based 3D printing simulations based on the level-set method</title>
<link>https://arxiv.org/abs/2508.20617</link>
<guid>https://arxiv.org/abs/2508.20617</guid>
<content:encoded><![CDATA[
<div> conservative level-set method, mass conservation, extrusion-based printing, numerical simulations, cross-sectional area<br />
Summary:<br />
The article investigates the mass conservation properties of the conservative level-set method in extrusion-based 3D printing. It focuses on tracking evolving material boundaries accurately to avoid mismatches between the extruded and simulated shapes. The study analyzes the impact of level set parameters on mass conservation accuracy, specifically looking at the cross-sectional area of deposited strands. Results show that reducing reinitialization and interface thickness parameters decreases errors in cross-sectional area calculations but may increase computational costs. Selecting an appropriate interface thickness can also reduce strong mesh requirements. Comparing simulated cross-sectional areas with ideal areas from a mass balance at steady state indicates good agreement, validating the method's accuracy. The research contributes valuable insights into improving mass conservation in extrusion-based 3D printing simulations. <br /><br />Summary: <div>
arXiv:2508.20617v1 Announce Type: new 
Abstract: Numerical simulations of extrusion-based printing require tracking evolving material bound- aries, a challenging task due to possible topological changes and mass conservation issues. Inaccurate conservation of mass can lead to a mismatch between the extruded and simulated shapes, and generally to unreliable predictions of the actual ink behavior. This work investigates the mass conservation properties of the conservative level-set method in extrusion-based 3D printing applications. We analyze the effects of the level set parameters on the accuracy of mass conservation using the cross-sectional area of the deposited strand. We compare the cross- sectional areas obtained in the simulation with the ideal areas obtained from a mass balance when the system reaches a steady-state condition. The numerical results indicate that reducing the reinitialization and the interface thickness parameters decreases the errors in the cross-sectional area obtained. However, the reductions in error tend to decline and could lead to excessive computational cost. Furthermore, we also found that the typical strong mesh requirements can be lessened by selecting an adequate interface thickness. Finally, we obtained the cross-sectional areas from simulations with different printing settings and found that they show good agreement with the simulated and experimental data published in previous work.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can News Predict the Direction of Oil Price Volatility? A Language Model Approach with SHAP Explanations</title>
<link>https://arxiv.org/abs/2508.20707</link>
<guid>https://arxiv.org/abs/2508.20707</guid>
<content:encoded><![CDATA[
<div> Keywords: financial markets, crude oil, news analysis, predictive modeling, sentiment analysis 

Summary:<br /><br />Financial markets are influenced by news, sentiment, and economic indicators, impacting asset price fluctuations. This study focuses on crude oil price volatility prediction using news data exclusively, comparing it to traditional market data methods. Utilizing a decade-long Eikon dataset, an ensemble learning framework incorporating sentiment analysis techniques and language models is developed. The model's performance is compared to the HAR model through the McNemar test, with raw news count being a significant predictor. FastText emerges as the most effective embedding technique for forecasting price movements. SHAP-based interpretation at the word level reveals evolving predictive drivers during different market regimes. Pre-pandemic factors included supply-demand and economic terms, early pandemic emphasized uncertainty and macroeconomic instability, post-shock focused on long-term recovery indicators, and war-period considered geopolitical and regional oil market disruptions. These findings highlight the potential of news-driven features and explainable NLP in financial forecasting. <div>
arXiv:2508.20707v1 Announce Type: new 
Abstract: Financial markets can be highly sensitive to news, investor sentiment, and economic indicators, leading to important asset price fluctuations. In this study we focus on crude oil, due to its crucial role in commodity markets and the global economy. Specifically, we are interested in understanding the directional changes of oil price volatility, and for this purpose we investigate whether news alone -- without incorporating traditional market data -- can effectively predict the direction of oil price movements. Using a decade-long dataset from Eikon (2014-2024), we develop an ensemble learning framework to extract predictive signals from financial news. Our approach leverages diverse sentiment analysis techniques and modern language models, including FastText, FinBERT, Gemini, and LLaMA, to capture market sentiment and textual patterns. We benchmark our model against the Heterogeneous Autoregressive (HAR) model and assess statistical significance using the McNemar test. While most sentiment-based indicators do not consistently outperform HAR, the raw news count emerges as a robust predictor. Among embedding techniques, FastText proves most effective for forecasting directional movements. Furthermore, SHAP-based interpretation at the word level reveals evolving predictive drivers across market regimes: pre-pandemic emphasis on supply-demand and economic terms; early pandemic focus on uncertainty and macroeconomic instability; post-shock attention to long-term recovery indicators; and war-period sensitivity to geopolitical and regional oil market disruptions. These findings highlight the predictive power of news-driven features and the value of explainable NLP in financial forecasting.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-consistent clustering analysis for homogenisation of heterogeneous plates</title>
<link>https://arxiv.org/abs/2508.20446</link>
<guid>https://arxiv.org/abs/2508.20446</guid>
<content:encoded><![CDATA[
<div> plate structures, reduced-order model, periodic micro-structures, self-consistent clustering analysis, Lippmann-Schwinger equation

Summary: 
This study presents a novel reduced-order model for plate structures with periodic micro-structures. By combining self-consistent clustering analysis (SCA) with the Lippmann-Schwinger equation, the model allows for fast multiscale homogenization of heterogeneous plates. A plate-specific SCA scheme is developed, incorporating an offline-online strategy utilizing Green's functions and k-means data compression, as well as an online self-consistent update leveraging the weak sensitivity of the reference medium. The framework is applicable to both linear and nonlinear problems in classical plate theory and first-order shear deformation theory, demonstrating its accuracy on various plate configurations. The proposed model matches the precision of FFT-based direct numerical simulation while significantly reducing computational cost. Examples include linear isotropic perforated plates, woven composites, and nonlinear elasto-plastic perforated plates with damage. This innovative approach enables efficient analysis of complex plate structures with periodic micro-structures. <div>
arXiv:2508.20446v1 Announce Type: cross 
Abstract: This work introduces a reduced-order model for plate structures with periodic micro-structures by coupling self-consistent clustering analysis (SCA) with the Lippmann-Schwinger equation, enabling rapid multiscale homogenisation of heterogeneous plates. A plate-specific SCA scheme is derived for the first time and features two key elements: (i) an offline-online strategy that combines Green's functions with k-means data compression, and (ii) an online self-consistent update that exploits the weak sensitivity of the reference medium. The framework handles both linear and nonlinear problems in classical plate theory and first-order shear deformation theory, and its performance is verified on linear isotropic perforated plates and woven composites, as well as on non-linear elasto-plastic perforated plates and woven composites with damage. Across all cases the proposed model matches the accuracy of FFT-based direct numerical simulation while reducing computational cost by over an order of magnitude.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation</title>
<link>https://arxiv.org/abs/2508.20806</link>
<guid>https://arxiv.org/abs/2508.20806</guid>
<content:encoded><![CDATA[
<div> epistemic uncertainty, Epistemic Support-Point Filter, possibility theory, ordinal logic, Choquet integral <br /> 
Summary: The Epistemic Support-Point Filter (ESPF) introduces a non-Bayesian filtering framework that addresses the limitations of traditional state estimation methods. ESPF is grounded in possibility theory and emphasizes epistemic humility by redefining belief evolution using compatibility-weighted support updates and surprisal-aware pruning. It adapts belief support through adaptive dispersion via sparse grid quadrature and employs the Choquet integral for multi-model inference. ESPF does not seek a posterior distribution but maintains a structured region of plausibility, updating using ordinal logic. This approach allows for dynamic contraction or expansion of belief support based on information structure without requiring prior statistical calibration. The framework supports robust estimation in sparse or adversarial sensing environments where priors are unavailable, misleading, or epistemically unjustified. <br /> 
Summary: <div>
arXiv:2508.20806v1 Announce Type: cross 
Abstract: Traditional state estimation methods rely on probabilistic assumptions that often collapse epistemic uncertainty into scalar beliefs, risking overconfidence in sparse or adversarial sensing environments. We introduce the Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework fully grounded in possibility theory and epistemic humility. ESPF redefines the evolution of belief over state space using compatibility-weighted support updates, surprisalaware pruning, and adaptive dispersion via sparse grid quadrature. Unlike conventional filters, ESPF does not seek a posterior distribution, but rather maintains a structured region of plausibility or non-rejection, updated using ordinal logic rather than integration. For multi-model inference, we employ the Choquet integral to fuse competing hypotheses based on a dynamic epistemic capacity function, generalizing classical winner-take-all strategies. The result is an inference engine capable of dynamically contracting or expanding belief support in direct response to information structure, without requiring prior statistical calibration. This work presents a foundational shift in how inference, evidence, and ignorance are reconciled, supporting robust estimation where priors are unavailable, misleading, or epistemically unjustified.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scale-invariant Monte Carlo and multilevel Monte Carlo estimation of mean and variance: An application to simulation of linear elastic bone tissue</title>
<link>https://arxiv.org/abs/2106.13723</link>
<guid>https://arxiv.org/abs/2106.13723</guid>
<content:encoded><![CDATA[
<div> scale-invariant, error estimators, Monte Carlo, multilevel Monte Carlo, mechanical simulation

Summary:
The article introduces novel scale-invariant error estimators for Monte Carlo and multilevel Monte Carlo methods used in estimating mean and variance. These estimators optimize computation costs across different grid levels for linear transformations of the quantity of interest, remaining robust to distribution variations. The proposed algorithms are demonstrated in a mechanical simulation of linear elastic bone tissue, incorporating material uncertainty with heterogeneity and random anisotropy in the constitutive law. The new error estimators are fully dimensionless and offer improved efficiency in estimating mean and variance, making them suitable for a wide range of applications where accurate estimation of uncertainty is essential.<br /><br />Summary: <div>
arXiv:2106.13723v3 Announce Type: replace-cross 
Abstract: We propose novel scale-invariant error estimators for the Monte Carlo and multilevel Monte Carlo estimation of mean and variance. For any linear transformation of the distribution of the quantity of interest, the computation cost across grid levels is optimized using a normalized error estimate, which is not only fully dimensionless but also remains robust to variation in characteristics of the distribution. We demonstrate the effectiveness of the algorithms through application to a mechanical simulation of linear elastic bone tissue, where material uncertainty incorporating both heterogeneity and random anisotropy is considered in the constitutive law.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infrastructure-enabled risk assessment of hazardous road conditions on rural roads during inclement weather</title>
<link>https://arxiv.org/abs/2508.19444</link>
<guid>https://arxiv.org/abs/2508.19444</guid>
<content:encoded><![CDATA[
<div> Keywords: Rural roadways, Commercial Motor Vehicle drivers, hazardous conditions, roadway hazard risk assessment, safe advisory speeds

Summary: 
The study addresses the lack of real-time reporting of hazardous conditions on rural roadways and limited infrastructure, increasing the risk of crashes for Commercial Motor Vehicle (CMV) drivers. The framework presented quantifies the probability and severity of crash occurrences due to specific roadway hazards, providing a comprehensive approach to assess combined driving risks. A synthetic dataset was used for a case study, confirming the coherence of the risk profile generated by the combined ProbabilitySeverity scoring. The results validate the practicality of the risk assessment approach and suggest implementing graduated safety measures in real-world roadway operations.<br /><br />Summary: <div>
arXiv:2508.19444v1 Announce Type: new 
Abstract: Rural roadways often expose Commercial Motor Vehicle (CMV) drivers to hazardous conditions, such as heavy fog, rain, snow, black ice, and flash floods, many of which remain unreported in real time. This lack of timely information, coupled with limited infrastructure in rural areas, significantly increases the risk of crashes. Although various sensing technologies exist to monitor individual hazards like low visibility or surface friction, they rarely assess the combined driving risk posed by multiple simultaneous hazards, nor do they provide actionable recommendations such as safe advisory speeds. To address this critical gap, in this study, we present a roadway hazard risk assessment framework that provides an approach to quantify the probability and severity of crash occurrences due to specific roadway hazards. To evaluate this framework, we presented a case study by constructing a synthetic "year-long" dataset that encompasses every possible pairing of road surface and visibility conditions. Our analysis confirms that the combined ProbabilitySeverity scoring yields a coherent, stepwise risk profile across all hazard scenarios. These results validate the practicality of our risk assessment approach and provide a foundation for deploying graduated safety measures in real-world roadway operations.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An assessment of estimation models and investment gaps for the deployment of high-speed broadband networks in NUTS3 regions to meet the objectives of the European Gigabit Society</title>
<link>https://arxiv.org/abs/2508.19921</link>
<guid>https://arxiv.org/abs/2508.19921</guid>
<content:encoded><![CDATA[
<div> European Union, high speed broadband networks, investment, European Gigabit Society, estimation model <br />
Summary: 
This paper examines the deployment of high speed broadband networks in the European Union, specifically focusing on the investment required to achieve the targets set by the European Commission for 2025 as part of the European Gigabit Society. The analysis includes assessing the availability and adoption of high capacity fixed and wireless networks in urban and rural areas. The estimation model used in the study incorporates data at the local level to determine the investment gap for each EGS objective. Three scenarios based on technology mixes are considered. The paper compares its methodology with existing literature and provides a dynamic view of the investment gap evolution from 2017 to 2019. The analysis proves the usefulness of the estimation models in evaluating the investment needed for high speed broadband infrastructure in the EU. <br /><br /> <div>
arXiv:2508.19921v1 Announce Type: new 
Abstract: This paper analyses the deployment of high speed broadband networks in the European Union (EU). Its aim is to assess the investment required to meet the targets set by the European Commission (EC) for 2025, within the framework of the European Gigabit Society (EGS). This plan aims to ensure the availability and take up of very high capacity fixed and wireless networks, in both urban and rural areas, among households and the main socioeconomic drivers. The estimation model presented here uses a methodology supported by data at the local (NUTS3) level to give a bottom up estimation of the investment gap for each of the EGS objectives, using three different scenarios depending on the mix of wired and wireless technologies offered. The methodology and estimation model used in the paper are examined against other examples and assumptions available in the literature. We also offer a dynamic perspective on the analysis of the evolution of this investment gap over the years 2017 2019, which includes an assessment of the usefulness of these estimation models.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-field decomposed hyper-reduced order modeling of damage-plasticity simulations</title>
<link>https://arxiv.org/abs/2508.19957</link>
<guid>https://arxiv.org/abs/2508.19957</guid>
<content:encoded><![CDATA[
<div> DEIM, ECSW, multi-field decomposed approach, hyper-reduced order modeling, gradient-extended damage-plasticity simulations <br />
Summary: <br />
This paper introduces a new approach for hyper-reduced order modeling to address the limitations of traditional model reduction techniques in gradient-extended damage-plasticity simulations. The method involves extending the discrete empirical interpolation method (DEIM) and the energy-conserving sampling and weighting method (ECSW) to accommodate the multi-field nature of the problem. By applying these methods, stable reduced order simulations are achieved while significantly reducing computational costs compared to full-order simulations. Through two numerical examples, the proposed approaches' performance and limitations are demonstrated. The decomposed ECSW method proves to have higher accuracy and lower computational cost than the decomposed DEIM method, showcasing its superiority in hyper-reduced order modeling for complex simulations. <div>
arXiv:2508.19957v1 Announce Type: new 
Abstract: This paper presents a multi-field decomposed approach for hyper-reduced order modeling to overcome the limitations of traditional model reduction techniques for gradient-extended damage-plasticity simulations. The discrete empirical interpolation method (DEIM) and the energy-conserving sampling and weighting method (ECSW) are extended to account for the multi-field nature of the problem. Both methods yield stable reduced order simulations, while significantly reducing the computational cost compared to full-order simulations. Two numerical examples are presented to demonstrate the performance and limitations of the proposed approaches. The decomposed ECSW method has overall higher accuracy and lower computational cost than the decomposed DEIM method.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From stand-up to start-up: exploring entrepreneurship competences and STEM womens intention</title>
<link>https://arxiv.org/abs/2508.20091</link>
<guid>https://arxiv.org/abs/2508.20091</guid>
<content:encoded><![CDATA[
<div> STEM, entrepreneurship competencies, intention, gender differences, European Commission

Summary: 
The study examines the relationship between entrepreneurship competencies and intention among potential STEM entrepreneurs. Contrary to the assumption, there is no significant difference in entrepreneurship intention between men and women. Gender does not act as a moderating factor in the relationship between competencies and intention. The analysis, based on the Entrepreneurship Competences Framework by the European Commission, highlights a positive correlation between competencies and entrepreneurship intention. Self-perceived competences show minor variations based on gender. These findings debunk the belief that women have lower rates of entrepreneurship intention due to perceived lack of competence. The study's results provide valuable insights for entrepreneurship education and business creation initiatives. <div>
arXiv:2508.20091v1 Announce Type: new 
Abstract: This study seeks to explore the relationship between entrepreneurship competencies and intention (EI) of a sample of potential STEM entrepreneurs in order to assess the conventional assumption on women exhibiting lower rates of entrepreneurship intention than men and that the lack of competence perceived is a higher barrier to be an entrepreneur for them. The model used for the analysis takes as reference the Entrepreneurship Competences Framework (EntreComp) proposed by the European Commission (EC) as a common guide to inspire entrepreneurship education. Data gathering is based on a structured questionnaire. The conducted analysis uses Students t test means comparison and factor analysis to define the model of competences, and a multiple regression model to study the relationship between competences and skill factors in EI. Findings do not validate the hypothesis that women have fewer entrepreneurship intentions than men. Also, slight differences on the self-perceived competences are obtained by gender. In addition, the study confirms the hypothesis of a positive relationship between competences and EI, but here gender is not a moderating factor. Results are expected to contribute to the entrepreneurship competences debate and provide useful insights of application in entrepreneurship education with orientation towards the business creation.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Trade-Off: An Analysis of the Operational Breakdown and Ontological Limits of "Certainty-Scope" in AI</title>
<link>https://arxiv.org/abs/2508.19304</link>
<guid>https://arxiv.org/abs/2508.19304</guid>
<content:encoded><![CDATA[
<div> Keywords: Floridi's conjecture, artificial intelligence, certainty, scope, operationalization

Summary:<br /><br />
Floridi's conjecture on the trade-off between certainty and scope in artificial intelligence systems is discussed in this paper. The conjecture, while conceptually sound, faces challenges in practical implementation due to its reliance on incomputable constructs and its assumption of AI systems as self-contained entities. This hinders its ability to inform the design, deployment, and governance of real-world AI systems in complex human-centric domains. The paper argues that these limitations prevent the conjecture from being actionable and verifiable in real-world scenarios. The authors propose a re-framing of Floridi's epistemic challenge to address the epistemic burdens of AI within dynamic socio-technical environments, aiming to bridge the gap between theoretical insights and practical applications in AI engineering and regulation. <div>
arXiv:2508.19304v1 Announce Type: cross 
Abstract: Floridi's conjecture offers a compelling intuition about the fundamental trade-off between certainty and scope in artificial intelligence (AI) systems. This exploration remains crucial, not merely as a philosophical exercise, but as a potential compass for guiding AI investments, particularly in safety-critical industrial domains where the level of attention will surely be higher in the future. However, while intellectually coherent, its formalization ultimately freezes this insight into a suspended epistemic truth, resisting operationalization within real-world systems. This paper is a result of an analysis arguing that the conjecture's ambition to provide insights to engineering design and regulatory decision-making is constrained by two critical factors: first, its reliance on incomputable constructs - rendering it practically unactionable and unverifiable; second, its underlying ontological assumption of AI systems as self-contained epistemic entities - separating it from the intricate and dynamic socio-technical environments in which knowledge is co-constructed. We conclude that this dual breakdown - an epistemic closure deficit and an embeddedness bypass - prevents the conjecture from transitioning into a computable and actionable framework suitable for informing the design, deployment, and governance of real-world AI hybrid systems. In response, we propose a contribution to the framing of Floridi's epistemic challenge, addressing the inherent epistemic burdens of AI within complex human-centric domains.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Topology Optimization for Self-Supporting Structures in Additive Manufacturing</title>
<link>https://arxiv.org/abs/2508.19169</link>
<guid>https://arxiv.org/abs/2508.19169</guid>
<content:encoded><![CDATA[
<div> machine learning, topology optimization, self-supporting structures, additive manufacturing, stress constraints
<br />
The paper introduces a novel machine learning-based framework for optimizing the topology of self-supporting structures for additive manufacturing. The framework utilizes a graph neural network (GNN) to predict material distributions over a finite element mesh, ensuring printability through an integrated AM filter. By minimizing structural compliance under volume and stress constraints, the framework generates stress-constrained manufacturable designs in various loading conditions. The stress constraint is enforced using a differentiable p-norm aggregation of von Mises stress to enhance mechanical reliability. The approach features a fully differentiable architecture, eliminating the need for explicit sensitivity derivation in the optimization loop. Numerical experiments demonstrate the efficacy of the framework in producing high-performance designs suitable for additive manufacturing with reduced post-processing requirements.
<br /><br />Summary: <div>
arXiv:2508.19169v1 Announce Type: new 
Abstract: This paper presents a machine learning-based framework for topology optimization of self-supporting structures, specifically tailored for additive manufacturing (AM). By employing a graph neural network (GNN) that acts as a neural field over the finite element mesh, the framework effectively learns and predicts continuous material distributions. An integrated AM filter ensures printability by eliminating unsupported overhangs, while the optimization process minimizes structural compliance under volume and stress constraints. The stress constraint is enforced using a differentiable p-norm aggregation of von Mises stress, promoting mechanical reliability in the optimized designs. A key advantage of the approach lies in its fully differentiable architecture, which leverages automatic differentiation throughout the optimization loop--eliminating the need for explicit sensitivity derivation for both the filter and the stress constraint. Numerical experiments demonstrate the ability of the framework to generate stress-constrained manufacturable topologies under various loading and boundary conditions, offering a practical pathway toward AM-ready high-performance designs with reduced post-processing requirements.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance</title>
<link>https://arxiv.org/abs/2508.19138</link>
<guid>https://arxiv.org/abs/2508.19138</guid>
<content:encoded><![CDATA[
<div> nanoscale electronic devices, nanoribbon field-effect transistors, NEGF, DFT, electron-electron interactions<br />
<br />
Summary:<br />
Designing nanoscale electronic devices like nanoribbon field-effect transistors (NRFETs) requires advanced quantum mechanical modeling tools. Current approaches combine NEGF and DFT, but with ultra-small device dimensions, electron-electron interactions become crucial. The NEGF+GW scheme presented here extends existing solvers to handle NRFET geometries with dimensions comparable to experiments. The QuaTrEx package utilizes a novel domain decomposition scheme, can handle devices with up to 84,480 atoms, and scales efficiently on supercomputers like Alps and Frontier. It achieves exascale FP64 performance on 42,240 atoms, reaching 1.15 Eflop/s. <div>
arXiv:2508.19138v1 Announce Type: cross 
Abstract: Designing nanoscale electronic devices such as the currently manufactured nanoribbon field-effect transistors (NRFETs) requires advanced modeling tools capturing all relevant quantum mechanical effects. State-of-the-art approaches combine the non-equilibrium Green's function (NEGF) formalism and density functional theory (DFT). However, as device dimensions do not exceed a few nanometers anymore, electrons are confined in ultra-small volumes, giving rise to strong electron-electron interactions. To account for these critical effects, DFT+NEGF solvers should be extended with the GW approximation, which massively increases their computational intensity. Here, we present the first implementation of the NEGF+GW scheme capable of handling NRFET geometries with dimensions comparable to experiments. This package, called QuaTrEx, makes use of a novel spatial domain decomposition scheme, can treat devices made of up to 84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80% weak scaling efficiency), and sustains an exascale FP64 performance on 42,240 atoms (1.15 Eflop/s).
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOFLUX: A Differentiable Topology Optimization Framework for Multiphysics Fluidic Problems</title>
<link>https://arxiv.org/abs/2508.17564</link>
<guid>https://arxiv.org/abs/2508.17564</guid>
<content:encoded><![CDATA[
<div> Topology Optimization, fluidic devices, automatic differentiation, JAX library, TOFLUX

Summary:
TOFLUX is a new framework for fluid devices that utilizes automatic differentiation for efficient design optimization. The complexity of fluid-based systems, with multiphysics nonlinear interactions, often hinders researchers, but TOFLUX aims to simplify the process. By using the JAX library, the framework enables rapid exploration of various objectives and constraints, even in challenging scenarios like thermo-fluidic coupling and fluid-structure interaction. The integration with neural networks and machine learning enhances scientific computing capabilities. TOFLUX provides a foundational resource to accelerate research and innovation in fluid-based Topology Optimization. The accompanying software can be accessed on GitHub at github.com/UW-ERSL/TOFLUX. <br /><br />Summary: <div>
arXiv:2508.17564v1 Announce Type: new 
Abstract: Topology Optimization (TO) holds the promise of designing next-generation compact and efficient fluidic devices. However, the inherent complexity of fluid-based TO systems, characterized by multiphysics nonlinear interactions, poses substantial barriers to entry for researchers.
  Beyond the inherent intricacies of forward simulation models, design optimization is further complicated by the difficulty of computing sensitivities, i.e., gradients. Manual derivation and implementation of sensitivities are often laborious and prone to errors, particularly for non-trivial objectives, constraints, and material models. An alternative solution is automatic differentiation (AD). Although AD has been previously demonstrated for simpler TO problems, extending its use to complex nonlinear multiphysics systems, specifically in fluidic optimization, is key to reducing the entry barrier.
  To this end, we introduce TOFLUX, a TO framework for fluid devices leveraging the JAX library for high-performance automatic differentiation. The flexibility afforded by AD enables the rapid exploration and evaluation of various objectives and constraints. We illustrate this capability through challenging examples encompassing thermo-fluidic coupling, fluid-structure interaction, and non-Newtonian flows. Additionally, we demonstrate the seamless integration of our framework with neural networks and machine learning methodologies, enabling modern approaches to scientific computing. Ultimately, the framework aims to provide a foundational resource to accelerate research and innovation in fluid-based TO. The software accompanying this educational paper can be accessed at github.com/UW-ERSL/TOFLUX.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing the exploration-exploitation trade-off in active learning for surrogate model-based reliability analysis via multi-objective optimization</title>
<link>https://arxiv.org/abs/2508.18170</link>
<guid>https://arxiv.org/abs/2508.18170</guid>
<content:encoded><![CDATA[
<div> active learning, reliability assessment, surrogate model, multi-objective optimization, sample acquisition

Summary: 
The article introduces a new approach for reliability assessment of engineering systems by using active learning to iteratively refine a surrogate model. This approach aims to reduce the number of expensive simulations by balancing exploration and exploitation through a multi-objective optimization (MOO) formulation. Traditional strategies like U and Expected Feasibility Function (EFF) are compared with the MOO approach, which explicitly considers the trade-off between exploration and exploitation. The MOO framework provides a unifying perspective and allows for the selection of samples based on a quantifiable exploration-exploitation trade-off. Different sample selection strategies, such as knee point and compromise solution, are evaluated across benchmark limit-state functions. Results show that the MOO approach is generally effective, with an adaptive strategy maintaining high reliability estimates and low relative errors. <div>
arXiv:2508.18170v1 Announce Type: new 
Abstract: Reliability assessment of engineering systems is often hindered by the need to evaluate limit-state functions through computationally expensive simulations, rendering standard sampling impractical. An effective solution is to approximate the limit-state function with a surrogate model iteratively refined through active learning, thereby reducing the number of expensive simulations. At each iteration, an acquisition strategy selects the next sample by balancing two competing goals: exploration, to reduce global predictive uncertainty, and exploitation, to improve accuracy near the failure boundary. Classical strategies, such as the U-function and the Expected Feasibility Function (EFF), implicitly condense exploration and exploitation into a scalar score derived from the surrogate predictive mean and variance, concealing the trade-off and biasing sampling. We introduce a multi-objective optimization (MOO) formulation for sample acquisition in reliability analysis, where exploration and exploitation are explicit, competing objectives. Within our framework, U and EFF correspond to specific Pareto-optimal solutions, providing a unifying perspective that connects classical and Pareto-based approaches. Solving the MOO problem discards dominated candidates, yielding a compact Pareto set, with samples representing a quantifiable exploration-exploitation trade-off. To select samples from the Pareto set, we adopt the knee point and the compromise solution, and further propose a strategy that adjusts the trade-off according to reliability estimates. Across benchmark limit-state functions, we assess the sample efficiency and active learning performance of all strategies. Results show that U and EFF exhibit case-dependent performance, knee and compromise are generally effective, and the adaptive strategy is robust, consistently reaching strict targets and maintaining relative errors below 0.1%.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience</title>
<link>https://arxiv.org/abs/2508.16836</link>
<guid>https://arxiv.org/abs/2508.16836</guid>
<content:encoded><![CDATA[
<div> Neural symbolic approach, complex networks, resilience prediction, physical information method, industrial chain<br />
<br />
Summary: 
This paper introduces a novel physically informative neural symbolic approach for predicting the resilience of complex networks, focusing on industrial chains. The approach integrates physical entity dynamics with spatiotemporal network evolution to enhance predictive accuracy. By jointly learning physical symbol dynamics and network topology, the model demonstrates superior prediction capabilities for industrial chain resilience. The experimental results showcase the effectiveness of the proposed approach in accurately and effectively predicting the elasticity of industrial chains. This advancement is crucial for sustainable development and has significant implications for the industry. <div>
arXiv:2508.16836v1 Announce Type: cross 
Abstract: Industrial chain plays an increasingly important role in the sustainable development of national economy. However, as a typical complex network, data-driven deep learning is still in its infancy in describing and analyzing the resilience of complex networks, and its core is the lack of a theoretical framework to describe the system dynamics. In this paper, we propose a physically informative neural symbolic approach to describe the evolutionary dynamics of complex networks for resilient prediction. The core idea is to learn the dynamics of the activity state of physical entities and integrate it into the multi-layer spatiotemporal co-evolution network, and use the physical information method to realize the joint learning of physical symbol dynamics and spatiotemporal co-evolution topology, so as to predict the industrial chain resilience. The experimental results show that the model can obtain better results and predict the elasticity of the industry chain more accurately and effectively, which has certain practical significance for the development of the industry.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.17086</link>
<guid>https://arxiv.org/abs/2508.17086</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial markets, Trade-based manipulation, Spoofing, Limit Order Book, Anomaly detection 

Summary: 
Financial markets are essential for global economic stability but are often undermined by trade-based manipulation (TBM), including deceptive strategies like spoofing. Detecting these anomalies in the rich information of the Limit Order Book (LOB) is challenging due to its high dimensionality and noise. To address this, a representation learning framework combining a cascaded LOB representation pipeline with supervised contrastive learning is proposed. Extensive experiments show improved detection performance across various models, with Transformer-based architectures achieving state-of-the-art results. Systematic analyses and ablation studies are conducted to investigate multilevel anomalies and the contributions of key components, providing insights into representation learning and anomaly detection for complex sequential data. The code for the framework will be released later at the provided URL. 

<br /><br />Summary: <div>
arXiv:2508.17086v1 Announce Type: cross 
Abstract: Financial markets are critical to global economic stability, yet trade-based manipulation (TBM) often undermines their fairness. Spoofing, a particularly deceptive TBM strategy, exhibits multilevel anomaly patterns that have not been adequately modeled. These patterns are usually concealed within the rich, hierarchical information of the Limit Order Book (LOB), which is challenging to leverage due to high dimensionality and noise. To address this, we propose a representation learning framework combining a cascaded LOB representation pipeline with supervised contrastive learning. Extensive experiments demonstrate that our framework consistently improves detection performance across diverse models, with Transformer-based architectures achieving state-of-the-art results. In addition, we conduct systematic analyses and ablation studies to investigate multilevel anomalies and the contributions of key components, offering broader insights into representation learning and anomaly detection for complex sequential data. Our code will be released later at this URL.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Easy Acceleration with Distributed Arrays</title>
<link>https://arxiv.org/abs/2508.17493</link>
<guid>https://arxiv.org/abs/2508.17493</guid>
<content:encoded><![CDATA[
<div> Keywords: high level programming languages, GPU accelerators, distributed arrays, memory bandwidth, scalability<br />
<br />
Summary: <br />
High level programming languages and GPU accelerators play a crucial role in enabling a wide range of applications. To achieve scalable vertical, horizontal, and temporal performance, effective abstractions are needed. Distributed arrays serve as one such abstraction, enabling high level programming to achieve highly scalable performance by deriving parallelism from data locality. Using the STREAM memory bandwidth benchmark on various hardware, this paper demonstrates scalable performance within and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across multiple nodes showed linear performance. The study also compared hardware improvements for memory bandwidth over decades, showing significant increases in CPU core, CPU node, and GPU node bandwidth. Finally, running on hundreds of MIT SuperCloud nodes simultaneously achieved a sustained bandwidth of over 1 PB/s. <br /><br />Summary: <div>
arXiv:2508.17493v1 Announce Type: cross 
Abstract: High level programming languages and GPU accelerators are powerful enablers for a wide range of applications. Achieving scalable vertical (within a compute node), horizontal (across compute nodes), and temporal (over different generations of hardware) performance while retaining productivity requires effective abstractions. Distributed arrays are one such abstraction that enables high level programming to achieve highly scalable performance. Distributed arrays achieve this performance by deriving parallelism from data locality, which naturally leads to high memory bandwidth efficiency. This paper explores distributed array performance using the STREAM memory bandwidth benchmark on a variety of hardware. Scalable performance is demonstrated within and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across multiple nodes was linear. The hardware used spans decades and allows a direct comparison of hardware improvements for memory bandwidth over this time range; showing a 10x increase in CPU core bandwidth over 20 years, 100x increase in CPU node bandwidth over 20 years, and 5x increase in GPU node bandwidth over 5 years. Running on hundreds of MIT SuperCloud nodes simultaneously achieved a sustained bandwidth $>$1 PB/s.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boltzina: Efficient and Accurate Virtual Screening via Docking-Guided Binding Prediction with Boltz-2</title>
<link>https://arxiv.org/abs/2508.17555</link>
<guid>https://arxiv.org/abs/2508.17555</guid>
<content:encoded><![CDATA[
<div> high-accuracy, computational efficiency, virtual screening, drug discovery, molecular docking<br />
Summary:
Boltzina is introduced as a novel framework in structure-based drug discovery to enhance the computational efficiency of high-accuracy binding affinity prediction. By omitting the structure prediction step and directly predicting affinity from AutoDock Vina docking poses, Boltzina achieves improved screening performance compared to traditional methods like AutoDock Vina and GNINA. While Boltzina falls slightly below Boltz-2 in accuracy, it offers significant speed enhancements, up to 11.8 times faster, through optimized iterations and batch processing. The study explores multi-pose selection strategies and proposes a two-stage screening approach combining Boltzina and Boltz-2 for increased accuracy and efficiency tailored to specific application requirements. This work marks the first successful integration of Boltz-2's accurate predictions into practical-scale screening, providing a comprehensive pipeline that balances accuracy and efficiency in computational biology.<br /><br /> <div>
arXiv:2508.17555v1 Announce Type: cross 
Abstract: In structure-based drug discovery, virtual screening using conventional molecular docking methods can be performed rapidly but suffers from limitations in prediction accuracy. Recently, Boltz-2 was proposed, achieving extremely high accuracy in binding affinity prediction, but requiring approximately 20 seconds per compound per GPU, making it difficult to apply to large-scale screening of hundreds of thousands to millions of compounds. This study proposes Boltzina, a novel framework that leverages Boltz-2's high accuracy while significantly improving computational efficiency. Boltzina achieves both accuracy and speed by omitting the rate-limiting structure prediction from Boltz-2's architecture and directly predicting affinity from AutoDock Vina docking poses. We evaluate on eight assays from the MF-PCBA dataset and show that while Boltzina performs below Boltz-2, it provides significantly higher screening performance compared to AutoDock Vina and GNINA. Additionally, Boltzina achieved up to 11.8$\times$ faster through reduced recycling iterations and batch processing. Furthermore, we investigated multi-pose selection strategies and two-stage screening combining Boltzina and Boltz-2, presenting optimization methods for accuracy and efficiency according to application requirements. This study represents the first attempt to apply Boltz-2's high-accuracy predictions to practical-scale screening, offering a pipeline that combines both accuracy and efficiency in computational biology. The Boltzina is available on github; https://github.com/ohuelab/boltzina.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation</title>
<link>https://arxiv.org/abs/2508.17568</link>
<guid>https://arxiv.org/abs/2508.17568</guid>
<content:encoded><![CDATA[
<div> Keywords: Metamaterials, MetaDSL, MetaDB, MetaBench, structure-representation-property relationships 

Summary: 
MetaDSL is introduced as a domain-specific language for capturing diverse metamaterial designs in a human-readable and machine-parsable form. MetaDB serves as a repository with a vast collection of parameterized MetaDSL programs and their derivatives, providing detailed information on geometry, renderings, and elastic properties. MetaBench offers benchmark suites for testing vision-language metamaterial assistants' core capabilities like structure reconstruction, inverse design driven by properties, and performance prediction. The study establishes baselines by fine-tuning advanced vision-language models and deploying an omni-model within an interactive CAD-like interface. Through case studies, the framework demonstrates a significant advancement in integrated design and comprehension of structure-representation-property relationships. 

<br /><br />Summary: <div>
arXiv:2508.17568v1 Announce Type: cross 
Abstract: Metamaterials are micro-architected structures whose geometry imparts highly tunable-often counter-intuitive-bulk properties. Yet their design is difficult because of geometric complexity and a non-trivial mapping from architecture to behaviour. We address these challenges with three complementary contributions. (i) MetaDSL: a compact, semantically rich domain-specific language that captures diverse metamaterial designs in a form that is both human-readable and machine-parsable. (ii) MetaDB: a curated repository of more than 150,000 parameterized MetaDSL programs together with their derivatives-three-dimensional geometry, multi-view renderings, and simulated elastic properties. (iii) MetaBench: benchmark suites that test three core capabilities of vision-language metamaterial assistants-structure reconstruction, property-driven inverse design, and performance prediction. We establish baselines by fine-tuning state-of-the-art vision-language models and deploy an omni-model within an interactive, CAD-like interface. Case studies show that our framework provides a strong first step toward integrated design and understanding of structure-representation-property relationships.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral-Prior Guided Multistage Physics-Informed Neural Networks for Highly Accurate PDE Solutions</title>
<link>https://arxiv.org/abs/2508.17902</link>
<guid>https://arxiv.org/abs/2508.17902</guid>
<content:encoded><![CDATA[
<div> PINNs, physics-informed neural networks, spectral prior, multistage strategy, accuracy improvement <br />
Summary:<br /> 
- This paper introduces two methods, SI-MSPINNs and RFF-MSPINNs, to enhance the accuracy of Physics-Informed Neural Networks (PINNs) by incorporating spectral information.
- SI-MSPINNs extract dominant spectral patterns to guide network initialization and use a multistage strategy to optimize resolution accuracy.
- RFF-MSPINNs combine random Fourier features with spectral weighting to prioritize learning high-energy physical modes based on residual power spectral density.
- Experimental verification on the Burgers equation and Helmholtz equation demonstrates significant accuracy improvements compared to traditional PINNs. 
- The proposed methods offer a practical solution to enhance the accuracy and performance of PINNs for solving high-dimensional problems efficiently. 
Summary: <div>
arXiv:2508.17902v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINNs) are becoming a popular method for solving PDEs, due to their mesh-free nature and their ability to handle high-dimensional problems where traditional numerical solvers often struggle. Despite their promise, the practical application of PINNs is still constrained by several fac- tors, a primary one being their often-limited accuracy. This paper is dedicated to enhancing the accuracy of PINNs by introducing spectral-prior guided multistage strategy. We propose two methods: Spectrum- Informed Multistage Physics-Informed Neural Networks (SI-MSPINNs) and Multistage Physics-Informed Neural Networks with Spectrum Weighted Random Fourier Features (RFF-MSPINNs). The SI-MSPINNs integrate the core mechanism of Spectrum-Informed Multistage Neural Network (SI-MSNNs) and PINNs, in which we extract the Dominant Spectral Pattern (DSP) of residuals by the discrete Fourier transform. This DSP guides the network initialization to alleviate spectral bias, and gradually optimizes the resolution accuracy using a multistage strategy. The RFF-MSPINNs combines random Fourier features with spectral weighting methods, dynamically adjusting the frequency sampling distribution based on the residual power spectral density, allowing the network to prioritize learning high-energy physical modes. Through experimental verification of the Burgers equation and the Helmholtz equation, we show that both models significantly improve the accuracy of the original PINNs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermodynamically Consistent Hybrid and Permutation-Invariant Neural Yield Functions for Anisotropic Plasticity</title>
<link>https://arxiv.org/abs/2508.15923</link>
<guid>https://arxiv.org/abs/2508.15923</guid>
<content:encoded><![CDATA[
<div> architecturally-constrained neural networks, plastic anisotropy, yield criteria, anisotropic yield function, data-driven frameworks
<br />
Summary:
The study addresses the challenge of modeling plastic anisotropy in metals by utilizing architecturally-constrained neural networks. Two data-driven frameworks are developed: one that combines the Hill yield criterion with Input Convex Neural Networks for anisotropic yield function representation, and another that uses a permutation-invariant input convex neural network to embed anisotropy through linear stress transformations. Calibration on an Al-7079 extrusion experimental dataset shows that the permutation-invariant input convex neural network frameworks outperform existing methods in terms of generalization capabilities. These frameworks accurately predict yield loci and Lankford ratios with minimal data, demonstrating the potential for rapid and thermodynamically consistent constitutive models for advanced forming simulations and microstructure-informed design in the future. 
<br /> <div>
arXiv:2508.15923v1 Announce Type: new 
Abstract: Plastic anisotropy in metals remains challenging to model. This is partly because conventional phenomenological yield criteria struggle to combine a highly descriptive, flexible representation with constraints, such as convexity, dictated by thermodynamic consistency. To address this gap, we employ architecturally-constrained neural networks and develop two data-driven frameworks: (i) a hybrid model that augments the Hill yield criterion with an Input Convex Neural Network (ICNN) to get an anisotropic yield function representation in the six-dimensional stress space and (ii) a permutation-invariant input convex neural network (PI-ICNN) that learns an isotropic yield function representation in the principal stress space and embeds anisotropy through linear stress transformations. We calibrate the proposed frameworks on a sparse Al-7079 extrusion experimental dataset comprising 12 uniaxial samples with measured yield stresses and Lankford ratios. To test the robustness of each framework, nine datasets were generated using k-fold cross-validation. These datasets were then used to quantitatively compare Hill-48, Yld2004-18p, pure ICNNs, the hybrid approach, and the PI-ICNN frameworks. While ICNNs and hybrid approaches can almost perfectly fit the training data, they exhibit significant over-fitting, resulting in high validation and test losses. In contrast, both PI-ICNN frameworks demonstrate better generalization capabilities, even outperforming Yld2004-18p on the validation and test data. These results demonstrate that PI-ICNNs unify physics-based constraints with the flexibility of neural networks, enabling the accurate prediction of both yield loci and Lankford ratios from minimal data. The approach opens a path toward rapid, thermodynamically consistent constitutive models for advanced forming simulations and future exploration of coupled hardening or microstructure-informed design.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise, Adaptation, and Strategy: Assessing LLM Fidelity in Decision-Making</title>
<link>https://arxiv.org/abs/2508.15926</link>
<guid>https://arxiv.org/abs/2508.15926</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, social science simulations, decision-making, variability, adaptability <br />
Summary: 
Large language models (LLMs) are being used in social science simulations, but their ability to simulate human decision-making variability and adaptability is not well understood. A new evaluation framework with progressive interventions was proposed to examine LLM agents' adaptability under different levels of external guidance and human-derived noise. The framework was validated on two classic economics tasks, highlighting behavioral gaps between LLMs and humans. By default, LLMs tend to converge on stable and conservative strategies that differ from human behaviors. Risk-framed instructions influence LLM behavior but do not capture human-like diversity. Incorporating human data through in-context learning helps narrow the gap but still falls short of replicating human subjects' strategic variability. These results underscore the need for more realistic evaluations of LLMs in dynamic decision-making tasks, providing guidance for their application in synthetic data for social science research. <br /><br /> <div>
arXiv:2508.15926v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in social science simulations. While their performance on reasoning and optimization tasks has been extensively evaluated, less attention has been paid to their ability to simulate human decision-making's variability and adaptability. We propose a process-oriented evaluation framework with progressive interventions (Intrinsicality, Instruction, and Imitation) to examine how LLM agents adapt under different levels of external guidance and human-derived noise. We validate the framework on two classic economics tasks, irrationality in the second-price auction and decision bias in the newsvendor problem, showing behavioral gaps between LLMs and humans.
  We find that LLMs, by default, converge on stable and conservative strategies that diverge from observed human behaviors. Risk-framed instructions impact LLM behavior predictably but do not replicate human-like diversity. Incorporating human data through in-context learning narrows the gap but fails to reach human subjects' strategic variability. These results highlight a persistent alignment gap in behavioral fidelity and suggest that future LLM evaluations should consider more process-level realism. We present a process-oriented approach for assessing LLMs in dynamic decision-making tasks, offering guidance for their application in synthetic data for social science research.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUEENS: An Open-Source Python Framework for Solver-Independent Analyses of Large-Scale Computational Models</title>
<link>https://arxiv.org/abs/2508.16316</link>
<guid>https://arxiv.org/abs/2508.16316</guid>
<content:encoded><![CDATA[
<div> Keywords: QUEENS, uncertainty quantification, simulation management, distributed computing, Bayesian analysis

Summary:
QUEENS is a Python framework designed to facilitate the analysis of large-scale computational models, specifically patient-specific digital twins of diseased human organs. It aims to streamline simulation management with arbitrary solvers on distributed systems, offering a range of state-of-the-art algorithms for convergence studies, optimization, uncertainty quantification, and Bayesian inverse analysis. The framework supports both deterministic and probabilistic analysis, featuring multi-fidelity uncertainty quantification and Bayesian analysis. With a modular architecture, QUEENS allows researchers to easily switch between different types of analyses and build sophisticated algorithms. The open-source repository for QUEENS is available on GitHub, providing researchers with access to cutting-edge research in probabilistic machine learning and efficient analysis methods. <div>
arXiv:2508.16316v1 Announce Type: new 
Abstract: A growing challenge in research and industrial engineering applications is the need for repeated, systematic analysis of large-scale computational models, for example, patient-specific digital twins of diseased human organs: The analysis requires efficient implementation, data, resource management, and parallelization, possibly on distributed systems. To tackle these challenges and save many researchers from annoying, time-consuming tasks, we present QUEENS (Quantification of Uncertain Effects in Engineering Systems), an open-source Python framework for composing and managing simulation analyses with arbitrary (physics-based) solvers on distributed computing infrastructures. Besides simulation management capabilities, QUEENS offers a comprehensive collection of efficiently implemented state-of-the-art algorithms ranging from routines for convergence studies and common optimization algorithms to more advanced sampling algorithms for uncertainty quantification and Bayesian inverse analysis. Additionally, we provide our latest cutting-edge research in multi-fidelity uncertainty quantification, efficient multi-fidelity Bayesian inverse analysis, and probabilistic machine learning. QUEENS adopts a Bayesian, probabilistic mindset but equally supports standard deterministic analysis without requiring prior knowledge of probability theory. The modular architecture allows rapid switching between common types of analyses and facilitates building sophisticated hierarchical algorithms. Encouraging natural incremental steps and scaling towards complexity allows researchers to consider the big picture while building towards it through smaller, manageable steps. The open-source repository is available at https://github.com/queens-py/queens.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Linear to Hierarchical: Evolving Tree-structured Thoughts for Efficient Alpha Mining</title>
<link>https://arxiv.org/abs/2508.16334</link>
<guid>https://arxiv.org/abs/2508.16334</guid>
<content:encoded><![CDATA[
<div> Alpha mining, Large Language Models, Tree-structured thought Evolution, hierarchical reasoning, automatic quantitative investment <br />
Summary: 

This paper introduces Tree-structured thought Evolution (TreEvo) as a solution to alpha mining using Large Language Models (LLMs). The goal is to automatically discover signals that predict asset returns without depending on handcrafted features or arithmetic operators. TreEvo evolves hierarchical reasoning ideas solely at the thought level, addressing the hierarchical tree structures of alphas. Experiments on real-market datasets show that TreEvo can generate better alphas in less computational time and with fewer expert efforts compared to traditional methods. The tree-structured thoughts and compatible evolutionary operators play a crucial role in achieving this improvement, demonstrating the importance of considering hierarchical structures in alpha mining. <br /><br />Summary: <div>
arXiv:2508.16334v1 Announce Type: new 
Abstract: Alpha mining, which discovers signals that predict asset returns, has long been attractive for automatic quantitative investment. This problem is typically formulated as a tree-based symbolic regression with handcrafted market data features and arithmetic operators. Unfortunately, existing symbolic methods are concerned with computational inefficiency and dependence on prior knowledge. Recent implementation of Large Language Models (LLMs) show that they can automatically generate executable codes for various tasks efficiently, thus can be considered as a new promising way for alpha mining. Specifically, LLMs-driven methods evolve a set of heuristics, including thoughts and codes, where the thoughts are usually represented as plain-text prompts of codes. Unfortunately, trivially adopting them in alpha mining ignores the fact that alphas are with hierarchical tree structures. This paper introduces Tree-structured thought Evolution (TreEvo), which evolves hierarchical reasoning ideas solely at the thought level. Experiments on four real-market datasets demonstrate that TreEvo can obtain better alphas with much less computational time and human expert efforts. And this superiority hardly holds without the tree-structured thoughts and the compatible evolutionary operators.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-optimized replacement strategies for water electrolysis systems affected by degradation</title>
<link>https://arxiv.org/abs/2508.16370</link>
<guid>https://arxiv.org/abs/2508.16370</guid>
<content:encoded><![CDATA[
<div> renewable hydrogen, electrolyzer stacks, degradation modeling, levelized cost, optimization<br />
<br />
Summary:<br />
A study was conducted to analyze the economics of green hydrogen production using water electrolysis systems. The focus was on minimizing the degradation of electrolyzer stacks to reduce project costs and increase stack lifetime. A linear optimization approach was used to calculate the levelized cost of hydrogen based on varying degradation thresholds, determining the optimal time for stack replacement. The study considered uncertainties such as degradation scale, load-dependency of degradation and energy demand, and electrolyzer costs. The findings showed that the optimal time for stack replacement could differ by up to 9 years depending on degradation scale. Understanding the impact of degradation is crucial for reducing project costs and supporting the growth of the hydrogen market. <div>
arXiv:2508.16370v1 Announce Type: new 
Abstract: A key factor in reducing the cost of green hydrogen production projects using water electrolysis systems is to minimize the degradation of the electrolyzer stacks, as this impacts the lifetime of the stacks and therefore the frequency of their replacement. To create a better understanding of the economics of stack degradation, we present a linear optimization approach minimizing the costs of a green hydrogen supply chain including an electrolyzer with degradation modeling. By calculating the levelized cost of hydrogen depending on a variable degradation threshold, the cost optimal time for stack replacement can be identified. We further study how this optimal time of replacement is affected by uncertainties such as the degradation scale, the load-dependency of both degradation and energy demand, and the costs of the electrolyzer. The variation of the identified major uncertainty degradation scale results in a difference of up to 9 years regarding the cost optimal time for stack replacement, respectively lifetime of the stacks. Therefore, a better understanding of the degradation impact is imperative for project cost reductions, which in turn would support a proceeding hydrogen market ramp-up.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment-Aware Mean-Variance Portfolio Optimization for Cryptocurrencies</title>
<link>https://arxiv.org/abs/2508.16378</link>
<guid>https://arxiv.org/abs/2508.16378</guid>
<content:encoded><![CDATA[
<div> Keywords: cryptocurrency, portfolio optimization, technical indicators, sentiment analysis, investment decision-making

Summary:
This paper introduces a dynamic cryptocurrency portfolio optimization strategy that combines technical indicators and sentiment analysis for better investment decision-making. The method uses the RSI and SMA to capture market momentum and sentiment scores from news articles with the VADER model. Google Gemini is employed to verify sentiment scores and make investment decisions. These signals are integrated into expected return estimates for mean-variance optimization with asset weight constraints. The strategy is tested through a rolling-window backtest on cryptocurrency market data, outperforming benchmarks of Bitcoin and an equal-weighted portfolio in cumulative return and Sharpe ratio. However, it also shows higher short-term downside risk. The results demonstrate the potential of integrating sentiment and technical signals to enhance cryptocurrency portfolio performance while emphasizing the importance of managing risk exposure in volatile markets.<br /><br />Summary: <div>
arXiv:2508.16378v1 Announce Type: new 
Abstract: This paper presents a dynamic cryptocurrency portfolio optimization strategy that integrates technical indicators and sentiment analysis to enhance investment decision-making. The proposed method employs the 14-day Relative Strength Index (RSI) and 14-day Simple Moving Average (SMA) to capture market momentum, while sentiment scores are extracted from news articles using the VADER (Valence Aware Dictionary and sEntiment Reasoner) model, with compound scores quantifying overall market tone. The large language model Google Gemini is used to further verify the sentiment scores predicted by VADER and give investment decisions. These technical indicator and sentiment signals are incorporated into the expected return estimates before applying mean-variance optimization with constraints on asset weights. The strategy is evaluated through a rolling-window backtest over cryptocurrency market data, with Bitcoin (BTC) and an equal-weighted portfolio of selected cryptocurrencies serving as benchmarks. Experimental results show that the proposed approach achieves a cumulative return of 38.72, substantially exceeding Bitcoin's 8.85 and the equal-weighted portfolio's 21.65 over the same period, and delivers a higher Sharpe ratio (1.1093 vs. 0.8853 and 1.0194, respectively). However, the strategy exhibits a larger maximum drawdown (-18.52%) compared to Bitcoin (-4.48%) and the equal-weighted portfolio (-11.02%), indicating higher short-term downside risk. These results highlight the potential of combining sentiment and technical signals to improve cryptocurrency portfolio performance, while also emphasizing the need to address risk exposure in volatile markets.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain</title>
<link>https://arxiv.org/abs/2508.15776</link>
<guid>https://arxiv.org/abs/2508.15776</guid>
<content:encoded><![CDATA[
<div> Keywords: pharmaceutical supply chain, cybersecurity, zero trust architecture, data protection, resilience

Summary:
The pharmaceutical supply chain is facing increasing cybersecurity challenges that threaten patient safety and operational continuity. This paper explores the potential of zero trust architecture in enhancing security and resilience in this critical ecosystem. By implementing principles such as continuous verification, least-privilege access, and data-centric security, organizations can strengthen security measures and protect sensitive data. Real-world case studies demonstrate the successful implementation of zero trust in pharmaceutical supply chains. One crucial area where zero trust can be effectively applied is in managing narcotics and high-health-risk drugs to ensure drug safety throughout the production process. By adopting zero trust principles, the pharmaceutical industry can safeguard its supply chain from evolving cyber threats, guaranteeing the reliability of critical medical operations.<br /><br />Summary: <div>
arXiv:2508.15776v1 Announce Type: cross 
Abstract: The pharmaceutical supply chain faces escalating cybersecurity challenges threatening patient safety and operational continuity. This paper examines the transformative potential of zero trust architecture for enhancing security and resilience within this critical ecosystem. We explore the challenges posed by data breaches, counterfeiting, and disruptions and introduce the principles of continuous verification, least-privilege access, and data-centric security inherent in zero trust. Real-world case studies illustrate successful implementations. Benefits include heightened security, data protection, and adaptable resilience. As recognized by researchers and industrialists, a reliable drug tracing system is crucial for ensuring drug safety throughout the pharmaceutical production process. One of the most pivotal domains within the pharmaceutical industry and its associated supply chains where zero trust can be effectively implemented is in the management of narcotics, high-health-risk drugs, and abusable substances. By embracing zero trust, the pharmaceutical industry fortifies its supply chain against constantly changing cyber threats, ensuring the trustworthiness of critical medical operations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms</title>
<link>https://arxiv.org/abs/2508.15963</link>
<guid>https://arxiv.org/abs/2508.15963</guid>
<content:encoded><![CDATA[
<div> Measurement System, Wheel Flange Wear, Machine Learning Algorithm, Infinite Impulse Response Filter, Rail Safety<br />
Summary:<br /> 
This paper presents an innovative onboard measurement system for monitoring wheel flange wear depth in railway systems. The system uses displacement and temperature sensors to accurately measure wear depth and surrounding temperature fluctuations. Machine learning algorithms based on regression models are trained dynamically using collected data, achieving an accuracy of 96.5%. An infinite impulse response filter (IIR) is designed to mitigate vehicle dynamics and sensor noise, further enhancing accuracy to 98.2%. The system also integrates with Internet of Things devices for real-time monitoring of wheel flange wear and track conditions. Overall, this advanced monitoring system ensures increased safety and efficiency in railway operations. <div>
arXiv:2508.15963v1 Announce Type: cross 
Abstract: Rail and wheel interaction functionality is pivotal to the railway system safety, requiring accurate measurement systems for optimal safety monitoring operation. This paper introduces an innovative onboard measurement system for monitoring wheel flange wear depth, utilizing displacement and temperature sensors. Laboratory experiments are conducted to emulate wheel flange wear depth and surrounding temperature fluctuations in different periods of time. Employing collected data, the training of machine learning algorithms that are based on regression models, is dynamically automated. Further experimentation results, using standards procedures, validate the system's efficacy. To enhance accuracy, an infinite impulse response filter (IIR) that mitigates vehicle dynamics and sensor noise is designed. Filter parameters were computed based on specifications derived from a Fast Fourier Transform analysis of locomotive simulations and emulation experiments data. The results show that the dynamic machine learning algorithm effectively counter sensor nonlinear response to temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime. The real-time noise reduction via IIR filter enhances the accuracy up to 98.2 %. Integrated with railway communication embedded systems such as Internet of Things devices, this advanced monitoring system offers unparalleled real-time insights into wheel flange wear and track irregular conditions that cause it, ensuring heightened safety and efficiency in railway systems operations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Scattering Matrix Synthesis: Independent Region Decomposition for Hybrid Antenna--Scatterer Systems</title>
<link>https://arxiv.org/abs/2503.17616</link>
<guid>https://arxiv.org/abs/2503.17616</guid>
<content:encoded><![CDATA[
<div> Keywords: generalized scattering matrix, hybrid electromagnetic systems, vector spherical wavefunctions, modular region decomposition, efficient analysis <br />
Summary: 
This paper introduces a unified formulation for synthesizing the generalized scattering matrix (GS-matrix) of hybrid electromagnetic systems consisting of various antennas and scatterers. The method utilizes a modular region decomposition framework to analyze electromagnetic interactions between separate structures, assuming they are separable by a plane. By using the addition theorem of vector spherical wavefunctions (VSWFs), a compact matrix representation is created to combine the GS- and S-matrices of individual components for the overall system response. This approach extends previous methods for multiple scattering or antenna array analysis, making it suitable for configurations where substructures can be repositioned or reused. Numerical examples demonstrate the accuracy and flexibility of the method, including cases with closely spaced components and rotational variations in substructure layout. <div>
arXiv:2503.17616v2 Announce Type: replace 
Abstract: This paper presents a unified formulation for synthesizing the generalized scattering matrix (GS-matrix) of hybrid electromagnetic systems comprising arbitrary numbers of antennas and scatterers. The proposed method provides a modular region decomposition framework that enables efficient analysis of electromagnetic interactions between distinct structures, under the relaxed geometric condition that the constituents are separable by a plane. By leveraging the addition theorem of vector spherical wavefunctions (VSWFs), a compact matrix representation is derived to assemble the GS- and S-matrices of individual components into the overall system response. This formulation generalizes and extends prior methods developed for either multiple scattering or antenna array analysis, and is particularly suited to configurations where substructures may be repositioned or reused. Numerical examples are provided to validate the accuracy and versatility of the method, including scenarios involving tightly spaced components and rotational variations in substructure layout.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa</title>
<link>https://arxiv.org/abs/2508.15110</link>
<guid>https://arxiv.org/abs/2508.15110</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, insurance sector, African insurance market, inclusive solutions
<br />
Summary: 
Artificial Intelligence, particularly Large Language Models (LLMs) and agentic AI, offer transformative potential in the insurance sector. Rapid performance improvements, open-source access, and decreasing deployment costs present unique opportunities and challenges for insurers. There is a need to address the complexity of LLM and agentic AI frameworks in the insurance industry. In the African insurance market, critical gaps exist, but there are also local efforts, players, and partnership opportunities that can be leveraged. It is essential for actuaries, insurers, regulators, and tech leaders to collaborate in creating inclusive, sustainable, and equitable AI strategies and solutions that cater to the specific needs of Africans.
<br /><br />Summary: <div>
arXiv:2508.15110v1 Announce Type: new 
Abstract: In this work, we highlight the transformative potential of Artificial Intelligence (AI), particularly Large Language Models (LLMs) and agentic AI, in the insurance sector. We consider and emphasize the unique opportunities, challenges, and potential pathways in insurance amid rapid performance improvements, increased open-source access, decreasing deployment costs, and the complexity of LLM or agentic AI frameworks. To bring it closer to home, we identify critical gaps in the African insurance market and highlight key local efforts, players, and partnership opportunities. Finally, we call upon actuaries, insurers, regulators, and tech leaders to a collaborative effort aimed at creating inclusive, sustainable, and equitable AI strategies and solutions: by and for Africans.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating profitable price bounds for prescriptive price optimization</title>
<link>https://arxiv.org/abs/2508.15248</link>
<guid>https://arxiv.org/abs/2508.15248</guid>
<content:encoded><![CDATA[
<div> bootstrap procedure, confidence intervals, Nelder-Mead simplex method, black-box optimization, total revenue

Summary:
The article discusses the importance of pricing in maximizing business profits, focusing on prescriptive price optimization. Two methods for estimating price bounds in prescriptive price optimization are proposed: one using the bootstrap procedure to estimate confidence intervals for optimal prices, and the other employing the Nelder-Mead simplex method for black-box price bounds optimization. Experimental results with synthetic price-demand datasets show that these methods successfully narrow down the price range while maintaining high revenues, especially with a small number of items or low demand noise levels. Additionally, the comparative advantage of these methods increases as more data accumulates. <div>
arXiv:2508.15248v1 Announce Type: cross 
Abstract: Pricing of products and services, which has a significant impact on consumer demand, is one of the most important factors in maximizing business profits. Prescriptive price optimization is a prominent data-driven pricing methodology consisting of two phases: demand forecasting and price optimization. In the practice of prescriptive price optimization, the price of each item is typically set within a predetermined range defined by lower and upper bounds. Narrow price ranges can lead to missed opportunities, while wide price ranges run the risk of proposing unrealistic prices; therefore, determining profitable price bounds while maintaining the reliability of the suggested prices is a critical challenge that directly affects the effectiveness of prescriptive price optimization. We propose two methods for estimating price bounds in prescriptive price optimization so that future total revenue derived from the optimized prices will be maximized. Our first method for price bounds estimation uses the bootstrap procedure to estimate confidence intervals for optimal prices. Our second method uses the Nelder--Mead simplex method for black-box price bounds optimization that maximizes total revenue estimated through $K$-fold cross-validation. Experimental results with synthetic price--demand datasets demonstrate that our methods successfully narrowed down the price range while maintaining high revenues, particularly when the number of items was small or the demand noise level was low. Moreover, as more data accumulated, the comparative advantage of our methods further increased.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEAS: Hierarchical Evolutionary Agent Simulation Framework for Cross-Scale Modeling and Multi-Objective Search</title>
<link>https://arxiv.org/abs/2508.15555</link>
<guid>https://arxiv.org/abs/2508.15555</guid>
<content:encoded><![CDATA[
<div> Framework, Agent-based modeling, Evolutionary optimization, Python, Hierarchical

Summary:
Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that integrates layered agent-based modeling with evolutionary optimization and tournament evaluation. HEAS organizes models as hierarchies of processes scheduled in layers, facilitating explicit cross-scale couplings. The framework provides a compact API for simulating, optimizing, and evaluating single- and multi-objective evolution, with PyTorch policy integration. HEAS standardizes evaluation metrics, persists data, and offers plotting tools for analysis. It emphasizes separating mechanism from orchestration, enabling easy composition and swapping of components. The framework is versatile and applicable for forward simulation, optimization, and comparisons across studies. Two example applications demonstrate the utility of HEAS in ecological systems and enterprise decision-making scenarios. HEAS serves as a reliable foundation for interdisciplinary, multi-level investigations, delivering reproducible results. 

<br /><br />Summary: <div>
arXiv:2508.15555v1 Announce Type: cross 
Abstract: Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that unifies layered agent-based modeling with evolutionary optimization and tournament evaluation in a single, reproducible workflow. HEAS represents models as hierarchies of lightweight processes ("streams") scheduled in deterministic layers that read and write a shared context, making cross-scale couplings explicit and auditable. A compact API and CLI-simulate, optimize, evaluate-expose single- and multi-objective evolution, PyTorch policy integration via parameter flattening/unflattening, and general tournament tooling with user-defined scoring and voting rules. The framework standardizes evaluation through uniform per-step and episode metrics, persists seeds, logbooks, and hall-of-fame archives, and provides plotting helpers for traces, Pareto fronts, and comparative outcomes, reducing glue code and improving comparability across studies. HEAS emphasizes separation of mechanism from orchestration, allowing exogenous drivers, endogenous agents, and aggregators to be composed and swapped without refactoring, while the same model can be used for forward simulation, optimization, or systematic comparison. We illustrate usage with two compact examples-an ecological system and an enterprise decision-making setting. HEAS offers a practical foundation for cross-disciplinary, multi-level inquiry, yielding reliable, reproducible results.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliability comparison of vessel trajectory prediction models via Probability of Detection</title>
<link>https://arxiv.org/abs/2508.14198</link>
<guid>https://arxiv.org/abs/2508.14198</guid>
<content:encoded><![CDATA[
<div> deep learning, vessel trajectory prediction, traffic complexity, model performance, reliability analysis

Summary:<br />
This study examines vessel trajectory prediction using deep learning approaches, focusing on evaluating model performance in various traffic complexities. Unlike previous models, this research considers specific traffic situations and assesses reliability through a probability of detection analysis. The models are tested on different traffic scenarios, with performance metrics and reliability estimates calculated for each category. The results provide insights into the strengths and limitations of the prediction approaches and their reliability in ensuring safe forecasts over different prediction horizons. By understanding these aspects, future developments can lead to more reliable vessel trajectory prediction methods, ultimately enhancing safety and efficiency in inland waterway navigation. <br /> <div>
arXiv:2508.14198v1 Announce Type: cross 
Abstract: This contribution addresses vessel trajectory prediction (VTP), focusing on the evaluation of different deep learning-based approaches. The objective is to assess model performance in diverse traffic complexities and compare the reliability of the approaches. While previous VTP models overlook the specific traffic situation complexity and lack reliability assessments, this research uses a probability of detection analysis to quantify model reliability in varying traffic scenarios, thus going beyond common error distribution analyses. All models are evaluated on test samples categorized according to their traffic situation during the prediction horizon, with performance metrics and reliability estimates obtained for each category. The results of this comprehensive evaluation provide a deeper understanding of the strengths and weaknesses of the different prediction approaches, along with their reliability in terms of the prediction horizon lengths for which safe forecasts can be guaranteed. These findings can inform the development of more reliable vessel trajectory prediction approaches, enhancing safety and efficiency in future inland waterways navigation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel geometric predictive algorithm for assessing Compressive Elastic Modulus in MEX additive processes, based on part nonlinearities and layers stiffness,validated with PETG and PLA materials</title>
<link>https://arxiv.org/abs/2508.13164</link>
<guid>https://arxiv.org/abs/2508.13164</guid>
<content:encoded><![CDATA[
<div> Algorithm, plastic materials, MEX, elastic modulus, compressive loads

Summary:
The paper introduces a new predictive algorithm developed by researchers for determining the elastic modulus and mechanical behavior of plastic materials manufactured using MEX under compressive loads. This algorithm requires input of the compressive elastic modulus of the material filament and MEX manufacturing parameters. It calculates layer stiffness based on the number of holes in the projected area and has been validated using PETG and PLA materials on test specimens and a variable topology case study. The algorithm is applicable to various print patterns and manufacturing directions, offering versatility for different plastic polymers suitable for MEX. It eliminates the need for costly mechanical analysis software or extensive experimental validations for complex component geometries under uniaxial compression loads. 

<br /><br />Summary: <div>
arXiv:2508.13164v1 Announce Type: new 
Abstract: The paper presents an innovative methodology based on the use of a new predictive algorithm created by the researchers capable of obtaining the elastic modulus of a plastic material manufactured with MEX and its mechanical behaviour in the elastic zone under compressive loads. The predictive algorithm only needs as input the compressive elastic modulus of the isotropic plastic material filament and the manufacturing parameters of the MEX process. The smart developed algorithm calculates the stiffness of each layer considering the number of holes in the projected area. The innovative predictive algorithm has been experimentally and numerically validated using PETG Polyethylene Terephthalate Glycol material and PLA Polylactic Acid on test specimens and on a case study of variable topology. The predictive algorithm is valid for each print pattern and manufacturing direction. The new algorithm improves the existing state of the art significantly since this algorithm extends its utility to most plastic polymer materials suitable for MEX 3D printing, provided that the mechanical and elastic properties of the filament are known. Its versatility extends to complex component geometries subjected to uniaxial compression loads, eliminating the need for mechanical analysis software or expensive experimental validations.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models</title>
<link>https://arxiv.org/abs/2508.13491</link>
<guid>https://arxiv.org/abs/2508.13491</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, FinCDM, CPA examination, Financial skills, Cognitive diagnosis evaluation

Summary:
FinCDM introduces a new cognitive diagnosis evaluation framework tailored for financial Large Language Models (LLMs). It allows for the evaluation of LLMs at the knowledge-skill level, uncovering hidden knowledge gaps and identifying under-tested areas such as tax and regulatory reasoning. The framework is supported by CPA-QKA, a dataset derived from the Certified Public Accountant examination, providing comprehensive coverage of real-world accounting and financial skills. The dataset is rigorously annotated by domain experts for fine-grained knowledge labels. Through extensive experiments on various LLMs, FinCDM reveals behavioral clusters among models, enabling interpretable, skill-aware diagnosis for more targeted model development. The approach supports trustworthy and targeted model development in the financial domain and aims to improve the overall performance and understanding of LLMs in high-stakes applications.<br /><br />Summary: <div>
arXiv:2508.13491v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promise for financial applications, yet their suitability for this high-stakes domain remains largely unproven due to inadequacies in existing benchmarks. Existing benchmarks solely rely on score-level evaluation, summarizing performance with a single score that obscures the nuanced understanding of what models truly know and their precise limitations. They also rely on datasets that cover only a narrow subset of financial concepts, while overlooking other essentials for real-world applications. To address these gaps, we introduce FinCDM, the first cognitive diagnosis evaluation framework tailored for financial LLMs, enabling the evaluation of LLMs at the knowledge-skill level, identifying what financial skills and knowledge they have or lack based on their response patterns across skill-tagged tasks, rather than a single aggregated number. We construct CPA-QKA, the first cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, with comprehensive coverage of real-world accounting and financial skills. It is rigorously annotated by domain experts, who author, validate, and annotate questions with high inter-annotator agreement and fine-grained knowledge labels. Our extensive experiments on 30 proprietary, open-source, and domain-specific LLMs show that FinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax and regulatory reasoning overlooked by traditional benchmarks, and uncovers behavioral clusters among models. FinCDM introduces a new paradigm for financial LLM evaluation by enabling interpretable, skill-aware diagnosis that supports more trustworthy and targeted model development, and all datasets and evaluation scripts will be publicly released to support further research.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Discovery of Multi-Dimensional Breakage Population Balance Equations</title>
<link>https://arxiv.org/abs/2508.13763</link>
<guid>https://arxiv.org/abs/2508.13763</guid>
<content:encoded><![CDATA[
<div> Sparse regression, multi-dimensional breakage, population balance equation, data-driven, Dynamic Mode Decomposition

Summary: 
The article introduces the Multi-Dimensional Breakage Population Balance Equation Identification (mPBE ID) algorithm, which aims to discover multi-dimensional breakage population balance equations directly from data. Current inverse solution techniques are limited to one-dimensional cases and require prior system knowledge, constraining their applicability. The mPBE-ID incorporates breakage-informed constrained sparse regression, constructs candidate library functions based on Dynamic Mode Decomposition (DMD) insights, and handles noisy/limited data through ensembling. The DMD is crucial for identifying dominant breakage dynamics and guiding the inclusion of candidate terms. The algorithm successfully discovers various forms of mPBE, even with noisy and limited data, offering a foundational framework for future extensions to generalize the discovery of multi-dimensional PBEs for high-dimensional particulate phenomena.<br /><br />Summary: <div>
arXiv:2508.13763v1 Announce Type: new 
Abstract: Multi-dimensional breakage is a ubiquitous phenomenon in natural systems, yet the systematic discovery of underlying governing equations remains a long-standing challenge. Current inverse solution techniques are restricted to one-dimensional cases and typically depend on the availability of a priori system knowledge, thus limiting their applicability. By leveraging advances in data-driven sparse regression techniques, we develop the Multi-Dimensional Breakage Population Balance Equation Identification (mPBE ID) algorithm for discovering multi-dimensional breakage population balance equations (mPBEs) directly from data. Our mPBE-ID enables tractable identification of mPBEs by incorporating several key strategies, namely, a breakage-informed constrained sparse regression, targeted candidate library functions construction via insights from Dynamic Mode Decomposition (DMD), and robust handling of noisy/limited data through ensembling (bagging/bragging). Notably, we demonstrate how the DMD is indispensable for distilling dominant breakage dynamics which can then be used to facilitate the systematic inclusion of candidate library terms. We showcase the ability of the mPBE-ID to discover different forms of mPBE (including those with discontinuous stoichiometric kernels) even when tested against noisy and limited data. We anticipate that the mPBE-ID will serve as a foundational framework for future extensions to generalize the discovery of multi-dimensional PBEs for various high-dimensional particulate phenomena.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Modelling of Infrastructure Asset Performance Deterioration -- a bounded gamma process approach</title>
<link>https://arxiv.org/abs/2508.13359</link>
<guid>https://arxiv.org/abs/2508.13359</guid>
<content:encoded><![CDATA[
<div> flexible deterioration model, infrastructure asset management systems, gamma process, bounded transformed gamma process, infrastructure performance deterioration

Summary: 
The article discusses the importance of a flexible deterioration model in infrastructure asset management systems and introduces a new bounded transformed gamma process (BTGP) model. This model is compared to a bounded nonstationary gamma process (BNGP) model in terms of deterioration modelling and asset management decision-making. An empirical study using real-world bridge condition data showcases the flexibility and significance of the proposed BTGP model. The BTGP model is deeply rooted in traditional regression modeling, providing a more flexible approach to characterizing different deterioration patterns in infrastructure systems. This study highlights the advantages of the BTGP model over existing alternatives and emphasizes its potential for improving infrastructure asset management practices. <div>
arXiv:2508.13359v1 Announce Type: cross 
Abstract: Infrastructure asset management systems require a flexible deterioration model that can handle various degradation patterns in a unified way. Owing to its appealing monotonic sample paths, independent increments and mathematical tractability, gamma process has been widely employed as an infrastructure performance deterioration model. This model was recently enhanced by introducing an upper bound to satisfy a practical modelling need that many infrastructure performance deterioration processes are constrained by physical or managerial limits. Several bounded transformed gamma process (BTGP) alternatives had been proposed; however, they lacked due flexibility to characterize different deterioration patterns. This paper proposed a new BTGP model that is deeply grounded upon the traditional regression modelling tradition in infrastructure asset management systems. Qualitative and quantitative comparisons were carried out between the proposed BTGP and a bounded nonstationary gamma process (BNGP) model from both deterioration modelling and asset management decision-making perspectives. An empirical study using the real-world historical bridge condition data was performed to examine the flexibility of the BTGP against the BNGP and six other BTGP alternatives. The results confirmed the flexibility and significance of the proposed BTGP model for infrastructure systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistence is All You Need -- A Topological Lens on Microstructural Characterization</title>
<link>https://arxiv.org/abs/2508.11967</link>
<guid>https://arxiv.org/abs/2508.11967</guid>
<content:encoded><![CDATA[
<div> Keywords: microstructure, materials, energy engineering, computational topology, deep neural network

Summary:
This study presents a novel approach to accurately design materials for energy and chemical engineering technologies by extracting key microstructural descriptors. By combining computational topology with assembly-learning-based regression, the researchers created a workflow that successfully predicted eight important microstructural features. Using a dataset of synthetic three-dimensional microstructures and a deep neural network trained on persistence images, the model achieved high accuracy in predicting the descriptors. The results showed an average R^2 of ~0.84 and Pearson r of ~0.92 in an independent test set, demonstrating both precision and generality of the approach. This unified and scalable tool offers a rapid characterization method for functional porous materials, potentially leading to improved design and performance in various applications. 

<br /><br />Summary: <div>
arXiv:2508.11967v1 Announce Type: new 
Abstract: The microstructure critically governs the properties of materials used in energy and chemical engineering technologies, from catalysts and filters to thermal insulators and sensors. Therefore, accurate design is based on quantitative descriptors of microstructural features. Here we show that eight key descriptors can be extracted by a single workflow that fuses computational topology with assembly-learning-based regression. First, 1312 synthetic three-dimensional microstructures were generated and evaluated using established algorithms, and a labeled data set of ground-truth parameters was built. Converting every structure into a persistence image allowed us to train a deep neural network that predicts the eight descriptors. In an independent test set, the model achieved on average R^2 ~ 0.84 and Pearson r ~ 0.92, demonstrating both precision and generality. The approach provides a unified and scalable tool for rapid characterization of functional porous materials.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Porous Convection in the Discrete Exterior Calculus with Geometric Multigrid</title>
<link>https://arxiv.org/abs/2508.12501</link>
<guid>https://arxiv.org/abs/2508.12501</guid>
<content:encoded><![CDATA[
<div> DEC, Discrete Exterior Calculus, Decapodes.jl, CombinatorialSpaces.jl, porous convection, geometric multigrid solver

Summary:
The article introduces the use of Discrete Exterior Calculus (DEC) in solving porous convection equations through the Decapodes.jl embedded domain-specific language. This approach is implemented using CombinatorialSpaces.jl, a Julia library that applies DEC over simplicial complexes and includes a geometric multigrid solver for maps between subdivided simplicial complexes. The study showcases numerical results of multigrid solvers for both the Poisson problem and porous convection problem, serving as a standalone solver or a preconditioner for open-source Julia iterative methods libraries. The DEC framework ensures the preservation of properties from the exterior calculus, making it a robust choice for solving multiphysics problems with efficiency and accuracy. <div>
arXiv:2508.12501v1 Announce Type: new 
Abstract: The discrete exterior calculus (DEC) defines a family of discretized differential operators which preserve certain desirable properties from the exterior calculus. We formulate and solve the porous convection equations in the DEC via the Decapodes.jl embedded domain-specific language (eDSL) for multiphysics problems discretized via CombinatorialSpaces.jl. CombinatorialSpaces.jl is an open-source Julia library which implements the DEC over simplicial complexes, and now offers a geometric multigrid solver over maps between subdivided simplicial complexes. We demonstrate numerical results of multigrid solvers for the Poisson problem and porous convection problem, both as a standalone solver and as a preconditioner for open-source Julia iterative methods libraries.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensured Energy: A simulation game to elicit preferences around Swiss energy transition pathways</title>
<link>https://arxiv.org/abs/2508.12799</link>
<guid>https://arxiv.org/abs/2508.12799</guid>
<content:encoded><![CDATA[
<div> Keywords: Paris Agreement, energy transition, serious game, public acceptance, sustainability <br />
Summary: 
The article discusses the analysis of Switzerland's energy and climate strategy towards achieving the objectives set in the 2015 Paris Agreement. Researchers examine different scenarios for transitioning towards renewable energy sources and assessing the impacts on society, environment, and economy. To gauge public acceptance of energy policies, a population survey was complemented with an online serious game that simulates the current and future energy provision, allowing players to make informed decisions. The game successfully attracted participants from various societal groups, highlighting the challenge of balancing complexity and entertainment. This approach provides valuable insights into public opinion and offers a more engaging way for stakeholders and policymakers to understand and address the challenges of transitioning to a sustainable energy future. <br /><br />Summary: <div>
arXiv:2508.12799v1 Announce Type: new 
Abstract: The 2015 Paris Agreement on global warming specifies national objectives for the reduction of greenhouse gas emissions. In support of Switzerland's energy and climate strategy for 2050, researchers investigate scenarios for the transition of energy systems towards a higher share of renewables, assessing their social, environmental and economic impact. Their results guide stakeholders and policy makers in designing resilient and sustainable systems. Political scientists use surveys to quantify public acceptance of energy policy, but the complexity and long time horizon of the subject creates difficulties, both for researchers in posing contextually relevant questions, and for respondents in assimilating enough information to give meaningful answers. A population survey was therefore augmented with an online serious game in which players experience an accurate simulation of current and future energy provision and manage transition towards a sustainable future. This interactive environment allows better informed and engaged decisions, and provides richer information on public opinion. In this paper we motivate and describe the design of the game and report initial findings on player characteristics and engagement. We show that a serious game can successfully attract participants from diverse societal groups and highlight the challenge of balancing complexity and entertainment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising diffusion models for inverse design of inflatable structures with programmable deformations</title>
<link>https://arxiv.org/abs/2508.13097</link>
<guid>https://arxiv.org/abs/2508.13097</guid>
<content:encoded><![CDATA[
<div> Keywords: Programmable structures, Inflatable structures, Generative design, Denoising diffusion probabilistic models, Pressure-driven actuation

Summary:
- The article discusses the inverse design of elastic structures undergoing large, nonlinear deformations under pressure-driven actuation.
- A generative design framework based on denoising diffusion probabilistic models (DDPMs) is presented for designing programmable structures.
- The method formulates the inverse design as a conditional generation task, using geometric descriptors of target deformed states as inputs.
- The framework quickly produces diverse undeformed configurations that achieve desired deformations when inflated, enabling parallel exploration of design candidates.
- Numerical experiments show the framework can accommodate complex constraints and efficiently explore viable design options. 

<br /><br />Summary: <div>
arXiv:2508.13097v1 Announce Type: new 
Abstract: Programmable structures are systems whose undeformed geometries and material property distributions are deliberately designed to achieve prescribed deformed configurations under specific loading conditions. Inflatable structures are a prominent example, using internal pressurization to realize large, nonlinear deformations in applications ranging from soft robotics and deployable aerospace systems to biomedical devices and adaptive architecture. We present a generative design framework based on denoising diffusion probabilistic models (DDPMs) for the inverse design of elastic structures undergoing large, nonlinear deformations under pressure-driven actuation. The method formulates the inverse design as a conditional generation task, using geometric descriptors of target deformed states as inputs and outputting image-based representations of the undeformed configuration. Representing these configurations as simple images is achieved by establishing a pre- and postprocessing pipeline that involves a fixed image processing, simulation setup, and descriptor extraction methods. Numerical experiments with scalar and higher-dimensional descriptors show that the framework can quickly produce diverse undeformed configurations that achieve the desired deformations when inflated, enabling parallel exploration of viable design candidates while accommodating complex constraints.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites</title>
<link>https://arxiv.org/abs/2508.12029</link>
<guid>https://arxiv.org/abs/2508.12029</guid>
<content:encoded><![CDATA[
<div> antibody-binding sites, epitopes, vaccine design, antigen sequences, convolutional neural networks 

Summary:
- Accurate prediction of antibody-binding sites is critical for various applications in immunology.
- In silico methods have limitations in predicting conformational epitopes effectively.
- A new conformer-based model leveraging CNNs and Transformers is proposed for epitope prediction.
- CNNs enhance the prediction of linear epitopes, while Transformers improve conformational epitope prediction.
- Experimental results show the model outperforms existing baselines in various performance metrics. 

<br /><br />Summary: <div>
arXiv:2508.12029v1 Announce Type: cross 
Abstract: Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and for advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose a conformer-based model trained on antigen sequences derived from 1,080 antigen-antibody complexes, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of PCC, ROC-AUC, PR-AUC, and F1 scores on conformational epitopes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems</title>
<link>https://arxiv.org/abs/2508.12569</link>
<guid>https://arxiv.org/abs/2508.12569</guid>
<content:encoded><![CDATA[
<div> framework, metriplectic, entropy, self-supervised learning, stochastic <br />
Summary: 
The article discusses a framework for machine learning coarse-grained dynamics of multiscale systems using the metriplectic bracket formalism. This framework preserves properties such as dissipative, history-dependent, and stochastic emergent physics, ensuring conservation laws and fluctuation-dissipation balance. A novel self-supervised learning strategy is introduced to identify emergent structural variables when labels are unavailable. The method is validated on benchmark systems and applied to challenging examples like coarse-graining star polymers and learning models from high-speed video of colloidal suspensions. Open-source implementations in PyTorch and LAMMPS are provided, enabling large-scale inference and applicability to various particle-based systems. <br /> <div>
arXiv:2508.12569v1 Announce Type: cross 
Abstract: Multiscale systems are ubiquitous in science and technology, but are notoriously challenging to simulate as short spatiotemporal scales must be appropriately linked to emergent bulk physics. When expensive high-dimensional dynamical systems are coarse-grained into low-dimensional models, the entropic loss of information leads to emergent physics which are dissipative, history-dependent, and stochastic. To machine learn coarse-grained dynamics from time-series observations of particle trajectories, we propose a framework using the metriplectic bracket formalism that preserves these properties by construction; most notably, the framework guarantees discrete notions of the first and second laws of thermodynamics, conservation of momentum, and a discrete fluctuation-dissipation balance crucial for capturing non-equilibrium statistics. We introduce the mathematical framework abstractly before specializing to a particle discretization. As labels are generally unavailable for entropic state variables, we introduce a novel self-supervised learning strategy to identify emergent structural variables. We validate the method on benchmark systems and demonstrate its utility on two challenging examples: (1) coarse-graining star polymers at challenging levels of coarse-graining while preserving non-equilibrium statistics, and (2) learning models from high-speed video of colloidal suspensions that capture coupling between local rearrangement events and emergent stochastic dynamics. We provide open-source implementations in both PyTorch and LAMMPS, enabling large-scale inference and extensibility to diverse particle-based systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shapley Values: Paired-Sampling Approximations</title>
<link>https://arxiv.org/abs/2508.12947</link>
<guid>https://arxiv.org/abs/2508.12947</guid>
<content:encoded><![CDATA[
<div> Shapley values, cooperative game theory, machine learning, sampling approximations, KernelSHAP, PermutationSHAP <br />
<br />
Summary: <br />
Shapley values, originally from cooperative game theory, are widely used in explaining machine learning predictions by assigning credit to input components based on their contribution. This study provides novel contributions by proving the asymptotic normality of sampling approximations like KernelSHAP and PermutationSHAP. Paired-sampling approaches offer exact results for interactions of maximal order two. Additionally, the paired-sampling PermutationSHAP exhibits the additive recovery property, while the kernel counterpart does not. These findings enhance the understanding and computation of Shapley values in explaining prediction outcomes. <div>
arXiv:2508.12947v1 Announce Type: cross 
Abstract: Originally introduced in cooperative game theory, Shapley values have become a very popular tool to explain machine learning predictions. Based on Shapley's fairness axioms, every input (feature component) gets a credit how it contributes to an output (prediction). These credits are then used to explain the prediction. The only limitation in computing the Shapley values (credits) for many different predictions is of computational nature. There are two popular sampling approximations, sampling KernelSHAP and sampling PermutationSHAP. Our first novel contributions are asymptotic normality results for these sampling approximations. Next, we show that the paired-sampling approaches provide exact results in case of interactions being of maximal order two. Furthermore, the paired-sampling PermutationSHAP possesses the additive recovery property, whereas its kernel counterpart does not.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Analysis in Frequency Domain: A Survey of Open Challenges, Opportunities and Benchmarks</title>
<link>https://arxiv.org/abs/2504.07099</link>
<guid>https://arxiv.org/abs/2504.07099</guid>
<content:encoded><![CDATA[
<div> Keywords: Frequency-domain analysis, Spectral methods, Causal structure preservation, Uncertainty quantification, Geometric deep learning<br />
Summary: 
Frequency-domain analysis is a powerful paradigm for time series analysis, offering advantages over traditional approaches. This survey covers classical Fourier analysis to modern neural operators, highlighting three key challenges: preserving causal structure during spectral transformations, quantifying uncertainty in learned frequency representations, and analyzing non-Euclidean data structures with topology awareness. Over 100 studies were reviewed to develop a unified taxonomy bridging conventional spectral techniques with machine learning approaches. The survey identifies knowledge gaps in geometric deep learning and quantum-enhanced spectral analysis. It provides a systematic framework for method selection and implementation and charts promising directions for future research in this rapidly evolving domain. <div>
arXiv:2504.07099v3 Announce Type: replace 
Abstract: Frequency-domain analysis has emerged as a powerful paradigm for time series analysis, offering unique advantages over traditional time-domain approaches while introducing new theoretical and practical challenges. This survey provides a comprehensive examination of spectral methods from classical Fourier analysis to modern neural operators, systematically summarizing three open challenges in current research: (1) causal structure preservation during spectral transformations, (2) uncertainty quantification in learned frequency representations, and (3) topology-aware analysis for non-Euclidean data structures. Through rigorous reviewing of over 100 studies, we develop a unified taxonomy that bridges conventional spectral techniques with cutting-edge machine learning approaches, while establishing standardized benchmarks for performance evaluation. Our work identifies key knowledge gaps in the field, particularly in geometric deep learning and quantum-enhanced spectral analysis. The survey offers practitioners a systematic framework for method selection and implementation, while charting promising directions for future research in this rapidly evolving domain.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantformer: from attention to profit with a quantitative transformer trading strategy</title>
<link>https://arxiv.org/abs/2404.00424</link>
<guid>https://arxiv.org/abs/2404.00424</guid>
<content:encoded><![CDATA[
<div> Transformer, Quantformer, investment factors, sentiment analysis, quantitative trading<br />
<br />
Summary:<br />
Quantitative trading faces challenges in capturing market variables for profit. Quantformer, a neural network based on transformer, uses transfer learning from sentiment analysis to build investment factors. It excels in modeling complex data relationships and forecasting stock trends accurately. With data from 2010 to 2023, Quantformer outperforms other quantitative strategies in predicting stock trends. Its innovative use of transformer-like models combined with market sentiment information enhances trading signal accuracy, promising advancements in quantitative trading strategies. <div>
arXiv:2404.00424v3 Announce Type: replace-cross 
Abstract: In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Fully capturing various market variables, including long-term information, as well as essential signals that may lead to profit remains a difficult task for learning algorithms. In order to tackle this challenge, this paper introduces quantformer, an enhanced neural network architecture based on transformer, to build investment factors. By transfer learning from sentiment analysis, quantformer not only exploits its original inherent advantages in capturing long-range dependencies and modeling complex data relationships, but is also able to solve tasks with numerical inputs and accurately forecast future returns over a given period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2023. The results of this study demonstrate the model's superior performance in predicting stock trends compared with other 100-factor-based quantitative strategies. Notably, the model's innovative use of transformer-like model to establish factors, in conjunction with market sentiment information, has been shown to enhance the accuracy of trading signals significantly, thereby offering promising implications for the future of quantitative trading strategies.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News</title>
<link>https://arxiv.org/abs/2508.10927</link>
<guid>https://arxiv.org/abs/2508.10927</guid>
<content:encoded><![CDATA[
<div> Keywords: company risk factors, news articles, machine learning models, supply chain, regulations<br />
Summary:<br />
- Importance of identifying company risk factors in financial market for investors and overall well-being.
- Computational framework developed to automatically extract risk factors from news articles.
- Schema comprising seven aspects such as supply chain, regulations, competitions.
- Experiment shows fine-tuned pre-trained language models performing better in identifying risk factors compared to zero-shot and few-shot prompting LLMs.
- Analysis of over 277K Bloomberg news articles demonstrates insight into company and industry operations through identification of risk factors.
<br /><br />Summary: Identifying company risk factors is crucial for investors and financial market stability. A computational framework was created to extract these factors from news articles, focusing on aspects like supply chain and regulations. Fine-tuned language models outperformed zero-shot and few-shot models in identifying risk factors. Analysis of Bloomberg news articles revealed valuable insights into company and industry operations through this approach. <div>
arXiv:2508.10927v1 Announce Type: cross 
Abstract: Identifying risks associated with a company is important to investors and the well-being of the overall financial market. In this study, we build a computational framework to automatically extract company risk factors from news articles. Our newly proposed schema comprises seven distinct aspects, such as supply chain, regulations, and competitions. We sample and annotate 744 news articles and benchmark various machine learning models. While large language models have achieved huge progress in various types of NLP tasks, our experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs (e.g. LLaMA-2) can only achieve moderate to low performances in identifying risk factors. And fine-tuned pre-trained language models are performing better on most of the risk factors. Using this model, we analyze over 277K Bloomberg news articles and demonstrate that identifying risk factors from news could provide extensive insight into the operations of companies and industries.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressive Meta-Learning</title>
<link>https://arxiv.org/abs/2508.11090</link>
<guid>https://arxiv.org/abs/2508.11090</guid>
<content:encoded><![CDATA[
<div> Keywords: compressive learning, parameter-learning, neural networks, meta-learning, data structure<br />
<br />
Summary: 
The article discusses the need for fast and efficient parameter-learning techniques due to the exponential growth of new datasets. Compressive learning is introduced as a framework that utilizes random, non-linear features to project large-scale databases onto compact representations, enabling efficient processing without access to the original samples. However, current compressive learning methods lack data structure exploitation. The proposed Compressive Meta-Learning framework meta-learns both encoding and decoding stages using neural networks, offering faster and more accurate systems. Various applications of the framework, such as compressive PCA, compressive ridge regression, and compressive k-means, are explored. This approach shows promise for improving the efficiency and privacy-friendliness of parameter learning in the face of growing dataset sizes. <br /><br />Summary: <div>
arXiv:2508.11090v1 Announce Type: cross 
Abstract: The rapid expansion in the size of new datasets has created a need for fast and efficient parameter-learning techniques. Compressive learning is a framework that enables efficient processing by using random, non-linear features to project large-scale databases onto compact, information-preserving representations whose dimensionality is independent of the number of samples and can be easily stored, transferred, and processed. These database-level summaries are then used to decode parameters of interest from the underlying data distribution without requiring access to the original samples, offering an efficient and privacy-friendly learning framework. However, both the encoding and decoding techniques are typically randomized and data-independent, failing to exploit the underlying structure of the data. In this work, we propose a framework that meta-learns both the encoding and decoding stages of compressive learning methods by using neural networks that provide faster and more accurate systems than the current state-of-the-art approaches. To demonstrate the potential of the presented Compressive Meta-Learning framework, we explore multiple applications -- including neural network-based compressive PCA, compressive ridge regression, compressive k-means, and autoencoders.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance</title>
<link>https://arxiv.org/abs/2508.11395</link>
<guid>https://arxiv.org/abs/2508.11395</guid>
<content:encoded><![CDATA[
<div> inflection point, stablecoins, "Banking 2.0", institutional adoption, macroeconomic imbalances <br />
<br />
Summary: The article discusses how stablecoins are revolutionizing the global financial system by seamlessly integrating cryptocurrency innovation with traditional banking infrastructure. It highlights the significance of stablecoins in addressing vulnerabilities in modern fiat currencies and tackling macroeconomic imbalances such as the inflation-productivity gap. The increasing institutional adoption of stablecoins, as evidenced by U.S. legislation and initiatives from major industry players like JPMorgan and PayPal, underscores their transformative potential. Stablecoins offer enhanced stability, reduced fraud risk, and facilitate unified global transactions that transcend national boundaries. By providing more robust and diversified backing mechanisms, stablecoins pave the way for a more interconnected international financial system while enabling deregulation and efficiency gains. The article provides real-world examples and current market data to support the argument that stablecoins are poised to reshape banking as we know it. <div>
arXiv:2508.11395v1 Announce Type: cross 
Abstract: The global financial system stands at an inflection point. Stablecoins represent the most significant evolution in banking since the abandonment of the gold standard, positioned to enable "Banking 2.0" by seamlessly integrating cryptocurrency innovation with traditional finance infrastructure. This transformation rivals artificial intelligence as the next major disruptor in the financial sector. Modern fiat currencies derive value entirely from institutional trust rather than physical backing, creating vulnerabilities that stablecoins address through enhanced stability, reduced fraud risk, and unified global transactions that transcend national boundaries. Recent developments demonstrate accelerating institutional adoption: landmark U.S. legislation including the GENIUS Act of 2025, strategic industry pivots from major players like JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive "Pay with Crypto" service. Widespread stablecoin implementation addresses critical macroeconomic imbalances, particularly the inflation-productivity gap plaguing modern monetary systems, through more robust and diversified backing mechanisms. Furthermore, stablecoins facilitate deregulation and efficiency gains, paving the way for a more interconnected international financial system. This whitepaper comprehensively explores how stablecoins are poised to reshape banking, supported by real-world examples, current market data, and analysis of their transformative potential.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models</title>
<link>https://arxiv.org/abs/2508.11542</link>
<guid>https://arxiv.org/abs/2508.11542</guid>
<content:encoded><![CDATA[
<div> approach, Operator Inference, reduced-order models, dynamic systems, snapshot data  
Summary:  
This paper presents a data-driven, nested Operator Inference (OpInf) approach for learning physics-informed reduced-order models (ROMs) from snapshot data of high-dimensional dynamical systems. The approach utilizes a hierarchy within the reduced space to iteratively construct initial guesses prioritizing interactions of dominant modes. The initial guess for any target reduced dimension yields a ROM with smaller or equal snapshot reconstruction error compared to standard OpInf. The nested OpInf algorithm supports warm-starting from previous models, allowing for dynamic basis and model form updates. Demonstrations on a cubic heat conduction problem showed nested OpInf achieved significantly smaller errors than standard OpInf with comparable offline time. Application to a large-scale Greenland ice sheet model produced a ROM with an average error of 3% and a computational speed-up factor exceeding 19,000. <div>
arXiv:2508.11542v1 Announce Type: cross 
Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach for learning physics-informed reduced-order models (ROMs) from snapshot data of high-dimensional dynamical systems. The approach exploits the inherent hierarchy within the reduced space to iteratively construct initial guesses for the OpInf learning problem that prioritize the interactions of the dominant modes. The initial guess computed for any target reduced dimension corresponds to a ROM with provably smaller or equal snapshot reconstruction error than with standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from previously learned models, enabling versatile application scenarios involving dynamic basis and model form updates. We demonstrate the performance of our algorithm on a cubic heat conduction problem, with nested OpInf achieving a four times smaller error than standard OpInf at a comparable offline time. Further, we apply nested OpInf to a large-scale, parameterized model of the Greenland ice sheet where, despite model form approximation errors, it learns a ROM with, on average, 3% error and computational speed-up factor above 19,000.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CarAT: Carbon Atom Tracing across Industrial Chemical Value Chains via Chemistry Language Models</title>
<link>https://arxiv.org/abs/2508.10216</link>
<guid>https://arxiv.org/abs/2508.10216</guid>
<content:encoded><![CDATA[
<div> Carbon Atom Tracker, biogenic carbon content, sustainability reporting, industrial value chains, sustainability.

Summary:
The chemical industry is focusing on sustainability and reducing carbon footprints. The Together for Sustainability consortium will soon require reporting of biogenic carbon content (BCC) in chemical products. Carbon-14 is impractical for continuous monitoring, so a new automated methodology called CarAT has been developed. CarAT uses Enterprise Resource Planning data to calculate BCC across industrial value chains by mapping carbon atoms in chemical reactions and applying a linear program. The methodology was validated on a toluene diisocyanate value chain with different scenarios. Results were visualized with Sankey diagrams, showing the flow of carbon attributes. CarAT enables real-time BCC calculation, supports compliance with reporting mandates, and facilitates the transition to sustainable manufacturing by tracking carbon sources transparently and empowering data-driven decisions. <br /><br />Summary: <div>
arXiv:2508.10216v1 Announce Type: new 
Abstract: The chemical industry is increasingly prioritising sustainability, with a focus on reducing carbon footprints to achieve net zero. By 2026, the Together for Sustainability (TfS) consortium will require reporting of biogenic carbon content (BCC) in chemical products, posing a challenge as BCC depends on feedstocks, value chain configuration, and process-specific variables. While carbon-14 isotope analysis can measure BCC, it is impractical for continuous industrial monitoring. This work presents CarAT (Carbon Atom Tracker), an automated methodology for calculating BCC across industrial value chains, enabling dynamic and accurate sustainability reporting. The approach leverages existing Enterprise Resource Planning data in three stages: (1) preparing value chain data, (2) performing atom mapping in chemical reactions using chemistry language models, and (3) applying a linear program to calculate BCC given known inlet compositions. The methodology is validated on a 27-node industrial toluene diisocyanate value chain. Three scenarios are analysed: a base case with fossil feedstocks, a case incorporating a renewable feedstock, and a butanediol value chain with a recycle stream. Results are visualised with Sankey diagrams showing the flow of carbon attributes across the value chain. The key contribution is a scalable, automated method for real-time BCC calculation under changing industrial conditions. CarAT supports compliance with upcoming reporting mandates and advances carbon neutrality goals by enabling systematic fossil-to-biogenic substitution. Through transparent, auditable tracking of carbon sources in production networks, it empowers data-driven decisions to accelerate the transition to sustainable manufacturing.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOBACO: Topology Optimization via Band-limited Coordinate Networks for Compositionally Graded Alloys</title>
<link>https://arxiv.org/abs/2508.10320</link>
<guid>https://arxiv.org/abs/2508.10320</guid>
<content:encoded><![CDATA[
<div> Keywords: Compositionally Graded Alloys, Additive Manufacturing, Topology Optimization, Coordinate Neural Network, Spatial Gradation<br />
Summary: <br />
Compositionally Graded Alloys (CGAs) offer design flexibility through spatial composition variations for stronger and lighter components. Advances in additive manufacturing (AM) have made CGA fabrication feasible, but manufacturing constraints on spatial gradation exist. This paper presents a topology optimization (TO) framework for optimized CGA designs with controlled compositional gradation. A band-limited coordinate neural network represents the composition distribution, ensuring compliance with gradation limits without explicit constraints. The approach benefits from TO advantages like mesh independence and high-resolution design extraction. Demonstrations in elastic and thermo-elastic TO examples showcase the framework's effectiveness. <div>
arXiv:2508.10320v1 Announce Type: new 
Abstract: Compositionally Graded Alloys (CGAs) offer unprecedented design flexibility by enabling spatial variations in composition; tailoring material properties to local loading conditions. This flexibility leads to components that are stronger, lighter, and more cost-effective than traditional monolithic counterparts. The fabrication of CGAs have become increasingly feasible owing to recent advancements in additive manufacturing (AM), particularly in multi-material printing and improved precision in material deposition. However, AM of CGAs requires imposition of manufacturing constraints; in particular limits on the maximum spatial gradation of composition.
  This paper introduces a topology optimization (TO) based framework for designing optimized CGA components with controlled compositional gradation. In particular, we represent the constrained composition distribution using a band-limited coordinate neural network. By regulating the network's bandwidth, we ensure implicit compliance with gradation limits, eliminating the need for explicit constraints. The proposed approach also benefits from the inherent advantages of TO using coordinate networks, including mesh independence, high-resolution design extraction, and end-to-end differentiability. The effectiveness of our framework is demonstrated through various elastic and thermo-elastic TO examples.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chem3DLLM: 3D Multimodal Large Language Models for Chemistry</title>
<link>https://arxiv.org/abs/2508.10696</link>
<guid>https://arxiv.org/abs/2508.10696</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D molecular structures, Chem3DLLM, multimodal, protein-conditioned, drug discovery

Summary:<br /><br />Chem3DLLM is introduced as a unified protein-conditioned multimodal large language model to generate 3D molecular structures. It addresses challenges faced by autoregressive-based language models in handling 3D molecular conformation. The model utilizes a reversible text encoding technique for 3D molecular structures that enables integration with protein pocket features. Reinforcement learning with stability-based rewards is employed to optimize chemical validity, and a lightweight protein embedding projector is incorporated for end-to-end training. Experimental results demonstrate state-of-the-art performance in structure-based drug design with a Vina score of -7.21, showcasing the efficacy of the unified multimodal approach for practical drug discovery applications. <div>
arXiv:2508.10696v1 Announce Type: new 
Abstract: In the real world, a molecule is a 3D geometric structure. Compared to 1D SMILES sequences and 2D molecular graphs, 3D molecules represent the most informative molecular modality. Despite the rapid progress of autoregressive-based language models, they cannot handle the generation of 3D molecular conformation due to several challenges: 1) 3D molecular structures are incompatible with LLMs' discrete token space, 2) integrating heterogeneous inputs like proteins, ligands, and text remains difficult within a unified model, and 3) LLMs lack essential scientific priors, hindering the enforcement of physical and chemical constraints during generation. To tackle these issues, we present Chem3DLLM, a unified protein-conditioned multimodal large language model. Our approach designs a novel reversible text encoding for 3D molecular structures using run-length compression, achieving 3x size reduction while preserving complete structural information. This enables seamless integration of molecular geometry with protein pocket features in a single LLM architecture. We employ reinforcement learning with stability-based rewards to optimize chemical validity and incorporate a lightweight protein embedding projector for end-to-end training. Experimental results on structure-based drug design demonstrate state-of-the-art performance with a Vina score of -7.21, validating our unified multimodal approach for practical drug discovery applications.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model</title>
<link>https://arxiv.org/abs/2508.10492</link>
<guid>https://arxiv.org/abs/2508.10492</guid>
<content:encoded><![CDATA[
<div> AI, clinical diagnosis, full-process, DxDirector-7B, deep thinking<br />
Summary: In the article, the authors propose a paradigm shift in which AI, specifically the DxDirector-7B large language model, takes on the primary role in driving the full-process clinical diagnosis, with human physicians as assistants. This model is equipped with advanced deep thinking capabilities and establishes an accountability framework for misdiagnoses. DxDirector-7B outperforms existing medical and general-purpose language models in accuracy and significantly reduces physician workload. It is evaluated across rare, complex, and real-world cases, showing potential to serve as a viable substitute for medical specialists. The shift towards AI driving the diagnostic process marks a new era in healthcare, with the potential to enhance diagnostic efficiency and reduce physicians' workload. <div>
arXiv:2508.10492v1 Announce Type: cross 
Abstract: Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians' workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians' assistant, now drives the entire diagnostic process to drastically reduce physicians' workload, indicating an efficient and accurate diagnostic solution.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Sensing for Solder Layer Degradation and Temperature Monitoring in IGBT Modules</title>
<link>https://arxiv.org/abs/2508.10515</link>
<guid>https://arxiv.org/abs/2508.10515</guid>
<content:encoded><![CDATA[
<div> solder degradation, IGBT modules, machine learning, virtual sensing, temperature estimation
Summary:
Machine learning-based virtual sensing is explored in this study to monitor solder degradation in IGBT modules, crucial for power electronic system reliability. With limited physical sensors, accurate estimation of degraded solder area (1.17% error) and surface temperature (max 4.56% relative error) is achieved. This approach offers a promising alternative to direct measurement of internal component degradation indicators, overcoming physical inaccessibility challenges in harsh environments. <div>
arXiv:2508.10515v1 Announce Type: cross 
Abstract: Monitoring the degradation state of Insulated Gate Bipolar Transistor (IGBT) modules is essential for ensuring the reliability and longevity of power electronic systems, especially in safety-critical and high-performance applications. However, direct measurement of key degradation indicators - such as junction temperature, solder fatigue or delamination - remains challenging due to the physical inaccessibility of internal components and the harsh environment. In this context, machine learning-based virtual sensing offers a promising alternative by bridging the gap from feasible sensor placement to the relevant but inaccessible locations. This paper explores the feasibility of estimating the degradation state of solder layers, and the corresponding full temperature maps based on a limited number of physical sensors. Based on synthetic data of a specific degradation mode, we obtain a high accuracy in the estimation of the degraded solder area (1.17% mean absolute error), and are able to reproduce the surface temperature of the IGBT with a maximum relative error of 4.56% (corresponding to an average relative error of 0.37%).
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Deep Contrast Source Inversion: A Unified Framework for Inverse Scattering Problems</title>
<link>https://arxiv.org/abs/2508.10555</link>
<guid>https://arxiv.org/abs/2508.10555</guid>
<content:encoded><![CDATA[
<div> inverse scattering problems, electromagnetic imaging, medical diagnostics, deep learning, contrast source inversion

Summary:
This paper proposes a physics-informed deep contrast source inversion framework (DeepCSI) for accurate medium reconstruction in inverse scattering problems. The approach utilizes a residual multilayer perceptron (ResMLP) to model current distributions under different transmitter excitations, linearizing the problem and reducing computational costs. By treating medium parameters as learnable tensors and employing a hybrid loss function, DeepCSI enables joint optimization of network parameters and medium properties. The framework is capable of handling diverse measurement scenarios, including phase-less and multi-frequency observation, offering simplicity and universal modeling capabilities compared to traditional methods. Simulations and experiments show that DeepCSI outperforms conventional contrast source inversion methods, providing high-precision and robust reconstruction in complex inverse scattering problems. <div>
arXiv:2508.10555v1 Announce Type: cross 
Abstract: Inverse scattering problems are critical in electromagnetic imaging and medical diagnostics but are challenged by their nonlinearity and diverse measurement scenarios. This paper proposes a physics-informed deep contrast source inversion framework (DeepCSI) for fast and accurate medium reconstruction across various measurement conditions. Inspired by contrast source inversion (CSI) and neural operator methods, a residual multilayer perceptron (ResMLP) is employed to model current distributions in the region of interest under different transmitter excitations, effectively linearizing the nonlinear inverse scattering problem and significantly reducing the computational cost of traditional full-waveform inversion. By modeling medium parameters as learnable tensors and utilizing a hybrid loss function that integrates state equation loss, data equation loss, and total variation regularization, DeepCSI establishes a fully differentiable framework for joint optimization of network parameters and medium properties. Compared with conventional methods, DeepCSI offers advantages in terms of simplicity and universal modeling capabilities for diverse measurement scenarios, including phase-less and multi-frequency observation. Simulations and experiments demonstrate that DeepCSI achieves high-precision, robust reconstruction under full-data, phaseless data, and multifrequency conditions, outperforming traditional CSI methods and providing an efficient and universal solution for complex inverse scattering problems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impacts of DEM Type and Resolution on Deep Learning-Based Flood Inundation Mapping</title>
<link>https://arxiv.org/abs/2309.13360</link>
<guid>https://arxiv.org/abs/2309.13360</guid>
<content:encoded><![CDATA[
<div> Keywords: flood mapping, deep learning, DEM resolution, flood prediction accuracy, data-scarce regions 

Summary:
- The study examines how DEM type and resolution impact flood prediction accuracy using a deep learning method.
- Synthetic hydrographs are used as training input with water depth data from a hydrodynamic model as target data.
- DSMs and DTMs derived from a 1 m LIDAR-based DTM were compared at resolutions from 15 to 30 m in the city of Carlisle, UK.
- Using a 30 m DTM outperformed a 30 m DSM by 21% in flood depth prediction accuracy during peak stages.
- Increasing DTM resolution to 15 m resulted in a minimum 50% increase in RMSE and a 20% increase in fit index across all flood stages.
- Coarser resolution DEMs may impact accuracy, but even a slight improvement in data resolution in data-scarce regions can enhance flood risk management.

<br /><br />Summary: <div>
arXiv:2309.13360v4 Announce Type: replace 
Abstract: The increasing availability of hydrological and physiographic spatiotemporal data has boosted machine learning's role in rapid flood mapping. Yet, data scarcity, especially high-resolution DEMs, challenges regions with limited access. This paper examines how DEM type and resolution affect flood prediction accuracy, utilizing a cutting-edge deep learning (DL) method called 1D convolutional neural network (CNN). It utilizes synthetic hydrographs as training input and water depth data obtained from LISFLOOD-FP, a 2D hydrodynamic model, as target data. This study investigates digital surface models (DSMs) and digital terrain models (DTMs) derived from a 1 m LIDAR-based DTM, with resolutions from 15 to 30 m. The methodology is applied and assessed in an established benchmark, the city of Carlisle, UK. The models' performance is then evaluated and compared against an observed flood event using RMSE, Bias, and Fit indices. Leveraging the insights gained from this region, the paper discusses the applicability of the methodology to address the challenges encountered in a data-scarce flood-prone region, exemplified by Pakistan. Results indicated that utilizing a 30 m DTM outperformed a 30 m DSM in terms of flood depth prediction accuracy by about 21% during the flood peak stage, highlighting the superior performance of DTM at lower resolutions. Increasing the resolution of DTM to 15 m resulted in a minimum 50% increase in RMSE and a 20% increase in fit index across all flood stages. The findings emphasize that while a coarser resolution DEM may impact the accuracy of machine learning models, it remains a viable option for rapid flood prediction. However, even a slight improvement in data resolution in data-scarce regions would provide significant added value, ultimately enhancing flood risk management.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Neural ODEs for Gene Regulatory Network Discovery under Perturbations</title>
<link>https://arxiv.org/abs/2501.02409</link>
<guid>https://arxiv.org/abs/2501.02409</guid>
<content:encoded><![CDATA[
<div> neural ODEs, gene regulatory network, perturbations, trajectory prediction, cell state<br />
Summary:<br />
The article focuses on developing PerturbODE, a framework that uses neural ordinary differential equations (ODEs) to model cell state trajectories and infer gene regulatory networks (GRNs) from large-scale perturbation datasets. By incorporating biologically informative neural ODEs, PerturbODE addresses the limitations of existing GRN inference models in terms of expressivity and scalability. It aims to capture causal gene regulatory relationships and account for the dynamic nature of biological processes such as cellular differentiation. The efficacy of PerturbODE is demonstrated through trajectory prediction and GRN inference on simulated and real over-expression datasets. The framework shows promising results in accurately predicting cell state trajectories under perturbations and deriving causal GRNs. <div>
arXiv:2501.02409v3 Announce Type: replace-cross 
Abstract: Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Differentiable causal graphical models have been proposed to infer a gene regulatory network (GRN) from large scale interventional datasets, capturing the causal gene regulatory relationships from genetic perturbations. However, existing models are limited in their expressivity and scalability while failing to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Topology Optimisation of Time-dependent Thermal Conduction Using Space-Time Finite Elements and a Parallel Space-Time Multigrid Preconditioner</title>
<link>https://arxiv.org/abs/2508.09589</link>
<guid>https://arxiv.org/abs/2508.09589</guid>
<content:encoded><![CDATA[
<div> Keywords: space-time topology optimisation, thermal conduction, finite element method, parallel computing, scalability

Summary:
This paper introduces a novel space-time topology optimization framework for time-dependent thermal conduction problems. Time is treated as an additional spatial dimension, and the governing equations are discretized using a stabilised continuous Galerkin space-time finite element method. A parallel-in-time method is implemented, demonstrating excellent scalability on a distributed-memory supercomputer. The framework offers up to 52x speed-up compared to traditional time-stepping approaches, with only moderate increases in total computational cost. Validation on benchmark problems with varying designs and material properties show the flexibility of the method. The proposed space-time method proves to be a promising approach for large-scale time-dependent topology optimization in thermal applications. 

<br /><br />Summary: <div>
arXiv:2508.09589v1 Announce Type: new 
Abstract: This paper presents a novel space-time topology optimisation framework for time-dependent thermal conduction problems, aiming to significantly reduce the time-to-solution. By treating time as an additional spatial dimension, we discretise the governing equations using a stabilised continuous Galerkin space-time finite element method. The resulting large all-at-once system is solved using an iterative Krylov solver preconditioned with a parallel space-time multigrid method employing a semi-coarsening strategy. Implemented in a fully parallel computing framework, the method yields a parallel-in-time method that demonstrates excellent scalability on a distributed-memory supercomputer, solving problems up to 4.2 billion degrees of freedom. Comparative studies show up to 52x speed-up over traditional time-stepping approaches, with only moderate increases in total computational cost in terms of core-hours. The framework is validated on benchmark problems with both time-constant and time-varying designs, and its flexibility is demonstrated through variations in material properties. These results establish the proposed space-time method as a promising approach for large-scale time-dependent topology optimisation in thermal applications.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisFinEval: A Scenario-Driven Chinese Multimodal Benchmark for Holistic Financial Understanding</title>
<link>https://arxiv.org/abs/2508.09641</link>
<guid>https://arxiv.org/abs/2508.09641</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, financial analysis, VisFinEval, Chinese benchmark, error analysis

Summary: 
Multimodal large language models (MLLMs) are promising for automating complex financial analysis. VisFinEval is a large-scale Chinese benchmark that covers various financial tasks using different image modalities. The benchmark includes 15,848 annotated question-answer pairs organized into three financial scenario depths. 21 state-of-the-art MLLMs were evaluated in a zero-shot setting, with the top model achieving 76.3% overall accuracy. However, it still trails behind financial experts. An error analysis revealed six recurring failure modes, indicating areas for future research such as cross-modal misalignment and lapses in business-process reasoning. VisFinEval aims to advance the development of domain-tailored MLLMs capable of integrating textual and visual financial information seamlessly.

<br /><br />Summary: <div>
arXiv:2508.09641v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) hold great promise for automating complex financial analysis. To comprehensively evaluate their capabilities, we introduce VisFinEval, the first large-scale Chinese benchmark that spans the full front-middle-back office lifecycle of financial tasks. VisFinEval comprises 15,848 annotated question-answer pairs drawn from eight common financial image modalities (e.g., K-line charts, financial statements, official seals), organized into three hierarchical scenario depths: Financial Knowledge & Data Analysis, Financial Analysis & Decision Support, and Financial Risk Control & Asset Optimization. We evaluate 21 state-of-the-art MLLMs in a zero-shot setting. The top model, Qwen-VL-max, achieves an overall accuracy of 76.3%, outperforming non-expert humans but trailing financial experts by over 14 percentage points. Our error analysis uncovers six recurring failure modes-including cross-modal misalignment, hallucinations, and lapses in business-process reasoning-that highlight critical avenues for future research. VisFinEval aims to accelerate the development of robust, domain-tailored MLLMs capable of seamlessly integrating textual and visual financial information. The data and the code are available at https://github.com/SUFE-AIFLM-Lab/VisFinEval.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finetuning Large Language Model as an Effective Symbolic Regressor</title>
<link>https://arxiv.org/abs/2508.09897</link>
<guid>https://arxiv.org/abs/2508.09897</guid>
<content:encoded><![CDATA[
<div> Keywords: Symbolic Regression, Large Language Models, Fine-tuning, SymbArena, SymbolicChat 

Summary: 
The article introduces the concept of Symbolic Regression (SR) and highlights the limitations of current Large Language Models (LLMs) in solving SR tasks efficiently. To address this, the authors propose fine-tuning LLMs for enhanced SR capability. However, the lack of dedicated datasets for SR-focused fine-tuning poses a challenge. To overcome this, the authors introduce SymbArena, a benchmark comprising a diverse set of equations for LLM training. SymbArena also introduces a new metric for evaluating form-level consistency in SR tasks. Through experiments, the authors demonstrate the effectiveness of SymbolicChat, a new LLM-based SR model that outperforms traditional numerical methods in both numerical precision and symbolic form accuracy. SymbolicChat achieves significant improvements in R2 score and form-level consistency score compared to other LLM baselines. This research paves the way for utilizing LLMs in SR tasks more effectively. 

<br /><br />Summary: <div>
arXiv:2508.09897v1 Announce Type: new 
Abstract: Deriving governing equations from observational data, known as Symbolic Regression (SR), is a cornerstone of scientific discovery. Large Language Models (LLMs) have shown promise in this task by leveraging their vast cross-disciplinary scientific knowledge. However, existing LLM-based methods primarily rely on direct inference or prompt engineering, often requiring excessive inference iterations to converge on correct formulas or failing to treating complex equation targets. These limitations in effectiveness and generalization stem from an inherent tension between pre-trained LLMs' proficiency in approximate reasoning and the high-precision demands of SR tasks. To bridge this gap, we propose to fine-tune LLMs for enhanced SR capability. Yet, the absence of dedicated datasets for SR-oriented fine-tuning remains a critical barrier. We thus introduce SymbArena, specifically engineered to optimize LLMs for SR. This benchmark comprises 148,102 diverse equations formulated as corpora of 1.83 billion tokens for LLM utilization, enabling effective training and inference. Further, SymbArena proposes a heuristics metric to precisely quantify form-level consistency, going beyond existing SR numerical-oriented evaluation strategies. With this benchmark, we explore mainstream LLM fine-tuning techniques for SR tasks and establish SymbolicChat, a simple yet effective LLM-based SR strong baseline. Experimental results validate SymbolicChat as the first LLM to exceed traditional numerical methods in both numerical precision and symbolic form accuracy, outperforming the second-best LLM baseline with improvements of 2-fold gains in R2 score and 8.37% in form-level consistency score.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction</title>
<link>https://arxiv.org/abs/2508.09227</link>
<guid>https://arxiv.org/abs/2508.09227</guid>
<content:encoded><![CDATA[
<div> Graph Attention Network, Recurrent Neural Network, Trajectory Prediction, Intelligent Transportation Systems, Task Corrector

Summary:
The proposed GSMT model integrates a Graph Attention Network (GAT) with a Recurrent Neural Network (RNN) for accurate trajectory prediction of buses in urban environments. A task corrector refines predictions by clustering historical trajectories and identifying distinct motion patterns. GSMT fuses dynamic bus and static station data through embedded networks for prediction and utilizes the corrector for further refinement. The approach allows for multi-node trajectory prediction in dense urban traffic conditions. Experimental results on a Kuala Lumpur dataset show superior performance compared to existing methods in both short-term and long-term prediction tasks. The GSMT model offers enhanced accuracy and efficiency in trajectory prediction for buses, especially in regions with limited multimodal data access.<br /><br />Summary: <div>
arXiv:2508.09227v1 Announce Type: cross 
Abstract: Accurate trajectory prediction for buses is crucial in intelligent transportation systems, particularly within urban environments. In developing regions where access to multimodal data is limited, relying solely on onboard GPS data remains indispensable despite inherent challenges. To address this problem, we propose GSMT, a hybrid model that integrates a Graph Attention Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and incorporates a task corrector capable of extracting complex behavioral patterns from large-scale trajectory data. The task corrector clusters historical trajectories to identify distinct motion patterns and fine-tunes the predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus information and static station information through embedded hybrid networks to perform trajectory prediction, and applies the task corrector for secondary refinement after the initial predictions are generated. This two-stage approach enables multi-node trajectory prediction among buses operating in dense urban traffic environments under complex conditions. Experiments conducted on a real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method significantly outperforms existing approaches, achieving superior performance in both short-term and long-term trajectory prediction tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</title>
<link>https://arxiv.org/abs/2508.09811</link>
<guid>https://arxiv.org/abs/2508.09811</guid>
<content:encoded><![CDATA[
<div> 3D scene geometry, appearance, physical information, dynamic multi-view videos, physics-informed losses

Summary:
The paper introduces a new framework named TRACE for modeling complex dynamic 3D scenes solely from multi-view videos without human labels. By treating each 3D point as a rigid particle with size and orientation, the method learns a translation rotation dynamics system for individual particles, accurately estimating physical parameters governing their motion. This approach enables the modeling of complex motion physics without requiring additional labels. Extensive experiments on various datasets demonstrate superior performance in future frame extrapolation compared to existing methods. Additionally, the framework allows for easy segmentation of multiple objects or parts by clustering the learned physical parameters. The method showcases remarkable capabilities in capturing 3D scene geometry, appearance, and physical information from dynamic multi-view videos, showcasing its potential for various applications in computer vision and scene understanding. 

<br /><br />Summary: <div>
arXiv:2508.09811v1 Announce Type: cross 
Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural nets, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. We propose a new framework named TRACE to model the motion physics of complex dynamic 3D scenes. The key novelty of our method is that, by formulating each 3D point as a rigid particle with size and orientation in space, we directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particle's motion over time. Extensive experiments on three existing dynamic datasets and one newly created challenging synthetic datasets demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Volume Element: Existence and Extent in Cracked Heterogeneous Medium</title>
<link>https://arxiv.org/abs/2508.08320</link>
<guid>https://arxiv.org/abs/2508.08320</guid>
<content:encoded><![CDATA[
<div> microscale failure, composites, multiscale modelling, representative volume element, periodic boundary conditions

Summary:
This paper addresses the growing demand for composites in engineering by focusing on microscale failure and improving composites through multiscale modelling. It aims to enhance the representativeness of volume elements by reducing mesh and size sensitivities in representative volume element (RVE) modelling. A technique is proposed to equalize fracture energy in computational analysis with real phenomena to mitigate mesh sensitivity. Modified periodic boundary conditions (MPBCs) are introduced to reduce size dependency in RVE modelling, validated through analysis of 1200 RVE samples under transverse loading. The study also examines factors influencing damage initiation in 2D composite RVEs, finding that the arrangement of closely spaced fibers can promote damage in the region between them. <div>
arXiv:2508.08320v1 Announce Type: new 
Abstract: Acknowledging the ever-increasing demand for composites in the engineering industry, this paper focuses on the failure of composites at the microscale and augmenting the use of multiscale modelling techniques to make them better for various applications. This work aims to increase the representativeness of the volume element by attenuating the mesh and size sensitivities in representative volume element (RVE) modelling. A technique to alleviate mesh sensitivity in RVE modelling is proposed, which equalises the fracture energy observed from computational analysis with the real phenomenon, thereby keeping the response independent of the bandwidth of strain localisation. Based on the hypothesis that ensuring periodicity of strain, in addition to displacement periodicity across the domain boundary and supplementing the capability of periodic boundary conditions (PBCs) to attenuate the size dependency in RVE modelling, a set of modified PBCs (MPBCs) are formulated. One thousand two hundred RVE samples falling into combinations of five fibre volume fractions and four RVE sizes are analysed under transverse loading, and the ability of MPBCs to attenuate the effect of RVE size on the precision of material response, particularly in the inelastic regime, is verified. This work also focuses on various factors affecting damage initiation in 2D composite RVEs. The arrangement of a pair of fibres with their members placed close to each other, such that the angle between the direction of loading and an imaginary line drawn between their centres is less, is observed to make the region between them more favourable to damage.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms</title>
<link>https://arxiv.org/abs/2508.08325</link>
<guid>https://arxiv.org/abs/2508.08325</guid>
<content:encoded><![CDATA[
<div> algorithm, pricing, advertising, competition, collusion
<br />
Summary:<br />
The study examines the impact of AI learning algorithms on product pricing and advertising decisions in online marketplaces. It investigates concerns about tacit collusion among sellers using these algorithms. The research shows that under certain conditions, algorithms can lead to mutually beneficial outcomes for consumers, sellers, and platforms by coordinating on lower prices. This coordination is achieved through lower advertising costs, resulting in decreased prices. Analysis of a large dataset from Amazon.com reveals varying consumer search costs across different product keywords. The study also identifies a negative relationship between consumer search costs and algorithm usage, indicating beneficial collusion. In terms of platform response, profit increases are seen through commission adjustments rather than reserve price changes. Overall, the findings suggest that competing learning algorithms may not have harmful effects and can assist in decision-making for sellers, platforms, and policymakers. 
 <div>
arXiv:2508.08325v1 Announce Type: cross 
Abstract: Online sellers have been adopting AI learning algorithms to automatically make product pricing and advertising decisions on e-commerce platforms. When sellers compete using such algorithms, one concern is that of tacit collusion - the algorithms learn to coordinate on higher than competitive. We empirically investigate whether these concerns are valid when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. Our empirical strategy is to analyze competition with multi-agent reinforcement learning, which we calibrate to a large-scale dataset collected from Amazon.com products. Our first contribution is to find conditions under which learning algorithms can facilitate win-win-win outcomes that are beneficial for consumers, sellers, and even the platform, when consumers have high search costs. In these cases the algorithms learn to coordinate on prices that are lower than competitive prices. The intuition is that the algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices. Our second contribution is an analysis of a large-scale, high-frequency keyword-product dataset for more than 2 million products on Amazon.com. Our estimates of consumer search costs show a wide range of costs for different product keywords. We generate an algorithm usage and find a negative interaction between the estimated consumer search costs and the algorithm usage index, providing empirical evidence of beneficial collusion. Finally, we analyze the platform's strategic response. We find that reserve price adjustments will not increase profits for the platform, but commission adjustments will. Our analyses help alleviate some worries about the potentially harmful effects of competing learning algorithms, and can help sellers, platforms and policymakers to decide on whether to adopt or regulate such algorithms.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Can Understand Spectra: A Multimodal Model for Molecular Structure Elucidation</title>
<link>https://arxiv.org/abs/2508.08441</link>
<guid>https://arxiv.org/abs/2508.08441</guid>
<content:encoded><![CDATA[
<div> Keywords: Structure elucidation, spectroscopic modalities, SpectraLLM, multi-modal reasoning, molecular structure

Summary: 
SpectraLLM is a novel language model designed to support multi-modal spectroscopic joint reasoning for structure elucidation. It can process single or multiple spectroscopic inputs and perform end-to-end structure elucidation by integrating continuous and discrete spectroscopic modalities. SpectraLLM learns to uncover substructural patterns that are consistent and complementary across spectra, enabling precise molecular structure elucidation. Pretrained and fine-tuned in small molecule domain, SpectraLLM achieves state-of-the-art performance on standardized chemical datasets. The model demonstrates strong robustness and generalization, even for single-spectrum inference, with its multi-modal reasoning capability further improving structural prediction accuracy.<br /><br />Summary: <div>
arXiv:2508.08441v1 Announce Type: cross 
Abstract: Structure elucidation is a fundamental technique for understanding the microscopic composition of matter and is widely applied across various disciplines in the natural sciences and engineering. However, existing methods often rely heavily on prior databases or known structural information, making it difficult to resolve unknown structures. In addition, complex structures typically require the joint analysis of multiple spectroscopic modalities. This process heavily depends on expert domain knowledge and is often accompanied by high costs in terms of both time and instrumentation. To address these challenges, we propose SpectraLLM, the first large language model designed to support multi-modal spectroscopic joint reasoning. SpectraLLM is capable of processing either single or multiple spectroscopic inputs and performing end-to-end structure elucidation. By integrating continuous and discrete spectroscopic modalities into a shared semantic space, SpectraLLM learns to uncover substructural patterns that are consistent and complementary across spectra, enabling precise molecular structure elucidation. We pretrain and fine-tune SpectraLLM in the domain of small molecules, and evaluate it on six standardized, publicly available chemical datasets. The model achieves state-of-the-art performance, significantly outperforming existing approaches trained on single modalities. Notably, SpectraLLM demonstrates strong robustness and generalization even for single-spectrum inference, while its multi-modal reasoning capability further improves the accuracy of structural prediction.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection-based multifidelity linear regression for data-scarce applications</title>
<link>https://arxiv.org/abs/2508.08517</link>
<guid>https://arxiv.org/abs/2508.08517</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, multifidelity methods, linear regression, high-dimensional outputs, data-limited applications

Summary:
Surrogate modeling for systems with high-dimensional outputs is challenging when training data are limited. This study introduces multifidelity methods for multiple-input multiple-output linear regression in such data-limited scenarios. Two projection-based approaches are proposed using principal component basis vectors for dimensionality reduction. These approaches combine high-fidelity and low-fidelity data through direct data augmentation and data augmentation with explicit linear corrections. The regression model is trained using weighted least squares with fidelity-specific weights, exploring various weighting schemes. The methods are applied to approximating the surface pressure field of a hypersonic vehicle, showing 3% - 12% improvement in median accuracy compared to single-fidelity methods with comparable computational cost in a low-data regime of up to ten high-fidelity samples.

<br /><br />Summary: <div>
arXiv:2508.08517v1 Announce Type: cross 
Abstract: Surrogate modeling for systems with high-dimensional quantities of interest remains challenging, particularly when training data are costly to acquire. This work develops multifidelity methods for multiple-input multiple-output linear regression targeting data-limited applications with high-dimensional outputs. Multifidelity methods integrate many inexpensive low-fidelity model evaluations with limited, costly high-fidelity evaluations. We introduce two projection-based multifidelity linear regression approaches that leverage principal component basis vectors for dimensionality reduction and combine multifidelity data through: (i) a direct data augmentation using low-fidelity data, and (ii) a data augmentation incorporating explicit linear corrections between low-fidelity and high-fidelity data. The data augmentation approaches combine high-fidelity and low-fidelity data into a unified training set and train the linear regression model through weighted least squares with fidelity-specific weights. Various weighting schemes and their impact on regression accuracy are explored. The proposed multifidelity linear regression methods are demonstrated on approximating the surface pressure field of a hypersonic vehicle in flight. In a low-data regime of no more than ten high-fidelity samples, multifidelity linear regression achieves approximately 3% - 12% improvement in median accuracy compared to single-fidelity methods with comparable computational cost.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Portable Multi-GPU Solver for Collisional Plasmas with Coulombic Interactions</title>
<link>https://arxiv.org/abs/2508.06771</link>
<guid>https://arxiv.org/abs/2508.06771</guid>
<content:encoded><![CDATA[
<div> Keywords: particle-in-cell methods, low-temperature plasmas, GPU acceleration, collisions, Python-based HPC tools

Summary: 
Particle-in-cell (PIC) methods for low-temperature plasmas (LTPs) are studied in this work, focusing on GPU acceleration of algorithms for velocity-space interactions, particularly collisions involving electrons with neutrals, ions, and electrons. The research explores both algorithmic analysis and the feasibility of implementing algorithms using Python-based HPC tools, specifically PyKokkos. Common PIC kernels are discussed, and performance results for NVIDIA Volta V100 and AMD MI250X GPUs are presented, with the MI250X showing slightly faster performance overall but being more sensitive to register pressure. Scaling results for a distributed memory implementation on up to 16 MPI ranks are also reported. This study contributes valuable insights into optimizing PIC methods for LTP simulations and highlights the potential of GPU acceleration in enhancing computational efficiency for studying plasma physics phenomena. 

<br /><br />Summary: <div>
arXiv:2508.06771v1 Announce Type: new 
Abstract: We study parallel particle-in-cell (PIC) methods for low-temperature plasmas (LTPs), which discretize kinetic formulations that capture the time evolution of the probability density function of particles as a function of position and velocity. We use a kinetic description for electrons and a fluid approximation for heavy species. In this paper, we focus on GPU acceleration of algorithms for velocity-space interactions and in particular, collisions of electrons with neutrals, ions, and electrons. Our work has two thrusts. The first is algorithmic exploration and analysis. The second is examining the viability of rapid-prototyping implementations using Python-based HPC tools, in particular PyKokkos. We discuss several common PIC kernels and present performance results on NVIDIA Volta V100 and AMD MI250X GPUs. Overall, the MI250X is slightly faster for most kernels but shows more sensitivity to register pressure. We also report scaling results for a distributed memory implementation on up to 16 MPI ranks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of association rule mining to assess forest species distribution in Italy considering abiotic and biotic factors</title>
<link>https://arxiv.org/abs/2508.07076</link>
<guid>https://arxiv.org/abs/2508.07076</guid>
<content:encoded><![CDATA[
<div> Association Rule Mining, biodiversity monitoring, forest community composition, ecological rules, Picea abies<br />
<br />Summary: The study explores the relationships among co-occurring plant species and environmental conditions using Association Rule Mining in forest biodiversity monitoring. By analyzing data from 6,784 plots in Italy, the Frequent Pattern Growth algorithm identified ecological rules, such as the strong correlation between temperature seasonality and precipitation seasonality with Picea abies. This tree species showed a specific association with cold, seasonal environments, indicating its ecological specificity. Some plant species acted as community "hubs," suggesting ties to particular environmental or biotic conditions. The findings provide valuable insights for future research in similar environmental settings, emphasizing the importance of accessible ecological data in understanding forest dynamics and biodiversity conservation.<br /> <div>
arXiv:2508.07076v1 Announce Type: new 
Abstract: Biodiversity monitoring represents a pressing global priority, and assessing forest community composition plays a crucial role due to its influence on ecosystem functions. The spatial distribution of forest species becomes essential for understanding biodiversity dynamics, territorial planning, aiding nature conservation and enhancing ecosystem resilience amid global change. Association Rule Mining, commonly applied to other scientific contexts, is now innovatively adopted in the ecological field to explore the relationships among co-occurring plant species and extract hidden interpretable patterns, also with abiotic and biotic conditions. Multiple heterogeneous data sources were integrated through data preprocessing into a unique dataset, including georeferenced information about 151 plant species monitored within 6,784 plots across Italy and several bioclimatic indices, soil-related factors, and variables from earth observations. The Frequent Pattern Growth algorithm, used for association rule mining, provided interesting and encouraging findings, suggesting ecological rules among plant species and environmental conditions. Indeed, temperature seasonality between 650-700 and precipitation seasonality between 45-50 resulted very correlated with Picea abies (confidence = 90.9%, lift = 7.13). Patterns detected for Picea abies highlighted its ecological specificity, indicating a strong association with cold, highly seasonal environments, and particular plant communities. Some species appeared acting as community "hubs", frequently co-occurring with other species, suggesting ties to specific environmental or biotic conditions. These findings represent a valuable resource for future research, especially in regions with similar environmental settings and when prior ecological knowledge exists, also underlining the importance of publicly accessible, high-quality ecological data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harmonic balance-automatic differentiation method: an out-of-the-box and efficient solver for general nonlinear dynamics simulation</title>
<link>https://arxiv.org/abs/2508.07309</link>
<guid>https://arxiv.org/abs/2508.07309</guid>
<content:encoded><![CDATA[
<div> method, Harmonic Balance-Alternating Frequency-Time domain, high-dimensional complex systems, Automatic Differentiation, computational efficiency

Summary:
The paper introduces the Harmonic Balance-Automatic Differentiation (HB-AD) method, which aims to enhance the dynamic response analysis of nonlinear systems, particularly in high-dimensional complex systems. HB-AD integrates Automatic Differentiation (AD) with the harmonic balance framework to eliminate manual derivations of Jacobian matrices, making it more efficient and accurate. The implementation of HB-AD leverages deep learning frameworks for parallel computing and CUDA acceleration, combined with arc-length continuation for high efficiency. Computational experiments on rotor systems demonstrate HB-AD's capability in handling complex nonlinear expressions with automated Jacobian calculations. Compared to traditional methods, HB-AD achieves significantly higher computational efficiency, making it a valuable tool for the dynamic characterization of high-dimensional engineering systems. <div>
arXiv:2508.07309v1 Announce Type: new 
Abstract: The Harmonic Balance-Alternating Frequency-Time domain (HB-AFT) method is extensively employed for dynamic response analysis of nonlinear systems. However, its application to high-dimensional complex systems is constrained by the manual derivation of Jacobian matrices during Newton-Raphson iterations, which become computationally intractable or error-prone for intricate nonlinearities. The Harmonic Balance-Automatic Differentiation (HB-AD) method is proposed to address this limitation, in which AD is integrated with the harmonic balance framework. This approach eliminates all manual derivations by leveraging AD to compute exact Jacobians numerically, enabling generic and efficient analysis of high-dimensional complex nonlinear systems. The implementation utilizes advanced deep learning frameworks for native parallel computing and CUDA acceleration, and combines AD with arc-length continuation, establishing an out-of-the-box and high efficiency computational architecture. Users need only supply the system's dynamic equations, HB-AD then autonomously trace the complete panorama of periodic responses -- including stable/unstable solution branches. Computational experiments on a rotor system with squeeze-film damper (SFD) demonstrate HB-AD's capability in handling complex nonlinear expressions with automated Jacobian calculations. For a high-dimensional aero-engine rotor-bearing-casing system with complex bearing nonlinearities, HB-AD achieves 17-fold higher efficiency than traditional HB-AFT and 144-fold acceleration over the Newmark method. The HB-AD method is a synergistic merger of computational mechanics and machine learning primitives, delivers an easy to use, general-purpose, high efficiency platform for high-fidelity dynamic characterization of high-dimensional engineering systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cardiotensor: A Python Library for Orientation Analysis and Tractography in 3D Cardiac Imaging</title>
<link>https://arxiv.org/abs/2508.07476</link>
<guid>https://arxiv.org/abs/2508.07476</guid>
<content:encoded><![CDATA[
<div> Keywords: cardiotensor, 3D cardiomyocyte orientation, structure tensor analysis, high-performance computing, tractography

Summary:
cardiotensor is a new open-source Python package designed to quantify 3D cardiomyocyte orientation in high-resolution imaging datasets of the human heart. It utilizes structure tensor analysis to extract directional metrics such as helical angle, intrusion angle, and fractional anisotropy. The package supports large teravoxel-scale datasets and is optimized for high-performance computing environments. In addition to providing detailed structural mapping of cardiac tissue, cardiotensor also includes tractography functionality to reconstruct continuous cardiomyocyte trajectories, allowing for multi-scale myoaggregate visualization down to the myocyte level. These capabilities enable the assessment of anatomical continuity and regional organization in the heart, providing valuable insights into its microstructural architecture. 

<br /><br />Summary: <div>
arXiv:2508.07476v1 Announce Type: new 
Abstract: Understanding the architecture of the human heart requires analysis of its microstructural organization across scales. With the advent of high-resolution imaging techniques such as synchrotron-based tomography, it has become possible to visualize entire hearts at micron-scale resolution. However, translating these large, complex volumetric datasets into interpretable, quantitative descriptors of cardiac organization remains a major challenge. Here we present cardiotensor, an open-source Python package designed to quantify 3D cardiomyocyte orientation in whole- or partial-heart imaging datasets. It provides efficient, scalable implementations of structure tensor analysis, enabling extraction of directional metrics such as helical angle (HA), intrusion angle (IA), and fractional anisotropy (FA). The package supports datasets reaching teravoxel-scale and is optimized for high-performance computing environments, including parallel and chunk-based processing pipelines. In addition, cardiotensor includes tractography functionality to reconstruct continuous cardiomyocyte trajectories. This enables multi-scale myoaggregate visualization down to the myocyte level, depending on resolution. These capabilities enable detailed structural mapping of cardiac tissue, supporting the assessment of anatomical continuity and regional organization.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Material Fingerprinting: A shortcut to material model discovery without solving optimization problems</title>
<link>https://arxiv.org/abs/2508.07831</link>
<guid>https://arxiv.org/abs/2508.07831</guid>
<content:encoded><![CDATA[
<div> fingerprinting, mechanical material models, hyperelastic materials, pattern recognition algorithm, experimental setups  
Summary:  
Material Fingerprinting is introduced as a novel method for rapidly discovering mechanical material models without solving complex optimization problems. The method is based on the assumption that each material has a unique response under standardized experimental conditions, creating a "fingerprint" that encodes mechanical characteristics. By establishing a database of fingerprints and corresponding models, unseen materials can be quickly characterized by matching their fingerprints. The study demonstrates that Material Fingerprinting is effective for model discovery in experiments with homogeneous and heterogeneous deformation fields, avoiding the challenges of non-convex optimization. This approach is shown to be applicable across different experimental setups and material behaviors, providing a versatile framework for rapid material model identification. This innovation holds promise for future developments in material characterization.  
<br /><br />Summary: <div>
arXiv:2508.07831v1 Announce Type: new 
Abstract: We propose Material Fingerprinting, a new method for the rapid discovery of mechanical material models from direct or indirect data that avoids solving potentially non-convex optimization problems. The core assumption of Material Fingerprinting is that each material exhibits a unique response when subjected to a standardized experimental setup. We can interpret this response as the material's fingerprint, essentially a unique identifier that encodes all pertinent information about the material's mechanical characteristics. Consequently, once we have established a database containing fingerprints and their corresponding mechanical models during an offline phase, we can rapidly characterize an unseen material in an online phase. This is accomplished by measuring its fingerprint and employing a pattern recognition algorithm to identify the best matching fingerprint in the database. In our study, we explore this concept in the context of hyperelastic materials, demonstrating the applicability of Material Fingerprinting across different experimental setups. Initially, we examine Material Fingerprinting through experiments involving homogeneous deformation fields, which provide direct strain-stress data pairs. We then extend this concept to experiments involving complexly shaped specimens with heterogeneous deformation fields, which provide indirect displacement and reaction force measurements. We show that, in both cases, Material Fingerprinting is an efficient tool for model discovery, bypassing the challenges of potentially non-convex optimization. We believe that Material Fingerprinting provides a powerful and generalizable framework for rapid material model identification across a wide range of experimental designs and material behaviors, paving the way for numerous future developments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Engineering Student Perceptions of Introductory CS Courses in an Indian Context</title>
<link>https://arxiv.org/abs/2508.06563</link>
<guid>https://arxiv.org/abs/2508.06563</guid>
<content:encoded><![CDATA[
<div> Keywords: student perceptions, assessment practices, computer science education, inclusive learning, programming course<br />
Summary: 
- The study focuses on engineering students' perceptions of assessment practices in an introductory computer science course and its associated lab in an Indian engineering institute.
- A survey involving 318 first-year students revealed that lab assignments were seen as effective, while exams and projects were viewed as authentic and skill-enhancing.
- Instructors played a significant role in shaping course content, and teaching assistants were found to be approachable and helpful despite some inconsistencies.
- Variations in academic performance and assessment perceptions were noted based on factors like prior programming experience, technology familiarity, gender, and academic branch.
- The study challenges common assumptions in grade modeling as the performance data did not follow a Gaussian distribution. A comparative analysis with European cohorts highlighted universal patterns and contextual differences, offering insights for designing inclusive assessment strategies in programming education.<br /><br />Summary: <div>
arXiv:2508.06563v1 Announce Type: cross 
Abstract: Understanding student perceptions of assessment is vital for designing inclusive and effective learning environments, especially in technical education. This study explores engineering students' perceptions of assessment practices in an introductory computer science/ programming course, and its associated laboratory within an Indian engineering institute context. A total of 318 first-year Bachelor of Technology students participated in a weekly 25-statement Likert-scale survey conducted over nine weeks. Using descriptive statistics and non-parametric tests (Mann-Whitney U and Kruskal-Wallis), the analysis reveals that students largely perceive lab assignments as effective learning activities and view exams and projects as authentic and skill-enhancing. Students appreciated the role of instructors in shaping course content and found teaching assistants to be approachable and helpful, despite some inconsistencies. The study also finds significant variations in students' academic performance and assessment perceptions based on prior programming experience, technology familiarity, gender, and academic branch. Notably, the performance data did not follow a Gaussian distribution, challenging common assumptions in grade modeling. A comparative analysis with European cohorts highlights both universal patterns and contextual differences, offering valuable insights for designing inclusive and equitable assessment strategies in programming education.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovery Learning accelerates battery design evaluation</title>
<link>https://arxiv.org/abs/2508.06985</link>
<guid>https://arxiv.org/abs/2508.06985</guid>
<content:encoded><![CDATA[
<div> Discovery Learning, batteries, machine learning, lifetime prediction, rapid feedback<br />
<br />
Summary: <br />
The article introduces Discovery Learning (DL), a scientific machine-learning approach that combines active learning, physics-guided learning, and zero-shot learning to efficiently evaluate novel battery designs. DL leverages historical data to predict battery lifetime for untested material-design combinations without the need for additional data labeling or extensive prototyping. Testing DL on a set of large-format lithium-ion pouch cells demonstrates its effectiveness in predicting cycle life with a 7.2% test error, leading to significant time and energy savings compared to traditional industrial practices. This approach showcases the potential of leveraging past designs to accelerate the development of next-generation battery technologies, making data-driven modeling more efficient and aiding scientific discovery and engineering innovation. <div>
arXiv:2508.06985v1 Announce Type: cross 
Abstract: Fast and reliable validation of novel designs in complex physical systems such as batteries is critical to accelerating technological innovation. However, battery research and development remain bottlenecked by the prohibitively high time and energy costs required to evaluate numerous new design candidates, particularly in battery prototyping and life testing. Despite recent progress in data-driven battery lifetime prediction, existing methods require labeled data of target designs to improve accuracy and cannot make reliable predictions until after prototyping, thus falling far short of the efficiency needed to enable rapid feedback for battery design. Here, we introduce Discovery Learning (DL), a scientific machine-learning paradigm that integrates active learning, physics-guided learning, and zero-shot learning into a human-like reasoning loop, drawing inspiration from learning theories in educational psychology. DL can learn from historical battery designs and actively reduce the need for prototyping, thus enabling rapid lifetime evaluation for unobserved material-design combinations without requiring additional data labeling. To test DL, we present 123 industrial-grade large-format lithium-ion pouch cells, spanning eight material-design combinations and diverse cycling protocols. Trained solely on public datasets of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting the average cycle life under unknown device variability. This results in savings of 98% in time and 95% in energy compared to industrial practices. This work highlights the potential of uncovering insights from historical designs to inform and accelerate the development of next-generation battery technologies. DL represents a key advance toward efficient data-driven modeling and helps realize the promise of machine learning for accelerating scientific discovery and engineering innovation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Enhanced Time-Series Forecasting via Large Language Models</title>
<link>https://arxiv.org/abs/2508.07697</link>
<guid>https://arxiv.org/abs/2508.07697</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, large language models, semantic enhancement, anomalous characteristics, Transformer-based models

Summary: 
- The article proposes a Semantic-Enhanced Large Language Model (SE-LLM) that integrates the periodicity and anomalies of time series data into a semantic space to enhance token embedding.
- By enhancing token interpretability for LLMs, SE-LLM effectively bridges the gap between linguistic knowledge structures and time series data patterns, improving semantic representation.
- A plugin module embedded within self-attention is introduced to enable LLMs to capture both long-term dependencies and short-term anomalies in time series data, enhancing their adaptability to temporal sequence analysis.
- The approach freezes the LLM model and reduces the dimensionality of token sequences, leading to significant computational efficiency gains.
- Experimental results demonstrate that SE-LLM outperforms existing state-of-the-art methods for time series forecasting. 

<br /><br />Summary: <div>
arXiv:2508.07697v1 Announce Type: cross 
Abstract: Time series forecasting plays a significant role in finance, energy, meteorology, and IoT applications. Recent studies have leveraged the generalization capabilities of large language models (LLMs) to adapt to time series forecasting, achieving promising performance. However, existing studies focus on token-level modal alignment, instead of bridging the intrinsic modality gap between linguistic knowledge structures and time series data patterns, greatly limiting the semantic representation. To address this issue, we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent periodicity and anomalous characteristics of time series to embed into the semantic space to enhance the token embedding. This process enhances the interpretability of tokens for LLMs, thereby activating the potential of LLMs for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel at capturing long-range dependencies but are weak at modeling short-term anomalies in time-series data. Hence, we propose a plugin module embedded within self-attention that models long-term and short-term dependencies to effectively adapt LLMs to time-series analysis. Our approach freezes the LLM and reduces the sequence dimensionality of tokens, greatly reducing computational consumption. Experiments demonstrate the superiority performance of our SE-LLM against the state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lagrangian method for solving the spherical shallow water equations using power diagrams</title>
<link>https://arxiv.org/abs/2508.08129</link>
<guid>https://arxiv.org/abs/2508.08129</guid>
<content:encoded><![CDATA[
<div> Eulerian viewpoint, Lagrangian viewpoint, spherical power cells, optimal transport problem, momentum conservation <br />
Summary: 
The article presents a new Lagrangian method for simulating the atmosphere using spherical power cells. Mass conservation is ensured through solving an optimal transport problem, while a semi-implicit time stepping procedure is used for time advancement. Artificial viscosity is not required for stabilization. The efficiency of computing spherical Voronoi diagrams is demonstrated, with calculations of 100 million sites completed in under 2 minutes. The new method is evaluated on benchmark tests, showing comparable momentum and energy conservation to the latest Lagrangian approach for the spherical shallow water equations. The study suggests that this Lagrangian approach can offer a competitive alternative to Eulerian simulations for global atmospheric simulations. <br /> <div>
arXiv:2508.08129v1 Announce Type: cross 
Abstract: Numerical simulations of the air in the atmosphere and water in the oceans are essential for numerical weather prediction. The state-of-the-art for performing these fluid simulations relies on an Eulerian viewpoint, in which the fluid domain is discretized into a mesh, and the governing equations describe the fluid motion as it passes through each cell of the mesh. However, it is unclear whether a Lagrangian viewpoint, in which the fluid is discretized by a collection of particles, can outperform Eulerian simulations in global atmospheric simulations. To date, Lagrangian approaches have shown promise, but tend to produce smoother solutions. In this work, a new Lagrangian method is developed to simulate the atmosphere in which particles are represented with spherical power cells. We introduce an efficient algorithm for computing these cells which are then used to discretize the spherical shallow water equations. Mass conservation is enforced by solving a semi-discrete optimal transport problem and a semi-implicit time stepping procedure is used to advance the solution in time. We note that, in contrast to previous work, artificial viscosity is not needed to stabilize the simulation. The performance of the spherical Voronoi diagram calculation is first assessed, which shows that spherical Voronoi diagrams of 100 million sites can be computed in under 2 minutes on a single machine. The new simulation method is then evaluated on standard benchmark test cases, which shows that momentum and energy conservation of this new method is comparable to the latest Lagrangian approach for simulating the spherical shallow water equations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Organizations, teams, and job mobility: A social microdynamics approach</title>
<link>https://arxiv.org/abs/2503.24117</link>
<guid>https://arxiv.org/abs/2503.24117</guid>
<content:encoded><![CDATA[
<div> social connections, worker mobility, organizational teams, job reunions, job change

Summary:
Workers in large organizations are influenced by preferring to reunite with past coworkers when changing jobs, with a significant percentage of job moves leading to worker reunions. The study introduces a new framework to describe organizations as composites of teams with specific tasks and social connections. The importance of worker reunions in determining job moves is highlighted, surpassing labor supply and demand considerations. Time spent together and team size are factors influencing the likelihood of reunions, indicating the role of familiarity and trust in job mobility. The study underscores the significance of teams structures and social ties in shaping internal job change within large organizations. <br /><br />Summary: <div>
arXiv:2503.24117v2 Announce Type: replace 
Abstract: Most of the modeling approaches used to understand organizational worker mobility are highly stylized, using idealizations such as structureless organizations, indistinguishable workers, and a lack of social bonding of the workers. In this article, aided by a decade of precise, temporally resolved data of a large civilian organization of the US Army in which employees can change jobs in a similar way to many private organizations, we introduce a new framework to describe organizations as composites of teams within which individuals perform specific tasks and where social connections develop. By tracking the personnel composition of organizational teams, we find that workers who change jobs are highly influenced by preferring to reunite with past coworkers. In this organization, 34% of all moves across temporally stable teams (and 32% of the totality of moves) lead to worker reunions, percentages that have not been reported and are well-above intuitive expectation. To assess the importance of worker reunions in determining job moves, we compare them to labor supply and demand with or without occupational specialization. The comparison shows that the most consistent information about job change is provided by reunions. We find that the greater the time workers spend together or the smaller the team they share both increase their likelihood to reunite, supporting the notion of increased familiarity and trust behind such reunions and the dominant role of social capital in the evolution of large organizations. Our study of this organization supports the idea that to correctly forecast job mobility inside large organizations, their teams structures and the social ties formed in those teams play a key role in shaping internal job change.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection</title>
<link>https://arxiv.org/abs/2403.06534</link>
<guid>https://arxiv.org/abs/2403.06534</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthetic Aperture Radar, object detection, dataset, SAR, Multi-Stage with Filter Augmentation<br />
<br />
Summary: 
This research introduces a new benchmark dataset, SARDet-100K, for large-scale Synthetic Aperture Radar (SAR) object detection. The dataset is a combination of 10 existing SAR detection datasets and is the first of its kind with multi-class objects at a COCO-level scale. The study identifies a challenge in SAR object detection related to the disparities between pretraining on RGB datasets and finetuning on SAR datasets. In response, a novel pretraining framework called Multi-Stage with Filter Augmentation (MSFA) is proposed to address these gaps in data domain and model structure. The MSFA method significantly improves the performance of SAR object detection models and demonstrates versatility across various models. By providing the SARDet-100K dataset and open-source code, this work aims to advance research in SAR object detection. <div>
arXiv:2403.06534v3 Announce Type: replace-cross 
Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at https://github.com/zcablii/SARDet_100K.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Sioux Falls Network: Insights from Path-Driven Higher-Order Network Analysis</title>
<link>https://arxiv.org/abs/2508.06234</link>
<guid>https://arxiv.org/abs/2508.06234</guid>
<content:encoded><![CDATA[
<div> Keywords: Benchmark scenarios, Higher-order network models, Mobility behavior, Structural complexity, Path diversity

Summary: 
The study introduces a mathematical framework based on higher-order network models to evaluate benchmark scenarios in transportation research, focusing on the Sioux Falls scenario. The framework aims to quantify the representativeness of benchmark networks by assessing structural and functional patterns. Results show that the classical Sioux Falls network has limited path diversity, rapid structural fragmentation, and weak alignment with empirical routing behavior. Higher-order network models are proposed as a way to bridge the gap between simulation-based and real-world mobility analysis, providing more accurate and generalizable insights in transportation research. This study highlights the importance of considering memory-aware network representations to improve the fidelity of benchmark scenarios in evaluating routing algorithms, infrastructure interventions, and new technologies in transportation research. 

Summary:<br /><br />Keywords: Benchmark scenarios, Higher-order network models, Mobility behavior, Structural complexity, Path diversity<br /><br />The study examines the representativeness of benchmark networks in transportation research using a mathematical framework based on higher-order network models. The analysis focuses on the widely used Sioux Falls scenario, revealing limited path diversity, rapid structural fragmentation, and weak alignment with empirical routing behavior in the classical Sioux Falls network. The study suggests that higher-order network models can enhance the accuracy and generalizability of simulation results, bridging the gap between simulation-based and real-world mobility analysis. By considering memory-aware network representations, researchers can improve the fidelity of benchmark scenarios and gain more meaningful insights in evaluating routing algorithms, infrastructure interventions, and new technologies in transportation research. <div>
arXiv:2508.06234v1 Announce Type: new 
Abstract: Benchmark scenarios are widely used in transportation research to evaluate routing algorithms, simulate infrastructure interventions, and test new technologies under controlled conditions. However, the structural and behavioral fidelity of these benchmarks remains largely unquantified, raising concerns about the external validity of simulation results. In this study, we introduce a mathematical framework based on higher-order network models to evaluate the representativeness of benchmark networks, focusing on the widely used Sioux Falls scenario. Higher-order network models encode empirical and simulated trajectory data into memory-aware network representations, which we use to quantify sequential dependencies in mobility behavior and assess how well benchmark networks capture real-world structural and functional patterns. Applying this framework to the Sioux Falls network, as well as real-world trajectory data, we quantify structural complexity, optimal memory length, link prediction accuracy, and centrality alignment. Our results show and statistically quantify that the classical Sioux Falls network exhibits limited path diversity, rapid structural fragmentation at higher orders, and weak alignment with empirical routing behavior. These results illustrate the potential of higher-order network models to bridge the gap between simulation-based and real-world mobility analysis, providing a robust foundation for more accurate and generalizable insights in transportation research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading</title>
<link>https://arxiv.org/abs/2508.06312</link>
<guid>https://arxiv.org/abs/2508.06312</guid>
<content:encoded><![CDATA[
<div> factor mining, quantitative trading, Large Language Models, automated, alpha discovery

Summary: 
Chain-of-Alpha introduces a novel framework for automated alpha mining in quantitative trading using Large Language Models (LLMs). The method utilizes a dual-chain architecture comprising a Factor Generation Chain and a Factor Optimization Chain to iteratively generate, evaluate, and refine alpha factors without human intervention. By leveraging market data, backtest feedback, and prior optimization knowledge, Chain-of-Alpha offers a high degree of automation, generality, and efficiency in alpha discovery. The framework outperforms existing baselines in real-world A-share benchmarks, highlighting its scalability and promising potential for LLM-driven quantitative research. <div>
arXiv:2508.06312v1 Announce Type: new 
Abstract: Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Farm Economics and Landscape Ecology for Global Sustainability through Hierarchical and Bayesian Optimization</title>
<link>https://arxiv.org/abs/2508.06386</link>
<guid>https://arxiv.org/abs/2508.06386</guid>
<content:encoded><![CDATA[
<div> Optimization, Agricultural landscapes, Biodiversity, Connectivity, Agri-environmental policies
Summary: 
The article introduces a novel hierarchical optimization framework addressing the challenge of sustaining food production while reversing biodiversity loss in agricultural landscapes. The framework consists of an Ecological Intensification (EI) model determining optimal allocation of land to margin and habitat interventions at the farm level, an Ecological Connectivity (EC) model arranging interventions across the landscape to enhance connectivity while maintaining profitability, and a Bayesian Optimization (BO) approach translating spatial outcomes into policy instruments. By applying this framework to a Canadian agricultural landscape, the study demonstrates improved connectivity under economic constraints. The approach aligns farm incentives with biodiversity goals, offering an effective tool for developing economically viable and ecologically sound agri-environmental policies. 
<br /><br />Summary: <div>
arXiv:2508.06386v1 Announce Type: new 
Abstract: Agricultural landscapes face the dual challenge of sustaining food production while reversing biodiversity loss. Agri-environmental policies often fall short of delivering ecological functions such as landscape connectivity, in part due to a persistent disconnect between farm-level economic decisions and landscape-scale spatial planning. We introduce a novel hierarchical optimization framework that bridges this gap. First, an Ecological Intensification (EI) model determines the economically optimal allocation of land to margin and habitat interventions at the individual farm level. These farm-specific intervention levels are then passed to an Ecological Connectivity (EC) model, which spatially arranges them across the landscape to maximize connectivity while preserving farm-level profitability. Finally, we introduce a Bayesian Optimization (BO) approach that translates these spatial outcomes into simple, cost effective, and scalable policy instruments, such as subsidies and eco-premiums, using non-spatial, farm-level policy parameters. Applying the framework to a Canadian agricultural landscape, we demonstrate how it enhances connectivity under real-world economic constraints. Our approach provides a globally relevant tool for aligning farm incentives with biodiversity goals, advancing the development of agri-environmental policies that are economically viable and ecologically effective.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Collocation Point Strategies For Physics Informed Neural Networks via the QR Discrete Empirical Interpolation Method</title>
<link>https://arxiv.org/abs/2501.07700</link>
<guid>https://arxiv.org/abs/2501.07700</guid>
<content:encoded><![CDATA[
<div> adaptive collocation point selection, physics-informed neural networks, partial differential equations, QR Discrete Empirical Interpolation Method, adaptive mesh refinement <br />
<br />
Summary: 
This article explores the impact of collocation point sampling on the performance of Physics-Informed Neural Networks (PINNs) for solving problems related to partial differential equations (PDEs). Traditional fixed sampling methods can be limited in capturing high-gradient regions, impacting the accuracy of PINNs for complex PDEs. The proposed adaptive collocation point selection strategies leverage the QR Discrete Empirical Interpolation Method (QR-DEIM) to dynamically update collocation points during training. Results on benchmark PDEs showcase that the QR-DEIM-based approaches enhance PINN accuracy compared to existing methods. This research opens up a promising pathway for improving the efficiency and effectiveness of adaptive collocation point strategies in the context of PINNs. <br /> <div>
arXiv:2501.07700v4 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) have gained significant attention for solving forward and inverse problems related to partial differential equations (PDEs). While advancements in loss functions and network architectures have improved PINN accuracy, the impact of collocation point sampling on their performance remains underexplored. Fixed sampling methods, such as uniform random sampling and equispaced grids, can fail to capture critical regions with high solution gradients, limiting their effectiveness for complex PDEs. Adaptive methods, inspired by adaptive mesh refinement from traditional numerical methods, address this by dynamically updating collocation points during training but may overlook residual dynamics between updates, potentially losing valuable information. To overcome this limitation, we propose two adaptive collocation point selection strategies utilizing the QR Discrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling technique for efficiently approximating nonlinear functions. Our results on benchmark PDEs demonstrate that our QR-DEIM-based approaches improve PINN accuracy compared to existing methods, offering a promising direction for adaptive collocation point strategies.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriceFM: Foundation Model for Probabilistic Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2508.04875</link>
<guid>https://arxiv.org/abs/2508.04875</guid>
<content:encoded><![CDATA[
<div> spatiotemporal forecasting, electricity price, Europe, deep learning, graph-based

Summary:
The paper introduces PriceFM, a spatiotemporal foundation model for electricity price forecasting in Europe that leverages graph-based inductive biases to capture spatial interdependencies across interconnected power markets. The model utilizes a comprehensive dataset spanning 24 European countries and 38 regions over a three-year period. PriceFM is designed for multi-region, multi-timestep, and multi-quantile probabilistic forecasting, outperforming competitive baselines in extensive experiments. The study highlights the importance of incorporating spatial context in electricity market forecasting. The dataset and code are available on GitHub for further research and development. <br /><br />Summary: <div>
arXiv:2508.04875v1 Announce Type: new 
Abstract: Electricity price forecasting in Europe presents unique challenges due to the continent's increasingly integrated and physically interconnected power market. While recent advances in deep learning and foundation models have led to substantial improvements in general time series forecasting, most existing approaches fail to capture the complex spatial interdependencies and uncertainty inherent in electricity markets. In this paper, we address these limitations by introducing a comprehensive and up-to-date dataset across 24 European countries (38 regions), spanning from 2022-01-01 to 2025-01-01. Building on this groundwork, we propose PriceFM, a spatiotemporal foundation model that integrates graph-based inductive biases to capture spatial interdependencies across interconnected electricity markets. The model is designed for multi-region, multi-timestep, and multi-quantile probabilistic electricity price forecasting. Extensive experiments and ablation studies confirm the model's effectiveness, consistently outperforming competitive baselines and highlighting the importance of spatial context in electricity markets. The dataset and code can be found at https://github.com/runyao-yu/PriceFM.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment-Aware Stock Price Prediction with Transformer and LLM-Generated Formulaic Alpha</title>
<link>https://arxiv.org/abs/2508.04975</link>
<guid>https://arxiv.org/abs/2508.04975</guid>
<content:encoded><![CDATA[
<div> Keywords: alpha decay, large language models, stock price prediction, financial data, interpretability

Summary:
Automating the generation of alpha decay strategies in trading and quantitative analysis is now possible through the integration of large language models (LLMs) with Transformer models. This novel framework utilizes structured inputs like historical stock features, technical indicators, and sentiment scores to generate diverse and adaptive alphas. These formulaic alphas serve as high-level features capturing complex dependencies within financial data. The alphas, not directly used for trading, are inputted into prediction models like Transformer, LSTM, TCN, SVR, and Random Forest to forecast future stock prices. Experimental results show that LLM-generated alphas significantly enhance predictive accuracy. Additionally, the natural language reasoning provided by the LLM enhances interpretability and transparency in financial decision-making. <br /><br />Summary: <div>
arXiv:2508.04975v1 Announce Type: new 
Abstract: Traditionally, traders and quantitative analysts address alpha decay by manually crafting formulaic alphas, mathematical expressions that identify patterns or signals in financial data, through domain expertise and trial-and-error. This process is often time-consuming and difficult to scale. With recent advances in large language models (LLMs), it is now possible to automate the generation of such alphas by leveraging the reasoning capabilities of LLMs. This paper introduces a novel framework that integrates a prompt-based LLM with a Transformer model for stock price prediction. The LLM first generates diverse and adaptive alphas using structured inputs such as historical stock features (Close, Open, High, Low, Volume), technical indicators, sentiment scores of both target and related companies. These alphas, instead of being used directly for trading, are treated as high-level features that capture complex dependencies within the financial data. To evaluate the effectiveness of these LLM-generated formulaic alphas, the alpha features are then fed into prediction models such as Transformer, LSTM, TCN, SVR, and Random Forest to forecast future stock prices. Experimental results demonstrate that the LLM-generated alphas significantly improve predictive accuracy. Moreover, the accompanying natural language reasoning provided by the LLM enhances the interpretability and transparency of the predictions, supporting more informed financial decision-making.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy Decisions on Fluid Instabilities: Autoencoder-Based Reconstruction meets Rule-Based Anomaly Classification</title>
<link>https://arxiv.org/abs/2508.05418</link>
<guid>https://arxiv.org/abs/2508.05418</guid>
<content:encoded><![CDATA[
<div> Keywords: shockwave classification, shadowgraph imaging, hybrid framework, unsupervised autoencoder, fuzzy inference system

Summary: 
This study introduces a novel approach for shockwave classification in shadowgraph imaging, addressing the challenges posed by limited labeled data and complex flow structures. The hybrid framework combines unsupervised autoencoder models with a fuzzy inference system to generate and interpret anomaly maps. Among the methods evaluated, the hybrid $\beta$-VAE autoencoder with a fuzzy rule-based system proves to be the most effective in capturing coherent shock features and integrating spatial context for enhanced anomaly classification. This approach allows for interpretable, unsupervised classification of flow disruptions, paving the way for real-time, physics-informed diagnostics in experimental and industrial fluid applications. The proposed methodology holds promise for improving understanding and analysis of shockwave phenomena in various fluid dynamics scenarios. 

<br /><br />Summary: <div>
arXiv:2508.05418v1 Announce Type: new 
Abstract: Shockwave classification in shadowgraph imaging is challenging due to limited labeled data and complex flow structures. This study presents a hybrid framework that combines unsupervised autoencoder models with a fuzzy inference system to generate and interpret anomaly maps. Among the evaluated methods, the hybrid $\beta$-VAE autoencoder with a fuzzy rule-based system most effectively captured coherent shock features, integrating spatial context to enhance anomaly classification. The resulting approach enables interpretable, unsupervised classification of flow disruptions and lays the groundwork for real-time, physics-informed diagnostics in experimental and industrial fluid applications.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorising SME Bank Transactions with Machine Learning and Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2508.05425</link>
<guid>https://arxiv.org/abs/2508.05425</guid>
<content:encoded><![CDATA[
<div> Financial, Small and Medium Enterprises, Cash flow lending, Synthetic data generation, Classification model<br />
Summary:<br />
This article explores the challenges faced by Small and Medium Enterprises (SMEs) in securing traditional financing due to information asymmetries. Cash flow lending is proposed as an alternative, but its effectiveness relies on accurate modeling of transaction-level data. The main obstacle in analyzing SME transactions is the unstructured nature of textual descriptions, characterized by abbreviations and imbalanced label distributions. To address these challenges, the authors propose a bank categorization pipeline leveraging synthetic data generation to enrich transaction datasets. Their approach consists of a synthetic data generation module, a fine-tuned classification model, and a calibration methodology. Experimental results show the model achieves high accuracy and robust generalization across different SMEs and transaction types, making it suitable for practical deployment in cash-flow lending applications. This framework offers a practical solution to data challenges in SME lending contexts, addressing scarcity, noise, and imbalance. <br /> <div>
arXiv:2508.05425v1 Announce Type: new 
Abstract: Despite their significant economic contributions, Small and Medium Enterprises (SMEs) face persistent barriers to securing traditional financing due to information asymmetries. Cash flow lending has emerged as a promising alternative, but its effectiveness depends on accurate modelling of transaction-level data. The main challenge in SME transaction analysis lies in the unstructured nature of textual descriptions, characterised by extreme abbreviations, limited context, and imbalanced label distributions. While consumer transaction descriptions often show significant commonalities across individuals, SME transaction descriptions are typically nonstandard and inconsistent across businesses and industries. To address some of these challenges, we propose a bank categorisation pipeline that leverages synthetic data generation to augment existing transaction data sets. Our approach comprises three core components: (1) a synthetic data generation module that replicates transaction properties while preserving context and semantic meaning; (2) a fine-tuned classification model trained on this enriched dataset; and (3) a calibration methodology that aligns model outputs with real-world label distributions. Experimental results demonstrate that our approach achieves 73.49% (+-5.09) standard accuracy on held-out data, with high-confidence predictions reaching 90.36% (+-6.52) accuracy. The model exhibits robust generalisation across different types of SMEs and transactions, which makes it suitable for practical deployment in cash-flow lending applications. By addressing core data challenges, namely, scarcity, noise, and imbalance, our framework provides a practical solution to build robust classification systems in data-sparse SME lending contexts.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deconstructing the Crystal Ball: From Ad-Hoc Prediction to Principled Startup Evaluation with the SAISE Framework</title>
<link>https://arxiv.org/abs/2508.05491</link>
<guid>https://arxiv.org/abs/2508.05491</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, startup evaluation, systematic literature review, prediction models, SAISE Framework

Summary: 
The article discusses the integration of Artificial Intelligence (AI) into startup evaluation and highlights the fragmented nature of existing academic research in this field. It points out the inconsistencies in definitions of success, lack of theoretical foundations, and insufficient validation methods in current predictive models. The study includes a systematic literature review of 57 empirical studies to understand the features, algorithms, data sources, and evaluation practices used in AI-driven startup prediction. The research identifies key weaknesses in the field, such as fragmented definition of success, lack of theory-driven feature engineering, inadequate model validation, and limited focus on data ethics and explainability. In response to these findings, the authors propose the Systematic AI-driven Startup Evaluation (SAISE) Framework, a five-stage roadmap aimed at guiding researchers towards a more principled and rigorous evaluation methodology. By emphasizing problem definition, data synthesis, feature engineering, validation, and interpretation, the SAISE framework aims to enhance the comparability, robustness, and practical relevance of research in this rapidly evolving domain.

Summary: <br /><br /> <div>
arXiv:2508.05491v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) into startup evaluation represents a significant technological shift, yet the academic research underpinning this transition remains methodologically fragmented. Existing studies often employ ad-hoc approaches, leading to a body of work with inconsistent definitions of success, atheoretical features, and a lack of rigorous validation. This fragmentation severely limits the comparability, reliability, and practical utility of current predictive models.
  To address this critical gap, this paper presents a comprehensive systematic literature review of 57 empirical studies. We deconstruct the current state-of-the-art by systematically mapping the features, algorithms, data sources, and evaluation practices that define the AI-driven startup prediction landscape. Our synthesis reveals a field defined by a central paradox: a strong convergence on a common toolkit -- venture databases and tree-based ensembles -- but a stark divergence in methodological rigor. We identify four foundational weaknesses: a fragmented definition of "success," a divide between theory-informed and data-driven feature engineering, a chasm between common and best-practice model validation, and a nascent approach to data ethics and explainability.
  In response to these findings, our primary contribution is the proposal of the Systematic AI-driven Startup Evaluation (SAISE) Framework. This novel, five-stage prescriptive roadmap is designed to guide researchers from ad-hoc prediction toward principled evaluation. By mandating a coherent, end-to-end methodology that emphasizes stage-aware problem definition, theory-informed data synthesis, principled feature engineering, rigorous validation, and risk-aware interpretation, the SAISE framework provides a new standard for conducting more comparable, robust, and practically relevant research in this rapidly maturing domain
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Space Diffusion for Topology Optimization</title>
<link>https://arxiv.org/abs/2508.05624</link>
<guid>https://arxiv.org/abs/2508.05624</guid>
<content:encoded><![CDATA[
<div> latent diffusion models, variational autoencoders, topology optimization, material distribution, generative process

Summary:<br />
This study introduces a novel framework that combines latent diffusion models (LDMs) with variational autoencoders (VAEs) for efficient topology optimization. The method allows for fast generation of optimized structures by conditioning the generative process on physically meaningful fields such as von Mises stress, strain energy density, volume fraction, and loading information. To improve design quality, auxiliary loss functions are introduced to penalize floating material, load imbalance, and volume fraction deviation, promoting realistic and manufacturable designs. Numerical experiments on a synthetic dataset demonstrate the framework's superior performance in compliance accuracy, volume control, and structural connectivity compared to existing diffusion-based methods. This approach offers a scalable alternative to traditional gradient-based methods, addressing issues of scalability and dimensionality in topology optimization. <br /><br /> <div>
arXiv:2508.05624v1 Announce Type: new 
Abstract: Topology optimization enables the automated design of efficient structures by optimally distributing material within a defined domain. However, traditional gradient-based methods often scale poorly with increasing resolution and dimensionality due to the need for repeated finite element analyses and sensitivity evaluations. In this work, we propose a novel framework that combines latent diffusion models (LDMs) with variational autoencoders (VAEs) to enable fast, conditional generation of optimized topologies. Unlike prior approaches, our method conditions the generative process on physically meaningful fields, specifically von Mises stress, strain energy density, volume fraction, and loading information, embedded as dense input channels. To further guide the generation process, we introduce auxiliary loss functions that penalize floating material, load imbalance, and volume fraction deviation, thereby encouraging physically realistic and manufacturable designs. Numerical experiments on a large synthetic dataset demonstrate that our VAE-LDM framework outperforms existing diffusion-based methods in compliance accuracy, volume control, and structural connectivity, providing a robust and scalable alternative to conventional
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle</title>
<link>https://arxiv.org/abs/2508.04755</link>
<guid>https://arxiv.org/abs/2508.04755</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, dynamic treatment regimes, large language models, implicit prior knowledge, clinical heuristics<br />
Summary: This study evaluates the use of large language models (LLMs) as dynamic insulin dosing agents in Type 1 diabetes treatment. LLMs demonstrate comparable performance to neural network-based agents when provided with carefully designed prompts. However, LLMs show limitations such as aggressive dosing due to reasoning errors like arithmetic hallucination and temporal misinterpretation. Explicit reasoning about latent states does not significantly improve performance. The study suggests cautious integration of LLMs into clinical workflows, emphasizing the need for targeted prompt engineering and validation. Hybrid approaches combining linguistic reasoning with structured physiological modeling may offer more effective decision-support systems.<br /><br />Summary: <div>
arXiv:2508.04755v1 Announce Type: cross 
Abstract: Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold promise for automating complex clinical decision-making, yet their practical deployment remains hindered by the intensive engineering required to inject clinical knowledge and ensure patient safety. Recent advancements in large language models (LLMs) suggest a complementary approach, where implicit prior knowledge and clinical heuristics are naturally embedded through linguistic prompts without requiring environment-specific training. In this study, we rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in silico Type 1 diabetes simulator, comparing their zero-shot inference performance against small neural network-based RL agents (SRAs) explicitly trained for the task. Our results indicate that carefully designed zero-shot prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or superior clinical performance relative to extensively trained SRAs, particularly in stable patient cohorts. However, LLMs exhibit notable limitations, such as overly aggressive insulin dosing when prompted with chain-of-thought (CoT) reasoning, highlighting critical failure modes including arithmetic hallucination, temporal misinterpretation, and inconsistent clinical logic. Incorporating explicit reasoning about latent clinical states (e.g., meals) yielded minimal performance gains, underscoring the current model's limitations in capturing complex, hidden physiological dynamics solely through textual inference. Our findings advocate for cautious yet optimistic integration of LLMs into clinical workflows, emphasising the necessity of targeted prompt engineering, careful validation, and potentially hybrid approaches that combine linguistic reasoning with structured physiological modelling to achieve safe, robust, and clinically effective decision-support systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations</title>
<link>https://arxiv.org/abs/2508.05020</link>
<guid>https://arxiv.org/abs/2508.05020</guid>
<content:encoded><![CDATA[
<div> AMR, Regent, Legion programming model, high-order solvers, compressible flows<br />
Summary:<br />
This study focuses on developing an adaptive mesh refinement (AMR) numerical solver using Regent, a high-level programming language designed for the Legion programming model. The implementation addresses challenges such as dynamic data structures for patch refinement/coarsening, mesh validity enforcement, and reducing task launch overhead through task fusion. Experimental results demonstrate significant speedups achieved with task fusion and automated GPU kernel generation using simple annotations. The approach is validated through simulations of two compressible flow problems governed by the Euler equations. Overall, the study showcases the effectiveness of using Regent and AMR for high-order solvers in scientific applications. <div>
arXiv:2508.05020v1 Announce Type: cross 
Abstract: High-order solvers for compressible flows are vital in scientific applications. Adaptive mesh refinement (AMR) is a key technique for reducing computational cost by concentrating resolution in regions of interest. In this work, we develop an AMR-based numerical solver using Regent, a high-level programming language for the Legion programming model. We address several challenges associated with implementing AMR in Regent. These include dynamic data structures for patch refinement/coarsening, mesh validity enforcement, and reducing task launch overhead via task fusion. Experimental results show that task fusion achieves 18x speedup, while automated GPU kernel generation via simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate our approach through simulations of two canonical compressible flow problems governed by the Euler equations.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo State Networks for Bitcoin Time Series Prediction</title>
<link>https://arxiv.org/abs/2508.05416</link>
<guid>https://arxiv.org/abs/2508.05416</guid>
<content:encoded><![CDATA[
<div> Keywords: stock prices, cryptocurrency prices, Echo State Networks, chaos analysis, machine learning methods

Summary:
Forecasting stock and cryptocurrency prices is a challenging task due to their high volatility and non-stationarity, influenced by various factors such as economic changes and market sentiment. This study investigates the use of Echo State Networks (ESNs) for predicting cryptocurrency prices, particularly during periods of extreme volatility. Results show that ESNs outperform other machine learning methods significantly, especially during chaotic periods, as reflected in the Lyapunov exponent analysis. The research demonstrates the robustness of ESNs during turbulent market conditions and their superior performance compared to Boosting and Nave methods. The findings suggest that ESNs are well-suited for capturing nonlinear patterns in dynamic data and can be effective tools for short-term forecasting of both stock and cryptocurrency prices. 

Summary: <div>
arXiv:2508.05416v1 Announce Type: cross 
Abstract: Forecasting stock and cryptocurrency prices is challenging due to high volatility and non-stationarity, influenced by factors like economic changes and market sentiment. Previous research shows that Echo State Networks (ESNs) can effectively model short-term stock market movements, capturing nonlinear patterns in dynamic data. To the best of our knowledge, this work is among the first to explore ESNs for cryptocurrency forecasting, especially during extreme volatility. We also conduct chaos analysis through the Lyapunov exponent in chaotic periods and show that our approach outperforms existing machine learning methods by a significant margin. Our findings are consistent with the Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods and excel under high chaos compared to Boosting and Na\"ive methods.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Variance Efficient Collaborative Filtering for Stock Recommendation</title>
<link>https://arxiv.org/abs/2306.06590</link>
<guid>https://arxiv.org/abs/2306.06590</guid>
<content:encoded><![CDATA[
<div> efficient collaborative filtering, stock recommendations, mean-variance, portfolio theory, personalized recommendations 
Summary:
The article introduces a novel mean-variance efficient collaborative filtering (MVECF) model for personalized stock recommendations that consider both user preferences and the risk-return characteristics of stocks. Traditional recommendation methods often overlook user preferences and focus solely on high-return stocks or diversified portfolios. The MVECF model aims to optimize the trade-off between risk and return by incorporating uncertainties in stock prices through regularization techniques. By enhancing the mean-variance efficiency of suggested portfolios, the MVECF model demonstrates improved performance while maintaining high average precision and recall. The model is designed for computational efficiency and can easily integrate with graph-based ranking models, making it a valuable tool for financial services in the era of FinTech. <br /><br />Summary: <div>
arXiv:2306.06590v2 Announce Type: replace-cross 
Abstract: The rise of FinTech has transformed financial services onto online platforms, yet stock investment recommender systems have received limited attention compared to other industries. Personalized stock recommendations can significantly impact customer engagement and satisfaction within the industry. However, traditional investment recommendations focus on high-return stocks or highly diversified portfolios based on the modern portfolio theory, often neglecting user preferences. On the other hand, collaborative filtering (CF) methods also may not be directly applicable to stock recommendations, because it is inappropriate to just recommend stocks that users like. The key is to optimally blend users preference with the portfolio theory. However, research on stock recommendations within the recommender system domain remains comparatively limited, and no existing model considers both the preference of users and the risk-return characteristics of stocks. In this regard, we propose a mean-variance efficient collaborative filtering (MVECF) model for stock recommendations that consider both aspects. Our model is specifically designed to improve the pareto optimality (mean-variance efficiency) in a trade-off between the risk (variance of return) and return (mean return) by systemically handling uncertainties in stock prices. Such improvements are incorporated into the MVECF model using regularization, and the model is restructured to fit into the ordinary matrix factorization scheme to boost computational efficiency. Experiments on real-world fund holdings data show that our model can increase the mean-variance efficiency of suggested portfolios while sacrificing just a small amount of mean average precision and recall. Finally, we further show MVECF is easily applicable to the state-of-the-art graph-based ranking models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tunable Plasmonic Absorption in Metal-Dielectric Multilayers via FDTD Simulations and an Explainable Machine Learning Approach</title>
<link>https://arxiv.org/abs/2508.04014</link>
<guid>https://arxiv.org/abs/2508.04014</guid>
<content:encoded><![CDATA[
<div> plasmonic devices, nanophotonics, machine learning, absorption behavior, multilayer systems
Summary:
- The study combines finite-difference time-domain simulations with machine learning to predict absorbed power behavior in multilayer plasmonic stacks.
- Varying Au and Ag thicknesses across a spectral range, spatial absorption maps and power metrics are generated.
- A multilayer perceptron and convolutional neural network models absorption behavior with high accuracy.
- Plasmonic layer thickness and excitation wavelength are identified as dominant contributors to absorption.
- Gold exhibits broader and sustained absorption compared to silver, with efficiency peaking between 450 and 850 nm.
<br /><br />Summary: <div>
arXiv:2508.04014v1 Announce Type: new 
Abstract: Plasmonic devices, fundamental to modern nanophotonics, exploit resonant interactions between light and free electrons in metals to achieve enhanced light trapping and electromagnetic field confinement. However, modeling their complex, nonlinear optical responses remains computationally intensive. In this work, we combine finite-difference time-domain simulations with machine learning to simulate and predict absorbed power behavior in multilayer plasmonic stacks composed of SiO2, gold, silver, and indium tin oxide. By varying Au and Ag thicknesses (10-50nm) across a spectral range of 300-1500nm, we generate spatial absorption maps and integrated power metrics from full-wave solutions to Maxwell's equations. A multilayer perceptron models global absorption behavior with a mean absolute error of 0.0953, while a convolutional neural network predicts spatial absorption distributions with an MAE of 0.0101. SHapley Additive exPlanations identify plasmonic layer thickness and excitation wavelength as dominant contributors to absorption, which peaks between 450 and 850~nm. Gold demonstrates broader and more sustained absorption compared to silver, although both metals exhibit reduced efficiency outside the resonance window. This integrated FDTD-ML framework offers a fast, explainable, and accurate approach for investigating tunable plasmonic behavior in multilayer systems, with applications in optical sensing, photovoltaics, and nanophotonic device design.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A GPU-Accelerated Three-Dimensional Crack Element Method for Transient Dynamic Fracture Simulation</title>
<link>https://arxiv.org/abs/2508.04076</link>
<guid>https://arxiv.org/abs/2508.04076</guid>
<content:encoded><![CDATA[
<div> Crack Element Method, dynamic crack propagation, quasi-brittle materials, element-splitting algorithm, fracture energy release rate<br />
Summary: <br />
This work introduces a novel three-dimensional Crack Element Method (CEM) for efficiently modeling transient dynamic crack propagation in quasi-brittle materials. The CEM features an advanced element-splitting algorithm that allows for element-wise crack growth and branching. A new formulation for calculating the fracture energy release rate in three dimensions is developed based on the evolving topology of split elements. The proposed 3D CEM is demonstrated to accurately simulate both single crack propagation and complex crack branching scenarios through a series of benchmark examples. Additionally, all three-dimensional simulations are GPU-accelerated, ensuring high levels of computational efficiency, consistency, and accuracy. <div>
arXiv:2508.04076v1 Announce Type: new 
Abstract: This work presents a novel three-dimensional Crack Element Method (CEM) designed to model transient dynamic crack propagation in quasi-brittle materials efficiently. CEM introduces an advanced element-splitting algorithm that enables element-wise crack growth, including crack branching. Based on the evolving topology of split elements, an original formulation for computing the fracture energy release rate in three dimensions is derived. A series of benchmark examples is conducted to demonstrate that the proposed 3D CEM accurately simulates both single crack propagation and complex crack branching scenarios. Furthermore, all three-dimensional simulations are GPU-accelerated, achieving high levels of computational efficiency, consistency, and accuracy.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional autoencoders for the reconstruction of three-dimensional interfacial multiphase flows</title>
<link>https://arxiv.org/abs/2508.04084</link>
<guid>https://arxiv.org/abs/2508.04084</guid>
<content:encoded><![CDATA[
<div> Keywords: autoencoders, reduced-order modeling, multiphase flows, convolutional architecture, interface representation

Summary:<br />
This study explores the use of autoencoders for reduced-order modeling of three-dimensional multiphase flows. The accuracy of reconstructing multiphase flow volume and mass fractions using a standard convolutional architecture is investigated, considering different interface representations such as diffuse, sharp, and level set. The research utilizes synthetic data with complex interface topologies and high-resolution simulation data of multiphase homogeneous isotropic turbulence for training and validation purposes. The findings establish best practices for reducing the dimensionality of multiphase flows with autoencoders, paving the way for separate training of accurate reconstruction and temporal or input/output models on the lower-dimensional latent space. This presents significant implications for the multiphase flow community and beyond, enabling advancements in modeling and understanding complex fluid dynamics efficiently. 

<br /><br />Summary: <div>
arXiv:2508.04084v1 Announce Type: new 
Abstract: In this work, we perform a comprehensive investigation of autoencoders for reduced-order modeling of three-dimensional multiphase flows. Focusing on the accuracy of reconstructing multiphase flow volume/mass fractions with a standard convolutional architecture, we examine the advantages and disadvantages of different interface representation choices (diffuse, sharp, level set). We use a combination of synthetic data with non-trivial interface topologies and high-resolution simulation data of multiphase homogeneous isotropic turbulence for training and validation. This study clarifies the best practices for reducing the dimensionality of multiphase flows via autoencoders. Consequently, this paves the path for uncoupling the training of autoencoders for accurate reconstruction and the training of temporal or input/output models such as neural operators (e.g., FNOs, DeepONets) and neural ODEs on the lower-dimensional latent space given by the autoencoders. As such, the implications of this study are significant and of interest to the multiphase flow community and beyond.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generic Framework for Optimization in Blockchain Simulators</title>
<link>https://arxiv.org/abs/2508.04157</link>
<guid>https://arxiv.org/abs/2508.04157</guid>
<content:encoded><![CDATA[
<div> Keywords: blockchain, simulation, optimization, warm starting technique, concurrent multiprocessing<br />
Summary: 
The paper introduces the Generic Framework for Optimization in Blockchain Simulators (GFOBS), a tool created to standardize and optimize blockchain simulations. GFOBS is designed to support various optimization algorithms, variables, and objectives, catering to a wide range of blockchain research needs. The key contributions of the paper include the development of GFOBS as a versatile tool, an innovative optimization method utilizing warm starting technique, and a novel concurrent multiprocessing technique for simultaneous simulation processes. These advancements aim to enhance the efficiency, replicability, and standardization of blockchain simulation experiments. The authors address the challenge of diverse and non-standardized simulation parameters that hinder the replicability and comparability of research methodologies in the rapidly evolving blockchain technology landscape. GFOBS provides a flexible platform for researchers to conduct blockchain simulations more effectively and efficiently. 

<br /><br />Summary: <div>
arXiv:2508.04157v1 Announce Type: new 
Abstract: As blockchain technology rapidly evolves, researchers face a significant challenge due to diverse and non-standardized simulation parameters, which hinder the replicability and comparability of research methodologies. This paper introduces a Generic Framework for Optimization in Blockchain Simulators (GFOBS), a comprehensive and adaptable solution designed to standardize and optimize blockchain simulations. GFOBS provides a flexible platform that supports various optimization algorithms, variables, and objectives, thereby catering to a wide range of blockchain research needs. The paper's key contributions are threefold: the development of GFOBS as a versatile tool for blockchain simulation optimization; the introduction of an innovative optimization method using warm starting technique; and the proposition of a novel concurrent multiprocessing technique for simultaneous simulation processes. These advancements collectively enhance the efficiency, replicability, and standardization of blockchain simulation experiments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Method-Based Reasoning for Large Language Models: Extraction, Reuse, and Continuous Improvement</title>
<link>https://arxiv.org/abs/2508.04289</link>
<guid>https://arxiv.org/abs/2508.04289</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, reasoning process, method-based model, continual learning, logical consistency

Summary: 
Large language models (LLMs) have shown impressive capabilities in various language tasks but are limited by their reliance on statistical patterns. To address this, a method-based model is proposed, augmenting LLMs with explicit procedures extracted from training data, responses, and user interactions. These methods are stored externally, ranked based on feedback, and retrieved to guide the LLM's response to new queries. The model enables continual learning, method reuse, and logical consistency beyond token prediction. Experimental results show improved factual verification and generalization in complex prompts. Furthermore, newly learned methods can surpass earlier ones through user-driven refinement. The approach shows promise in enhancing the reasoning abilities of LLMs by incorporating reusable procedures to handle novel problems and improve logical reasoning. 

<br /><br />Summary: <div>
arXiv:2508.04289v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive capabilities across a wide range of language tasks. However, their reasoning process is primarily guided by statistical patterns in training data, which limits their ability to handle novel problems and perform consistent logical reasoning. In this paper, we propose a method-based model that enhances LLMs with explicit, reusable procedures extracted from training content, generated responses, and user interactions. Each method is represented as a pair consisting of a problem and its corresponding solution, stored externally and ranked based on feedback. When a new query is received, the system retrieves and applies the most relevant methods to guide the LLM's response. Our model enables continual learning, method reuse, and logical consistency beyond next-token prediction. Experimental results demonstrate that the system improves factual verification and generalization in complex prompts, and that newly learned methods can outperform earlier ones through user-driven refinement.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extreme Event Precursor Prediction in Turbulent Dynamical Systems via CNN-Augmented Recurrence Analysis</title>
<link>https://arxiv.org/abs/2508.04301</link>
<guid>https://arxiv.org/abs/2508.04301</guid>
<content:encoded><![CDATA[
<div> framework, predict, precursors, extreme events, turbulent dynamical systems 

Summary:
- A general framework is presented to predict precursors to extreme events in turbulent dynamical systems.
- The approach combines phase-space reconstruction techniques with recurrence matrices and convolutional neural networks.
- Three testbed systems were evaluated: a triad turbulent interaction model, a stochastic anisotropic turbulent flow, and the Kolmogorov flow.
- The method offers a threshold-free classification strategy, efficient training with a small number of recurrence matrices, and generalizability to unseen systems.
- Results show robust predictive performance with detection rates of 96% for the triad model, 96% for the anisotropic turbulent flow, and 93% for the Kolmogorov flow, with varying mean lead times. 

<br /><br />Summary: <div>
arXiv:2508.04301v1 Announce Type: new 
Abstract: We present a general framework to predict precursors to extreme events in turbulent dynamical systems. The approach combines phase-space reconstruction techniques with recurrence matrices and convolutional neural networks to identify precursors to extreme events. We evaluate the framework across three distinct testbed systems: a triad turbulent interaction model, a prototype stochastic anisotropic turbulent flow, and the Kolmogorov flow. This method offers three key advantages: (1) a threshold-free classification strategy that eliminates subjective parameter tuning, (2) efficient training using only $\mathcal{O}(100)$ recurrence matrices, and (3) ability to generalize to unseen systems. The results demonstrate robust predictive performance across all test systems: 96\% detection rate for the triad model with a mean lead time of 1.8 time units, 96\% for the anisotropic turbulent flow with a mean lead time of 6.1 time units, and 93\% for the Kolmogorov flow with a mean lead time of 22.7 units.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation</title>
<link>https://arxiv.org/abs/2508.04306</link>
<guid>https://arxiv.org/abs/2508.04306</guid>
<content:encoded><![CDATA[
<div> Keywords: literature reviews, large language models, automated systems, Multi-Agent Taskforce Collaboration, benchmark dataset 

Summary: 
The article discusses the importance of literature reviews in scientific research and the role of large language models (LLMs) in automating the literature review process. The Multi-Agent Taskforce Collaboration (MATC) framework is proposed to address challenges of compounding errors in the review workflow. MATC consists of a manager agent and four executor agents for different tasks. Three collaboration paradigms are introduced to organize agents effectively and mitigate errors. Experimental results show that MATC outperforms existing benchmarks and a new dataset with diverse topics is introduced for literature review generation. The framework aims to improve the faithfulness and quality of automated literature reviews. <div>
arXiv:2508.04306v1 Announce Type: new 
Abstract: Literature reviews play an important role in scientific research. Recent advances in large language models (LLMs) have boosted the development of automated systems for the entire literature review workflow, from retrieval to manuscript drafting. However, a key challenge is that mistakes made in early stages can propagate and amplify in subsequent steps, leading to compounding errors that undermine the faithfulness of the final review. To tackle this issue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework, which consists of a manager agent and four executor agents for literature searching, outline generation, fact localization, and manuscript drafting. We propose three novel collaboration paradigms, forming exploration, exploitation, and experience taskforces, to effectively organize agents and mitigate compounding errors both between and within executor agents. Experimental results show that MATC achieves state-of-the-art performance on existing benchmarks. We further propose a new benchmark dataset featuring more diverse topics for faithful literature review generation.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing Large Language Models with PCA Without Performance Loss</title>
<link>https://arxiv.org/abs/2508.04307</link>
<guid>https://arxiv.org/abs/2508.04307</guid>
<content:encoded><![CDATA[
<div> Keywords: Principal Component Analysis, extreme compression, neural models, transformer, token sequences

Summary:<br />
- The study demonstrates that applying Principal Component Analysis (PCA) in a structured manner to polar-transformed images or token sequences allows for extreme compression of neural models while maintaining performance.
- A one-layer classifier trained on PCA-compressed polar MNIST achieves over 98 percent accuracy with only 840 parameters.
- A two-layer transformer utilizing 70-dimensional PCA-reduced MiniLM embeddings achieves 76.62 percent accuracy on the 20 Newsgroups dataset with just 81000 parameters.
- A decoder-only transformer generates coherent token sequences from 70-dimensional PCA embeddings, preserving over 97 percent cosine similarity with full MiniLM representations while using less than 17 percent of the parameter count of GPT-2.
- These findings underscore the effectiveness of PCA-based input compression as a strategy to align model capacity with information content, facilitating the development of lightweight architectures across various modalities.

<br /><br />Summary: <div>
arXiv:2508.04307v1 Announce Type: new 
Abstract: We demonstrate that Principal Component Analysis (PCA), when applied in a structured manner, either to polar-transformed images or segment-wise to token sequences, enables extreme compression of neural models without sacrificing performance. Across three case studies, we show that a one-layer classifier trained on PCA-compressed polar MNIST achieves over 98 percent accuracy using only 840 parameters. A two-layer transformer trained on 70-dimensional PCA-reduced MiniLM embeddings reaches 76.62 percent accuracy on the 20 Newsgroups dataset with just 81000 parameters. A decoder-only transformer generates coherent token sequences from 70-dimensional PCA embeddings while preserving over 97 percent cosine similarity with full MiniLM representations, using less than 17 percent of the parameter count of GPT-2. These results highlight PCA-based input compression as a general and effective strategy for aligning model capacity with information content, enabling lightweight architectures across multiple modalities.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Simulation and Experiment: A Self-Supervised Domain Adaptation Framework for Concrete Damage Classification</title>
<link>https://arxiv.org/abs/2508.04538</link>
<guid>https://arxiv.org/abs/2508.04538</guid>
<content:encoded><![CDATA[
<div> Keywords: concrete degradation, coda wave signals, domain adaptation, ultrasonic wave propagation simulations, neural networks

Summary: 
The study introduces a self-supervised domain adaptation framework for accurate concrete damage classification using coda wave signals. A virtual testing platform is developed for generating large-scale labeled synthetic data to reduce reliance on costly experimental labeling. The framework integrates domain adversarial training, minimum class confusion loss, and the BYOL strategy to bridge the domain gap between simulation and experimental data. Extensive experiments demonstrate notable performance improvements, with an accuracy of 0.7762 and a macro F1 score of 0.7713, outperforming baseline methods and domain adaptation techniques. The framework exhibits high robustness and minimal additional computational cost, showcasing its practical potential for structural health monitoring applications. 

<br /><br />Summary: <div>
arXiv:2508.04538v1 Announce Type: new 
Abstract: Reliable assessment of concrete degradation is critical for ensuring structural safety and longevity of engineering structures. This study proposes a self-supervised domain adaptation framework for robust concrete damage classification using coda wave signals. To support this framework, an advanced virtual testing platform is developed, combining multiscale modeling of concrete degradation with ultrasonic wave propagation simulations. This setup enables the generation of large-scale labeled synthetic data under controlled conditions, reducing the dependency on costly and time-consuming experimental labeling. However, neural networks trained solely on synthetic data often suffer from degraded performance when applied to experimental data due to domain shifts. To bridge this domain gap, the proposed framework integrates domain adversarial training, minimum class confusion loss, and the Bootstrap Your Own Latent (BYOL) strategy. These components work jointly to facilitate effective knowledge transfer from the labeled simulation domain to the unlabeled experimental domain, achieving accurate and reliable damage classification in concrete. Extensive experiments demonstrate that the proposed method achieves notable performance improvements, reaching an accuracy of 0.7762 and a macro F1 score of 0.7713, outperforming both the plain 1D CNN baseline and six representative domain adaptation techniques. Moreover, the method exhibits high robustness across training runs and introduces only minimal additional computational cost. These findings highlight the practical potential of the proposed simulation-driven and label-efficient framework for real-world applications in structural health monitoring.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Stablecoins for Digital Transformation -- Design, Metrics, and Application with Real World Asset Tokenization as a Case Study</title>
<link>https://arxiv.org/abs/2508.02403</link>
<guid>https://arxiv.org/abs/2508.02403</guid>
<content:encoded><![CDATA[
<div> taxonomy, stablecoin systems, performance evaluation framework, digital systems, Real World Asset tokenization

Summary: 
This study addresses the fragmented academic research on stablecoins by providing a unified taxonomy based on custodial structure, stabilization mechanism, and governance. It also introduces a comprehensive performance evaluation framework tailored to diverse stakeholder needs and offers transparency through an open-source benchmarking pipeline. Additionally, a case study on Real World Asset tokenization demonstrates how stablecoins function as programmable monetary infrastructure in cross-border digital systems. By combining conceptual theory with empirical tools, the paper contributes to the development of trusted, inclusive, and transparent digital monetary infrastructure. This research aims to bridge the gap between economics, law, and computer science in the study of stablecoins and provide a solid foundation for future research in this area. <br /><br />Summary: <div>
arXiv:2508.02403v1 Announce Type: cross 
Abstract: Stablecoins have become a foundational component of the digital asset ecosystem, with their market capitalization exceeding 230 billion USD as of May 2025. As fiat-referenced and programmable assets, stablecoins provide low-latency, globally interoperable infrastructure for payments, decentralized finance, DeFi, and tokenized commerce. Their accelerated adoption has prompted extensive regulatory engagement, exemplified by the European Union's Markets in Crypto-assets Regulation, MiCA, the US Guiding and Establishing National Innovation for US Stablecoins Act, GENIUS Act, and Hong Kong's Stablecoins Bill. Despite this momentum, academic research remains fragmented across economics, law, and computer science, lacking a unified framework for design, evaluation, and application. This study addresses that gap through a multi-method research design. First, it synthesizes cross-disciplinary literature to construct a taxonomy of stablecoin systems based on custodial structure, stabilization mechanism, and governance. Second, it develops a performance evaluation framework tailored to diverse stakeholder needs, supported by an open-source benchmarking pipeline to ensure transparency and reproducibility. Third, a case study on Real World Asset tokenization illustrates how stablecoins operate as programmable monetary infrastructure in cross-border digital systems. By integrating conceptual theory with empirical tools, the paper contributes: a unified taxonomy for stablecoin design; a stakeholder-oriented performance evaluation framework; an empirical case linking stablecoins to sectoral transformation; and reproducible methods and datasets to inform future research. These contributions support the development of trusted, inclusive, and transparent digital monetary infrastructure.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification</title>
<link>https://arxiv.org/abs/2508.03750</link>
<guid>https://arxiv.org/abs/2508.03750</guid>
<content:encoded><![CDATA[
<div> Keywords: glaucoma, risk prediction, multimodal, interpretability, XGBoost

Summary: 
GlaBoost is a novel multimodal gradient boosting framework designed for early and accurate detection of glaucoma. It integrates structured clinical features, fundus image embeddings, and textual descriptions for glaucoma risk prediction. GlaBoost utilizes pretrained convolutional encoders for visual representation extraction from retinal fundus photos and transformer-based language models for encoding neuroretinal rim assessments. By combining these heterogeneous signals with manually assessed risk scores and ophthalmic indicators, GlaBoost creates a unified feature space for classification using an enhanced XGBoost model. Experimental results on a real-world dataset show that GlaBoost outperforms baseline models with a validation accuracy of 98.71%. Feature importance analysis highlights the significant contributions of cup-to-disc ratio, rim pallor, and textual embeddings in model decisions. GlaBoost offers a transparent and scalable solution for interpretable glaucoma diagnosis and has the potential for extension to other ophthalmic disorders.<br /><br />Summary: <div>
arXiv:2508.03750v1 Announce Type: cross 
Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible vision loss. However, existing methods often rely on unimodal data and lack interpretability, limiting their clinical utility. In this paper, we present GlaBoost, a multimodal gradient boosting framework that integrates structured clinical features, fundus image embeddings, and expert-curated textual descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual representations from retinal fundus photographs using a pretrained convolutional encoder and encodes free-text neuroretinal rim assessments using a transformer-based language model. These heterogeneous signals, combined with manually assessed risk scores and quantitative ophthalmic indicators, are fused into a unified feature space for classification via an enhanced XGBoost model. Experiments conducted on a real-world annotated dataset demonstrate that GlaBoost significantly outperforms baseline models, achieving a validation accuracy of 98.71%. Feature importance analysis reveals clinically consistent patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings contributing most to model decisions. GlaBoost offers a transparent and scalable solution for interpretable glaucoma diagnosis and can be extended to other ophthalmic disorders.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations</title>
<link>https://arxiv.org/abs/2508.03839</link>
<guid>https://arxiv.org/abs/2508.03839</guid>
<content:encoded><![CDATA[
<div> surrogate model, trainable by parts, parameterized nonlinear PDEs, encoder, latent space

Summary: 
The article introduces a new trainable-by-parts surrogate model for solving forward and inverse parameterized nonlinear partial differential equations. The model consists of an encoder to reduce input dimensionality, a neural network to map to the solution space, and a decoder for reconstruction. The key innovation is the independent training of the model components, leading to decreased time and energy requirements compared to existing models like FNO and DeepONet. The model, named VAE-DNN, is evaluated on solving the nonlinear diffusion equation for groundwater flow, demonstrating higher efficiency and accuracy in both forward and inverse solutions. The separable training approach through a variational autoencoder framework enhances the overall performance of the model. <div>
arXiv:2508.03839v1 Announce Type: cross 
Abstract: We propose a trainable-by-parts surrogate model for solving forward and inverse parameterized nonlinear partial differential equations. Like several other surrogate and operator learning models, the proposed approach employs an encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct $h(\bm{x},t)$. The innovative aspect of our model is its ability to train its three components independently. This approach leads to a substantial decrease in both the time and energy required for training when compared to leading operator learning models such as FNO and DeepONet. The separable training is achieved by training the encoder as part of the variational autoencoder (VAE) for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet models for obtaining forward and inverse solutions to the nonlinear diffusion equation governing groundwater flow in an unconfined aquifer. Our findings indicate that VAE-DNN not only demonstrates greater efficiency but also delivers superior accuracy in both forward and inverse solutions compared to the FNO and DeepONet models.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging</title>
<link>https://arxiv.org/abs/2508.04625</link>
<guid>https://arxiv.org/abs/2508.04625</guid>
<content:encoded><![CDATA[
<div> benchmark, financial reasoning, multimodal, numerical reasoning, large language models

Summary:
- FinMMR is a new benchmark designed to evaluate the reasoning abilities of multimodal large language models in financial numerical tasks.
- It introduces multimodality by incorporating images in addition to text-based questions, covering 14 categories in the financial domain.
- The benchmark is comprehensive, spanning 14 financial subdomains such as corporate finance and banking, surpassing existing benchmarks.
- The challenge lies in requiring models to perform precise numerical reasoning by combining financial knowledge with understanding complex financial images and text.
- The best-performing model achieves 53.0% accuracy on difficult problems, indicating the need for advancement in this area.

<br /><br />Summary: <div>
arXiv:2508.04625v1 Announce Type: cross 
Abstract: We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation</title>
<link>https://arxiv.org/abs/2307.07694</link>
<guid>https://arxiv.org/abs/2307.07694</guid>
<content:encoded><![CDATA[
<div> algorithm, portfolio optimisation, deep reinforcement learning, Kelly criterion, market impact

Summary:
- The study assessed various deep reinforcement learning algorithms for portfolio optimization using simulated data.
- The simulator utilized correlated geometric Brownian motion with the Bertsimas-Lo market impact model to generate data.
- By applying the Kelly criterion as the objective, the optimal policy without market impact was analytically derived, serving as a performance benchmark.
- Off-policy algorithms like DDPG, TD3, and SAC struggled to learn the correct $Q$-function due to noisy rewards, leading to inferior performance.
- On-policy algorithms PPO and A2C, alongside generalised advantage estimation, effectively managed noise and generated policies close to optimal. The clipping variant of PPO was crucial in maintaining policy convergence. In a more complex setting with changing GBM parameters, PPO combined with a hidden Markov model successfully learned and adapted policies to different regimes. However, the algorithms demonstrated high sample complexity, requiring substantial steps for effective learning in real data applications. <br /><br />Summary: <div>
arXiv:2307.07694v3 Announce Type: replace 
Abstract: We evaluate benchmark deep reinforcement learning algorithms on the task of portfolio optimisation using simulated data. The simulator to generate the data is based on correlated geometric Brownian motion with the Bertsimas-Lo market impact model. Using the Kelly criterion (log utility) as the objective, we can analytically derive the optimal policy without market impact as an upper bound to measure performance when including market impact. We find that the off-policy algorithms DDPG, TD3 and SAC are unable to learn the right $Q$-function due to the noisy rewards and therefore perform poorly. The on-policy algorithms PPO and A2C, with the use of generalised advantage estimation, are able to deal with the noise and derive a close to optimal policy. The clipping variant of PPO was found to be important in preventing the policy from deviating from the optimal once converged. In a more challenging environment where we have regime changes in the GBM parameters, we find that PPO, combined with a hidden Markov model to learn and predict the regime context, is able to learn different policies adapted to each regime. Overall, we find that the sample complexity of these algorithms is too high for applications using real data, requiring more than 2m steps to learn a good policy in the simplest setting, which is equivalent to almost 8,000 years of daily prices.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Insight into Eco-Choices: Simulating the Fast Fashion Shift</title>
<link>https://arxiv.org/abs/2407.18814</link>
<guid>https://arxiv.org/abs/2407.18814</guid>
<content:encoded><![CDATA[
<div> Keywords: fashion, fast fashion, Spain, consumer behavior, Agent-Based Modeling

Summary: 
The study focuses on the impact of fast fashion on the environment and society, particularly in Spain. It highlights the detrimental effects of fast fashion, such as waste and human rights abuses, while also acknowledging its economic significance. Through Agent-Based Modeling, the research examines individual decision-making processes in purchasing fast fashion and the influence of awareness on sustainable fashion practices. The study emphasizes the role of government interventions in shaping consumer behavior, with campaigns playing a crucial role in driving progress. However, the success of such interventions is dependent on factors like social media influence and public polarization. The research suggests that a balanced approach by the government, along with targeted social media strategies, can lead to more significant shifts in consumer habits towards sustainable fashion choices. <div>
arXiv:2407.18814v2 Announce Type: replace 
Abstract: Fashion is a powerful force in the modern world. It is one of the most accessible means of self-expression, thereby playing a significant role in our society. Yet, it is plagued by well-documented issues of waste and human rights abuses. Fast fashion in particular, characterized by its disposable nature, contributes extensively to environmental degradation and CO$_2$ emissions, surpassing the combined outputs of France, Germany, and the UK, but its economic contributions have somewhat shielded it from criticism. In this paper, we examine the demand for fast fashion, with a focus on Spain. We explore the individual decision-making process involved in choosing to buy fast fashion and the role of awareness regarding working conditions, environmental consequences, and education on sustainable fashion in influencing consumer behavior. By employing Agent-Based Modeling, we investigate the factors influencing garment consumption patterns and how shifts in public opinion can be achieved through peer pressure, social media influence, and government interventions. Our study revealed that government interventions are pivotal, with the state's campaigns setting the overall tone for progress, although its success is conditioned by social media and polarization levels of the population. Importantly, the state does not need to adopt an extremely proactive stance or continue the campaigns indefinitely to achieve optimal results, as excessive interventions yield diminishing returns.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A CFL condition for the finite cell method</title>
<link>https://arxiv.org/abs/2502.13675</link>
<guid>https://arxiv.org/abs/2502.13675</guid>
<content:encoded><![CDATA[
<div> boundary-conforming mesh generation, finite element methods, critical time step size, explicit time integration, finite cell method
<br />
Summary: 
The study focuses on the finite cell method's effect on the critical time step size for explicit time integration in immersed wave propagation simulations. By analyzing a one-degree-of-freedom model, the influence of -stabilization on the maximum eigenvalue and critical time step size for corner and sliver cuts was systematically studied. It was found that the critical time step size does not decrease below a limit even as the cut fraction tends to zero, with the lower bound controlled by . Sliver cuts were identified as more detrimental than corner cuts in higher dimensions. Increasing polynomial degree had minimal impact on degradation. An estimate of the minimum critical time step size as a function of  was derived to propose a modified CFL condition for the finite cell method, validated on a two-dimensional perforated plate example. <div>
arXiv:2502.13675v2 Announce Type: replace 
Abstract: Immersed boundary finite element methods allow the user to bypass the potentially troublesome task of boundary-conforming mesh generation. When combined with explicit time integration, poorly cut elements with little support in the physical domain lead to a severely reduced critical time step size, posing a major challenge for immersed wave propagation simulations. The finite cell method stabilizes cut elements by defining the weak form of the problem also in the fictitious domain, but scaled by a small value $\alpha$. This paper investigates the effect of the finite cell method on the critical time step size for explicit time integration. Starting with an analytical one-degree-of-freedom model, we systematically study the influence of $\alpha$-stabilization on the maximum eigenvalue, and thus on the critical time step size, for corner and sliver cuts. The analysis is complemented by a numerical study of an example with one element and increasing polynomial degree, confirming that the critical time step size does not decrease below a certain limit, even as the cut fraction tends to zero. This lower bound is controlled by the choice of $\alpha$. In higher dimensions, sliver cuts are found to be more detrimental than corner cuts, thus determining the minimum critical time step size. Increasing the polynomial degree has only little effect on this degradation. Based on these observations, we derive an estimate of the minimum critical time step size as a function of $\alpha$, which we use to propose a modified CFL condition for the finite cell method. The validity of this condition is demonstrated on a two-dimensional perforated plate example.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical Errors in Quantitative System Analysis With Decision Diagrams</title>
<link>https://arxiv.org/abs/2508.02673</link>
<guid>https://arxiv.org/abs/2508.02673</guid>
<content:encoded><![CDATA[
<div> DDs, state-space explosion problem, probabilistic systems, quantum systems, floating-point numbers <br />
<br />
Matrix-vector multiplication with multi-terminal binary decision diagrams (MTBDDs) is crucial for computing successor states in probabilistic and quantum systems. This paper delves into the numerical stability of this algorithm, as floating-point computations can introduce errors affecting result correctness and DD compression effectiveness. The study demonstrates that the MTBDD matrix-vector multiplication algorithm can be made numerically stable under specific conditions, though real-world MTBDD implementations often fail to meet these criteria. A case study on quantum circuit simulation reveals varying degrees of numerical errors in practice. The research underscores the importance of addressing numerical stability challenges in DD-based approaches for handling probabilistic and quantum systems effectively. <br /><br />Summary: <div>
arXiv:2508.02673v1 Announce Type: new 
Abstract: Decision diagrams (DDs) are a powerful data structure that is used to tackle the state-space explosion problem, not only for discrete systems, but for probabilistic and quantum systems as well. While many of the DDs used in the probabilistic and quantum domains make use of floating-point numbers, this is not without challenges. Floating-point computations are subject to small rounding errors, which can affect both the correctness of the result and the effectiveness of the DD's compression. In this paper, we investigate the numerical stability, i.e. the robustness of an algorithm to small numerical errors, of matrix-vector multiplication with multi-terminal binary decision diagrams (MTBDDs). Matrix-vector multiplication is of particular interest because it is the function that computes successor states for both probabilistic and quantum systems. We prove that the MTBDD matrix-vector multiplication algorithm can be made numerically stable under certain conditions, although in many practical implementations of MTBDDs these conditions are not met. Additionally, we provide a case study of the numerical errors in the simulation of quantum circuits, which shows that the extent of numerical errors in practice varies greatly between instances.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming the Loss Conditioning Bottleneck in Optimization-Based PDE Solvers: A Novel Well-Conditioned Loss Function</title>
<link>https://arxiv.org/abs/2508.02692</link>
<guid>https://arxiv.org/abs/2508.02692</guid>
<content:encoded><![CDATA[
<div> optimization, PDE solvers, loss function, Stabilized Gradient Residual, convergence

Summary:
The article introduces a new Stabilized Gradient Residual (SGR) loss for optimization-based PDE solvers that aims to address the slow convergence issues associated with the mean squared error (MSE) loss. The SGR loss allows for flexible modulation of the condition number, leading to faster convergence compared to the MSE loss. By systematically benchmarking the performance of the SGR loss in both the ODIL and PINNs frameworks, the study demonstrates significant improvements in convergence speed and optimization stability. The SGR loss outperforms the MSE loss in various numerical experiments on benchmark problems within the ODIL framework and shows consistent better performance within the PINNs framework despite high nonlinearity. These findings emphasize the importance of loss conditioning in the design of more efficient PDE solvers, bridging the performance gap between classical iterative solvers and optimization-based solvers. <br /><br />Summary: <div>
arXiv:2508.02692v1 Announce Type: new 
Abstract: Optimization-based PDE solvers that minimize scalar loss functions have gained increasing attention in recent years. These methods either define the loss directly over discrete variables, as in Optimizing a Discrete Loss (ODIL), or indirectly through a neural network surrogate, as in Physics-Informed Neural Networks (PINNs). However, despite their promise, such methods often converge much more slowly than classical iterative solvers and are commonly regarded as inefficient. This work provides a theoretical insight, attributing the inefficiency to the use of the mean squared error (MSE) loss, which implicitly forms the normal equations, squares the condition number, and severely impairs optimization. To address this, we propose a novel Stabilized Gradient Residual (SGR) loss. By tuning a weight parameter, it flexibly modulates the condition number between the original system and its normal equations, while reducing to the MSE loss in the limiting case. We systematically benchmark the convergence behavior and optimization stability of the SGR loss within both the ODIL framework and PINNs-employing either numerical or automatic differentiation-and compare its performance against classical iterative solvers. Numerical experiments on a range of benchmark problems demonstrate that, within the ODIL framework, the proposed SGR loss achieves orders-of-magnitude faster convergence than the MSE loss. Further validation within the PINNs framework shows that, despite the high nonlinearity of neural networks, SGR consistently outperforms the MSE loss. These theoretical and empirical findings help bridge the performance gap between classical iterative solvers and optimization-based solvers, highlighting the central role of loss conditioning, and provide key insights for the design of more efficient PDE solvers.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using numerical-experimental analysis to evaluate rPET mechanical behavior under compressive stresses and FFF additive manufacturing for new sustainable designs</title>
<link>https://arxiv.org/abs/2508.02728</link>
<guid>https://arxiv.org/abs/2508.02728</guid>
<content:encoded><![CDATA[
<div> Keywords: recycled polymer, compressive stresses, mechanical behavior modeling, sustainable design, numerical-experimental study

Summary:<br />
The study focuses on investigating the mechanical behavior modeling of recycled polyethylene terephthalate (rPET) manufactured using a deposition FFF process under compressive stresses for sustainable designs. Experimental tests revealed that rPET behaves linearly until the elastic limit along manufacturing axes. Numerical analyses based on experimental data validated the design's structural safety and confirmed rPET could be configured as isotropic in simulation software without material modeling modifications. The results support the use of recycled rPET for sustainable product production using MEX technology under compressive stress. Major design companies are now incorporating recycled plastic materials in their designs. The validation results, presented through experimental testing and numerical simulations, demonstrate the feasibility and efficacy of using recycled rPET in real-world applications.<br /><br />Summary: <div>
arXiv:2508.02728v1 Announce Type: new 
Abstract: The purpose of this study is to investigate the numerical-experimental mechanical behavior modeling of the recycled polymer, that is, recyclable polyethylene terephthalate (rPET), manufactured by a deposition FFF process under compressive stresses for new sustainable designs. In all, 42 test specimens were manufactured and analyzed according to the ASTM D695-15 standards. Eight numerical analyzes were performed on a real design manufactured with rPET using Young's compression modulus from the experimental tests. Finally, eight additional experimental tests under uniaxial compression loads were performed on the real sustainable design for validating its mechanical behavior versus computational numerical tests. As a result of the experimental tests, rPET behaves linearly until it reaches the elastic limit, along each manufacturing axis. The results of this study confirmed the design's structural safety by the load scenario and operating boundary conditions. Experimental and numerical results show a difference of 0.001-0.024 mm, allowing for the rPET to be configured as isotropic in numerical simulation software without having to modify its material modeling equations. The results obtained are of great help to industry, designers and researchers because they validate the use of recycled rPET for the ecological production of real-sustainable products using MEX technology under compressive stress and its configuration for numerical simulations. Major design companies are now using recycled plastic materials in their high-end designs. Validation results have been presented on test specimens and real items, comparing experimental material configuration values with numerical results.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fluid--peridynamic structure model of deformation and damage of microchannels</title>
<link>https://arxiv.org/abs/2508.02875</link>
<guid>https://arxiv.org/abs/2508.02875</guid>
<content:encoded><![CDATA[
<div> fluid-structure interaction, microchannels, peridynamic formulation, failure scenarios, wave propagation <br />
<br />
Summary: 
This study investigates fluid-structure interaction in soft-walled microchannels, applying a nonlocal mechanical theory to model the compliant top wall's behavior, including potential failure scenarios. A computational model coupling viscous flow and a peridynamic Euler-Bernoulli beam formulation is developed to analyze steady and time-dependent responses. Through dispersion analysis, the study reveals that increasing nonlocal influence leads to a gradual suppression of phase velocity in wave propagation. The research identifies a dividing curve in a parameter space, based on the Strouhal number and compliance number, distinguishing potential failure scenarios during transient and steady loads. This work lays the foundation for understanding and predicting failure modes in soft-walled microchannels under hydrodynamic forces. <br /> <div>
arXiv:2508.02875v1 Announce Type: new 
Abstract: Soft-walled microchannels arise in many applications, ranging from organ-on-a-chip platforms to soft-robotic actuators. However, despite extensive research on their static and dynamic response, the potential failure of these devices has not been addressed. To this end, we explore fluid--structure interaction in microchannels whose compliant top wall is governed by a nonlocal mechanical theory capable of simulating both deformation and material failure. We develop a one-dimensional model by coupling viscous flow under the lubrication approximation to a state-based peridynamic formulation of an Euler--Bernoulli beam. The peridynamic formulation enables the wall to be modeled as a genuinely nonlocal beam, and the integral form of its equation of motion remains valid whether the deformation field is smooth or contains discontinuities. Through the proposed computational model, we explore the steady and time-dependent behaviors of this fluid--peridynamic structure interaction. We rationalize the wave and damping dynamics observed in the simulations through a dispersion (linearized) analysis of the coupled system, finding that, with increasing nonlocal influence, wave propagation exhibits a clear departure from classical behavior, characterized by a gradual suppression of the phase velocity. The main contribution of our study is to outline the potential failure scenarios of the microchannel's soft wall under the hydrodynamic load of the flow. Specifically, we find a dividing curve in the space spanned by the dimensionless Strouhal number (quantifying unsteady inertia of the beam) and the compliance number (quantifying the strength of the fluid--structure coupling) separating scenarios of potential failure during transient conditions from potential failure at the steady load.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closed-Loop Multi-Agent Framework for Aerodynamics-Aware Automotive Styling Design</title>
<link>https://arxiv.org/abs/2508.03370</link>
<guid>https://arxiv.org/abs/2508.03370</guid>
<content:encoded><![CDATA[
<div> Keywords: automotive design, aerodynamic performance, multi-agent framework, concept generation, rapid engineering validation<br />
Summary:<br />
The article introduces a new approach to automotive exterior design that combines aesthetics with aerodynamic performance using a multi-agent framework driven by LLM. The framework automates the workflow from ambiguous requirements to 3D concept model validation in two stages: conceptual generation and performance validation. In the conceptual generation stage, agents collaborate to interpret design requirements and produce concept sketches and renderings using diffusion models. The renderings are then converted to 3D point clouds for rapid validation using a Drag Prediction Agent based on a lightweight surrogate model. This approach seamlessly integrates creative exploration with engineering validation in an automated system, offering a new paradigm for balancing design creativity with engineering constraints in the early stages of automotive design. <br /><br />Summary: <div>
arXiv:2508.03370v1 Announce Type: new 
Abstract: The core challenge in automotive exterior design is balancing subjective aesthetics with objective aerodynamic performance while dramatically accelerating the development cycle. To address this, we propose a novel, LLM-driven multi-agent framework that automates the end-to-end workflow from ambiguous requirements to 3D concept model performance validation. The workflow is structured in two stages: conceptual generation and performance validation. In the first stage, agents collaborate to interpret fuzzy design requirements, generate concept sketches, and produce photorealistic renderings using diffusion models. In the second stage, the renderings are converted to 3D point clouds, where a Drag Prediction Agent, built upon a lightweight surrogate model, provides near-instantaneous predictions of the drag coefficient and pressure fields, replacing time-consuming CFD simulations. The primary contribution of this work is the seamless integration of creative generation with a rapid engineering validation loop within a unified, automated system, which provides a new paradigm for efficiently balancing creative exploration with engineering constraints in the earliest stages of design.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Incentivize: LLM-Empowered Contract for AIGC Offloading in Teleoperation</title>
<link>https://arxiv.org/abs/2508.03464</link>
<guid>https://arxiv.org/abs/2508.03464</guid>
<content:encoded><![CDATA[
<div> service providers, incentive mechanisms, AI-generated content, online learning, contract design
<br />
Summary:
This paper addresses the challenge of designing incentive mechanisms for edge AI-generated content service providers (ASPs) with information asymmetry. The study focuses on bonus design between a teleoperator and an ASP, where the teleoperator cannot observe the ASP's private settings and actions. The problem is formulated as an online learning contract design issue, divided into ASP's settings inference and contract derivation subproblems. A large language model (LLM)-empowered framework is introduced to tackle the NP-hard setting-inference problem. By leveraging the LLM's domain expertise, the framework refines a seed solver iteratively. Subsequently, the contract derivation problem is addressed using convex optimization techniques to obtain a near-optimal contract. Simulation results on a Unity-based teleoperation platform demonstrate that the proposed method significantly increases the teleoperator's utility compared to benchmarks, while maintaining positive incentives for the ASP. The code for this study is available on GitHub for further exploration. 
 <div>
arXiv:2508.03464v1 Announce Type: new 
Abstract: With the rapid growth in demand for AI-generated content (AIGC), edge AIGC service providers (ASPs) have become indispensable. However, designing incentive mechanisms that motivate ASPs to deliver high-quality AIGC services remains a challenge, especially in the presence of information asymmetry. In this paper, we address bonus design between a teleoperator and an edge ASP when the teleoperator cannot observe the ASP's private settings and chosen actions (diffusion steps). We formulate this as an online learning contract design problem and decompose it into two subproblems: ASP's settings inference and contract derivation. To tackle the NP-hard setting-inference subproblem with unknown variable sizes, we introduce a large language model (LLM)-empowered framework that iteratively refines a naive seed solver using the LLM's domain expertise. Upon obtaining the solution from the LLM-evolved solver, we directly address the contract derivation problem using convex optimization techniques and obtain a near-optimal contract. Simulation results on our Unity-based teleoperation platform show that our method boosts the teleoperator's utility by $5 \sim 40\%$ compared to benchmarks, while preserving positive incentives for the ASP. The code is available at https://github.com/Zijun0819/llm4contract.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Dynamically Changing Domains</title>
<link>https://arxiv.org/abs/2508.02697</link>
<guid>https://arxiv.org/abs/2508.02697</guid>
<content:encoded><![CDATA[
<div> planning, first-order logic, bounded planning, sequential generalized planning, conformant planning
Summary:
In the article, the authors challenge the Domain Closure Assumption commonly made in classical and conformant planning by introducing a dynamic object set that can change during actions. They formulate the planning problem in first-order logic, considering a finite consistent set of fluent literals as the initial theory. By imposing a finite integer bound on plan length and organizing search over grounded action sequences, they ensure soundness and completeness in solving bounded planning problems without DCA. Their approach combines elements of sequential generalized planning and conformant planning, without the use of disjunction over fluent literals. A proof-of-concept implementation of the planner is discussed, showcasing its practical application in scenarios where object sets evolve dynamically. <div>
arXiv:2508.02697v1 Announce Type: cross 
Abstract: In classical planning and conformant planning, it is assumed that there are finitely many named objects given in advance, and only they can participate in actions and in fluents. This is the Domain Closure Assumption (DCA). However, there are practical planning problems where the set of objects changes dynamically as actions are performed; e.g., new objects can be created, old objects can be destroyed. We formulate the planning problem in first-order logic, assume an initial theory is a finite consistent set of fluent literals, discuss when this guarantees that in every situation there are only finitely many possible actions, impose a finite integer bound on the length of the plan, and propose to organize search over sequences of actions that are grounded at planning time. We show the soundness and completeness of our approach. It can be used to solve the bounded planning problems without DCA that belong to the intersection of sequential generalized planning (without sensing actions) and conformant planning, restricted to the case without the disjunction over fluent literals. We discuss a proof-of-the-concept implementation of our planner.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model</title>
<link>https://arxiv.org/abs/2508.02734</link>
<guid>https://arxiv.org/abs/2508.02734</guid>
<content:encoded><![CDATA[
<div> Keywords: Location-Based Service, human mobility, activity sequences, Variable Selection Network, Insertion Transformer

Summary: 
The study addresses the challenge of incomplete trip and activity sequences in Location-Based Service (LBS) data, proposing a novel solution named Variable Selection Network-fused Insertion Transformer (VSNIT). VSNIT combines the flexibility of the Insertion Transformer with the dynamic covariate handling capability of the Variable Selection Network to recover missing segments in activity sequences at the individual level. Results show that VSNIT generates more diverse and realistic activity patterns, effectively restoring disrupted activity transitions. Compared to baseline models, VSNIT outperforms in accuracy and diversity metrics, showcasing its potential to enhance LBS data utility for mobility analysis. The proposed approach offers a promising framework for future research and applications in location-based studies.<br /><br />Summary: <div>
arXiv:2508.02734v1 Announce Type: cross 
Abstract: Location-Based Service (LBS) data provides critical insights into human mobility, yet its sparsity often yields incomplete trip and activity sequences, making accurate inferences about trips and activities difficult. We raise a research problem: Can we use activity sequences derived from high-quality LBS data to recover incomplete activity sequences at the individual level? This study proposes a new solution, the Variable Selection Network-fused Insertion Transformer (VSNIT), integrating the Insertion Transformer's flexible sequence construction with the Variable Selection Network's dynamic covariate handling capability, to recover missing segments in incomplete activity sequences while preserving existing data. The findings show that VSNIT inserts more diverse, realistic activity patterns, more closely matching real-world variability, and restores disrupted activity transitions more effectively aligning with the target. It also performs significantly better than the baseline model across all metrics. These results highlight VSNIT's superior accuracy and diversity in activity sequence recovery tasks, demonstrating its potential to enhance LBS data utility for mobility analysis. This approach offers a promising framework for future location-based research and applications.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreditARF: A Framework for Corporate Credit Rating with Annual Report and Financial Feature Integration</title>
<link>https://arxiv.org/abs/2508.02738</link>
<guid>https://arxiv.org/abs/2508.02738</guid>
<content:encoded><![CDATA[
<div> Keywords: corporate credit rating, FinBERT, annual reports, non-financial data, dataset

Summary:
Corporate credit rating is essential for the market economy, maintaining economic order. This paper introduces a framework that combines financial data with features extracted from annual reports using FinBERT. It aims to leverage the value of unstructured text data typically overlooked in credit rating models. The Comprehensive Corporate Rating Dataset (CCRD) incorporates both financial and textual data, enhancing the accuracy of rating predictions by 8-12%. By integrating non-financial data into the credit rating process, the method proposed in this study improves the effectiveness and reliability of corporate credit ratings. <div>
arXiv:2508.02738v1 Announce Type: cross 
Abstract: Corporate credit rating serves as a crucial intermediary service in the market economy, playing a key role in maintaining economic order. Existing credit rating models rely on financial metrics and deep learning. However, they often overlook insights from non-financial data, such as corporate annual reports. To address this, this paper introduces a corporate credit rating framework that integrates financial data with features extracted from annual reports using FinBERT, aiming to fully leverage the potential value of unstructured text data. In addition, we have developed a large-scale dataset, the Comprehensive Corporate Rating Dataset (CCRD), which combines both traditional financial data and textual data from annual reports. The experimental results show that the proposed method improves the accuracy of the rating predictions by 8-12%, significantly improving the effectiveness and reliability of corporate credit ratings.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTBench: Cryptocurrency Time Series Generation Benchmark</title>
<link>https://arxiv.org/abs/2508.02758</link>
<guid>https://arxiv.org/abs/2508.02758</guid>
<content:encoded><![CDATA[
<div> benchmark, cryptocurrency, time series generation, trading, forecasting

Summary:
The article introduces CTBench, a Time Series Generation (TSG) benchmark specifically tailored for cryptocurrency markets. It addresses the limitations of existing TSG methods by considering the unique characteristics of crypto trading, such as 24/7 trading, extreme volatility, and rapid regime shifts. CTBench evaluates TSG models across 13 metrics in key dimensions including forecasting accuracy, rank fidelity, trading performance, risk assessment, and computational efficiency. A dual-task evaluation framework is introduced, measuring both Predictive Utility for forecasting and Statistical Arbitrage for trading signals. The benchmark analyzes eight models from five methodological families across four market regimes, revealing trade-offs between statistical fidelity and real-world profitability. CTBench provides actionable guidance for selecting and deploying TSG models in cryptocurrency analytics and strategy development. <br /><br />Summary: <div>
arXiv:2508.02758v1 Announce Type: cross 
Abstract: Synthetic time series are essential tools for data augmentation, stress testing, and algorithmic prototyping in quantitative finance. However, in cryptocurrency markets, characterized by 24/7 trading, extreme volatility, and rapid regime shifts, existing Time Series Generation (TSG) methods and benchmarks often fall short, jeopardizing practical utility. Most prior work (1) targets non-financial or traditional financial domains, (2) focuses narrowly on classification and forecasting while neglecting crypto-specific complexities, and (3) lacks critical financial evaluations, particularly for trading applications. To address these gaps, we introduce \textsf{CTBench}, the first comprehensive TSG benchmark tailored for the cryptocurrency domain. \textsf{CTBench} curates an open-source dataset from 452 tokens and evaluates TSG models across 13 metrics spanning 5 key dimensions: forecasting accuracy, rank fidelity, trading performance, risk assessment, and computational efficiency. A key innovation is a dual-task evaluation framework: (1) the \emph{Predictive Utility} task measures how well synthetic data preserves temporal and cross-sectional patterns for forecasting, while (2) the \emph{Statistical Arbitrage} task assesses whether reconstructed series support mean-reverting signals for trading. We benchmark eight representative models from five methodological families over four distinct market regimes, uncovering trade-offs between statistical fidelity and real-world profitability. Notably, \textsf{CTBench} offers model ranking analysis and actionable guidance for selecting and deploying TSG models in crypto analytics and strategy development.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal wall pressure forecast of a rectangular cylinder with physics-aware DeepUFNet</title>
<link>https://arxiv.org/abs/2508.03183</link>
<guid>https://arxiv.org/abs/2508.03183</guid>
<content:encoded><![CDATA[
<div> deep learning, wall pressure, spatiotemporal, Fourier neural network, forecasting

Summary:
The study introduces the DeepUFNet model to predict spatiotemporal wall pressure generated by fluid flow past a rectangular cylinder. The model combines UNet structure and Fourier neural network while embedding physical high-frequency loss control in model training. Wind tunnel testing provides data for training and testing the DeepUFNet model, demonstrating its accurate forecast of wall pressure information. Results show agreement with experimental data in statistical information, temporal pressure variation, spatial distribution, and spatiotemporal correlation. Incorporating a physical high-frequency loss control coefficient improves the model's performance, particularly in forecasting high-order frequency fluctuation and wall pressure variance. Additionally, the model exhibits a satisfactory extrapolation capability, even with sparse spatial information input. <div>
arXiv:2508.03183v1 Announce Type: cross 
Abstract: The wall pressure is of great importance in understanding the forces and structural responses induced by fluid. Recent works have investigated the potential of deep learning techniques in predicting mean pressure coefficients and fluctuating pressure coefficients, but most of existing deep learning frameworks are limited to predicting a single snapshot using full spatial information. To forecast spatiotemporal wall pressure of flow past a rectangular cylinder, this study develops a physics-aware DeepU-Fourier neural Network (DeepUFNet) deep learning model. DeepUFNet comprises the UNet structure and the Fourier neural network, with physical high-frequency loss control embedded in the model training stage to optimize model performance, where the parameter $\beta$ varies with the development of the training epoch. Wind tunnel testing is performed to collect wall pressures of a two-dimensional rectangular cylinder with a side ratio of 1.5 at an angle of attack of zero using high-frequency pressure scanning, thereby constructing a database for DeepUFNet training and testing. The DeepUFNet model is found to forecast spatiotemporal wall pressure information with high accuracy. The comparison between forecast results and experimental data presents agreement in statistical information, temporal pressure variation, power spectrum density, spatial distribution, and spatiotemporal correlation. It is also found that embedding a physical high-frequency loss control coefficient $\beta$ in the DeepUFNet model can significantly improve model performance in forecasting spatiotemporal wall pressure information, in particular, in forecasting high-order frequency fluctuation and wall pressure variance. Furthermore, the DeepUFNet extrapolation capability is tested with sparse spatial information input, and the model presents a satisfactory extrapolation ability
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A neural network machine-learning approach for characterising hydrogen trapping parameters from TDS experiments</title>
<link>https://arxiv.org/abs/2508.03371</link>
<guid>https://arxiv.org/abs/2508.03371</guid>
<content:encoded><![CDATA[
<div> machine learning, trapping behaviour, thermal desorption spectroscopy, parameter identification, neural network

Summary:
This study presents a novel approach to analyze the hydrogen trapping behavior of metallic alloys using machine learning and Thermal Desorption Spectroscopy (TDS). Traditional methods struggle to extract key trapping parameters accurately, but this work introduces a multi-Neural Network (NN) model trained on synthetic data to predict these parameters directly from experimental TDS spectra. The model consists of two NNs - a classification model to predict trap types and a regression model to determine trap densities and binding energies. Through optimization of architecture, hyperparameters, and data pre-processing, the model demonstrated high predictive accuracy when applied to three tempered martensitic steels with different compositions. The code developed for this model is openly available for use. This innovative approach shows promise in enhancing the efficiency and accuracy of hydrogen trapping analysis in metallic alloys. 

Summary: <div>
arXiv:2508.03371v1 Announce Type: cross 
Abstract: The hydrogen trapping behaviour of metallic alloys is generally characterised using Thermal Desorption Spectroscopy (TDS). However, as an indirect method, extracting key parameters (trap binding energies and densities) remains a significant challenge. To address these limitations, this work introduces a machine learning-based scheme for parameter identification from TDS spectra. A multi-Neural Network (NN) model is developed and trained exclusively on synthetic data to predict trapping parameters directly from experimental data. The model comprises two multi-layer, fully connected, feed-forward NNs trained with backpropagation. The first network (classification model) predicts the number of distinct trap types. The second network (regression model) then predicts the corresponding trap densities and binding energies. The NN architectures, hyperparameters, and data pre-processing were optimised to minimise the amount of training data. The proposed model demonstrated strong predictive capabilities when applied to three tempered martensitic steels of different compositions. The code developed is freely provided.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Memory Non-Binary LDPC Decoding</title>
<link>https://arxiv.org/abs/2508.03567</link>
<guid>https://arxiv.org/abs/2508.03567</guid>
<content:encoded><![CDATA[
<div> near-memory non-binary LDPC decoders, PiM paradigm, parallel processing, data movement bottleneck, UPMEM system<br />
<br />
Summary:
This paper discusses the challenge of data movement bottleneck in parallel processing systems when using low-density parity-check (LDPC) codes for error correction. The processing in-memory (PiM) paradigm is proposed as a solution, focusing on near-memory non-binary LDPC decoders in the UPMEM system. The study introduces a novel efficient solution for PiM-based non-binary LDPC decoders, benchmarked against low-power GPU parallel solutions. The results show that PiM-based non-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, proving to be competitive even compared to edge GPUs. This research highlights the importance of addressing the data movement bottleneck in parallel processing systems and demonstrates the effectiveness of PiM-based solutions for LDPC decoding. <br /><br />Summary: <div>
arXiv:2508.03567v1 Announce Type: cross 
Abstract: Low-density parity-check (LDPC) codes are an important feature of several communication and storage applications, offering a flexible and effective method for error correction. These codes are computationally complex and require the exploitation of parallel processing to meet real-time constraints. As advancements in arithmetic and logic unit technology allowed for higher performance of computing systems, memory technology has not kept the same pace of development, creating a data movement bottleneck and affecting parallel processing systems more dramatically. To alleviate the severity of this bottleneck, several solutions have been proposed, namely the processing in-memory (PiM) paradigm that involves the design of compute units to where (or near) the data is stored, utilizing thousands of low-complexity processing units to perform out bit-wise and simple arithmetic operations. This paper presents a novel efficient solution for near-memory non-binary LDPC decoders in the UPMEM system, for the best of our knowledge the first real hardware PiM-based non-binary LDPC decoder that is benchmarked against low-power GPU parallel solutions highly optimized for throughput performance. PiM-based non-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, which is even competitive when compared against implementations running in edge GPUs.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA</title>
<link>https://arxiv.org/abs/2508.03590</link>
<guid>https://arxiv.org/abs/2508.03590</guid>
<content:encoded><![CDATA[
<div> Keywords: solar irradiance forecasting, artificial intelligence, high resolution, data assimilation, net-zero energy systems

Summary:
SolarSeer is introduced as an end-to-end artificial intelligence model for accurate solar irradiance forecasting in the Contiguous United States (CONUS). By directly mapping historical satellite observations to future forecasts, SolarSeer eliminates the need for computationally intensive data assimilation and solving complex partial differential equations, making it over 1,500 times faster than traditional numerical weather prediction models. With a resolution of 5-kilometers, SolarSeer outperforms the state-of-the-art High-Resolution Rapid Refresh (HRRR) model by reducing the root mean squared error of solar irradiance forecasting by 27.28% in reanalysis data and 15.35% across 1,800 stations. SolarSeer also excels in capturing solar irradiance fluctuations and enhances first-order irradiance difference forecasting accuracy. The ultrafast and accurate 24-hour solar irradiance forecasts provided by SolarSeer play a crucial role in supporting the shift towards sustainable, net-zero energy systems.<br /><br />Summary: <div>
arXiv:2508.03590v1 Announce Type: cross 
Abstract: Accurate 24-hour solar irradiance forecasting is essential for the safe and economic operation of solar photovoltaic systems. Traditional numerical weather prediction (NWP) models represent the state-of-the-art in forecasting performance but rely on computationally costly data assimilation and solving complicated partial differential equations (PDEs) that simulate atmospheric physics. Here, we introduce SolarSeer, an end-to-end large artificial intelligence (AI) model for solar irradiance forecasting across the Contiguous United States (CONUS). SolarSeer is designed to directly map the historical satellite observations to future forecasts, eliminating the computational overhead of data assimilation and PDEs solving. This efficiency allows SolarSeer to operate over 1,500 times faster than traditional NWP, generating 24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometer resolution in under 3 seconds. Compared with the state-of-the-art NWP in the CONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantly reduces the root mean squared error of solar irradiance forecasting by 27.28% in reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectively captures solar irradiance fluctuations and significantly enhances the first-order irradiance difference forecasting accuracy. SolarSeer's ultrafast, accurate 24-hour solar irradiance forecasts provide strong support for the transition to sustainable, net-zero energy systems.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements</title>
<link>https://arxiv.org/abs/2502.04592</link>
<guid>https://arxiv.org/abs/2502.04592</guid>
<content:encoded><![CDATA[
<div> Dataset, CAMEF, Multi-modal framework, Causal relationships, Counterfactual event augmentation<br />
<br />
Summary:<br />
The article introduces a new framework called CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting) that combines textual and time-series data to forecast the impact of macroeconomic events on financial markets. CAMEF integrates a causal learning mechanism and an event augmentation technique to capture causal relationships between policy texts and historical price data. A new financial dataset with various macroeconomic releases and high-frequency trading data for U.S. financial assets is introduced for analysis. The framework is compared to transformer-based models and ablation studies confirm the effectiveness of the causal learning mechanism and event types in enhancing financial forecasting accuracy. <div>
arXiv:2502.04592v2 Announce Type: replace-cross 
Abstract: Accurately forecasting the impact of macroeconomic events is critical for investors and policymakers. Salient events like monetary policy decisions and employment reports often trigger market movements by shaping expectations of economic growth and risk, thereby establishing causal relationships between events and market behavior. Existing forecasting methods typically focus either on textual analysis or time-series modeling, but fail to capture the multi-modal nature of financial markets and the causal relationship between events and price movements. To address these gaps, we propose CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a multi-modality framework that effectively integrates textual and time-series data with a causal learning mechanism and an LLM-based counterfactual event augmentation technique for causal-enhanced financial forecasting. Our contributions include: (1) a multi-modal framework that captures causal relationships between policy texts and historical price data; (2) a new financial dataset with six types of macroeconomic releases from 2008 to April 2024, and high-frequency real trading data for five key U.S. financial assets; and (3) an LLM-based counterfactual event augmentation strategy. We compare CAMEF to state-of-the-art transformer-based time-series and multi-modal baselines, and perform ablation studies to validate the effectiveness of the causal learning mechanism and event types.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance Agent Benchmark: Benchmarking LLMs on Real-world Financial Research Tasks</title>
<link>https://arxiv.org/abs/2508.00828</link>
<guid>https://arxiv.org/abs/2508.00828</guid>
<content:encoded><![CDATA[
<div> SEC filings, Finance Agent Benchmark, Large Language Models, AI capabilities, Financial analysis <br />
<br />
Summary: 
The article discusses the use of Large Language Models (LLMs) in financial analysis through the creation of the Finance Agent Benchmark. This benchmark consists of real-world finance research problems that require LLMs to analyze SEC filings. Developed with input from industry experts, the benchmark includes 537 questions spanning nine financial task categories. An agent harness provides LLMs with tools like Google Search and EDGAR database access to generate accurate responses. Evaluation results show that current AI capabilities, with the best model achieving only 46.8% accuracy at a cost of $3.79 per query, have limitations that need to be addressed before reliable deployment in high-stakes finance settings. <div>
arXiv:2508.00828v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) technology has emerged as a transformative force in financial analysis and the finance industry, though significant questions remain about the full capabilities of Large Language Model (LLM) agents in this domain. We present the Finance Agent Benchmark, featuring challenging and diverse real-world finance research problems that require LLMs to perform complex analysis using recent SEC filings. We construct the benchmark using a taxonomy of nine financial task categories, developed in consultation with experts from banks, hedge funds, and private equity firms. The dataset includes 537 expert-authored questions covering tasks from information retrieval to complex financial modeling, each validated through a rigorous review process to ensure accuracy and relevance. Moreover, we implement an agentic harness that equips LLMs with tools sufficient to produce accurate responses, including Google Search and EDGAR database access. Overall, the Finance Agent Benchmark provides a comprehensive testbed for measuring the progress of LLM-driven finance agents. Our evaluation reveals significant limitations in current AI capabilities - even the best-performing model (OpenAI o3) achieved only 46.8% accuracy at an average cost of $3.79 per query. This underscores the need for further advancements before reliable deployment in high-stakes finance settings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bike-Bench: A Bicycle Design Benchmark for Generative Models with Objectives and Constraints</title>
<link>https://arxiv.org/abs/2508.00830</link>
<guid>https://arxiv.org/abs/2508.00830</guid>
<content:encoded><![CDATA[
<div> engineering design benchmark, generative models, multi-objective, constraints, Bike-Bench<br />
Summary:<br />
The article introduces Bike-Bench, an engineering design benchmark for assessing generative models in solving real-world problems with multiple objectives and constraints. Bike-Bench evaluates AI models on their ability to generate designs that meet specific performance goals and restrictions across various domains such as aerodynamics, ergonomics, and human usability. The benchmark includes datasets of simulation results, human-rated bicycle assessments, and a large dataset of design variations. Results from experiments show that Language Models (LLMs) and tabular generative models do not perform as well as optimization-based models in terms of validity and optimality scores. This highlights the need for improvement in generative AI approaches for constrained multi-objective engineering design challenges. Bike-Bench aims to drive progress in this area and provides code, data, and resources for further research.<br /> <div>
arXiv:2508.00830v1 Announce Type: new 
Abstract: We introduce Bike-Bench, an engineering design benchmark for evaluating generative models on problems with multiple real-world objectives and constraints. As generative AI's reach continues to grow, evaluating its capability to understand physical laws, human guidelines, and hard constraints grows increasingly important. Engineering product design lies at the intersection of these difficult tasks, providing new challenges for AI capabilities. Bike-Bench evaluates AI models' capability to generate designs that not only resemble the dataset, but meet specific performance objectives and constraints. To do so, Bike-Bench quantifies a variety of human-centered and multiphysics performance characteristics, such as aerodynamics, ergonomics, structural mechanics, human-rated usability, and similarity to subjective text or image prompts. Supporting the benchmark are several datasets of simulation results, a dataset of 10K human-rated bicycle assessments, and a synthetically-generated dataset of 1.4M designs, each with a parametric, CAD/XML, SVG, and PNG representation. Bike-Bench is uniquely configured to evaluate tabular generative models, LLMs, design optimization, and hybrid algorithms side-by-side. Our experiments indicate that LLMs and tabular generative models fall short of optimization and optimization-augmented generative models in both validity and optimality scores, suggesting significant room for improvement. We hope Bike-Bench, a first-of-its-kind benchmark, will help catalyze progress in generative AI for constrained multi-objective engineering design problems. Code, data, and other resources are published at decode.mit.edu/projects/bikebench/.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EngiBench: A Framework for Data-Driven Engineering Design Research</title>
<link>https://arxiv.org/abs/2508.00831</link>
<guid>https://arxiv.org/abs/2508.00831</guid>
<content:encoded><![CDATA[
<div> Keywords: Engineering design optimization, Data-driven, Open-source library, Benchmarking, Machine learning algorithms

Summary: 
EngiBench is an open-source library that provides datasets and benchmarks for data-driven engineering design optimization. It offers a unified API and curated benchmarks in various domains such as aeronautics, heat conduction, and photonics. EngiOpt is another companion library that includes optimization and machine learning algorithms compatible with EngiBench. Both libraries are modular, allowing users to add new algorithms, automate experiment workflows, and use utilities for visualization and performance analysis. Experiments showed that these engineering design problems are challenging for standard machine learning methods due to sensitive and constrained design parameters. This initiative enables fair and reproducible comparisons of algorithms and facilitates faster experimentation in engineering design optimization. 

<br /><br />Summary: <div>
arXiv:2508.00831v1 Announce Type: new 
Abstract: Engineering design optimization seeks to automatically determine the shapes, topologies, or parameters of components that maximize performance under given conditions. This process often depends on physics-based simulations, which are difficult to install, computationally expensive, and require domain-specific expertise. To mitigate these challenges, we introduce EngiBench, the first open-source library and datasets spanning diverse domains for data-driven engineering design. EngiBench provides a unified API and a curated set of benchmarks -- covering aeronautics, heat conduction, photonics, and more -- that enable fair, reproducible comparisons of optimization and machine learning algorithms, such as generative or surrogate models. We also release EngiOpt, a companion library offering a collection of such algorithms compatible with the EngiBench interface. Both libraries are modular, letting users plug in novel algorithms or problems, automate end-to-end experiment workflows, and leverage built-in utilities for visualization, dataset generation, feasibility checks, and performance analysis. We demonstrate their versatility through experiments comparing state-of-the-art techniques across multiple engineering design problems, an undertaking that was previously prohibitively time-consuming to perform. Finally, we show that these problems pose significant challenges for standard machine learning methods due to highly sensitive and constrained design manifolds.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Kernel Bayesian Optimisation for Closed-Loop Electrode Microstructure Design with User-Defined Properties based on GANs</title>
<link>https://arxiv.org/abs/2508.00833</link>
<guid>https://arxiv.org/abs/2508.00833</guid>
<content:encoded><![CDATA[
<div> Keywords: porous electrode microstructures, lithium-ion batteries, deep convolutional Generative Adversarial Network, Gaussian Process Regression, Bayesian optimisation framework

Summary:
In the design of enhanced electrochemical energy storage devices like lithium-ion batteries, generating multiphase porous electrode microstructures with optimized properties is crucial. A closed-loop algorithm has been developed for designing microstructures with tailored properties. This approach involves using a deep convolutional Generative Adversarial Network to create synthetic three-phase three-dimensional images of a battery cathode material. A Gaussian Process Regression model correlates morphological and transport properties, and a deep kernel Bayesian optimization framework optimizes cathode properties based on the generator's latent space. Objective functions are defined for maximizing morphological and transport properties, and the optimized latent space shows correlation with morphological properties. This innovative method allows for efficient generation of visually realistic microstructures with customized properties. <div>
arXiv:2508.00833v1 Announce Type: new 
Abstract: The generation of multiphase porous electrode microstructures with optimum morphological and transport properties is essential in the design of improved electrochemical energy storage devices, such as lithium-ion batteries. Electrode characteristics directly influence battery performance by acting as the main sites where the electrochemical reactions coupled with transport processes occur. This work presents a generation-optimisation closed-loop algorithm for the design of microstructures with tailored properties. A deep convolutional Generative Adversarial Network is used as a deep kernel and employed to generate synthetic three-phase three-dimensional images of a porous lithium-ion battery cathode material. A Gaussian Process Regression uses the latent space of the generator and serves as a surrogate model to correlate the morphological and transport properties of the synthetic microstructures. This surrogate model is integrated into a deep kernel Bayesian optimisation framework, which optimises cathode properties as a function of the latent space of the generator. A set of objective functions were defined to perform the maximisation of morphological properties (e.g., volume fraction, specific surface area) and transport properties (relative diffusivity). We demonstrate the ability to perform simultaneous maximisation of correlated properties (specific surface area and relative diffusivity), as well as constrained optimisation of these properties. This is the maximisation of morphological or transport properties constrained by constant values of the volume fraction of the phase of interest. Visualising the optimised latent space reveals its correlation with morphological properties, enabling the fast generation of visually realistic microstructures with customised properties.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Baseline-free Damage Detection and Localization on Composite Structures with Unsupervised Kolmogorov-Arnold Autoencoder and Guided Waves</title>
<link>https://arxiv.org/abs/2508.01081</link>
<guid>https://arxiv.org/abs/2508.01081</guid>
<content:encoded><![CDATA[
<div> Keywords: Structural health monitoring, damage detection, composite structures, Kolmogorov-Arnold autoencoder, probabilistic elliptical imaging algorithm

Summary:
A novel hybrid baseline-free damage detection and localization framework has been proposed for composite structures. The framework combines an unsupervised Kolmogorov-Arnold autoencoder (KAE) with a modified probabilistic elliptical imaging algorithm (MRAPID). The KAE processes guided wave signals without prior feature extraction, continuously learning and adapting to the baseline model of each structure. The predictions from KAE are then combined with MRAPID to generate a damage probability map. The method was tested on simulated damage data from wind turbine blades and real damage data from composite flat plates, showing effective detection and localization capabilities, including the ability to detect multiple damages. Comparative analysis demonstrated superior performance over classical algorithms and state-of-the-art baseline-free methods, particularly in terms of damage localization accuracy.<br /><br />Summary: <div>
arXiv:2508.01081v1 Announce Type: new 
Abstract: Structural health monitoring (SHM) ensures the safety and longevity of structures such as aerospace equipment and wind power installations. Developing a simple, highly flexible, and scalable SHM method that does not depend on baseline models is significant for ensuring the operational integrity of advanced composite structures. In this regard, a hybrid baseline-free damage detection and localization framework incorporating an unsupervised Kolmogorov-Arnold autoencoder (KAE) and modified probabilistic elliptical imaging algorithm (MRAPID) is proposed for damage detection and localization in composite structures. Specifically, KAE was used to process the guided wave signals (GW) without any prior feature extraction process. The KAE continuously learns and adapts to the baseline model of each structure, learning from the response characteristics of its undamaged state. Then, the predictions from KAE are processed, combined with the MRAPID to generate a damage probability map. The performance of the proposed method for damage detection and localization was verified using the simulated damage data obtained on wind turbine blades and the actual damage data obtained on composite flat plates. The results show that the proposed method can effectively detect and localize damage and can achieve multiple damage localization. In addition, the method outperforms classical damage detection algorithms and state-of-the-art baseline-free damage detection and localization methods in terms of damage localization accuracy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FluidFormer: Transformer with Continuous Convolution for Particle-based Fluid Simulation</title>
<link>https://arxiv.org/abs/2508.01537</link>
<guid>https://arxiv.org/abs/2508.01537</guid>
<content:encoded><![CDATA[
<div> Fluid Attention Block, local-global hierarchy, continuous convolutions, self-attention, Transformer architecture, neural fluid simulation, convolution-based features, attention-based modeling, FluidFormer, stability.

Summary: 
The article introduces a novel approach for fluid simulation using neural networks, emphasizing the importance of global context integration for stabilizing complex simulations. The proposed Fluid Attention Block (FAB) incorporates a local-global hierarchy, combining continuous convolutions for local features with self-attention for capturing global dependencies. This fusion helps suppress error accumulation and model long-range physical phenomena. Additionally, a specialized Transformer architecture is developed for continuous fluid simulation, integrated within a dual-pipeline framework. The method, named FluidFormer, showcases state-of-the-art performance and enhanced stability in various complex fluid scenarios. This innovative approach unifies convolution-based local features with attention-based global context modeling, setting a new standard for neural fluid simulation techniques. <div>
arXiv:2508.01537v1 Announce Type: new 
Abstract: Learning-based fluid simulation networks have been proven as viable alternatives to traditional numerical solvers for the Navier-Stokes equations. Existing neural methods follow Smoothed Particle Hydrodynamics (SPH) frameworks, which inherently rely only on local inter-particle interactions. However, we emphasize that global context integration is also essential for learning-based methods to stabilize complex fluid simulations. We propose the first Fluid Attention Block (FAB) with a local-global hierarchy, where continuous convolutions extract local features while self-attention captures global dependencies. This fusion suppresses the error accumulation and models long-range physical phenomena. Furthermore, we pioneer the first Transformer architecture specifically designed for continuous fluid simulation, seamlessly integrated within a dual-pipeline architecture. Our method establishes a new paradigm for neural fluid simulation by unifying convolution-based local features with attention-based global context modeling. FluidFormer demonstrates state-of-the-art performance, with stronger stability in complex fluid scenarios.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Model Fidelity Evaluation to Support Design Decisions for Complex, Novel Systems of Systems</title>
<link>https://arxiv.org/abs/2508.02456</link>
<guid>https://arxiv.org/abs/2508.02456</guid>
<content:encoded><![CDATA[
<div> trustworthiness, model validation, model fidelity, systems engineering, design process

Summary: 
Model trust is vital in systems design processes where real-world data is scarce, especially for complex systems with emergent behavior. Trustworthy models are crucial for supporting designers in making informed decisions. Model fidelity, defined as the model's adherence to real-world physics, is closely linked to trust and validity, enhancing a designer's ability to rely on physics-based models. The complexity and accuracy of a model's representation of physical phenomena play a significant role in determining its fidelity. Methods for evaluating and selecting models that do not require real-world data are essential challenges in systems engineering. Validating models based on their fidelity to real-world physics helps designers choose the most appropriate model for a given design decision. <div>
arXiv:2508.02456v1 Announce Type: new 
Abstract: Systems design processes are increasingly reliant on simulation models to inform design decisions. A pervasive issue within the systems engineering community is trusting in the models used to make decisions about complex systems. This work presents a method of evaluating the trustworthiness of a model to provide utility to a designer making a decision within a design process. Trusting the results of a model is especially important in design processes where the system is complex, novel, or displays emergent phenomena. Additionally, systems that are in the pre-prototype stages of development often do not have sources of ground truth for validating the models. Developing methods of model validation and trust that do not require real-world data is a key challenge facing systems engineers. Model fidelity in this work refers to the adherence of a model to real-world physics and is closely tied to model trust and model validity. Trust and validity directly support a designer's ability to make decisions using physics-based models. The physics that are captured in a model and the complexity of the mathematical representation of the physics contribute to a model's fidelity, and this work leverages the included physical phenomena to develop a means of selecting the most appropriate for a given design decision.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gearshift Fellowship: A Next-Generation Neurocomputational Game Platform to Model and Train Human-AI Adaptability</title>
<link>https://arxiv.org/abs/2508.00850</link>
<guid>https://arxiv.org/abs/2508.00850</guid>
<content:encoded><![CDATA[
<div> Keywords: Supertask paradigm, adaptive behavior, cognitive neuroscience, computational psychiatry, serious gaming

Summary: 
Gearshift Fellowship (GF) is a new Supertask paradigm that models adaptive behavior in humans and artificial agents by combining cognitive neuroscience, computational psychiatry, economics, and artificial intelligence. It creates a dynamic, multi-mission environment for assessing mechanisms of adaptive behavior across cognitive and social contexts. GF allows for neurocognitive modeling of individual differences in perceptual decisions, learning, and meta-cognitive levels, making it a flexible testbed for understanding cognitive-affective control processes, learning styles, and motivation shifts. Results from an online study show that GF recovers effects from traditional neuropsychological tasks, uncovers novel patterns in learning across contexts, and maps clinical features onto distinct adaptations. This research paves the way for developing in-game interventions that promote self-efficacy and coping skills for real-world stress. GF aims to accelerate science, transform clinical care, and support individual growth by creating an adaptive ecosystem where humans and machines can co-develop greater flexibility and awareness.<br /><br />Summary: <div>
arXiv:2508.00850v1 Announce Type: cross 
Abstract: How do we learn when to persist, when to let go, and when to shift gears? Gearshift Fellowship (GF) is the prototype of a new Supertask paradigm designed to model how humans and artificial agents adapt to shifting environment demands. Grounded in cognitive neuroscience, computational psychiatry, economics, and artificial intelligence, Supertasks combine computational neurocognitive modeling with serious gaming. This creates a dynamic, multi-mission environment engineered to assess mechanisms of adaptive behavior across cognitive and social contexts. Computational parameters explain behavior and probe mechanisms by controlling the game environment. Unlike traditional tasks, GF enables neurocognitive modeling of individual differences across perceptual decisions, learning, and meta-cognitive levels. This positions GF as a flexible testbed for understanding how cognitive-affective control processes, learning styles, strategy use, and motivational shifts adapt across contexts and over time. It serves as an experimental platform for scientists, a phenotype-to-mechanism intervention for clinicians, and a training tool for players aiming to strengthen self-regulated learning, mood, and stress resilience. Online study (n = 60, ongoing) results show that GF recovers effects from traditional neuropsychological tasks (construct validity), uncovers novel patterns in how learning differs across contexts and how clinical features map onto distinct adaptations. These findings pave the way for developing in-game interventions that foster self-efficacy and agency to cope with real-world stress and uncertainty. GF builds a new adaptive ecosystem designed to accelerate science, transform clinical care, and foster individual growth. It offers a mirror and training ground where humans and machines co-develop together deeper flexibility and awareness.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Residual Guided strategy with Generative Adversarial Networks in training Physics-Informed Transformer Networks</title>
<link>https://arxiv.org/abs/2508.00855</link>
<guid>https://arxiv.org/abs/2508.00855</guid>
<content:encoded><![CDATA[
<div> Transformer, Physics-Informed Neural Networks, Generative Adversarial Networks, Residual Guided Training, Partial Differential Equations<br />
<br />
Summary: 
The article introduces a novel Residual Guided Training strategy for Physics-Informed Transformer via Generative Adversarial Networks (GAN) to improve resolving residuals and enforcing temporal causality in modeling nonlinear partial differential equations (PDEs). The proposed framework combines a decoder-only Transformer for capturing temporal correlations with a residual-aware GAN to prioritize high-residual regions. By incorporating a causal penalty term and an adaptive sampling mechanism, the method enhances accuracy in critical spatiotemporal regions. Experimental results on various PDEs like Allen-Cahn, Klein-Gordon, and Navier-Stokes equations demonstrate substantial improvements with up to three orders of magnitude reduction in Mean Squared Error (MSE) compared to conventional methods. This approach effectively bridges the gap between deep learning and physics-driven modeling, offering a robust solution for modeling multiscale and time-dependent PDE systems. <br /><br /> <div>
arXiv:2508.00855v1 Announce Type: cross 
Abstract: Nonlinear partial differential equations (PDEs) are pivotal in modeling complex physical systems, yet traditional Physics-Informed Neural Networks (PINNs) often struggle with unresolved residuals in critical spatiotemporal regions and violations of temporal causality. To address these limitations, we propose a novel Residual Guided Training strategy for Physics-Informed Transformer via Generative Adversarial Networks (GAN). Our framework integrates a decoder-only Transformer to inherently capture temporal correlations through autoregressive processing, coupled with a residual-aware GAN that dynamically identifies and prioritizes high-residual regions. By introducing a causal penalty term and an adaptive sampling mechanism, the method enforces temporal causality while refining accuracy in problematic domains. Extensive numerical experiments on the Allen-Cahn, Klein-Gordon, and Navier-Stokes equations demonstrate significant improvements, achieving relative MSE reductions of up to three orders of magnitude compared to baseline methods. This work bridges the gap between deep learning and physics-driven modeling, offering a robust solution for multiscale and time-dependent PDE systems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Fleet Upgrade Decisions with Machine-Learning Enhanced Optimization</title>
<link>https://arxiv.org/abs/2508.00915</link>
<guid>https://arxiv.org/abs/2508.00915</guid>
<content:encoded><![CDATA[
<div> Keywords: Rental-based business models, fleet management, machine learning, optimization, sustainability

Summary:
Rental-based business models and sustainability requirements are driving the need for efficient strategies to manage large machine and vehicle fleets. Traditional fleet optimization methods based on integer programming are computationally expensive, especially for large fleets. This study proposes two approaches for fleet upgrade optimization: an extended integer programming approach and a machine learning-based method. In a real-world automotive industry case study, the machine learning approach demonstrated near-optimal solutions with improved scalability and computational performance compared to the traditional method. This makes it a practical alternative for large-scale fleet management. By integrating machine learning into fleet upgrade decision-making processes, organizations can achieve optimal solutions that balance utility, cost, and sustainability considerations effectively. 

<br /><br />Summary: <div>
arXiv:2508.00915v1 Announce Type: cross 
Abstract: Rental-based business models and increasing sustainability requirements intensify the need for efficient strategies to manage large machine and vehicle fleet renewal and upgrades. Optimized fleet upgrade strategies maximize overall utility, cost, and sustainability. However, conventional fleet optimization does not account for upgrade options and is based on integer programming with exponential runtime scaling, which leads to substantial computational cost when dealing with large fleets and repeated decision-making processes. This contribution firstly suggests an extended integer programming approach that determines optimal renewal and upgrade decisions. The computational burden is addressed by a second, alternative machine learning-based method that transforms the task to a mixed discrete-continuous optimization problem. Both approaches are evaluated in a real-world automotive industry case study, which shows that the machine learning approach achieves near-optimal solutions with significant improvements in the scalability and overall computational performance, thus making it a practical alternative for large-scale fleet management.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reward-Directed Diffusion Framework for Generative Design Optimization</title>
<link>https://arxiv.org/abs/2508.01509</link>
<guid>https://arxiv.org/abs/2508.01509</guid>
<content:encoded><![CDATA[
<div> Diffusion models, reward-directed sampling, high-performance engineering designs, soft value function, Markov decision process <br />
<br />
Summary: This study introduces a generative optimization framework utilizing a fine-tuned diffusion model and reward-directed sampling to create high-performance engineering designs. The framework uses a parametric design representation to generate new parameter sets with improved performance metrics. It employs a soft value function within a Markov decision process to guide the decoding process, reducing computational costs and achieving high-reward designs. Empirical results show significant enhancements in 3D ship hull design and 2D airfoil design, with samples surpassing the training data distribution. The proposed approach leads to a 25 percent reduction in resistance for ship design and a 10 percent improvement in the lift-to-drag ratio for 2D airfoil design. Integration of this framework in the engineering design cycle can boost designer efficiency and overall design performance. <br /> <div>
arXiv:2508.01509v1 Announce Type: cross 
Abstract: This study presents a generative optimization framework that builds on a fine-tuned diffusion model and reward-directed sampling to generate high-performance engineering designs. The framework adopts a parametric representation of the design geometry and produces new parameter sets corresponding to designs with enhanced performance metrics. A key advantage of the reward-directed approach is its suitability for scenarios in which performance metrics rely on costly engineering simulations or surrogate models (e.g. graph-based, ensemble models, or tree-based) are non-differentiable or prohibitively expensive to differentiate. This work introduces the iterative use of a soft value function within a Markov decision process framework to achieve reward-guided decoding in the diffusion model. By incorporating soft-value guidance during both the training and inference phases, the proposed approach reduces computational and memory costs to achieve high-reward designs, even beyond the training data. Empirical results indicate that this iterative reward-directed method substantially improves the ability of the diffusion models to generate samples with reduced resistance in 3D ship hull design and enhanced hydrodynamic performance in 2D airfoil design tasks. The proposed framework generates samples that extend beyond the training data distribution, resulting in a greater 25 percent reduction in resistance for ship design and over 10 percent improvement in the lift-to-drag ratio for the 2D airfoil design. Successful integration of this model into the engineering design life cycle can enhance both designer productivity and overall design performance.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Policy Iteration for Stochastic Optimal Control: A Physics-Informed Approach</title>
<link>https://arxiv.org/abs/2508.01718</link>
<guid>https://arxiv.org/abs/2508.01718</guid>
<content:encoded><![CDATA[
<div> physics-informed neural network, stochastic optimal control, Hamilton-Jacobi-Bellman equation, policy iteration, value function approximation 
Summary:
The proposed Physics-Informed Neural Network Policy Iteration (PINN-PI) framework tackles stochastic optimal control problems through second-order Hamilton-Jacobi-Bellman equations. A neural network is trained at each iteration to approximate the value function by minimizing the residual of a linear PDE based on a fixed policy, ensuring systematic error control. Explicit Lipschitz-type bounds quantify the propagation of value gradient errors to policy updates, enhancing interpretability during training. Extending deterministic PINN approaches to stochastic environments, the method guarantees global exponential convergence under mild conditions. Demonstrated effectiveness on various benchmark problems like stochastic cartpole, pendulum, and high-dimensional linear quadratic regulation (LQR) challenges up to 10D showcases the versatility and reliability of the approach. <div>
arXiv:2508.01718v1 Announce Type: cross 
Abstract: We propose a physics-informed neural network policy iteration (PINN-PI) framework for solving stochastic optimal control problems governed by second-order Hamilton--Jacobi--Bellman (HJB) equations. At each iteration, a neural network is trained to approximate the value function by minimizing the residual of a linear PDE induced by a fixed policy. This linear structure enables systematic $L^2$ error control at each policy evaluation step, and allows us to derive explicit Lipschitz-type bounds that quantify how value gradient errors propagate to the policy updates. This interpretability provides a theoretical basis for evaluating policy quality during training. Our method extends recent deterministic PINN-based approaches to stochastic settings, inheriting the global exponential convergence guarantees of classical policy iteration under mild conditions. We demonstrate the effectiveness of our method on several benchmark problems, including stochastic cartpole, pendulum problems and high-dimensional linear quadratic regulation (LQR) problems in up to 10D.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization</title>
<link>https://arxiv.org/abs/2508.02002</link>
<guid>https://arxiv.org/abs/2508.02002</guid>
<content:encoded><![CDATA[
<div> transformers, diffusers, trajectory generation, auto-bidding, GRAD
<br />
Summary:
GRAD is a novel approach in the field of auto-bidding systems, combining an Action-Mixture-of-Experts module and the Value Estimator of Causal Transformer. This model addresses challenges faced by generative methods in online advertising, such as distribution shift and limited exploration of the action space, by incorporating constraint-aware optimization techniques. GRAD has been successfully implemented at Meituan, a leading online food delivery platform, resulting in a significant increase in platform revenue, Gross Merchandise Value (GMV), and Return on Investment (ROI). The model demonstrates its effectiveness in enhancing the performance of auto-bidding systems by catering to the dynamic and diverse requirements of modern advertisers. <div>
arXiv:2508.02002v1 Announce Type: cross 
Abstract: Modern auto-bidding systems are required to balance overall performance with diverse advertiser goals and real-world constraints, reflecting the dynamic and evolving needs of the industry. Recent advances in conditional generative models, such as transformers and diffusers, have enabled direct trajectory generation tailored to advertiser preferences, offering a promising alternative to traditional Markov Decision Process-based methods. However, these generative methods face significant challenges, such as the distribution shift between offline and online environments, limited exploration of the action space, and the necessity to meet constraints like marginal Cost-per-Mille (CPM) and Return on Investment (ROI). To tackle these challenges, we propose GRAD (Generative Reward-driven Ad-bidding with Mixture-of-Experts), a scalable foundation model for auto-bidding that combines an Action-Mixture-of-Experts module for diverse bidding action exploration with the Value Estimator of Causal Transformer for constraint-aware optimization. Extensive offline and online experiments demonstrate that GRAD significantly enhances platform revenue, highlighting its effectiveness in addressing the evolving and diverse requirements of modern advertisers. Furthermore, GRAD has been implemented in multiple marketing scenarios at Meituan, one of the world's largest online food delivery platforms, leading to a 2.18% increase in Gross Merchandise Value (GMV) and 10.68% increase in ROI.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval</title>
<link>https://arxiv.org/abs/2508.02222</link>
<guid>https://arxiv.org/abs/2508.02222</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hierarchical queries, passage retrieval, indirect positives mining, Financial Passage Retrieval Generated dataset

Summary: 
The paper introduces a bidirectional generation pipeline for constructing passage retrieval datasets using large language models (LLMs). This pipeline generates 3-level hierarchical queries for intra-doc and cross-doc scenarios by disassembling single-doc text and dividing multi-doc titles into clusters based on industry, topic, and time. It incorporates both bottom-up and top-down query generation methods to enhance query expression and relevance. The pipeline includes a direct mapping annotation process and an indirect positives mining method to enrich relevance labels. The Financial Passage Retrieval Generated dataset (FinCPRG) was created from Chinese financial research reports, containing hierarchical queries and rich relevance labels. Evaluations and experiments demonstrated the quality and effectiveness of FinCPRG as a training and benchmarking dataset for passage retrieval tasks. 

Summary: <div>
arXiv:2508.02222v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have demonstrated significant potential in constructing passage retrieval datasets. However, existing methods still face limitations in expressing cross-doc query needs and controlling annotation quality. To address these issues, this paper proposes a bidirectional generation pipeline, which aims to generate 3-level hierarchical queries for both intra-doc and cross-doc scenarios and mine additional relevance labels on top of direct mapping annotation. The pipeline introduces two query generation methods: bottom-up from single-doc text and top-down from multi-doc titles. The bottom-up method uses LLMs to disassemble and generate structured queries at both sentence-level and passage-level simultaneously from intra-doc passages. The top-down approach incorporates three key financial elements--industry, topic, and time--to divide report titles into clusters and prompts LLMs to generate topic-level queries from each cluster. For relevance annotation, our pipeline not only relies on direct mapping annotation from the generation relationship but also implements an indirect positives mining method to enrich the relevant query-passage pairs. Using this pipeline, we constructed a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k Chinese financial research reports, which includes hierarchical queries and rich relevance labels. Through evaluations of mined relevance labels, benchmarking and training experiments, we assessed the quality of FinCPRG and validated its effectiveness as a passage retrieval dataset for both training and benchmarking.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte Space</title>
<link>https://arxiv.org/abs/2508.02247</link>
<guid>https://arxiv.org/abs/2508.02247</guid>
<content:encoded><![CDATA[
<div> Generative modeling, high-frequency limit order book, finance, ByteGen, raw byte streams<br />
Summary:<br />
The article introduces ByteGen, a novel generative model for high-frequency limit order book dynamics in finance. It works directly on raw byte streams of market events, eliminating the need for feature engineering and tokenization. ByteGen treats the problem as an autoregressive next-byte prediction task, using a compact 32-byte packed binary format for efficient data representation. The H-Net architecture, a hybrid Mamba-Transformer model, is used to discover the structure of market messages without predefined rules. Trained on CME Bitcoin futures data, ByteGen successfully replicates key characteristics of financial markets, such as realistic price distributions and bursty event timing. This approach shows promise for modeling complex financial systems without the biases of tokenization, achieving competitive performance on market quality metrics. <br /> <div>
arXiv:2508.02247v1 Announce Type: cross 
Abstract: Generative modeling of high-frequency limit order book (LOB) dynamics is a critical yet unsolved challenge in quantitative finance, essential for robust market simulation and strategy backtesting. Existing approaches are often constrained by simplifying stochastic assumptions or, in the case of modern deep learning models like Transformers, rely on tokenization schemes that affect the high-precision, numerical nature of financial data through discretization and binning. To address these limitations, we introduce ByteGen, a novel generative model that operates directly on the raw byte streams of LOB events. Our approach treats the problem as an autoregressive next-byte prediction task, for which we design a compact and efficient 32-byte packed binary format to represent market messages without information loss. The core novelty of our work is the complete elimination of feature engineering and tokenization, enabling the model to learn market dynamics from its most fundamental representation. We achieve this by adapting the H-Net architecture, a hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to discover the inherent structure of market messages without predefined rules. Our primary contributions are: 1) the first end-to-end, byte-level framework for LOB modeling; 2) an efficient packed data representation; and 3) a comprehensive evaluation on high-frequency data. Trained on over 34 million events from CME Bitcoin futures, ByteGen successfully reproduces key stylized facts of financial markets, generating realistic price distributions, heavy-tailed returns, and bursty event timing. Our findings demonstrate that learning directly from byte space is a promising and highly flexible paradigm for modeling complex financial systems, achieving competitive performance on standard market quality metrics without the biases of tokenization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Continuous-Time MILP for Integrated Aircraft Hangar Scheduling and Layout</title>
<link>https://arxiv.org/abs/2508.02640</link>
<guid>https://arxiv.org/abs/2508.02640</guid>
<content:encoded><![CDATA[
<div> continuous-time mixed-integer linear programming, aircraft maintenance hangars, aircraft scheduling, spatial allocation, scalability limitations <br />
<br />
Summary: 
This paper presents a novel continuous-time mixed-integer linear programming model for efficient management of aircraft maintenance hangars. The model addresses the complex decisions involved in aircraft scheduling and spatial allocation, overcoming the scalability limitations of traditional approaches by treating time as a continuous variable. Benchmarking against a heuristic shows the model's superior performance, solving instances with up to 25 aircraft quickly and delivering high-quality solutions for cases with up to 40 aircraft. The model's economic benefits and managerial insights are highlighted, with solutions consistently outperforming the heuristic. A custom-built visualization dashboard demonstrates the practical applicability of the model, showcasing its ability to provide optimized solutions quickly and effectively. <div>
arXiv:2508.02640v1 Announce Type: cross 
Abstract: Efficient management of aircraft maintenance hangars is a critical operational challenge, involving complex, interdependent decisions regarding aircraft scheduling and spatial allocation. This paper introduces a novel continuous-time mixed-integer linear programming (MILP) model to solve this integrated spatio-temporal problem. By treating time as a continuous variable, our formulation overcomes the scalability limitations of traditional discrete-time approaches. The performance of the exact model is benchmarked against a constructive heuristic, and its practical applicability is demonstrated through a custom-built visualization dashboard. Computational results are compelling: the model solves instances with up to 25 aircraft to proven optimality, often in mere seconds, and for large-scale cases of up to 40 aircraft, delivers high-quality solutions within known optimality gaps. In all tested scenarios, the resulting solutions consistently and significantly outperform the heuristic, which highlights the framework's substantial economic benefits and provides valuable managerial insights into the trade-off between solution time and optimality.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A locking-free isogeometric thin shell formulation based on higher order accurate diagonalized strain projection via approximate dual splines</title>
<link>https://arxiv.org/abs/2406.16685</link>
<guid>https://arxiv.org/abs/2406.16685</guid>
<content:encoded><![CDATA[
<div> spline basis functions, Kirchhoff-Love shell formulation, membrane locking, Hellinger-Reissner variational principle, isogeometric discretization <br />
Summary:<br />
The article presents a novel approach for isogeometric discretization of the Kirchhoff-Love shell formulation, addressing membrane locking issues. Independent strains are discretized using spline basis functions one degree lower than displacements to enhance accuracy. Variations of strains are discretized using approximate dual splines to obtain a projection matrix that is diagonalized through row-sum lumping for efficient condensation. This diagonalization simplifies static condensation of strain fields without the need for matrix inversion, maintaining higher-order accuracy with optimal convergence rates. Numerical benchmarks, such as a curved Euler-Bernoulli beam and shell obstacle course examples, demonstrate the approach's numerical properties and performance. <br /> 
Summary: <div>
arXiv:2406.16685v4 Announce Type: replace 
Abstract: We present a novel isogeometric discretization approach for the Kirchhoff-Love shell formulation based on the Hellinger-Reissner variational principle. For mitigating membrane locking, we discretize the independent strains with spline basis functions that are one degree lower than those used for the displacements. To enable computationally efficient condensation of the independent strains, we first discretize the variations of the independent strains with approximate dual splines to obtain a projection matrix that is close to a diagonal matrix. We then diagonalize this strain projection matrix via row-sum lumping. Due to this diagonalization, the static condensation of the independent strain fields becomes computationally inexpensive, as no matrix needs to be inverted. At the same time, our approach maintains higher-order accuracy at optimal rates of convergence. We illustrate the numerical properties and the performance of our approach through numerical benchmarks, including a curved Euler-Bernoulli beam and the examples of the shell obstacle course.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADformer: A Multi-Granularity Spatial-Temporal Transformer for EEG-Based Alzheimer Detection</title>
<link>https://arxiv.org/abs/2409.00032</link>
<guid>https://arxiv.org/abs/2409.00032</guid>
<content:encoded><![CDATA[
<div> EEG, Alzheimer's Disease, ADformer, transformer, spatial-temporal<br />
<br />
Summary: <br />
The article introduces ADformer, a novel spatial-temporal transformer for EEG-based Alzheimer's Disease (AD) detection. Existing approaches for AD detection often suffer from information loss and limited generalizability due to manual feature engineering. ADformer addresses these challenges by capturing both temporal and spatial features from raw EEG signals, enabling end-to-end representation learning. It incorporates multi-granularity embedding strategies and a two-stage intra-inter granularity self-attention mechanism to learn local patterns and global dependencies. ADformer is evaluated on 4 large-scale datasets with 1,713 subjects, achieving superior performance in distinguishing AD from healthy control subjects. The model outperforms existing methods with subject-level F1 scores of 92.82%, 89.83%, 67.99%, and 83.98% on the 4 datasets, showcasing its effectiveness for EEG-based AD detection. <div>
arXiv:2409.00032v2 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) has emerged as a cost-effective and efficient tool to support neurologists in the detection of Alzheimer's Disease (AD). However, most existing approaches rely heavily on manual feature engineering or data transformation. While such techniques may provide benefits when working with small-scale datasets, they often lead to information loss and distortion when applied to large-scale data, ultimately limiting model performance. Moreover, the limited subject scale and demographic diversity of datasets used in prior studies hinder comprehensive evaluation of model robustness and generalizability, thus restricting their applicability in real-world clinical settings. To address these challenges, we propose ADformer, a novel multi-granularity spatial-temporal transformer designed to capture both temporal and spatial features from raw EEG signals, enabling effective end-to-end representation learning. Our model introduces multi-granularity embedding strategies across both spatial and temporal dimensions, leveraging a two-stage intra-inter granularity self-attention mechanism to learn both local patterns within each granularity and global dependencies across granularities. We evaluate ADformer on 4 large-scale datasets comprising a total of 1,713 subjects, representing one of the largest corpora for EEG-based AD detection to date, under a cross-validated, subject-independent setting. Experimental results demonstrate that ADformer consistently outperforms existing methods, achieving subject-level F1 scores of 92.82%, 89.83%, 67.99%, and 83.98% on the 4 datasets, respectively, in distinguishing AD from healthy control (HC) subjects.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Tick-Size Too Small: A General Method for Modelling Small Tick Limit Order Books</title>
<link>https://arxiv.org/abs/2410.08744</link>
<guid>https://arxiv.org/abs/2410.08744</guid>
<content:encoded><![CDATA[
<div> tick-sizes, market agents behavior, Limit Order Book, Hawkes Process model, stylized facts<br />
<br />Summary:
This study examines the impact of tick-sizes on the microstructural properties of assets in the market. By analyzing a variety of assets with different tick-sizes, the researchers identify distinct stylized facts that differentiate between large, medium, and small-tick assets. They propose a Hawkes Process model that effectively captures the characteristics of different tick-size assets, including sparsity, multi-tick level price moves, and the shape of the Limit Order Book. Through simulations, they demonstrate the model's versatility and its ability to transition between large and small-tick assets based on key variables. The study also assesses the model's assumptions, highlights challenges, and suggests potential directions for future research in this area. <div>
arXiv:2410.08744v3 Announce Type: replace-cross 
Abstract: Tick-sizes not only influence the granularity of the price formation process but also affect market agents' behavior. We investigate the disparity in the microstructural properties of the Limit Order Book (LOB) across a basket of assets with different relative tick-sizes. A key contribution of this study is the identification of several stylized facts, which are used to differentiate between large, medium, and small-tick assets, along with clear metrics for their measurement. We provide cross-asset visualizations to illustrate how these attributes vary with relative tick-size. Further, we propose a Hawkes Process model that {\color{black}not only fits well for large-tick assets, but also accounts for }sparsity, multi-tick level price moves, and the shape of the LOB in small-tick assets. Through simulation studies, we demonstrate the {\color{black} versatility} of the model and identify key variables that determine whether a simulated LOB resembles a large-tick or small-tick asset. Our tests show that stylized facts like sparsity, shape, and relative returns distribution can be smoothly transitioned from a large-tick to a small-tick asset using our model. We test this model's assumptions, showcase its challenges and propose questions for further directions in this area of research.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Operator Networks for Bayesian Parameter Estimation in PDEs</title>
<link>https://arxiv.org/abs/2501.10684</link>
<guid>https://arxiv.org/abs/2501.10684</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Operator Networks, Physics-Informed Neural Networks, partial differential equations, uncertainty quantification, surrogate modeling

Summary: 

Deep Operator Networks (DeepONets) and Physics-Informed Neural Networks (PINNs) are combined to solve partial differential equations (PDEs) and estimate their parameters. The framework integrates data-driven learning with physical constraints, achieving robust and accurate solutions in various scenarios. Bayesian training via variational inference enables comprehensive uncertainty quantification for aleatoric and epistemic uncertainties, ensuring reliable predictions even in noisy conditions or when governing equations are missing. This approach proves effective in solving forward and inverse problems, including the 1D unsteady heat equation and 2D reaction-diffusion equations, as well as regression tasks with sparse, noisy observations. The framework offers a computationally efficient and generalizable method for addressing uncertainty quantification in PDE surrogate modeling.<br /><br />Summary: <div>
arXiv:2501.10684v2 Announce Type: replace-cross 
Abstract: We present a novel framework combining Deep Operator Networks (DeepONets) with Physics-Informed Neural Networks (PINNs) to solve partial differential equations (PDEs) and estimate their unknown parameters. By integrating data-driven learning with physical constraints, our method achieves robust and accurate solutions across diverse scenarios. Bayesian training is implemented through variational inference, allowing for comprehensive uncertainty quantification for both aleatoric and epistemic uncertainties. This ensures reliable predictions and parameter estimates even in noisy conditions or when some of the physical equations governing the problem are missing. The framework demonstrates its efficacy in solving forward and inverse problems, including the 1D unsteady heat equation and 2D reaction-diffusion equations, as well as regression tasks with sparse, noisy observations. This approach provides a computationally efficient and generalizable method for addressing uncertainty quantification in PDE surrogate modeling.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Finite Element Approach for Simulating Dynamic Crack Growth in Cu/Ultra Low-k Interconnect Structures</title>
<link>https://arxiv.org/abs/2508.00193</link>
<guid>https://arxiv.org/abs/2508.00193</guid>
<content:encoded><![CDATA[
<div> Keywords: finite element modeling, dynamic crack propagation, Crack Element Method, Edge-based Smoothed Finite Element Method, fracture energy release rate <br />
Summary: 
The article introduces the Crack Element Method (CEM) for simulating dynamic crack propagation in 2D structures. CEM utilizes an element-splitting algorithm based on the Edge-based Smoothed Finite Element Method (ES-FEM) to capture crack growth while minimizing poorly shaped elements. A fracture energy release rate formulation is developed using split element topology. Validation on benchmark problems confirms accuracy and robustness. A case study on patterned Cu/Ultra Low-k interconnect structures showcases CEM's applicability.<br /><br />Summary: <div>
arXiv:2508.00193v1 Announce Type: new 
Abstract: This work presents a practical finite element modeling strategy, the Crack Element Method (CEM), for simulating the dynamic crack propagation in two-dimensional structures. The method employs an element-splitting algorithm based on the Edge-based Smoothed Finite Element Method (ES-FEM) to capture the element-wise crack growth while reducing the formation of poorly shaped elements that can compromise numerical accuracy and computational performance. A fracture energy release rate formulation is also developed based on the evolving topology of the split elements. The proposed approach is validated through a series of classical benchmark problems, demonstrating its accuracy and robustness in addressing dynamic fracture scenarios. Finally, the applicability of the CEM is illustrated in a case study involving patterned Cu/Ultra Low-k interconnect structures.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeightFlow: Learning Stochastic Dynamics via Evolving Weight of Neural Network</title>
<link>https://arxiv.org/abs/2508.00451</link>
<guid>https://arxiv.org/abs/2508.00451</guid>
<content:encoded><![CDATA[
<div> Keywords: stochastic dynamics, neural network, weight space, optimal transport, high-dimensional

Summary: 
This article introduces a novel approach, WeightFlow, to model stochastic dynamics directly in the weight space of a neural network. By projecting the evolving probability distribution onto the weight space, WeightFlow connects dynamic optimal transport in measure space to an energy functional in weight space. The neural network weights are constructed into a graph, and their evolution is learned through a graph-controlled differential equation. Experimental results on interdisciplinary datasets show that WeightFlow outperforms state-of-the-art methods by an average of 43.02%. This approach provides an effective and scalable solution for modeling high-dimensional stochastic dynamics. <div>
arXiv:2508.00451v1 Announce Type: new 
Abstract: Modeling stochastic dynamics from discrete observations is a key interdisciplinary challenge. Existing methods often fail to estimate the continuous evolution of probability densities from trajectories or face the curse of dimensionality. To address these limitations, we presents a novel paradigm: modeling dynamics directly in the weight space of a neural network by projecting the evolving probability distribution. We first theoretically establish the connection between dynamic optimal transport in measure space and an equivalent energy functional in weight space. Subsequently, we design WeightFlow, which constructs the neural network weights into a graph and learns its evolution via a graph controlled differential equation. Experiments on interdisciplinary datasets demonstrate that WeightFlow improves performance by an average of 43.02\% over state-of-the-art methods, providing an effective and scalable solution for modeling high-dimensional stochastic dynamics.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources</title>
<link>https://arxiv.org/abs/2508.00654</link>
<guid>https://arxiv.org/abs/2508.00654</guid>
<content:encoded><![CDATA[
<div> management, microscopy, data integration, interoperability, web-based platform
<br />
Summary:<br />
- Managing and integrating large volumes of microscopy data stored across different platforms is a challenge in research. 
- Data types such as bioimages, experimental records, and spectral information are often stored separately, hindering data linkage and alignment with FAIR data management principles. 
- The lack of tools for effectively integrating heterogeneous data sources prompted the development of LEO, a web-based platform. 
- LEO initially linked Electronic Lab Notebooks (ELNs) with OMERO but can now integrate other data sources through a plugin-based architecture. 
- LEO's extensibility makes it a scalable and flexible solution for various microscopy research workflows. 
<br />Summary: <div>
arXiv:2508.00654v1 Announce Type: new 
Abstract: In the interdisciplinary field of microscopy research, managing and integrating large volumes of data stored across disparate platforms remains a major challenge. Data types such as bioimages, experimental records, and spectral information are often maintained in separate repositories, each following different management standards. However, linking these data sources across the research lifecycle is essential to align with the FAIR principles of data management: Findability, Accessibility, Interoperability, and Reusability. Despite this need, there is a notable lack of tools capable of effectively integrating and linking data from heterogeneous sources. To address this gap, we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based platform designed to create and manage links between distributed data systems. LEO was initially developed to link objects between Electronic Lab Notebooks (ELNs) and OMERO, but its functionality has since been extended through a plugin-based architecture, allowing the integration of additional data sources. This extensibility makes LEO a scalable and flexible solution for a wide range of microscopy research workflows.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contact Sensors to Remote Cameras: Quantifying Cardiorespiratory Coupling in High-Altitude Exercise Recovery</title>
<link>https://arxiv.org/abs/2508.00773</link>
<guid>https://arxiv.org/abs/2508.00773</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiorespiratory coupling, high altitude, exercise, remote photoplethysmography, autonomic regulation

Summary: 
Cardiorespiratory coupling (CRC) is the dynamic interaction between the heart and lungs, which is enhanced during physical exercise and associated with improved physiological function. A study examined CRC at high altitudes during rest and post-exercise recovery, revealing significant differences. The analysis showed that recovery involved more frequent yet less stable synchronization between breathing and pulse. The feasibility of non-contact CRC measurement using remote photoplethysmography (rPPG) was explored, showing a strong correlation with oximeter-based metrics. These findings suggest that CRC could serve as a sensitive marker for autonomic regulation and have potential applications in contactless monitoring. Source code for the study is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2508.00773v1 Announce Type: new 
Abstract: Cardiorespiratory coupling (CRC) captures the dynamic interaction between the cardiac and respiratory systems--an interaction strengthened by physical exercise and linked to improved physiological function. We examined CRC at high altitude in two states, rest and post-exercise recovery, and found significant differences (p < 0.05). Quantitative analysis revealed that recovery involved more frequent yet less stable episodes of synchronization between respiration and pulse. Furthermore, we explored the feasibility of non-contact CRC measurement with remote photoplethysmography (rPPG), observing a strong correlation with oximeter-based metrics (Pearson r = 0.96). These findings highlight the potential of CRC as a sensitive marker for autonomic regulation and its future application in contactless monitoring. Source code is available at GitHub: https://github.com/McJackTang/CRC.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>{\tau}-Ring: A Smart Ring Platform for Multimodal Physiological and Behavioral Sensing</title>
<link>https://arxiv.org/abs/2508.00778</link>
<guid>https://arxiv.org/abs/2508.00778</guid>
<content:encoded><![CDATA[
<div> Keyword: Smart rings, wearable, physiological sensing, open-source, reproducible <br />
<br />
Summary: 
The article introduces the {\tau}-Ring platform, designed to address the limitations of proprietary smart rings for continuous physiological and behavioral sensing. The platform offers accessible hardware with multi-channel PPG, IMU, temperature sensing, NFC, and on-board storage. It features adjustable firmware for quick reconfiguration of settings, allowing researchers to customize sampling rates, power modes, and wireless protocols. The platform also includes an open-source Android software suite for real-time streaming and offline logging. These capabilities enable easy acquisition of rich datasets, facilitating research prototyping and standardization. The platform is validated through studies in heart-rate monitoring and ring-based handwriting recognition. Overall, {\tau}-Ring provides a commercial-ready solution that promotes reproducibility in wearable research and accelerates innovation in physiological and behavioral sensing technologies. Source code is available on GitHub for reference and collaboration. <br /><br />Summary: <div>
arXiv:2508.00778v1 Announce Type: new 
Abstract: Smart rings have emerged as uniquely convenient devices for continuous physiological and behavioral sensing, offering unobtrusive, constant access to metrics such as heart rate, motion, and skin temperature. Yet most commercial solutions remain proprietary, hindering reproducibility and slowing innovation in wearable research. We introduce {\tau}-Ring, a commercial-ready platform that bridges this gap through: (i) accessible hardware combining time-synchronized multi-channel PPG, 6-axis IMU, temperature sensing, NFC, and on-board storage; (ii) adjustable firmware that lets researchers rapidly reconfigure sampling rates, power modes, and wireless protocols; and (iii) a fully open-source Android software suite that supports both real-time streaming and 8-hour offline logging. Together, these features enable out-of-the-box, reproducible acquisition of rich physiological and behavioral datasets, accelerating prototyping and standardizing experimentation. We validate the platform with demonstration studies in heart-rate monitoring and ring-based handwriting recognition. Source code is available at GitHub: https://github.com/thuhci/OpenRing.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models</title>
<link>https://arxiv.org/abs/2508.00804</link>
<guid>https://arxiv.org/abs/2508.00804</guid>
<content:encoded><![CDATA[
<div> structured state space models, real-time recurrent learning, online adaptation, linear-recurrent-unit, prediction error 

Summary: 
This paper presents a novel approach for enhancing the predictions of structured state space models (SSMs) using real-time recurrent learning during inference. SSMs are traditionally trained offline and do not adapt to new data during deployment. The proposed method enables online adaptation by continuously updating model parameters based on incoming data. The study evaluated this approach using a small carbon emission dataset from embedded automotive hardware and found that it consistently reduced prediction error during inference. This highlights the potential of the method in dynamic and resource-constrained environments. The use of linear-recurrent-unit SSMs demonstrated the effectiveness of the approach in improving prediction accuracy in real-time scenarios. <div>
arXiv:2508.00804v1 Announce Type: new 
Abstract: This paper introduces a new approach for fine-tuning the predictions of structured state space models (SSMs) at inference time using real-time recurrent learning. While SSMs are known for their efficiency and long-range modeling capabilities, they are typically trained offline and remain static during deployment. Our method enables online adaptation by continuously updating model parameters in response to incoming data. We evaluate our approach for linear-recurrent-unit SSMs using a small carbon emission dataset collected from embedded automotive hardware. Experimental results show that our method consistently reduces prediction error online during inference, demonstrating its potential for dynamic, resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak Values as Geometric Lenses: Deformations of Hilbert Space and the Emergence of superoscillations</title>
<link>https://arxiv.org/abs/2508.00023</link>
<guid>https://arxiv.org/abs/2508.00023</guid>
<content:encoded><![CDATA[
<div> weak measurement, quantum mechanics, signal processing, superoscillations, geometric structure <br />
Summary: 
This paper explores the relationship between weak measurement in quantum mechanics and superoscillations, showing that superoscillations are a natural consequence of the geometric structure underlying weak values. The weak value is described as a ratio of geometric deformation, representing how an observable transforms Hilbert space relative to the standard inner product. This deformation warps quantum states locally, producing oscillations that exceed the global Fourier bandwidth. The weak value is interpreted as a comparison between a deformed sesquilinear form and the standard one, revealing connections to generalized Rayleigh quotients and projective geometry of quantum states. This perspective unifies weak values and superoscillations as manifestations of a single geometric principle. <div>
arXiv:2508.00023v1 Announce Type: cross 
Abstract: The formalism of weak measurement in quantum mechanics has revealed profound connections between measurement theory, quantum foundations, and signal processing. In this paper, we develop a pointer-free derivation of superoscillations, demonstrating that they are a natural and necessary consequence of the geometric structure underlying weak values. We argue that the weak value is best understood as a ratio of geometric deformation, quantifying how an observable transforms the structure of Hilbert space relative to a reference provided by the standard inner product. This deformation acts as a conceptual lens, warping the local structure of quantum states to produce oscillations far exceeding the global Fourier bandwidth. We formalize this by interpreting the weak value as a comparison between a deformed sesquilinear form and the standard one, and explore its deep connections to generalized Rayleigh quotients and the projective geometry of quantum states. This perspective unifies weak values and superoscillations as two facets of a single underlying geometric principle.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis</title>
<link>https://arxiv.org/abs/2508.00381</link>
<guid>https://arxiv.org/abs/2508.00381</guid>
<content:encoded><![CDATA[
<div> marine environment, welding defect detection, neural networks, interpretability, offshore environment
<br />
Summary:<br />
The paper introduces "Adapt-WeldNet", an adaptive framework for welding defect detection in marine and offshore environments. It evaluates pre-trained architectures, transfer learning strategies, and adaptive optimizers to optimize defect detection. A Defect Detection Interpretability Analysis (DDIA) framework enhances system transparency through Explainable AI techniques and domain-specific evaluations by NDE Level II professionals. The Human-in-the-Loop (HITL) approach and Trustworthy AI principles ensure reliability and accountability. By improving performance and interpretability, the system enhances trust, safety, and reliability of welding defect detection, supporting critical operations in challenging environments. <div>
arXiv:2508.00381v1 Announce Type: cross 
Abstract: Weld defect detection is crucial for ensuring the safety and reliability of piping systems in the oil and gas industry, especially in challenging marine and offshore environments. Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment. To address these challenges, this paper introduces ``Adapt-WeldNet", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation. By improving both performance and interpretability, this work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models</title>
<link>https://arxiv.org/abs/2508.00383</link>
<guid>https://arxiv.org/abs/2508.00383</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial transcriptomics, Precision oncology, Vision foundation models, State space models, Colorectal cancer

Summary:
Spatial transcriptomics is a valuable tool for predicting treatment responses in oncology, but its high cost and complexity hinder clinical adoption. Current vision foundation models (VFMs) based on ViT backbones struggle to meet clinical standards. To address this, a hybrid backbone architecture named MVHybrid, combining state space models with ViT, is proposed. Pretrained on colorectal cancer datasets using self-supervised learning, MVHybrid outperforms ViT in predicting gene expression and exhibits superior robustness in leave-one-study-out evaluation. It also performs well in classification, patch retrieval, and survival prediction tasks, showing promise as a next-generation pathology VFM backbone. The code is publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2508.00383v1 Announce Type: cross 
Abstract: Spatial transcriptomics reveals gene expression patterns within tissue context, enabling precision oncology applications such as treatment response prediction, but its high cost and technical complexity limit clinical adoption. Predicting spatial gene expression (biomarkers) from routine histopathology images offers a practical alternative, yet current vision foundation models (VFMs) in pathology based on Vision Transformer (ViT) backbones perform below clinical standards. Given that VFMs are already trained on millions of diverse whole slide images, we hypothesize that architectural innovations beyond ViTs may better capture the low-frequency, subtle morphological patterns correlating with molecular phenotypes. By demonstrating that state space models initialized with negative real eigenvalues exhibit strong low-frequency bias, we introduce $MV_{Hybrid}$, a hybrid backbone architecture combining state space models (SSMs) with ViT. We compare five other different backbone architectures for pathology VFMs, all pretrained on identical colorectal cancer datasets using the DINOv2 self-supervised learning method. We evaluate all pretrained models using both random split and leave-one-study-out (LOSO) settings of the same biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher correlation than the best-performing ViT and shows 43% smaller performance degradation compared to random split in gene expression prediction, demonstrating superior performance and robustness, respectively. Furthermore, $MV_{Hybrid}$ shows equal or better downstream performance in classification, patch retrieval, and survival prediction tasks compared to that of ViT, showing its promise as a next-generation pathology VFM backbone. Our code is publicly available at: https://github.com/deepnoid-ai/MVHybrid.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The time slot allocation problem in liberalised passenger railway markets: a multi-objective approach</title>
<link>https://arxiv.org/abs/2401.12073</link>
<guid>https://arxiv.org/abs/2401.12073</guid>
<content:encoded><![CDATA[
<div> competition, railway market, time slot allocation, multi-objective model, market equilibrium
<br />
Summary:
The article discusses the time slot allocation problem in the European passenger railway market post-liberalization. The Infrastructure Manager assesses bids and allocates resources to Railway Undertakings, influencing market equilibrium. A multi-objective model is proposed for time slot allocation, with the Infrastructure Manager choosing a point from the Pareto front. Two selection criteria are suggested: one based on priorities for time slot allocation to companies, and the other introducing fairness to incentivize competition. The impact of these rules on market equilibrium was evaluated on a high-speed corridor in the Spanish railway network. <div>
arXiv:2401.12073v2 Announce Type: replace 
Abstract: The liberalisation of the European passenger railway markets through the European Directive EU 91/440/EEC states a new scenario where different Railway Undertakings compete with each other in a bidding process for time slots. The infrastructure resources are provided by the Infrastructure Manager, who analyses and assesses the bids received, allocating the resources to each Railway Undertaking. Time slot allocation is a fact that drastically influences the market equilibrium. In this paper, we address the time slot allocation problem within the context of a liberalized passenger railway market as a multi-objective model. The Infrastructure Manager is tasked with selecting a point from the Pareto front as the solution to the time slot allocation problem. We propose two criteria for making this selection: the first one allocates time slots to each company according to a set of priorities, while the second one introduces a criterion of fairness in the treatment of companies to incentive competition. The assessment of the impact of these rules on market equilibrium has been conducted on a liberalized high-speed corridor within the Spanish railway network.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-physics Model of Flow from Coronary Angiography: Insights to Microvascular Function</title>
<link>https://arxiv.org/abs/2412.04798</link>
<guid>https://arxiv.org/abs/2412.04798</guid>
<content:encoded><![CDATA[
<div> coronary microvascular dysfunction, computational fluid dynamics, angiography, contrast intensity profile, lumped parameter model<br />
Summary:<br />
Coronary Microvascular Dysfunction (CMD) causes impaired vasodilation and insufficient blood flow to the myocardium during stress. Invasive wire-based diagnosis techniques like index of microcirculatory resistance (IMR) and coronary flow reserve (CFR) are underutilized due to complexity. A 3D-0D coupled multi-physics computational fluid dynamics (CFD) model was developed to simulate contrast injection during angiography. A contrast intensity profile (CIP) was introduced to describe angiography data dynamics. Sensitivity studies showed that resistance impacts CIP slopes more than capacitance, with higher resistance amplifying the effect. The model offers a tool for interpreting angiographic data, potentially transforming the understanding and utilization of coronary angiography in diagnosing CMD. <br />Summary: <div>
arXiv:2412.04798v2 Announce Type: replace 
Abstract: Coronary Microvascular Dysfunction (CMD) is characterized by impaired vasodilation and can lead to insufficient blood flow to the myocardium during stress or exertion, affecting millions of people globally. Despite their diagnostic value, invasive, wire-based diagnosis techniques of CMD, such as index of microcirculatory resistance (IMR) and coronary flow reserve (CFR), are underutilized due to their complexity and inconsistency. Coronary angiography, one of the most commonly used imaging modalities, offers valuable flow information that assists in diagnosing CMD. However, this information is not fully understood or utilized in current clinical practice. In this study, a 3D-0D coupled multi-physics computational fluid dynamics (CFD) model was developed and calibrated to simulate and study the process of contrast injection and washout during clinical angiography. A contrast intensity profile (CIP) was introduced to describe the dynamics of coronary angiography data. Additionally, sensitivity studies were conducted to evaluate the influence of various coronary lumped parameter model (LPM) parameters on the shapes of CIPs. The results demonstrate that the multi-physics model can be effectively calibrated to produce physiologically meaningful hemodynamic results. Sensitivity studies reveal that resistance has a greater impact on the rising and falling slopes of CIP than capacitance, with higher resistance amplifying this effect. The model and results are presented here. These results are potentially transformative, as they provide a tool for interpreting angiographic data and ultimately extracting information concerning coronary microcirculation.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PATH: A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly Detection Approaches for Multivariate Time Series</title>
<link>https://arxiv.org/abs/2411.13951</link>
<guid>https://arxiv.org/abs/2411.13951</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, multivariate time series, automotive powertrain, semi-supervised learning, threshold selection <br />
Summary: <br />
- Benchmarking anomaly detection approaches for multivariate time series is challenging due to the lack of high-quality datasets.
- A diverse, extensive, and non-trivial dataset generated via simulation tools reflecting realistic behavior of an automotive powertrain is proposed.
- The dataset represents a discrete-sequence problem not addressed by previous literature.
- Different versions of the dataset are provided for unsupervised and semi-supervised anomaly detection settings, time series generation, and forecasting.
- Baseline results from deterministic and variational autoencoders, as well as a non-parametric approach, show the importance of robustness to contaminated training data and the influence of the threshold on detection performance. Further work is needed to improve threshold selection methods without requiring labeled data. <br /> <div>
arXiv:2411.13951v5 Announce Type: replace-cross 
Abstract: Benchmarking anomaly detection approaches for multivariate time series is a challenging task due to a lack of high-quality datasets. Current publicly available datasets are too small, not diverse and feature trivial anomalies, which hinders measurable progress in this research area. We propose a solution: a diverse, extensive, and non-trivial dataset generated via state-of-the-art simulation tools that reflects realistic behaviour of an automotive powertrain, including its multivariate, dynamic and variable-state properties. Additionally, our dataset represents a discrete-sequence problem, which remains unaddressed by previously-proposed solutions in literature. To cater for both unsupervised and semi-supervised anomaly detection settings, as well as time series generation and forecasting, we make different versions of the dataset available, where training and test subsets are offered in contaminated and clean versions, depending on the task. We also provide baseline results from a selection of approaches based on deterministic and variational autoencoders, as well as a non-parametric approach. As expected, the baseline experimentation shows that the approaches trained on the semi-supervised version of the dataset outperform their unsupervised counterparts, highlighting a need for approaches more robust to contaminated training data. Furthermore, results show that the threshold used can have a large influence on detection performance, hence more work needs to be invested in methods to find a suitable threshold without the need for labelled data.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Axioms for Model Fidelity Evaluation</title>
<link>https://arxiv.org/abs/2507.23020</link>
<guid>https://arxiv.org/abs/2507.23020</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital engineering, model fidelity, simulation, evaluation framework, ground vehicle model

Summary:
Digital engineering has revolutionized the design process, with simulations playing a crucial role in providing information consistent with reality. The concept of model fidelity, which refers to the similarity between a simulation and reality, is essential in this context. However, existing definitions of model fidelity lack formal rigor, leading to ambiguity in evaluation processes. This paper introduces seven axioms to guide the development of future fidelity evaluation frameworks. By applying these axioms to a ground vehicle model, the study demonstrates their practicality. The axioms serve as a foundation for potential advancements in evaluating model fidelity and can be used as reference points for future research in this area. This work highlights the importance of ensuring the accuracy and reliability of simulations in digital engineering practices. 

<br /><br />Summary: <div>
arXiv:2507.23020v1 Announce Type: new 
Abstract: Digital engineering has transformed the design and development process. However, the utility of digital engineering is fundamentally dependent on the assumption that a simulation provides information consistent with reality. This relationship is described as model fidelity. Despite the widespread use of the term, existing definitions of model fidelity often lack formal rigor in practical application, which leaves ambiguity in how this similarity should be evaluated. This paper presents seven fundamental axioms to aid the development of future fidelity evaluation frameworks. An example of a ground vehicle model is used under an existing fidelity evaluation framework to observe the applicability of these axioms. In addition, these axioms are used as a reference point for considering future opportunities in future work related to model fidelity.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information Bottleneck Asset Pricing Model</title>
<link>https://arxiv.org/abs/2507.23218</link>
<guid>https://arxiv.org/abs/2507.23218</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep neural networks, financial asset pricing, information bottleneck, mutual information, nonlinear relationships 

Summary: 
Deep neural networks (DNNs) are often used in financial asset pricing due to their ability to model complex nonlinear relationships in financial data. However, these models can over-fit to noise in the data, leading to subpar performance. To combat this issue, a new information bottleneck asset pricing model is proposed. This model compresses data with low signal-to-noise ratios by eliminating redundant information while retaining critical information for asset pricing. By imposing constraints of mutual information during the nonlinear mapping process, the model progressively reduces the mutual information between input data and compressed representation while increasing mutual information between the compressed representation and output prediction. This approach ensures that irrelevant information (noise) is filtered out during the modeling of financial nonlinear relationships, ultimately improving asset pricing accuracy. <div>
arXiv:2507.23218v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) have garnered significant attention in financial asset pricing, due to their strong capacity for modeling complex nonlinear relationships within financial data. However, sophisticated models are prone to over-fitting to the noise information in financial data, resulting in inferior performance. To address this issue, we propose an information bottleneck asset pricing model that compresses data with low signal-to-noise ratios to eliminate redundant information and retain the critical information for asset pricing. Our model imposes constraints of mutual information during the nonlinear mapping process. Specifically, we progressively reduce the mutual information between the input data and the compressed representation while increasing the mutual information between the compressed representation and the output prediction. The design ensures that irrelevant information, which is essentially the noise in the data, is forgotten during the modeling of financial nonlinear relationships without affecting the final asset pricing. By leveraging the constraints of the Information bottleneck, our model not only harnesses the nonlinear modeling capabilities of deep networks to capture the intricate relationships within financial data but also ensures that noise information is filtered out during the information compression process.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adjoint-Based Aerodynamic Shape Optimization with a Manifold Constraint Learned by Diffusion Models</title>
<link>https://arxiv.org/abs/2507.23443</link>
<guid>https://arxiv.org/abs/2507.23443</guid>
<content:encoded><![CDATA[
<div> shape optimization, aerodynamics, adjoint method, diffusion model, automatic differentiation

Summary:
This article presents a novel approach to aerodynamic shape optimization using an adjoint-based framework. The framework incorporates a diffusion model trained on existing designs to learn a smooth manifold of aerodynamically viable shapes, which is enforced as an equality constraint in the optimization problem. By computing adjoint gradients of design objectives with respect to the manifold space, the method eliminates the need for ad hoc parameter tuning and variable scaling. The framework integrates seamlessly into existing adjoint-based design workflows with minimal modification and demonstrates superior aerodynamic performance compared to conventional approaches in transonic RANS airfoil design cases. By combining AI-generated priors with adjoint methods, this approach enables robust, high-fidelity aerodynamic shape optimization through automatic differentiation. <div>
arXiv:2507.23443v1 Announce Type: new 
Abstract: We introduce an adjoint-based aerodynamic shape optimization framework that integrates a diffusion model trained on existing designs to learn a smooth manifold of aerodynamically viable shapes. This manifold is enforced as an equality constraint to the shape optimization problem. Central to our method is the computation of adjoint gradients of the design objectives (e.g., drag and lift) with respect to the manifold space. These gradients are derived by first computing shape derivatives with respect to conventional shape design parameters (e.g., Hicks-Henne parameters) and then backpropagating them through the diffusion model to its latent space via automatic differentiation. Our framework preserves mathematical rigor and can be integrated into existing adjoint-based design workflows with minimal modification. Demonstrated on extensive transonic RANS airfoil design cases using off-the-shelf and general-purpose nonlinear optimizers, our approach eliminates ad hoc parameter tuning and variable scaling, maintains robustness across initialization and optimizer choices, and achieves superior aerodynamic performance compared to conventional approaches. This work establishes how AI generated priors integrates effectively with adjoint methods to enable robust, high-fidelity aerodynamic shape optimization through automatic differentiation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis</title>
<link>https://arxiv.org/abs/2507.22936</link>
<guid>https://arxiv.org/abs/2507.22936</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Financial Natural Language Processing, Comparative Evaluation, 10-K Filings, Model Performance

Summary: 
Large Language Models (LLMs) have shown significant capabilities in various Financial Natural Language Processing (FinNLP) tasks. This study compares five leading LLMs, including GPT, Claude, Perplexity, Gemini, and DeepSeek, using 10-K filings from top technology companies. Evaluation methods include human annotation, automated metrics, and model behavior diagnostics. GPT performs best in coherence, semantic alignment, and contextual relevance, followed by Claude and Perplexity. Gemini and DeepSeek exhibit more variability and disagreement. Model outputs differ based on prompts and source material, varying across companies and over time. The study highlights the importance of careful prompt design and data selection in influencing LLM performance in financial analysis.<br /><br />Summary: <div>
arXiv:2507.22936v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide variety of Financial Natural Language Processing (FinNLP) tasks. However, systematic comparisons among widely used LLMs remain underexplored. Given the rapid advancement and growing influence of LLMs in financial analysis, this study conducts a thorough comparative evaluation of five leading LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the 'Magnificent Seven' technology companies. We create a set of domain-specific prompts and then use three methodologies to evaluate model performance: human annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics (prompt-level variance and across-model similarity). The results show that GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement. Also, the similarity and stability of outputs change from company to company and over time, showing that they are sensitive to how prompts are written and what source material is used.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Machine Learning with Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2507.22959</link>
<guid>https://arxiv.org/abs/2507.22959</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific machine learning, Kolmogorov-Arnold Networks, data encoding, interpretability, nonlinear interactions<br />
Summary:<br />
The article discusses the shift from multilayer perceptrons (MLPs) to Kolmogorov-Arnold Networks (KANs) in scientific machine learning. KANs offer enhanced interpretability, flexibility, and improved capability in capturing complex nonlinear interactions, overcoming the limitations of MLPs. The review categorizes recent progress in KAN-based models from three perspectives: data-driven learning, physics-informed modeling, and deep operator learning. It highlights the advantages of KANs in accuracy, convergence, and spectral representation compared to MLPs. The review also identifies challenges in KAN development such as computational efficiency, theoretical guarantees, hyperparameter tuning, and algorithm complexity. Future research directions focus on improving the robustness, scalability, and physical consistency of KAN-based frameworks.<br /><br /> <div>
arXiv:2507.22959v1 Announce Type: cross 
Abstract: The field of scientific machine learning, which originally utilized multilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold Networks (KANs) for data encoding. This shift is driven by the limitations of MLPs, including poor interpretability, fixed activation functions, and difficulty capturing localized or high-frequency features. KANs address these issues with enhanced interpretability and flexibility, enabling more efficient modeling of complex nonlinear interactions and effectively overcoming the constraints associated with conventional MLP architectures. This review categorizes recent progress in KAN-based models across three distinct perspectives: (i) data-driven learning, (ii) physics-informed modeling, and (iii) deep operator learning. Each perspective is examined through the lens of architectural design, training strategies, application efficacy, and comparative evaluation against MLP-based counterparts. By benchmarking KANs against MLPs, we highlight consistent improvements in accuracy, convergence, and spectral representation, clarifying KANs' advantages in capturing complex dynamics while learning more effectively. Finally, this review identifies critical challenges and open research questions in KAN development, particularly regarding computational efficiency, theoretical guarantees, hyperparameter tuning, and algorithm complexity. We also outline future research directions aimed at improving the robustness, scalability, and physical consistency of KAN-based frameworks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Readiness for Scientific AI at Scale</title>
<link>https://arxiv.org/abs/2507.23018</link>
<guid>https://arxiv.org/abs/2507.23018</guid>
<content:encoded><![CDATA[
<div> Data Readiness for AI, leadership-scale scientific datasets, foundation models, preprocessing patterns, domain-specific constraints <br />
<br />
Summary: This paper explores how Data Readiness for AI (DRAI) principles apply to leadership-scale scientific datasets for training foundation models. By analyzing workflows in climate, nuclear fusion, bio/health, and materials domains, common preprocessing patterns and domain-specific constraints are identified. A two-dimensional readiness framework is introduced, consisting of Data Readiness Levels and Data Processing Stages tailored to high performance computing environments. This framework highlights challenges in transforming scientific data for scalable AI training, particularly focusing on transformer-based generative models. By incorporating these dimensions, a conceptual maturity matrix is formed to characterize scientific data readiness and guide infrastructure development towards standardized, cross-domain support for scalable and reproducible AI applications in science. <br /> <div>
arXiv:2507.23018v1 Announce Type: cross 
Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to leadership-scale scientific datasets used to train foundation models. We analyze archetypal workflows across four representative domains - climate, nuclear fusion, bio/health, and materials - to identify common preprocessing patterns and domain-specific constraints. We introduce a two-dimensional readiness framework composed of Data Readiness Levels (raw to AI-ready) and Data Processing Stages (ingest to shard), both tailored to high performance computing (HPC) environments. This framework outlines key challenges in transforming scientific data for scalable AI training, emphasizing transformer-based generative models. Together, these dimensions form a conceptual maturity matrix that characterizes scientific data readiness and guides infrastructure development toward standardized, cross-domain support for scalable and reproducible AI for science.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution</title>
<link>https://arxiv.org/abs/2507.23600</link>
<guid>https://arxiv.org/abs/2507.23600</guid>
<content:encoded><![CDATA[
<div> matrix factorization, multivariate curve resolution, deep learning, signal unmixing, generative modeling

Summary:
This study introduces a novel energy-based deep learning solver, EB-gMCR, for signal unmixing analysis. By reformulating multivariate curve resolution as a generative process, EB-gMCR automatically determines the optimal component set for faithful data reconstruction. The solver utilizes a differentiable gating network to select active components and estimate their concentrations, achieving high accuracy even in the presence of noise. Additional chemical priors can be easily incorporated into the framework, allowing for adaptation to various instruments and domains without changing the core learning process. With the ability to handle large datasets and unknown component counts, EB-gMCR provides a practical approach to scalable signal unmixing analysis, particularly in chemical library-driven scenarios. The source code for EB-gMCR is available on GitHub for further exploration and use. <br /><br />Summary: <div>
arXiv:2507.23600v1 Announce Type: cross 
Abstract: Signal unmixing analysis decomposes data into basic patterns and is widely applied in chemical and biological research. Multivariate curve resolution (MCR), a branch of signal unmixing, separates mixed chemical signals into base patterns (components) and their concentrations, playing a key role in understanding composition. Classical MCR is typically framed as matrix factorization (MF) and requires a user-specified component count, usually unknown in real data. As dataset size or component count increases, the scalability and reliability of MF-based MCR face significant challenges. This study reformulates MCR as a generative process (gMCR), and introduces an energy-based deep learning solver, EB-gMCR, that automatically discovers the smallest component set able to reconstruct the data faithfully. EB-gMCR starts from a large candidate pool (e.g., 1024 spectra) and employs a differentiable gating network to retain only active components while estimating their concentrations. On noisy synthetic datasets containing up to 256 latent sources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count within 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near exact component estimation. Additional chemical priors, such as non-negativity or nonlinear mixing, enter as simple plug-in functions, enabling adaptation to other instruments or domains without altering the core learning process. By uniting high-capacity generative modeling and hard component selection, EB-gMCR offers a practical route to large-scale signal unmixing analysis, including chemical library-driven scenarios. The source code is available at https://github.com/b05611038/ebgmcr_solver.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An entropy-stable and kinetic energy-preserving macro-element HDG method for compressible flows</title>
<link>https://arxiv.org/abs/2507.22195</link>
<guid>https://arxiv.org/abs/2507.22195</guid>
<content:encoded><![CDATA[
<div> Efficient, robust simulation, compressible flows, high order numerical framework, macro element HDG method, turbulence simulation<br />
Summary:<br />
This paper presents a new high order numerical framework for simulating compressible flows efficiently and robustly. The approach, called macro element HDG method, embeds continuous Galerkin structure within macro-elements to reduce degrees of freedom and enable highly parallel local solves. By using entropy variables and a flux differencing approach, the method extends its robustness, making it suitable for under resolved or turbulent regimes. The formulations ensure entropy stability and kinetic energy preservation while maintaining high order accuracy. The method's performance is demonstrated on benchmark problems, showing optimal accuracy, improved robustness, and significant speedup compared to standard HDG methods. These advancements mark a significant progress in high order methods for direct numerical simulation (DNS) of compressible flows.<br /> <div>
arXiv:2507.22195v1 Announce Type: new 
Abstract: This paper introduces a high order numerical framework for efficient and robust simulation of compressible flows. To address the inefficiencies of standard hybridized discontinuous Galerkin (HDG) methods in large scale settings, we develop a macro element HDG method that reduces global and local degrees of freedom by embedding continuous Galerkin structure within macro-elements. This formulation supports matrix free implementations and enables highly parallel local solves, leading to substantial performance gains and excellent scalability on modern architectures. To enhance robustness in under resolved or turbulent regimes, we extend the method using entropy variables and a flux differencing approach to construct entropy stable and kinetic energy preserving variants. These formulations satisfy a discrete entropy inequality and improve stability without compromising high order accuracy. We demonstrate the performance of the proposed method on benchmark problems including the inviscid isentropic vortex and the Taylor Green vortex in both inviscid and turbulent regimes. Numerical results confirm optimal accuracy, improved robustness, and up to an order of magnitude speedup over standard HDG methods. These developments mark a significant advancement in high order methods for direct numerical simulation (DNS) of compressible flows.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cycles Protocol: A Peer-to-Peer Electronic Clearing System</title>
<link>https://arxiv.org/abs/2507.22309</link>
<guid>https://arxiv.org/abs/2507.22309</guid>
<content:encoded><![CDATA[
<div> Keywords: financial institutions, liquidity challenges, blockchain communities, decentralized settlement systems, Cycles <br />
<br />
Summary: Financial institutions have historically formed closed clearing clubs to address liquidity challenges, excluding many small enterprises and communities. Blockchain communities offer decentralized settlement systems, but they have yet to significantly impact the real economy. To tackle these issues, Cycles introduces an open, decentralized protocol for clearing, settlement, and issuance. It aims to help firms reduce payment inefficiencies, lower working capital costs, and access diverse assets and liquidity sources. Cycles uses a privacy-preserving multilateral settlement platform based on a graph optimization algorithm, recognizing that liquidity can be found within cycles in the payment network structure. By optimizing settlement flows to reduce debt, Cycles seeks to transform the way firms manage liquidity challenges and improve financial access for all actors in the ecosystem. <br /> <div>
arXiv:2507.22309v1 Announce Type: new 
Abstract: For centuries, financial institutions have responded to liquidity challenges by forming closed, centralized clearing clubs with strict rules and membership that allow them to collaborate on using the least money to discharge the most debt. As closed clubs, much of the general public has been excluded from participation. But the vast majority of private sector actors consists of micro or small firms that are vulnerable to late payments and generally ineligible for bank loans. This low liquidity environment often results in gridlock and leads to insolvency, and it disproportionately impacts small enterprises and communities.
  On the other hand, blockchain communities have developed open, decentralized settlement systems, along with a proliferation of store of value assets and new lending protocols, allowing anyone to permissionlessly transact and access credit. However, these protocols remain used primarily for speculative purposes, and so far have fallen short of the large-scale positive impact on the real economy prophesied by their promoters.
  We address these challenges by introducing Cycles, an open, decentralized clearing, settlement, and issuance protocol. Cycles is designed to enable firms to overcome payment inefficiencies, to reduce their working capital costs, and to leverage diverse assets and liquidity sources, including cryptocurrencies, stablecoins, and lending protocols, in service of clearing more debt with less money. Cycles solves real world liquidity challenges through a privacy-preserving multilateral settlement platform based on a graph optimization algorithm. The design is based on a core insight: liquidity resides within cycles in the payment network's structure and can be accessed via settlement flows optimized to reduce debt.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A holomorphic Kolmogorov-Arnold network framework for solving elliptic problems on arbitrary 2D domains</title>
<link>https://arxiv.org/abs/2507.22678</link>
<guid>https://arxiv.org/abs/2507.22678</guid>
<content:encoded><![CDATA[
<div> holomorphic neural networks, Physics-informed, differential equations, Kolmogorov-Arnold representation, Laurent series theory
Summary:
Physics-informed holomorphic neural networks (PIHNNs) have become efficient surrogate models for differential problems. They embed the problem structure into the network, needing training only for boundary conditions. Introduction of a new holomorphic network architecture, PIHKAN, based on Kolmogorov-Arnold representation, improves accuracy with reduced complexity. Mathematical extensions broaden PIHNNs' applicability to a wider class of elliptic partial differential equations like the Helmholtz equation. A new method using Laurent series theory allows holomorphic networks to be applied to multiply-connected plane domains, removing limitations to simply-connected geometries. This advancement enhances accuracy and computational efficiency in solving two-dimensional differential problems. <br /><br />Summary: <div>
arXiv:2507.22678v1 Announce Type: new 
Abstract: Physics-informed holomorphic neural networks (PIHNNs) have recently emerged as efficient surrogate models for solving differential problems. By embedding the underlying problem structure into the network, PIHNNs require training only to satisfy boundary conditions, often resulting in significantly improved accuracy and computational efficiency compared to traditional physics-informed neural networks (PINNs). In this work, we improve and extend the application of PIHNNs to two-dimensional problems. First, we introduce a novel holomorphic network architecture based on the Kolmogorov-Arnold representation (PIHKAN), which achieves higher accuracy with reduced model complexity. Second, we develop mathematical extensions that broaden the applicability of PIHNNs to a wider class of elliptic partial differential equations, including the Helmholtz equation. Finally, we propose a new method based on Laurent series theory that enables the application of holomorphic networks to multiply-connected plane domains, thereby removing the previous limitation to simply-connected geometries.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep reinforcement learning for efficient exploration of combinatorial structural design spaces</title>
<link>https://arxiv.org/abs/2507.22804</link>
<guid>https://arxiv.org/abs/2507.22804</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, structural design, form finding, sequential decision-making, material efficiency

Summary:
The paper introduces a novel reinforcement learning framework for performance-driven structural design, departing from traditional top-down optimization methods. Structures are modeled as compositions of predefined elements to align with practical constraints like constructability. The framework transforms the design task into a sequential decision-making problem and utilizes a training algorithm inspired by human learning. By applying reinforcement learning to structural design, the method efficiently searches large combinatorial design spaces. Through experimentation on steel braced truss frame cantilever structures, the trained policies consistently generate high-performing and structurally efficient designs based on known engineering principles. The analysis reveals that the agent effectively focuses its search on promising regions of the design space, showcasing transferable structural knowledge. <br /><br />Summary: <div>
arXiv:2507.22804v1 Announce Type: new 
Abstract: This paper proposes a reinforcement learning framework for performance-driven structural design that combines bottom-up design generation with learned strategies to efficiently search large combinatorial design spaces. Motivated by the limitations of conventional top-down approaches such as optimization, the framework instead models structures as compositions of predefined elements, aligning form finding with practical constraints like constructability and component reuse. With the formulation of the design task as a sequential decision-making problem and a human learning inspired training algorithm, the method adapts reinforcement learning for structural design. The framework is demonstrated by designing steel braced truss frame cantilever structures, where trained policies consistently generate distinct, high-performing designs that display structural performance and material efficiency with the use of structural strategies that align with known engineering principles. Further analysis shows that the agent efficiently narrows its search to promising regions of the design space, revealing transferable structural knowledge.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling and simulation of electro-mechanically coupled dielectric elastomers and myocardial tissue using smoothed finite element methods</title>
<link>https://arxiv.org/abs/2507.22838</link>
<guid>https://arxiv.org/abs/2507.22838</guid>
<content:encoded><![CDATA[
<div> tetrahedral meshes, cardiac electro-mechanics, finite element method, di-electric elastomer actuators, smoothed finite element methods 

Summary: 
This study explores the use of smoothed finite element methods (S-FEMs) in cardiac electro-mechanics to address issues with stiffness and volume locking that arise from automatically generated tetrahedral meshes. Four approaches, including standard linear FEM, face-based S-FEM (FS-FEM), node-based S-FEM (NS-FEM), and the hybrid FSNS-FEM, were implemented and evaluated in modeling electrically induced contraction in dielectric elastomers and orthotropic myocardial tissue samples. Results show that FSNS-FEM offers the best balance of accuracy and computational efficiency, closely matching reference data. NS-FEM produces softer results, leading to an overestimation of deformation, while FS-FEM and standard FEM exhibit overly stiff behavior and volume locking. These findings suggest that S-FEMs, particularly FSNS-FEM, hold promise for accurately simulating coupled electro-mechanical behavior in complex biomedical applications.<br /><br /> <div>
arXiv:2507.22838v1 Announce Type: new 
Abstract: Computational modelling offers a cost-effective and time-efficient alternative to experimental studies in biomedical engineering. In cardiac electro-mechanics, finite element method (FEM)-based simulations provide valuable insights into diseased tissue behaviour and the development of assistive systems such as di-electric elastomer actuators. However, the use of automatically generated tetrahedral meshes, commonly applied due to geometric complexity, often leads to numerical issues including overly stiff responses and volume locking, particularly in incompressible materials. Smoothed finite element methods (S-FEMs) offer a promising alternative by softening the stiffness matrix through gradient smoothing over defined smoothing domains. This work extends S-FEM formulations to electro-mechanically coupled problems and compares their performance against standard linear FEM. We implement and evaluate four approaches in the Abaqus environment via custom user elements: standard linear FEM, face-based S-FEM (FS-FEM), node-based S-FEM (NS-FEM), and the hybrid face/node-based S-FEM (FSNS-FEM). Two benchmark problems are studied: the electrically induced contraction of a compressible dielectric elastomer and an incompressible, orthotropic myocardial tissue sample. Reference solutions are obtained using a mesh consisting of higher-order elements. Our results demonstrate that FSNS-FEM provides the best balance between accuracy and computational efficiency, closely matching reference data. NS-FEM produces softer results, which leads to an overestimation of the true deformation. FS-FEM and standard FEM consistently exhibit overly stiff behaviour, with pronounced volume locking in the myocardial case. These findings support the potential of S-FEMs, in particular FSNS-FEM, for accurate simulation of coupled electro-mechanical behaviour in complex biomedical applications.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh based segmentation for automated margin line generation on incisors receiving crown treatment</title>
<link>https://arxiv.org/abs/2507.22859</link>
<guid>https://arxiv.org/abs/2507.22859</guid>
<content:encoded><![CDATA[
<div> Keywords: dental crowns, deep learning, segmentation, margin line, dataset 

Summary: 
- Dental crowns are crucial for restoring damaged teeth, with current design methods relying on manual input for margin line definition.
- A new framework utilizing deep learning was developed to automatically and accurately determine margin lines in dental preparations.
- An ensemble model combined with maximum probability showed the highest success rate in predicting margin lines.
- The quality of the preparation directly affected the accuracy of margin line prediction.
- The study provides the community with the datasets used for training and testing the deep learning model. 

<br /><br />Summary: <div>
arXiv:2507.22859v1 Announce Type: new 
Abstract: Dental crowns are essential dental treatments for restoring damaged or missing teeth of patients. Recent design approaches of dental crowns are carried out using commercial dental design software. Once a scan of a preparation is uploaded to the software, a dental technician needs to manually define a precise margin line on the preparation surface, which constitutes a non-repeatable and inconsistent procedure. This work proposes a new framework to determine margin lines automatically and accurately using deep learning. A dataset of incisor teeth was provided by a collaborating dental laboratory to train a deep learning segmentation model. A mesh-based neural network was modified by changing its input channels and used to segment the prepared tooth into two regions such that the margin line is contained within the boundary faces separating the two regions. Next, k-fold cross-validation was used to train 5 models, and a voting classifier technique was used to combine their results to enhance the segmentation. After that, boundary smoothing and optimization using the graph cut method were applied to refine the segmentation results. Then, boundary faces separating the two regions were selected to represent the margin line faces. A spline was approximated to best fit the centers of the boundary faces to predict the margin line. Our results show that an ensemble model combined with maximum probability predicted the highest number of successful test cases (7 out of 13) based on a maximum distance threshold of 200 m (representing human error) between the predicted and ground truth point clouds. It was also demonstrated that the better the quality of the preparation, the smaller the divergence between the predicted and ground truth margin lines (Spearman's rank correlation coefficient of -0.683). We provide the train and test datasets for the community.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach</title>
<link>https://arxiv.org/abs/2507.22070</link>
<guid>https://arxiv.org/abs/2507.22070</guid>
<content:encoded><![CDATA[
<div> metaclass, Protocol Buffers, test data generation, statistical analysis, enterprise systems <br />
<br />
In this paper, a novel framework for generating test data for enterprise systems using Protocol Buffers is proposed. The framework leverages Python's metaclass system for dynamic type enhancement and statistical analysis of production logs to extract realistic value domains. It combines automatic schema introspection, statistical value distribution analysis, and recursive descent algorithms to handle complex nested data structures. Experimental results show significant reduction in test data preparation time and improvement in test coverage compared to existing approaches. The framework is capable of handling protobuf structures with up to 15 levels of nesting and generating over 100,000 test cases within seconds. This approach addresses the challenges posed by large-scale enterprise systems with intricate hierarchical and graph-like structures, making it a valuable tool for performance testing in such environments. <br /><br />Summary: <div>
arXiv:2507.22070v1 Announce Type: cross 
Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present significant challenges for performance testing, particularly when targeting intermediate business interfaces with complex nested data structures. Traditional test data generation approaches are inadequate for handling the intricate hierarchical and graph-like structures inherent in enterprise protobuf schemas. This paper presents a novel test data generation framework that leverages Python's metaclass system for dynamic type enhancement and statistical analysis of production logs for realistic value domain extraction. Our approach combines automatic schema introspection, statistical value distribution analysis, and recursive descent algorithms for handling deeply nested structures. Experimental evaluation on three real-world enterprise systems demonstrates up to 95\% reduction in test data preparation time and 80\% improvement in test coverage compared to existing approaches. The framework successfully handles protobuf structures with up to 15 levels of nesting and generates comprehensive test suites containing over 100,000 test cases within seconds.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Variance Optimization and Algorithm for Finite-Horizon Markov Decision Processes</title>
<link>https://arxiv.org/abs/2507.22327</link>
<guid>https://arxiv.org/abs/2507.22327</guid>
<content:encoded><![CDATA[
<div> Mean-variance optimization, multi-period, Markov decision processes, pseudo mean, pseudo variance

Summary: 
This paper addresses the challenge of multi-period mean-variance optimization in finite-horizon discrete-time Markov decision processes. By introducing pseudo mean and pseudo variance concepts, the problem is transformed into a bilevel MDP. The bilevel MDP consists of an outer optimization for pseudo mean and an inner MDP with augmented state space. The properties of the bilevel MDP are explored, and an iterative algorithm is proposed to efficiently solve it, converging to a local optimum. Conditions for global optimum convergence are also derived. The approach is applied to multi-period portfolio selection and other scenarios such as queueing control and inventory management. The results align with classic financial engineering findings. This innovative approach presents a new method for mean-variance optimization problems in MDP models with broad applicability. <br /><br />Summary: <div>
arXiv:2507.22327v1 Announce Type: cross 
Abstract: Multi-period mean-variance optimization is a long-standing problem, caused by the failure of dynamic programming principle. This paper studies the mean-variance optimization in a setting of finite-horizon discrete-time Markov decision processes (MDPs), where the objective is to maximize the combined metrics of mean and variance of the accumulated rewards at terminal stage. By introducing the concepts of pseudo mean and pseudo variance, we convert the original mean-variance MDP to a bilevel MDP, where the outer is a single parameter optimization of the pseudo mean and the inner is a standard finite-horizon MDP with an augmented state space by adding an auxiliary state of accumulated rewards. We further study the properties of this bilevel MDP, including the optimality of history-dependent deterministic policies and the piecewise quadratic concavity of the inner MDPs' optimal values with respect to the pseudo mean. To efficiently solve this bilevel MDP, we propose an iterative algorithm that alternatingly updates the inner optimal policy and the outer pseudo mean. We prove that this algorithm converges to a local optimum. We also derive a sufficient condition under which our algorithm converges to the global optimum. Furthermore, we apply this approach to study the mean-variance optimization of multi-period portfolio selection problem, which shows that our approach exactly coincides with the classical result by Li and Ng (2000) in financial engineering. Our approach builds a new avenue to solve mean-variance optimization problems and has wide applicability to any problem modeled by MDPs, which is further demonstrated by examples of mean-variance optimization for queueing control and inventory management.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A surrogate model for topology optimisation of elastic structures via parametric autoencoders</title>
<link>https://arxiv.org/abs/2507.22539</link>
<guid>https://arxiv.org/abs/2507.22539</guid>
<content:encoded><![CDATA[
<div> surrogate-based, topology optimisation, linear elastic structures, parametric loads, boundary conditions <br />
Summary: 
A new surrogate-based algorithm for topology optimization of linear elastic structures under parametric loads and boundary conditions is introduced. The approach involves using a surrogate model to predict quasi-optimal topologies based on system parameters, trained through a feed-forward neural network. This predicted topology serves as an initial guess for a computationally efficient optimization algorithm, allowing for correction of errors and refinement of the design. The method demonstrates superior performance compared to high-fidelity optimizers, reducing the average number of iterations by 53% and maintaining discrepancies below 4% in the optimal functional value. Various architectures are proposed and evaluated for their approximation and generalization capabilities. The quasi-optimal topologies generated by the surrogate model enable efficient and accurate topology optimization even when extrapolating beyond the training and validation domain. <br /> <div>
arXiv:2507.22539v1 Announce Type: cross 
Abstract: A surrogate-based topology optimisation algorithm for linear elastic structures under parametric loads and boundary conditions is proposed. Instead of learning the parametric solution of the state (and adjoint) problems or the optimisation trajectory as a function of the iterations, the proposed approach devises a surrogate version of the entire optimisation pipeline. First, the method predicts a quasi-optimal topology for a given problem configuration as a surrogate model of high-fidelity topologies optimised with the homogenisation method. This is achieved by means of a feed-forward net learning the mapping between the input parameters characterising the system setup and a latent space determined by encoder/decoder blocks reducing the dimensionality of the parametric topology optimisation problem and reconstructing a high-dimensional representation of the topology. Then, the predicted topology is used as an educated initial guess for a computationally efficient algorithm penalising the intermediate values of the design variable, while enforcing the governing equations of the system. This step allows the method to correct potential errors introduced by the surrogate model, eliminate artifacts, and refine the design in order to produce topologies consistent with the underlying physics. Different architectures are proposed and the approximation and generalisation capabilities of the resulting models are numerically evaluated. The quasi-optimal topologies allow to outperform the high-fidelity optimiser by reducing the average number of optimisation iterations by $53\%$ while achieving discrepancies below $4\%$ in the optimal value of the objective functional, even in the challenging scenario of testing the model to extrapolate beyond the training and validation domain.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASCA: LLM based-Multi Agents System for Credit Assessment</title>
<link>https://arxiv.org/abs/2507.22758</link>
<guid>https://arxiv.org/abs/2507.22758</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, agent-based systems, credit assessment, MASCA, hierarchical multi-agent systems

Summary: 
MASCA is introduced as a multi-agent system driven by LLMs to improve credit assessment through a layered architecture approach. The system integrates contrastive learning for risk and reward evaluation and incorporates a signaling game theory perspective for theoretical insights. A detailed bias analysis is included to address fairness concerns in credit assessment. Experimental results show that MASCA outperforms baseline methods, showcasing the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring. <div>
arXiv:2507.22758v1 Announce Type: cross 
Abstract: Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic flux surrogate-based partitioned methods for interface problems</title>
<link>https://arxiv.org/abs/2402.03560</link>
<guid>https://arxiv.org/abs/2402.03560</guid>
<content:encoded><![CDATA[
<div> dynamic mode decomposition, partitioned methods, multiphysics problems, surrogate modeling, parametric PDEs  
Summary:  
Loosely coupled partitioned methods for multiphysics problems are beneficial for code reuse, concurrency, and plug-and-play simulations. However, they can compromise accuracy and stability. This study introduces a data-driven partitioned method for coupled parametric PDEs that improves accuracy without sacrificing performance. By replacing field transfers with a surrogate for interface flux dynamics, the approach uses dynamic mode decomposition on a staggered-in-time state. The offline training phase handles the computational load, while applying the surrogate in the online phase involves a single matrix-vector multiplication. Stability analysis of the scheme is provided, along with numerical results showcasing its effectiveness. <div>
arXiv:2402.03560v2 Announce Type: replace 
Abstract: Loosely coupled partitioned methods for multiphysics problems treat each subproblem as a separate entity and advance them independently in time. In so doing these methods enable code reuse, increase concurrency and provide a convenient framework for plug-and-play multiphysics simulations. However, mathematically loosely coupled schemes are equivalent to a single step of an iterative solution method, which can compromise their accuracy and stability. We present a new data-driven partitioned method for coupled parametric PDEs that can improve upon the accuracy of traditional loosely coupled methods without incurring a performance penalty. To that end, we replace conventional field transfers across the interface by a surrogate for the dynamics of the interface flux exchanged between the subdomains. To develop this surrogate we apply dynamic mode decomposition to a non-standard staggered-in-time state, comprising the interface flux and small solution patches near the interface. The new approach shifts the main computational burden to an offline training phase, whereas application of the surrogate in the online phase amounts to a single matrix-vector multiplication. We provide stability analysis of the surrogate-based partitioned scheme and include numerical results that demonstrate its potential.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Neural Network Training using Dynamic Learning Rate Schedule for PINNs and Image Classification</title>
<link>https://arxiv.org/abs/2507.21749</link>
<guid>https://arxiv.org/abs/2507.21749</guid>
<content:encoded><![CDATA[
<div> dynamic learning rate scheduler, neural networks, training, hyperparameters, optimization

Summary:
The paper introduces a dynamic learning rate scheduler (DLRS) algorithm to address the challenges in training neural networks, particularly in complex problems. The conventional static learning rate approach can lead to tedious training processes and suboptimal results. The DLRS adapts the learning rate based on loss values calculated during training, allowing for more efficient navigation of varying gradients and improved optimization. Experiments on physics-informed neural networks (PINNs) and image classification tasks using multilayer perceptrons and convolutional neural networks show that the DLRS accelerates training and enhances stability. This adaptive approach to learning rate optimization proves to be beneficial in improving the training process and achieving better performance in neural network tasks. <div>
arXiv:2507.21749v1 Announce Type: new 
Abstract: Training neural networks can be challenging, especially as the complexity of the problem increases. Despite using wider or deeper networks, training them can be a tedious process, especially if a wrong choice of the hyperparameter is made. The learning rate is one of such crucial hyperparameters, which is usually kept static during the training process. Learning dynamics in complex systems often requires a more adaptive approach to the learning rate. This adaptability becomes crucial to effectively navigate varying gradients and optimize the learning process during the training process. In this paper, a dynamic learning rate scheduler (DLRS) algorithm is presented that adapts the learning rate based on the loss values calculated during the training process. Experiments are conducted on problems related to physics-informed neural networks (PINNs) and image classification using multilayer perceptrons and convolutional neural networks, respectively. The results demonstrate that the proposed DLRS accelerates training and improves stability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge</title>
<link>https://arxiv.org/abs/2507.21990</link>
<guid>https://arxiv.org/abs/2507.21990</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, chemistry, reasoning, interpretable outputs, human-AI collaboration <br />
Summary: 
ChemDFM-R is a Chemical Reasoner LLM developed to improve reasoning capabilities in chemistry. The model is trained on a comprehensive dataset of atomized knowledge points to enhance its understanding of fundamental principles. A mix-sourced distillation strategy is used to integrate expert-curated knowledge and general-domain reasoning skills, followed by domain-specific reinforcement learning for enhanced chemical reasoning. ChemDFM-R achieves state-of-the-art performance on diverse chemical benchmarks and provides interpretable, rationale-driven outputs. The model's explicit reasoning chains improve reliability, transparency, and practical utility in real-world human-AI collaboration scenarios. <div>
arXiv:2507.21990v1 Announce Type: new 
Abstract: While large language models (LLMs) have achieved impressive progress, their application in scientific domains such as chemistry remains hindered by shallow domain understanding and limited reasoning capabilities. In this work, we focus on the specific field of chemistry and develop a Chemical Reasoner LLM, ChemDFM-R. We first construct a comprehensive dataset of atomized knowledge points to enhance the model's understanding of the fundamental principles and logical structure of chemistry. Then, we propose a mix-sourced distillation strategy that integrates expert-curated knowledge with general-domain reasoning skills, followed by domain-specific reinforcement learning to enhance chemical reasoning. Experiments on diverse chemical benchmarks demonstrate that ChemDFM-R achieves state-of-the-art performance while providing interpretable, rationale-driven outputs. Further case studies illustrate how explicit reasoning chains significantly improve the reliability, transparency, and practical utility of the model in real-world human-AI collaboration scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2507.07426</link>
<guid>https://arxiv.org/abs/2507.07426</guid>
<content:encoded><![CDATA[
<div> drug repurposing, large language models, structured reasoning, multi-agent collaboration, Monte Carlo Tree Search

Summary:
DrugMCTS is a novel framework that combines Retrieval-Augmented Generation (RAG), multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing. It utilizes five specialized agents to retrieve and analyze molecular and protein information, enabling structured and iterative reasoning without domain-specific fine-tuning. The framework outperforms Deepseek-R1 by over 20%, achieving higher recall and robustness on DrugBank and KIBA datasets. The results demonstrate the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in enhancing the application of Large Language Models (LLMs) in drug discovery. <div>
arXiv:2507.07426v2 Announce Type: cross 
Abstract: Recent advances in large language models have demonstrated considerable potential in scientific domains such as drug discovery. However, their effectiveness remains constrained when reasoning extends beyond the knowledge acquired during pretraining. Conventional approaches, such as fine-tuning or retrieval-augmented generation, face limitations in either imposing high computational overhead or failing to fully exploit structured scientific data. To overcome these challenges, we propose DrugMCTS, a novel framework that synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing. The framework employs five specialized agents tasked with retrieving and analyzing molecular and protein information, thereby enabling structured and iterative reasoning. Without requiring domain-specific fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially higher recall and robustness compared to both general-purpose LLMs and deep learning baselines. Our results highlight the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in advancing LLM applications for drug discovery.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bubbleformer: Forecasting Boiling with Transformers</title>
<link>https://arxiv.org/abs/2507.21244</link>
<guid>https://arxiv.org/abs/2507.21244</guid>
<content:encoded><![CDATA[
<div> Transformer-based, spatiotemporal model, boiling dynamics, nucleation, interface evolution, heat transfer <br /> 
Summary: Bubbleformer is a transformer-based spatiotemporal model that accurately forecasts boiling dynamics, including nucleation, interface evolution, and heat transfer, without relying on future input during inference. It integrates factorized axial attention, frequency-aware scaling, and conditions on thermophysical parameters to generalize across various fluid types, geometries, and operating conditions. The model is evaluated using physics-based metrics that assess heat-flux consistency, interface geometry, and mass conservation in chaotic systems. The BubbleML 2.0 dataset accompanying the model includes diverse working fluids and boiling configurations. Bubbleformer achieves new benchmark results in both prediction and forecasting of two-phase boiling flows. <br /> <div>
arXiv:2507.21244v1 Announce Type: cross 
Abstract: Modeling boiling (an inherently chaotic, multiphase process central to energy and thermal systems) remains a significant challenge for neural PDE surrogates. Existing models require future input (e.g., bubble positions) during inference because they fail to learn nucleation from past states, limiting their ability to autonomously forecast boiling dynamics. They also fail to model flow boiling velocity fields, where sharp interface-momentum coupling demands long-range and directional inductive biases. We introduce Bubbleformer, a transformer-based spatiotemporal model that forecasts stable and long-range boiling dynamics including nucleation, interface evolution, and heat transfer without dependence on simulation data during inference. Bubbleformer integrates factorized axial attention, frequency-aware scaling, and conditions on thermophysical parameters to generalize across fluids, geometries, and operating conditions. To evaluate physical fidelity in chaotic systems, we propose interpretable physics-based metrics that evaluate heat-flux consistency, interface geometry, and mass conservation. We also release BubbleML 2.0, a high-fidelity dataset that spans diverse working fluids (cryogens, refrigerants, dielectrics), boiling configurations (pool and flow boiling), flow regimes (bubbly, slug, annular), and boundary conditions. Bubbleformer sets new benchmark results in both prediction and forecasting of two-phase boiling flows.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>evoxels: A differentiable physics framework for voxel-based microstructure simulations</title>
<link>https://arxiv.org/abs/2507.21748</link>
<guid>https://arxiv.org/abs/2507.21748</guid>
<content:encoded><![CDATA[
<div> Keywords: materials science, advanced microscopy, predictive simulations, inverse modeling, machine learning

Summary:
Materials science research involves collaboration between experimentalists using advanced microscopy and theorists using computational models to understand the relationship between processing, structure, and properties. Inverse material design, starting from desired performance and working backwards to determine optimal microstructures and manufacturing routes, requires the integration of high-resolution imaging with predictive simulations and data-driven optimization. The evoxels framework, based on a Pythonic voxel-based approach, combines segmented 3D microscopy data with physical simulations, inverse modeling, and machine learning to accelerate discovery and deepen understanding of process-structure-property relationships.<br /><br />Summary: <div>
arXiv:2507.21748v1 Announce Type: cross 
Abstract: Materials science inherently spans disciplines: experimentalists use advanced microscopy to uncover micro- and nanoscale structure, while theorists and computational scientists develop models that link processing, structure, and properties. Bridging these domains is essential for inverse material design where you start from desired performance and work backwards to optimal microstructures and manufacturing routes. Integrating high-resolution imaging with predictive simulations and data-driven optimization accelerates discovery and deepens understanding of process-structure-property relationships. The differentiable physics framework evoxels is based on a fully Pythonic, unified voxel-based approach that integrates segmented 3D microscopy data, physical simulations, inverse modeling, and machine learning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predict Patient Self-reported Race from Skin Histological Images</title>
<link>https://arxiv.org/abs/2507.21912</link>
<guid>https://arxiv.org/abs/2507.21912</guid>
<content:encoded><![CDATA[
<div> Deep learning, Artificial Intelligence, computational pathology, race prediction, bias mitigation  
Summary:   
- The study investigates the use of deep learning models to predict self-reported race from digitized dermatopathology slides.  
- Attention-based mechanisms are applied to uncover race-associated morphological features.  
- Different dataset curation strategies were evaluated to control for confounding factors.  
- White and Black demographic groups showed high prediction performance, while overall performance decreased when considering all groups.  
- Attention analysis revealed the epidermis as a key predictive feature, emphasizing the importance of careful data curation and bias mitigation for equitable AI deployment in pathology.  
<br /><br />Summary: <div>
arXiv:2507.21912v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) has demonstrated success in computational pathology (CPath) for disease detection, biomarker classification, and prognosis prediction. However, its potential to learn unintended demographic biases, particularly those related to social determinants of health, remains understudied. This study investigates whether deep learning models can predict self-reported race from digitized dermatopathology slides and identifies potential morphological shortcuts. Using a multisite dataset with a racially diverse population, we apply an attention-based mechanism to uncover race-associated morphological features. After evaluating three dataset curation strategies to control for confounding factors, the final experiment showed that White and Black demographic groups retained high prediction performance (AUC: 0.799, 0.762), while overall performance dropped to 0.663. Attention analysis revealed the epidermis as a key predictive feature, with significant performance declines when these regions were removed. These findings highlight the need for careful data curation and bias mitigation to ensure equitable AI deployment in pathology. Code available at: https://github.com/sinai-computational-pathology/CPath_SAIF.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falling through the cracks: energy storage along segmented brittle crack fronts</title>
<link>https://arxiv.org/abs/2507.19406</link>
<guid>https://arxiv.org/abs/2507.19406</guid>
<content:encoded><![CDATA[
<div> disjoint crack front, stepped crack, material ligament, 3D measurements, strain energy density
Summary: 
The study focuses on the mechanics of crack propagation in brittle materials, specifically the formation of stepped cracks and material ligaments. Through in-situ 3D measurements using laser scanning, researchers observed the deformation field around stepped cracks and within the ligament feature. They discovered that the ligament concentrates strain energy density, leading to an increase in apparent fracture energy proportional to the strain energy within the ligament. This finding highlights the importance of understanding the role of material ligaments in controlling crack propagation behavior and provides valuable insights into the mechanics of brittle fracture. <div>
arXiv:2507.19406v2 Announce Type: replace 
Abstract: During brittle crack propagation, a smooth crack front curve frequently becomes disjoint, generating a stepped crack and a material ligament that unites the newly formed crack fronts. These universal features fundamentally alter the singular field structure and stability of propagating cracks; however, a quantitative analysis of their mechanics is lacking. Here, we perform in-situ 3D measurements to resolve the deformation field around stepped cracks, and crucially, within the ligament feature. The 3D kinematic data are obtained by scanning a thin laser sheet through the brittle hydrogel samples, while recording the scattered intensity from the embedded tracer particles. We find that the ligament concentrates the strain energy density, and moreover, the apparent fracture energy increases proportionally to the strain energy within the ligament.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IFD: A Large-Scale Benchmark for Insider Filing Violation Detection</title>
<link>https://arxiv.org/abs/2507.20162</link>
<guid>https://arxiv.org/abs/2507.20162</guid>
<content:encoded><![CDATA[
<div> Dataset, Insider trading, Form 4 filings, Regulatory compliance, MaBoost <br />
Summary: 
The article introduces the Insider Filing Delay (IFD) dataset, the largest dataset for insider disclosure behavior, covering over one million Form 4 transactions from 2002 to 2025. It addresses the challenge of insider trading violations and delayed disclosures in financial markets by presenting IFD as a benchmark for detecting strategic disclosure violations. The MaBoost framework, a hybrid model combining Mamba-based state space encoder with XGBoost, achieves high accuracy and interpretability in identifying high-risk behavioral patterns. Experiments show that MaBoost outperforms previous approaches, with an F1-score of up to 99.47% under regulatory settings. IFD serves as a realistic and reproducible benchmark for developing AI models in financial compliance, regulatory forensics, and interpretable time-series classification. The dataset and codes are publicly available for further research and analysis. <br /> <div>
arXiv:2507.20162v1 Announce Type: new 
Abstract: Insider trading violations, particularly delayed disclosures of Form 4 filings, remain a persistent challenge for financial market surveillance. Despite regulatory requirements such as the two-business-day rule of the Securities and Exchange Commission (SEC), enforcement is limited by the lack of large-scale, labeled datasets and task-specific benchmarks. In this paper, we introduce Insider Filing Delay (IFD), the first and largest publicly available dataset for insider disclosure behavior, comprising over one million Form 4 transactions spanning two decades (2002-2025), with structured annotations on delay status, insider roles, governance factors, and firm-level financial indicators. IFD enables the first large-scale formulation of strategic disclosure violation detection as a binary classification task grounded in regulatory compliance. To demonstrate the utility of IFD, we propose MaBoost, a hybrid framework combining a Mamba-based state space encoder with XGBoost, achieving high accuracy and interpretability in identifying high-risk behavioral patterns. Experiments across statistical baselines, deep learning models, and large language models confirm that MaBoost outperforms prior approaches, achieving an F1-score of up to 99.47% under constrained regulatory settings. IFD provides a realistic, reproducible, and behavior-rich benchmark for developing AI models in financial compliance, regulatory forensics, and interpretable time-series classification. All data and codes are available: https://github.com/CH-YellowOrange/MaBoost-and-IFD.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Explainable Stock Predictions with Tweets Using Mixture of Experts</title>
<link>https://arxiv.org/abs/2507.20535</link>
<guid>https://arxiv.org/abs/2507.20535</guid>
<content:encoded><![CDATA[
<div> Keywords: Stock price movements, Textual information, LLMs, FTS-Text-MoE model, Financial time series prediction

Summary: 
The FTS-Text-MoE model proposed in this study aims to improve stock price prediction by integrating numerical data with key summaries from news and social media using point embeddings. This model utilizes a Mixture of Experts Transformer decoder to process both data types, reducing computational costs by activating only a subset of model parameters. Multi-resolution prediction heads allow for flexible forecasting of financial time series at different scales. Experimental results demonstrate that FTS-Text-MoE outperforms baseline methods in terms of investment returns and Sharpe ratio, showcasing its superior accuracy and ability to predict future market trends. This approach addresses limitations in prompt-based methods and enhances financial analysis by leveraging factual textual data alongside historical price data. <div>
arXiv:2507.20535v1 Announce Type: new 
Abstract: Stock price movements are influenced by many factors, and alongside historical price data, tex-tual information is a key source. Public news and social media offer valuable insights into market sentiment and emerging events. These sources are fast-paced, diverse, and significantly impact future stock trends. Recently, LLMs have enhanced financial analysis, but prompt-based methods still have limitations, such as input length restrictions and difficulties in predicting sequences of varying lengths. Additionally, most models rely on dense computational layers, which are resource-intensive. To address these challenges, we propose the FTS- Text-MoE model, which combines numerical data with key summaries from news and tweets using point embeddings, boosting prediction accuracy through the integration of factual textual data. The model uses a Mixture of Experts (MoE) Transformer decoder to process both data types. By activating only a subset of model parameters, it reduces computational costs. Furthermore, the model features multi-resolution prediction heads, enabling flexible forecasting of financial time series at different scales. Experimental results show that FTS-Text-MoE outperforms baseline methods in terms of investment returns and Sharpe ratio, demonstrating its superior accuracy and ability to predict future market trends.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exascale Implicit Kinetic Plasma Simulations on El~Capitan for Solving the Micro-Macro Coupling in Magnetospheric Physics</title>
<link>https://arxiv.org/abs/2507.20719</link>
<guid>https://arxiv.org/abs/2507.20719</guid>
<content:encoded><![CDATA[
<div> Keywords: kinetic simulations, magnetospheres, Particle-in-Cell method, AMD Instinct MI300A Accelerated Processing Units, multi-scale coupling <br />
Summary: 
Our study presents fully kinetic, implicit Particle-in-Cell (PIC) simulations of global magnetospheres using El Capitan's AMD Instinct MI300A Accelerated Processing Units. This computational approach addresses the challenge of resolving the multi-scale coupling between microscopic and macroscopic dynamics in planetary magnetospheres. The implicit scheme of iPIC3D allows for larger time steps and grid spacing compared to explicit methods, without compromising accuracy. This capability enables the simulation of magnetospheres while preserving fine-scale electron physics crucial for processes like magnetic reconnection and plasma turbulence. Our innovations in algorithmic and technological aspects, including GPU-optimized kernels and data compression techniques, support the simulation of global-scale dynamics in small-to-medium planetary magnetospheres like Mercury and Ganymede. This advancement extends the reach of fully kinetic PIC codes to systems previously beyond their capabilities.  <br /><br />Summary: <div>
arXiv:2507.20719v1 Announce Type: new 
Abstract: Our fully kinetic, implicit Particle-in-Cell (PIC) simulations of global magnetospheres on up to 32,768 of El Capitan's AMD Instinct MI300A Accelerated Processing Units (APUs) represent an unprecedented computational capability that addresses a fundamental challenge in space physics: resolving the multi-scale coupling between microscopic (electron-scale) and macroscopic (global-scale) dynamics in planetary magnetospheres. The implicit scheme of iPIC3D supports time steps and grid spacing that are up to 10 times larger than those of explicit methods, without sacrificing physical accuracy. This enables the simulation of magnetospheres while preserving fine-scale electron physics, which is critical for key processes such as magnetic reconnection and plasma turbulence. Our algorithmic and technological innovations include GPU-optimized kernels, particle control, and physics-aware data compression using Gaussian Mixture Models. With simulation domains spanning 100-1,000 ion skin depths, we reach the global scale of small-to-medium planetary magnetospheres, such as those of Mercury and Ganymede, which supports fully kinetic treatment of global-scale dynamics in systems previously out of reach for fully kinetic PIC codes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Virtual Humans Toward Human Physiologically-Based Drug Discovery</title>
<link>https://arxiv.org/abs/2507.19568</link>
<guid>https://arxiv.org/abs/2507.19568</guid>
<content:encoded><![CDATA[
<div> AI, drug discovery, virtual experiments, programmable virtual humans, translational gap
Summary:
Artificial intelligence in drug discovery has primarily focused on digitizing existing experiments without addressing the challenges of predicting drug effects in humans. Biomedical digital twins, while useful in late-phase development, lack the resolution for early-stage discovery. To overcome this disconnect, programmable virtual humans have emerged as a solution, utilizing AI, high-throughput assays, and omics data to simulate drug actions in the human body. By bridging the translational gap, programmable virtual humans offer a new paradigm for drug discovery centered on human physiology. While offering transformative potential, key opportunities and challenges must be addressed for their realization. <div>
arXiv:2507.19568v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has sparked immense interest in drug discovery, but most current approaches only digitize existing high-throughput experiments. They remain constrained by conventional pipelines. As a result, they do not address the fundamental challenges of predicting drug effects in humans. Similarly, biomedical digital twins, largely grounded in real-world data and mechanistic models, are tailored for late-phase drug development and lack the resolution to model molecular interactions or their systemic consequences, limiting their impact in early-stage discovery. This disconnect between early discovery and late development is one of the main drivers of high failure rates in drug discovery. The true promise of AI lies not in augmenting current experiments but in enabling virtual experiments that are impossible in the real world: testing novel compounds directly in silico in the human body. Recent advances in AI, high-throughput perturbation assays, and single-cell and spatial omics across species now make it possible to construct programmable virtual humans: dynamic, multiscale models that simulate drug actions from molecular to phenotypic levels. By bridging the translational gap, programmable virtual humans offer a transformative path to optimize therapeutic efficacy and safety earlier than ever before. This perspective introduces the concept of programmable virtual humans, explores their roles in a new paradigm of drug discovery centered on human physiology, and outlines key opportunities, challenges, and roadmaps for their realization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Domain Shift in Multi-source CT-Scan Classification via Input-Space Standardization</title>
<link>https://arxiv.org/abs/2507.19858</link>
<guid>https://arxiv.org/abs/2507.19858</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-source CT-scan, domain shifts, cross-source generalization, input-space standardization, medical imaging

Summary:
Spatial-Slice Feature Learning (SSFL++) and Kernel-Density-based Slice Sampling (KDS) preprocessing pipelines address domain shifts in multi-source CT-scan classification. This study investigates the mechanisms behind the domain robustness of these pipelines and how they manage the trade-off between local discriminability and cross-source generalization. SSFL++ and KDS preprocess the input data, reducing inter-source variance and aligning inputs into a consistent target space to mitigate domain shift. This alignment simplifies the learning task for network optimization and consistently improves performance across different architectures. Experimental validation confirmed the effectiveness of the preprocessing approach, leading to a first-place finish in a competitive challenge. This study highlights the practicality and robustness of input-space standardization in multi-institutional medical imaging.<br /><br />Summary: Input-space standardization through SSFL++ and KDS preprocessing pipelines effectively addresses domain shifts in multi-source CT-scan classification. By reducing inter-source variance and aligning inputs into a consistent target space, these pipelines improve performance across architectures, simplifying the learning task and supporting cross-source generalization. Experimental validation and a first-place finish in a competitive challenge demonstrate the effectiveness and practicality of this preprocessing approach for multi-institutional medical imaging. <div>
arXiv:2507.19858v1 Announce Type: cross 
Abstract: Multi-source CT-scan classification suffers from domain shifts that impair cross-source generalization. While preprocessing pipelines combining Spatial-Slice Feature Learning (SSFL++) and Kernel-Density-based Slice Sampling (KDS) have shown empirical success, the mechanisms underlying their domain robustness remain underexplored. This study analyzes how this input-space standardization manages the trade-off between local discriminability and cross-source generalization. The SSFL++ and KDS pipeline performs spatial and temporal standardization to reduce inter-source variance, effectively mapping disparate inputs into a consistent target space. This preemptive alignment mitigates domain shift and simplifies the learning task for network optimization. Experimental validation demonstrates consistent improvements across architectures, proving the benefits stem from the preprocessing itself. The approach's effectiveness was validated by securing first place in a competitive challenge, supporting input-space standardization as a robust and practical solution for multi-institutional medical imaging.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQUA: A Large Language Model for Aquaculture &amp; Fisheries</title>
<link>https://arxiv.org/abs/2507.20520</link>
<guid>https://arxiv.org/abs/2507.20520</guid>
<content:encoded><![CDATA[
<div> Keywords: Aquaculture, artificial intelligence, large language model, AQUA, AQUADAPT 

Summary: 
Aquaculture is vital for global food security and economies. Challenges like disease outbreaks, inefficient feeding, and hatchery issues persist. Existing machine learning methods lack domain-specific solutions for aquaculture. AQUA, the first large language model tailored for aquaculture, aims to support farmers and researchers. AQUADAPT, an Agentic Framework, generates high-quality synthetic data combining expert knowledge and automated evaluation techniques. This innovation paves the way for LLM-driven advancements in aquaculture research, advisory systems, and decision-making tools. <div>
arXiv:2507.20520v1 Announce Type: cross 
Abstract: Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery</title>
<link>https://arxiv.org/abs/2507.20954</link>
<guid>https://arxiv.org/abs/2507.20954</guid>
<content:encoded><![CDATA[
<div> Python package, SHRED, deep learning strategy, dynamical systems, spatiotemporal data<br />
<br />
Summary: <br />
PySHRED is a Python package that implements SHRED, a deep learning strategy for modeling high-dimensional dynamical systems and spatiotemporal data. The version 1.0 release of PySHRED includes data preprocessors and cutting-edge SHRED methods designed for handling real-world data that may be noisy, multi-scale, parameterized, high-dimensional, and nonlinear. The package is easy to install, well-documented, and includes extensive code examples. It is modularly-structured to support future additions and is released under the MIT license. PySHRED provides extensions for robust sensing, reduced order modeling, and physics discovery, making it a valuable tool for researchers analyzing complex systems. The codebase is available on GitHub, allowing for collaboration and further development in the field of deep learning for dynamical systems. <div>
arXiv:2507.20954v1 Announce Type: cross 
Abstract: SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for modeling high-dimensional dynamical systems and/or spatiotemporal data from dynamical system snapshot observations. PySHRED is a Python package that implements SHRED and several of its major extensions, including for robust sensing, reduced order modeling and physics discovery. In this paper, we introduce the version 1.0 release of PySHRED, which includes data preprocessors and a number of cutting-edge SHRED methods specifically designed to handle real-world data that may be noisy, multi-scale, parameterized, prohibitively high-dimensional, and strongly nonlinear. The package is easy to install, thoroughly-documented, supplemented with extensive code examples, and modularly-structured to support future additions. The entire codebase is released under the MIT license and is available at https://github.com/pyshred-dev/pyshred.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order transmissibility and its linear approximation for in-service crack identification in train wheelset axles</title>
<link>https://arxiv.org/abs/2507.18636</link>
<guid>https://arxiv.org/abs/2507.18636</guid>
<content:encoded><![CDATA[
<div> Keywords: structural health monitoring, crack detection, train wheelset axles, higher-order harmonics, crack identification

Summary: 
Structural health monitoring is a potential method for early crack detection in train wheelset axles. A new crack detection feature called Higher-Order Transmissibility (HOTr) based on higher-order harmonics of breathing crack is proposed. The sensitivity and efficacy of this feature in crack identification are assessed, and a surrogate model based on linear system theory is developed to speed up the crack identification process. The method accurately reproduces the HOTr feature while eliminating the need for iterative solutions of nonlinear equations, reducing computational burden. The results indicate the potential for adoption in in-service damage identification for wheelset axles in near real-time applications.<br /><br />Summary: <div>
arXiv:2507.18636v1 Announce Type: new 
Abstract: In-service structural health monitoring is a so far rarely exploited, yet potent option for early-stage crack detection and identification in train wheelset axles. This procedure is non-trivial to enforce on the basis of a purely data-driven approach and typically requires the adoption of numerical, e.g. finite element-based, simulation schemes of the dynamic behavior of these axles. Damage in this particular case can be formulated as a breathing crack problem, which further complicates simulation by introducing response-dependent nonlinearities into the picture. In this study, first, a new crack detection feature based on higher-order harmonics of the breathing crack is proposed, termed Higher-Order Transmissibility (HOTr), and, secondly, its sensitivity and efficacy are assessed within the context of crack identification. Next, the mentioned feature is approximated via use of linear system theory, delivering a surrogate model which facilitates the computation and speeds up the crack identification procedure. The accuracy of the proposed method in reproducing the delivered HOTr is compared against the nonlinear simulation model. The obtained results suggest that the approximation of the HOTr can significantly reduce the computational burden by eliminating the need for an iterative solution of the governing nonlinear equation of motion, while maintaining a high level of accuracy when compared against the reference model. This implies great potential for adoption in in-service damage identification for wheelset axles, feasibly within a near real-time context.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning coupled Allen-Cahn and Cahn-Hilliard phase-field equations using Physics-informed neural operator(PINO)</title>
<link>https://arxiv.org/abs/2507.18731</link>
<guid>https://arxiv.org/abs/2507.18731</guid>
<content:encoded><![CDATA[
<div> PINOs, phase-field equations, mesoscale microstructural evolution, Physics informed Neural Operators, Al-Cu alloys<br />
<br />
Summary:<br />
Physics informed Neural Operators (PINOs) offer an alternative approach to predict microstructural evolution in materials subjected to periodic boundary conditions. In this study, PINO successfully predicted the growth of $\theta^{\prime}$ precipitates in Al-Cu alloys by solving three coupled physics equations simultaneously, involving two second-order Allen-Cahn equations and one fourth-order Cahn-Hilliard equation. The use of Fourier derivatives, specifically a pseudo-spectral method and Fourier extension, significantly improved the Cahn-Hilliard equation's accuracy. By leveraging the Fourier domain's properties, computing fourth derivatives of the Cahn-Hilliard equation was made more efficient. This research showcases the potential of PINOs in accurately predicting material microstructural evolution with reduced computational cost. <div>
arXiv:2507.18731v1 Announce Type: new 
Abstract: Phase-field equations, mostly solved numerically, are known for capturing the mesoscale microstructural evolution of a material. However, such numerical solvers are computationally expensive as it needs to generate fine mesh systems to solve the complex Partial Differential Equations(PDEs) with good accuracy. Therefore, we propose an alternative approach of predicting the microstructural evolution subjected to periodic boundary conditions using Physics informed Neural Operators (PINOs).
  In this study, we have demonstrated the capability of PINO to predict the growth of $\theta^{\prime}$ precipitates in Al-Cu alloys by learning the operator as well as by solving three coupled physics equations simultaneously. The coupling is of two second-order Allen-Cahn equation and one fourth-order Cahn-Hilliard equation. We also found that using Fourier derivatives(pseudo-spectral method and Fourier extension) instead of Finite Difference Method improved the Cahn-Hilliard equation loss by twelve orders of magnitude. Moreover, since differentiation is equivalent to multiplication in the Fourier domain, unlike Physics informed Neural Networks(PINNs), we can easily compute the fourth derivative of Cahn-Hilliard equation without converting it to coupled second order derivative.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThermoRL:Structure-Aware Reinforcement Learning for Protein Mutation Design to Enhance Thermostability</title>
<link>https://arxiv.org/abs/2507.18816</link>
<guid>https://arxiv.org/abs/2507.18816</guid>
<content:encoded><![CDATA[
<div> Keywords: protein thermostability, mutation design, reinforcement learning, graph neural networks, computational efficiency

Summary: 
ThermoRL is a novel framework utilizing reinforcement learning and graph neural networks to optimize protein thermostability through mutation design. Traditional methods face challenges in balancing sequence variations, structural dynamics, and thermostability. ThermoRL overcomes these limitations by incorporating a hierarchical Q-learning network and a surrogate model for reward feedback, guiding the agent on optimal mutation positions and amino acids. Experimental results demonstrate ThermoRL's ability to outperform baselines in rewards while efficiently filtering out destabilizing mutations and identifying stabilizing mutations aligned with experimental data. The framework's generalizability is highlighted by its accurate detection of key mutation sites in previously unseen proteins. ThermoRL represents a robust alternative to traditional methods for enhancing protein thermostability. 

<br /><br />Summary: <div>
arXiv:2507.18816v1 Announce Type: new 
Abstract: Designing mutations to optimize protein thermostability remains challenging due to the complex relationship between sequence variations, structural dynamics, and thermostability, often assessed by \delta\delta G
  (the change in free energy of unfolding). Existing methods rely on experimental random mutagenesis or prediction models tested with pre-defined datasets, using sequence-based heuristics and treating enzyme design as a one-step process without iterative refinement, which limits design space exploration and restricts discoveries beyond known variations. We present ThermoRL, a framework based on reinforcement learning (RL) that leverages graph neural networks (GNN) to design mutations with enhanced thermostability. It combines a pre-trained GNN-based encoder with a hierarchical Q-learning network and employs a surrogate model for reward feedback, guiding the RL agent on where (the position) and which (mutant amino acid) to apply for enhanced thermostability. Experimental results show that ThermoRL achieves higher or comparable rewards than baselines while maintaining computational efficiency. It filters out destabilizing mutations and identifies stabilizing mutations aligned with experimental data. Moreover, ThermoRL accurately detects key mutation sites in unseen proteins, highlighting its strong generalizability. This RL-guided approach powered by GNN embeddings offers a robust alternative to traditional protein mutation design.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrinityDNA: A Bio-Inspired Foundational Model for Efficient Long-Sequence DNA Modeling</title>
<link>https://arxiv.org/abs/2507.19229</link>
<guid>https://arxiv.org/abs/2507.19229</guid>
<content:encoded><![CDATA[
<div> TrinityDNA, DNA foundational model, long-range dependencies, structural features, Gated Reverse Complement (GRC), multi-scale attention mechanism. 

Summary:
TrinityDNA is a novel DNA foundational model designed to address the challenges of genomic sequence modeling. It integrates biologically informed components like Groove Fusion and GRC to capture DNA's structural features and symmetry. The model also utilizes a multi-scale attention mechanism to attend to varying levels of sequence dependencies. An evolutionary training strategy adapts the model to prokaryotic and eukaryotic genomes. TrinityDNA offers improvements in gene function prediction and regulatory mechanism discovery in genomics applications. It bridges machine learning techniques with biological insights for more effective genomic data analysis. Additionally, a new DNA long-sequence CDS annotation benchmark is introduced for comprehensive evaluations oriented towards practical applications. 

<br /><br />Summary: 
TrinityDNA, a novel DNA foundational model, integrates biologically informed components like Groove Fusion and GRC, along with a multi-scale attention mechanism and evolutionary training strategy, improving gene function prediction and regulatory mechanism discovery in genomics applications. It bridges machine learning techniques with biological insights for more effective genomic data analysis and introduces a DNA long-sequence CDS annotation benchmark for comprehensive evaluations focused on practical applications. <div>
arXiv:2507.19229v1 Announce Type: new 
Abstract: The modeling of genomic sequences presents unique challenges due to their length and structural complexity. Traditional sequence models struggle to capture long-range dependencies and biological features inherent in DNA. In this work, we propose TrinityDNA, a novel DNA foundational model designed to address these challenges. The model integrates biologically informed components, including Groove Fusion for capturing DNA's structural features and Gated Reverse Complement (GRC) to handle the inherent symmetry of DNA sequences. Additionally, we introduce a multi-scale attention mechanism that allows the model to attend to varying levels of sequence dependencies, and an evolutionary training strategy that progressively adapts the model to both prokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate and efficient approach to genomic sequence modeling, offering significant improvements in gene function prediction, regulatory mechanism discovery, and other genomics applications. Our model bridges the gap between machine learning techniques and biological insights, paving the way for more effective analysis of genomic data. Additionally, we introduced a new DNA long-sequence CDS annotation benchmark to make evaluations more comprehensive and oriented toward practical applications.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Level Monte Carlo sampling with Parallel-in-Time Integration for Uncertainty Quantification in Electric Machine Simulation</title>
<link>https://arxiv.org/abs/2507.19246</link>
<guid>https://arxiv.org/abs/2507.19246</guid>
<content:encoded><![CDATA[
<div> Efficient Uncertainty Quantification, Monte Carlo sampling, Multi-Level Monte Carlo method, Parallel-in-Time integration, computational effort <br />
Summary: <br />
This article introduces a method that combines Multi-Level Monte Carlo sampling with Parallel-in-Time integration to improve the efficiency of Uncertainty Quantification in high-dimensional uncertainty scenarios. While the Multi-Level Monte Carlo method reduces computational effort, it struggles to decrease time to solution in highly parallel computing environments. By leveraging Parallel-in-Time integration for select samples, the proposed method accelerates computation without sacrificing accuracy. Results from numerical examples show a significant speedup of 12-45% compared to Multi-Level Monte Carlo sampling, with a modest increase of 15-18% in total computational effort. The study delves into the tradeoff between time-to-solution and computational effort, presenting theoretical considerations and practical implications for this combined approach. <div>
arXiv:2507.19246v1 Announce Type: new 
Abstract: While generally considered computationally expensive, Uncertainty Quantification using Monte Carlo sampling remains beneficial for applications with uncertainties of high dimension. As an extension of the naive Monte Carlo method, the Multi-Level Monte Carlo method reduces the overall computational effort, but is unable to reduce the time to solution in a sufficiently parallel computing environment. In this work, we propose a Uncertainty Quantification method combining Multi-Level Monte Carlo sampling and Parallel-in-Time integration for select samples, exploiting remaining parallel computing capacity to accelerate the computation. While effective at reducing the time-to-solution, Parallel-in-Time integration methods greatly increase the total computational effort. We investigate the tradeoff between time-to-solution and total computational effort of the combined method, starting from theoretical considerations and comparing our findings to two numerical examples. There, a speedup of 12 - 45% compared to Multi-Level Monte Carlo sampling is observed, with an increase of 15 - 18% in computational effort.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning electromagnetic fields based on finite element basis functions</title>
<link>https://arxiv.org/abs/2507.19255</link>
<guid>https://arxiv.org/abs/2507.19255</guid>
<content:encoded><![CDATA[
<div> Keywords: parametric surrogate models, electric machines, isogeometric analysis, proper orthogonal decomposition, deep learning

Summary: 
Parametric surrogate models are essential for efficient design optimization and operational monitoring of electric machines. This study introduces a novel approach that combines isogeometric analysis, proper orthogonal decomposition, and deep learning to predict spline basis coefficients rapidly and accurately. By directly learning these coefficients, the method enables efficient and physically consistent predictions, particularly for parametric nonlinear magnetostatic models of permanent magnet synchronous machines. The effectiveness of this approach is demonstrated in the study, showcasing its potential for enhancing the design and analysis of electric machines. <br /><br />Summary: <div>
arXiv:2507.19255v1 Announce Type: new 
Abstract: Parametric surrogate models of electric machines are widely used for efficient design optimization and operational monitoring. Addressing geometry variations, spline-based computer-aided design representations play a pivotal role. In this study, we propose a novel approach that combines isogeometric analysis, proper orthogonal decomposition and deep learning to enable rapid and physically consistent predictions by directly learning spline basis coefficients. The effectiveness of this method is demonstrated using a parametric nonlinear magnetostatic model of a permanent magnet synchronous machine.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel multi-thickness topology optimization method for balancing structural performance and manufacturability</title>
<link>https://arxiv.org/abs/2507.19388</link>
<guid>https://arxiv.org/abs/2507.19388</guid>
<content:encoded><![CDATA[
<div> Keywords: Topology optimization, multi-thickness, density-based, manufacturability, compliance

Summary:
This paper introduces a novel multi-thickness, density-based topology optimization method that aims to strike a balance between structural performance and manufacturability. By guiding the design towards a predefined set of discrete allowable thicknesses through a multilevel penalization scheme and smoothed Heaviside projection, the method transitions designs from truss-like structures to high-performance sheet-like structures as the number of allowable thickness levels increases. The approach, validated on standard benchmarks, achieves compliance values within 2% of fully unpenalized optimization while outperforming standard SIMP results. The method eliminates impractically thin regions and features, making designs suitable for both additive manufacturing and conventional fabrication. This approach maximizes both performance and manufacturability, addressing the trade-off commonly encountered in two-dimensional topology optimization.<br /><br />Summary: <div>
arXiv:2507.19388v1 Announce Type: new 
Abstract: Topology optimization (TO) in two dimensions often presents a trade-off between structural performance and manufacturability, with unpenalized (variable-thickness) methods yielding superior but complex designs, and penalized (SIMP) methods producing simpler, truss-like structures with compromised performance. This paper introduces a multi-thickness, density-based topology optimization method designed to bridge this gap. The proposed approach guides the design towards a predefined set of discrete, allowable thicknesses by employing a novel multilevel penalization scheme and a multilevel smoothed Heaviside projection. A continuation strategy for the penalization and projection parameters, combined with an adaptive mesh refinement technique, ensures robust convergence and high-resolution geometric features. The method is validated on standard cantilever and MBB beam benchmarks. Results demonstrate that as the number of allowable thicknesses increases, the designs systematically transition from conventional truss-like structures to high-performance, sheet-like structures. Notably, designs with as few as three discrete thickness levels achieve compliance values within 2\% of those from fully unpenalized, variable-thickness optimization, while significantly outperforming standard SIMP results. The method inherently eliminates impractically thin regions and features, both in the out-of-plane and in-plane directions and produces designs well-suited for both additive manufacturing and conventional fabrication using standard-thickness stock materials, thus maximizing both performance and manufacturability.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance as Extended Biology: Reciprocity as the Cognitive Substrate of Financial Behavior</title>
<link>https://arxiv.org/abs/2506.00099</link>
<guid>https://arxiv.org/abs/2506.00099</guid>
<content:encoded><![CDATA[
<div> reciprocity, financial behaviors, credit, insurance, trade, artificial intelligence
Summary:
- The article argues that financial behaviors such as credit, insurance, trade, and token exchange are not products of institutional design but extensions of reciprocity.
- Reciprocity is considered the foundational logic of early human societies, governing the circulation of goods and maintenance of long-term cooperation.
- Trade is seen as the canonical form of reciprocity, involving simultaneous, symmetric, and partner-contingent interactions.
- The four core financial functions mentioned - credit, insurance, token exchange, and investment - are reconstructed as expressions of the underlying principle of reciprocity under different conditions.
- By focusing on the minimal dynamics of reciprocal interaction, the framework shifts the focus from institutional engineering to behavioral computation, providing a new foundation for modeling decentralized financial behavior in both human and artificial agents. 

<br /><br />Summary: <div>
arXiv:2506.00099v2 Announce Type: cross 
Abstract: A central challenge in economics and artificial intelligence is explaining how financial behaviors-such as credit, insurance, and trade-emerge without formal institutions. We argue that these functions are not products of institutional design, but structured extensions of a single behavioral substrate: reciprocity. Far from being a derived strategy, reciprocity served as the foundational logic of early human societies-governing the circulation of goods, regulation of obligation, and maintenance of long-term cooperation well before markets, money, or formal rules. Trade, commonly regarded as the origin of financial systems, is reframed here as the canonical form of reciprocity: simultaneous, symmetric, and partner-contingent. Building on this logic, we reconstruct four core financial functions-credit, insurance, token exchange, and investment-as expressions of the same underlying principle under varying conditions. By grounding financial behavior in minimal, simulateable dynamics of reciprocal interaction, this framework shifts the focus from institutional engineering to behavioral computation-offering a new foundation for modeling decentralized financial behavior in both human and artificial agents.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable inverse design of optical multilayer thin films based on extended neural adjoint and regression activation mapping</title>
<link>https://arxiv.org/abs/2507.18644</link>
<guid>https://arxiv.org/abs/2507.18644</guid>
<content:encoded><![CDATA[
<div> Neural adjoint, optical multilayer thin films, inverse design, material loss function, interpretability<br />
<br />
Summary:
The article introduces an Extended Neural Adjoint (ENA) framework for the inverse design of optical multilayer thin films (OMTs). It meets six key criteria: accuracy, efficiency, diversity, scalability, flexibility, and interpretability. The ENA framework incorporates a material loss function in the neural adjoint method to explore different material configurations of OMTs. The forward neural network architecture (F-RAM) enhances scalability and interpretability by visualizing feature importance. Ablation studies show that the material loss improves accuracy and diversity of OMT solutions. Compared to a residual network-based method (Res-GLOnet), the ENA achieves higher accuracy and better diversity in inverse design. The interpretability of the ENA method is demonstrated by consistent feature importance distributions across OMT structures with similar optical properties. The flexibility of the ENA method is showcased by imposing constraints on the initial layer of OMTs. <br /><br />Summary: <div>
arXiv:2507.18644v1 Announce Type: cross 
Abstract: We propose an extended neural adjoint (ENA) framework, which meets six key criteria for artificial intelligence-assisted inverse design of optical multilayer thin films (OMTs): accuracy, efficiency, diversity, scalability, flexibility, and interpretability. To enhance the scalability of the existing neural adjoint method, we present a novel forward neural network architecture for OMTs and introduce a material loss function into the existing neural adjoint loss function, facilitating the exploration of material configurations of OMTs. Furthermore, we present the detailed formulation of the regression activation mapping for the presented forward neural network architecture (F-RAM), a feature visualization method aimed at improving interpretability. We validated the efficacy of the material loss by conducting an ablation study, where each component of the loss function is systematically removed and evaluated. The results indicated that the inclusion of the material loss significantly improves accuracy and diversity. To substantiate the performance of the ENA-based inverse design, we compared it against the residual network-based global optimization network (Res-GLOnet). The ENA yielded the OMT solutions of an inverse design with higher accuracy and better diversity compared to the Res-GLOnet. To demonstrate the interpretability, we applied F-RAM to diverse OMT structures with similar optical properties, obtained by the proposed ENA method. We showed that distributions of feature importance for various OMT structures exhibiting analogous optical properties are consistent, despite variations in material configurations, layer number, and thicknesses. Furthermore, we demonstrate the flexibility of the ENA method by restricting the initial layer of OMTs to SiO2 and 100 nm.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report</title>
<link>https://arxiv.org/abs/2507.19402</link>
<guid>https://arxiv.org/abs/2507.19402</guid>
<content:encoded><![CDATA[
<div> Keywords: financial transactions, fraud detection, classical machine learning, quantum machine learning, hybrid models
Summary: 
The report investigates and compares classical, quantum, and hybrid machine learning models for detecting fraudulent financial activities using a comprehensive behavioural feature engineering framework. Classical models such as Random Forest outperformed quantum models on the IBM Anti-Money Laundering dataset, achieving high accuracy and F-measure. The proposed FD4QC architecture offers a classical-first, quantum-enhanced approach for real-world deployment. While classical models showed better performance in this study, the Quantum Support Vector Machine (QSVM) demonstrated promise with high precision and low false-positive rates. The results highlight the current limitations of quantum machine learning in financial fraud detection and suggest avenues for future research. 
<br /><br />Summary: <div>
arXiv:2507.19402v1 Announce Type: cross 
Abstract: The increasing complexity and volume of financial transactions pose significant challenges to traditional fraud detection systems. This technical report investigates and compares the efficacy of classical, quantum, and quantum-hybrid machine learning models for the binary classification of fraudulent financial activities.
  As of our methodology, first, we develop a comprehensive behavioural feature engineering framework to transform raw transactional data into a rich, descriptive feature set. Second, we implement and evaluate a range of models on the IBM Anti-Money Laundering (AML) dataset. The classical baseline models include Logistic Regression, Decision Tree, Random Forest, and XGBoost. These are compared against three hybrid classic quantum algorithms architectures: a Quantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC), and a Hybrid Quantum Neural Network (HQNN).
  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a practical, API-driven system architecture designed for real-world deployment, featuring a classical-first, quantum-enhanced philosophy with robust fallback mechanisms.
  Our results demonstrate that classical tree-based models, particularly \textit{Random Forest}, significantly outperform the quantum counterparts in the current setup, achieving high accuracy (\(97.34\%\)) and F-measure (\(86.95\%\)). Among the quantum models, \textbf{QSVM} shows the most promise, delivering high precision (\(77.15\%\)) and a low false-positive rate (\(1.36\%\)), albeit with lower recall and significant computational overhead.
  This report provides a benchmark for a real-world financial application, highlights the current limitations of quantum machine learning in this domain, and outlines promising directions for future research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Twin Technologies in Predictive Maintenance: Enabling Transferability via Sim-to-Real and Real-to-Sim Transfer</title>
<link>https://arxiv.org/abs/2507.18449</link>
<guid>https://arxiv.org/abs/2507.18449</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, Artificial Intelligence, Digital Twins, Reality Gap Analysis, Simulation Model

Summary:
The article discusses the development of Digital Twins (DTs) in the context of the Internet of Things (IoT) and Artificial Intelligence. It emphasizes the importance of standardized frameworks for transitioning DTs from academia to industry. The focus is on transferability, particularly the transfer of knowledge between simulations and real-world operations, known as sim-to-real and real-to-sim transfer. The concept of the "reality gap," the difference between simulated predictions and actual outcomes, is explored. The authors propose integrating a Reality Gap Analysis (RGA) module into existing DT frameworks to manage sim-to-real and real-to-sim transfers effectively. Data pipelines connect the RGA module with historical repositories and simulation models to facilitate bidirectional knowledge transfer. A case study on a pedestrian bridge at Carnegie Mellon University demonstrates the performance of this approach, showcasing efficient bidirectional knowledge transfer without compromising effectiveness.<br /><br />Summary: The article highlights the importance of standardizing DT frameworks for industry adoption and emphasizes the need for bidirectional knowledge transfer between simulations and real-world operations. The integration of a Reality Gap Analysis module enhances this transferability, addressing the challenge of the "reality gap" discrepancy. The proposed approach, demonstrated through a case study, showcases efficient transfer capabilities without compromising effectiveness. <div>
arXiv:2507.18449v1 Announce Type: new 
Abstract: The advancement of the Internet of Things (IoT) and Artificial Intelligence has catalyzed the evolution of Digital Twins (DTs) from conceptual ideas to more implementable realities. Yet, transitioning from academia to industry is complex due to the absence of standardized frameworks. This paper builds upon the authors' previously established functional and informational requirements supporting standardized DT development, focusing on a crucial aspect: transferability. While existing DT research primarily centers on asset transfer, the significance of "sim-to-real transfer" and "real-to-sim transfer"--transferring knowledge between simulations and real-world operations--is vital for comprehensive lifecycle management in DTs. A key challenge in this process is calibrating the "reality gap," the discrepancy between simulated predictions and actual outcomes. Our research investigates the impact of integrating a single Reality Gap Analysis (RGA) module into an existing DT framework to effectively manage both sim-to-real and real-to-sim transfers. This integration is facilitated by data pipelines that connect the RGA module with the existing components of the DT framework, including the historical repository and the simulation model. A case study on a pedestrian bridge at Carnegie Mellon University showcases the performance of different levels of integration of our approach with an existing framework. With full implementation of an RGA module and a complete data pipeline, our approach is capable of bidirectional knowledge transfer between simulations and real-world operations without compromising efficiency.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents</title>
<link>https://arxiv.org/abs/2507.18067</link>
<guid>https://arxiv.org/abs/2507.18067</guid>
<content:encoded><![CDATA[
<div> supervised deep learning framework, neural operators, PDEs, downscaling models, Copernicus ocean current data <br />
Summary: 
This paper introduces a supervised deep learning framework utilizing neural operators to solve partial differential equations (PDEs) and provide high-resolution solutions for ocean current data. The proposed method aims to address the limitations of available satellite products by downscaling models and predicting solutions at arbitrary resolutions. By applying this approach to Copernicus ocean data, the study demonstrates the ability to enhance spatial granularity for detailed local analyses crucial in oceanography. Additionally, the model's versatility allows it to model surrogate PDEs and predict solutions at varying resolutions, irrespective of the input resolution. Evaluation on real-world Copernicus data and synthetic Navier-Stokes simulation datasets showcases the effectiveness of this approach in generating accurate and detailed current data, with potential applications in coastal management, environmental monitoring, and maritime safety. <br /> <div>
arXiv:2507.18067v1 Announce Type: cross 
Abstract: Accurate modeling of physical systems governed by partial differential equations is a central challenge in scientific computing. In oceanography, high-resolution current data are critical for coastal management, environmental monitoring, and maritime safety. However, available satellite products, such as Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and global ocean models, often lack the spatial granularity required for detailed local analyses. In this work, we (a) introduce a supervised deep learning framework based on neural operators for solving PDEs and providing arbitrary resolution solutions, and (b) propose downscaling models with an application to Copernicus ocean current data. Additionally, our method can model surrogate PDEs and predict solutions at arbitrary resolution, regardless of the input resolution. We evaluated our model on real-world Copernicus ocean current data and synthetic Navier-Stokes simulation datasets.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On zero-order consistency residue and background pressure for the conservative SPH fluid dynamics</title>
<link>https://arxiv.org/abs/2507.18210</link>
<guid>https://arxiv.org/abs/2507.18210</guid>
<content:encoded><![CDATA[
<div> SPH method, smoothed particle hydrodynamics, zero-order consistency issue, numerical dissipation, gravity-driven flow<br />
<br />
Summary: 
The study examines the zero-order consistency issue in conservative smoothed particle hydrodynamics (SPH) and its impact on flow simulation in pressure-driven channels and gravity-driven free-surface flows. It identifies the common root cause of non-physical numerical damping in these scenarios as the zero-order gradient consistency residue. The background pressure exacerbates this issue, leading to excessive numerical dissipation. The study conducts theoretical analysis and numerical experiments to understand and mitigate this residue effect, testing sensitive factors like water depth, input dynamic pressure, channel length, resolution, and outlet pressure. The reverse kernel gradient correction technique is effective but has limitations in reducing the residue effect. The study highlights the necessity of correction schemes, especially in scenarios with high background pressure, as demonstrated in the FDA nozzle engineering benchmark. <div>
arXiv:2507.18210v1 Announce Type: cross 
Abstract: As one of the major challenges for the conservative smoothed particle hydrodynamics (SPH) method, the zero-order consistency issue, although thought to be mitigated by the particle regularization scheme, such as the transport velocity formulation, significantly damps the flow in a long channel for both laminar and turbulent simulations. Building on this finding, this paper not only thoroughly analyzes the damping reason in this pressure-driven channel flow, but also relates this problem with the excessive numerical dissipation in the gravity-driven free-surface flow. The common root cause of the non-physical numerical damping in the two typical flow scenarios, the zero-order gradient consistency residue, is exposed. The adverse influence of the background pressure on the residue for the two scenarios is revealed and discussed. To comprehensively understand the behavior of the residue and mitigate its potential adverse effects, we conduct both theoretical analysis and numerical experiments focusing on the key sensitive factors. For studying the residue-induced non-physical energy dissipation in the gravity-driven free-surface flow, the water depth and input dynamic pressure in the inviscid standing wave case are tested. To investigate the velocity loss in the pressure-driven channel flow, we examine the effects of the channel length, resolution, and outlet pressure. The state-of-the-art reverse kernel gradient correction technique is introduced for the two typical flows, and proved to be effective in reducing the residue effect, but we find its correction capability is fundamentally limited. Finally, the FDA nozzle, an engineering benchmark, is tested to demonstrate the residue influence in a complex geometry, highlighting the necessity of correction schemes in scenarios with unavoidable high background pressure.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A stabilized Two-Step Formulation of Maxwell's Equations in the time-domain</title>
<link>https://arxiv.org/abs/2507.18235</link>
<guid>https://arxiv.org/abs/2507.18235</guid>
<content:encoded><![CDATA[
<div> time-domain, Maxwell's equations, Galerkin discretization, low-frequency instability, numerical stability

Summary:
This study presents a novel approach to simulating electromagnetic fields across broad frequency ranges by extending a stabilized two-step formulation of Maxwell's equations to the time-domain. Utilizing a Galerkin discretization in space, the researchers apply two time-discretization schemes tailored to the first- and second-order partial differential equations. To combat low-frequency instabilities, a generalized tree-cotree gauge is incorporated to eliminate the singularity of the curl-curl operator, ensuring robustness even in the static limit. Numerical experimentation on various 3D problems demonstrates the method's stability, accuracy, and its ability to handle nonlinear, temperature-dependent materials. The results affirm the reliability and applicability of this approach in simulating electromagnetic phenomena with diverse frequency characteristics. 

<br /><br />Summary: <div>
arXiv:2507.18235v1 Announce Type: cross 
Abstract: Simulating electromagnetic fields across broad frequency ranges is challenging due to numerical instabilities at low frequencies. This work extends a stabilized two-step formulation of Maxwell's equations to the time-domain. Using a Galerkin discretization in space, we apply two different time-discretization schemes that are tailored to the first- and second-order in time partial differential equations of the two-step solution procedure used here. To address the low-frequency instability, we incorporate a generalized tree-cotree gauge that removes the singularity of the curl-curl operator, ensuring robustness even in the static limit. Numerical results on academic and application-oriented 3D problems confirm stability, accuracy, and the method's applicability to nonlinear, temperature-dependent materials.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Select2Drive: Pragmatic Communications for Real-Time Collaborative Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.12040</link>
<guid>https://arxiv.org/abs/2501.12040</guid>
<content:encoded><![CDATA[
<div> keyword: Vehicle-to-everything communications, autonomous driving, collaborative perception, decision-making, Select2Drive <br />
Summary: <br />
Vehicle-to-everything communications play a crucial role in advancing autonomous driving, with the emergence of PragComm as a promising paradigm. The Select2Drive framework is proposed to optimize limited computational and communication resources for collaborative driving. It introduces distributed predictive perception to reduce latency and enhance decision-making efficiency. By prioritizing critical regions in communication using area-of-importance-based PragComm, Select2Drive boosts both communication efficiency and decision-making efficacy. Empirical evaluations on V2Xverse and real-world DAIR-V2X demonstrate significant improvements in offline perception tasks and driving performance, especially in dense traffic scenarios. Select2Drive showcases a promising approach to enhance collaborative driving through efficient resource utilization and improved decision-making processes. <br /> <div>
arXiv:2501.12040v2 Announce Type: replace 
Abstract: Vehicle-to-everything communications-assisted autonomous driving has witnessed remarkable advancements in recent years, with pragmatic communications (PragComm) emerging as a promising paradigm for real-time collaboration among vehicles and other agents. Simultaneously, extensive research has explored the interplay between collaborative perception and decision-making in end-to-end driving frameworks. In this work, we revisit the collaborative driving problem and propose the Select2Drive framework to optimize the utilization of limited computational and communication resources. Particularly, to mitigate cumulative latency in perception and decision-making, Select2Drive introduces distributed predictive perception by formulating an active prediction paradigm and simplifying high-dimensional semantic feature prediction into a computation cost-efficient, motion-aware reconstruction. Given the ``less is more" principle that an over-broadened perceptual horizon possibly confuses the decision module rather than contributing to it, Select2Drive utilizes area-of-importance-based PragComm to prioritize the communications of critical regions, thus boosting both communication efficiency and decision-making efficacy. Empirical evaluations on the V2Xverse and real-world DAIR-V2X demonstrate that Select2Drive achieves a $2.60$\% and $1.99$\% improvement in offline perception tasks under limited bandwidth (resp., pose error conditions). Moreover, it delivers at most $8.35$\% and $2.65$\% enhancement in closed-loop driving scores and route completion rates, particularly in scenarios characterized by dense traffic and high-speed dynamics.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoadBench: A Vision-Language Foundation Model and Benchmark for Road Damage Understanding</title>
<link>https://arxiv.org/abs/2507.17353</link>
<guid>https://arxiv.org/abs/2507.17353</guid>
<content:encoded><![CDATA[
<div> dataset, multimodal, road damage, vision language model, infrastructure monitoring <br />
<br /> Summary: RoadBench is introduced as a multimodal benchmark for road damage understanding, combining high-resolution images with textual descriptions for richer context. The RoadCLIP vision language model enhances CLIP with disease-aware positional encoding and road-condition priors for improved road damage recognition. A data generation pipeline using GPT expands the dataset, improving data diversity without manual annotation. Experimental results show RoadCLIP outperforms vision-only models by 19.2%, demonstrating the benefits of combining visual and textual information for enhanced road condition analysis. This work sets new benchmarks for the field and advances infrastructure monitoring through multimodal learning. <div>
arXiv:2507.17353v1 Announce Type: new 
Abstract: Accurate road damage detection is crucial for timely infrastructure maintenance and public safety, but existing vision-only datasets and models lack the rich contextual understanding that textual information can provide. To address this limitation, we introduce RoadBench, the first multimodal benchmark for comprehensive road damage understanding. This dataset pairs high resolution images of road damages with detailed textual descriptions, providing a richer context for model training. We also present RoadCLIP, a novel vision language model that builds upon CLIP by integrating domain specific enhancements. It includes a disease aware positional encoding that captures spatial patterns of road defects and a mechanism for injecting road-condition priors to refine the model's understanding of road damages. We further employ a GPT driven data generation pipeline to expand the image to text pairs in RoadBench, greatly increasing data diversity without exhaustive manual annotation. Experiments demonstrate that RoadCLIP achieves state of the art performance on road damage recognition tasks, significantly outperforming existing vision-only models by 19.2%. These results highlight the advantages of integrating visual and textual information for enhanced road condition analysis, setting new benchmarks for the field and paving the way for more effective infrastructure monitoring through multimodal learning.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning-Driven Retrosynthesis Prediction with Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.17448</link>
<guid>https://arxiv.org/abs/2507.17448</guid>
<content:encoded><![CDATA[
<div> Reversal planning; organic synthesis; drug discovery; AI-driven advancements; RetroDFM-R <br />
Summary: RetroDFM-R is a reasoning-based large language model (LLM) developed for chemical retrosynthesis to address limitations in existing methods. Through reinforcement learning guided by chemically verifiable rewards, RetroDFM-R improves prediction accuracy and explainability. It outperforms state-of-the-art methods with a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Human assessments confirm the model's chemical plausibility and practicality. RetroDFM-R accurately predicts multistep retrosynthetic routes for drug molecules and perovskite materials from the literature. The model's reasoning process provides interpretable insights, enhancing trust and practical value in real-world retrosynthesis applications.<br /><br />Summary: RetroDFM-R, a reasoning-based large language model, enhances prediction accuracy and explainability in chemical retrosynthesis. It outperforms existing methods on benchmarks and accurately predicts multistep routes for various compounds. Human assessments confirm its utility and the practical value of its reasoning process. <div>
arXiv:2507.17448v1 Announce Type: new 
Abstract: Retrosynthesis planning, essential in organic synthesis and drug discovery, has greatly benefited from recent AI-driven advancements. Nevertheless, existing methods frequently face limitations in both applicability and explainability. Traditional graph-based and sequence-to-sequence models often lack generalized chemical knowledge, leading to predictions that are neither consistently accurate nor easily explainable. To address these challenges, we introduce RetroDFM-R, a reasoning-based large language model (LLM) designed specifically for chemical retrosynthesis. Leveraging large-scale reinforcement learning guided by chemically verifiable rewards, RetroDFM-R significantly enhances prediction accuracy and explainability. Comprehensive evaluations demonstrate that RetroDFM-R significantly outperforms state-of-the-art methods, achieving a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Double-blind human assessments further validate the chemical plausibility and practical utility of RetroDFM-R's predictions. RetroDFM-R also accurately predicts multistep retrosynthetic routes reported in the literature for both real-world drug molecules and perovskite materials. Crucially, the model's explicit reasoning process provides human-interpretable insights, thereby enhancing trust and practical value in real-world retrosynthesis applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous Sustainability Assessment via Multimodal AI Agents</title>
<link>https://arxiv.org/abs/2507.17012</link>
<guid>https://arxiv.org/abs/2507.17012</guid>
<content:encoded><![CDATA[
<div> AI agents, life cycle assessment, sustainability, carbon emissions, electronic devices
Summary: 
- AI agents are introduced to streamline the process of calculating cradle-to-gate carbon emissions for electronic devices, reducing expert time while maintaining accuracy.
- A method is developed to estimate environmental impacts by comparing products with similar descriptions, providing quick and accurate results.
- A data-driven approach is implemented to generate emission factors, improving accuracy compared to traditional methods.
- The scalability and implications of this approach for future LCA workflows are analyzed.
- The innovative use of AI and data abstraction tools addresses data availability gaps and enhances the efficiency of sustainability assessments in product manufacturing. <br /><br /> <div>
arXiv:2507.17012v1 Announce Type: cross 
Abstract: Interest in sustainability information has surged in recent years. However, the data required for a life cycle assessment (LCA) that maps the materials and processes from product manufacturing to disposal into environmental impacts (EI) are often unavailable. Here we reimagine conventional LCA by introducing multimodal AI agents that emulate interactions between LCA experts and stakeholders like product managers and engineers to calculate the cradle-to-gate (production) carbon emissions of electronic devices. The AI agents iteratively generate a detailed life-cycle inventory leveraging a custom data abstraction and software tools that extract information from online text and images from repair communities and government certifications. This approach reduces weeks or months of expert time to under one minute and closes data availability gaps while yielding carbon footprint estimates within 19% of expert LCAs with zero proprietary data. Additionally, we develop a method to directly estimate EI by comparing an input to a cluster of products with similar descriptions and known carbon footprints. This runs in 3 ms on a laptop with a MAPE of 12.28% on electronic products. Further, we develop a data-driven method to generate emission factors. We use the properties of an unknown material to represent it as a weighted sum of emission factors for similar materials. Compared to human experts picking the closest LCA database entry, this improves MAPE by 120.26%. We analyze the data and compute scaling of this approach and discuss its implications for future LCA workflows.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image memorability predicts social media virality and externally-associated commenting</title>
<link>https://arxiv.org/abs/2409.14659</link>
<guid>https://arxiv.org/abs/2409.14659</guid>
<content:encoded><![CDATA[
<div> memorability, viral potential, social media, neural network, image categorization  
Summary:  
Memorable images play a crucial role in predicting viral potential on social media platforms. The study utilized neural network ResMem to assess image memorability and correlated it with virality metrics using Reddit image posts. Results showed that memorable images consistently received more comments, even after accounting for image categories. Semantic analysis indicated that memorable images elicited neutral-affect comments, suggesting a unique pathway to virality compared to emotional content. Visual consistency analysis revealed that memorable posts stimulated diverse comments from external sources. Analyses of ResMem's layers highlighted the importance of semantic distinctiveness in both memorability and virality, independent of image category effects. This study sheds light on the link between memorability and social media engagement, emphasizing the role of visual features and human cognitive interactions in online content dissemination.  
<br /><br />Summary: <div>
arXiv:2409.14659v2 Announce Type: replace-cross 
Abstract: Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images are consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional contents. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found that semantic distinctiveness was key to both memorability and virality even after accounting for image category effects. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting CFD Surrogates through Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2507.16069</link>
<guid>https://arxiv.org/abs/2507.16069</guid>
<content:encoded><![CDATA[
<div> Keywords: surrogate models, computational fluid dynamics, interpretability, sparse autoencoders, latent features

Summary: 
Surrogate models are increasingly being used as alternatives to high-fidelity CFD solvers, but their opaque latent representations limit their adoption in safety-critical or regulated environments. This study introduces a posthoc interpretability framework for graph-based surrogate models in CFD, utilizing sparse autoencoders (SAEs). By extracting an overcomplete basis in the node embedding space of a pretrained surrogate, the method generates a dictionary of interpretable latent features. This approach allows for the identification of monosemantic concepts aligned with physical phenomena like vorticity and flow structures. By enhancing explainability and trustworthiness in CFD applications, this model-agnostic pathway provides a valuable tool for improving the transparency of surrogate models. 

<br /><br />Summary: <div>
arXiv:2507.16069v1 Announce Type: new 
Abstract: Learning-based surrogate models have become a practical alternative to high-fidelity CFD solvers, but their latent representations remain opaque and hinder adoption in safety-critical or regulation-bound settings. This work introduces a posthoc interpretability framework for graph-based surrogate models used in computational fluid dynamics (CFD) by leveraging sparse autoencoders (SAEs). By obtaining an overcomplete basis in the node embedding space of a pretrained surrogate, the method extracts a dictionary of interpretable latent features. The approach enables the identification of monosemantic concepts aligned with physical phenomena such as vorticity or flow structures, offering a model-agnostic pathway to enhance explainability and trustworthiness in CFD applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational design of personalized drugs via robust optimization under uncertainty</title>
<link>https://arxiv.org/abs/2507.16470</link>
<guid>https://arxiv.org/abs/2507.16470</guid>
<content:encoded><![CDATA[
<div> Keywords: drug composition, release profile, inverse design, topology optimization, uncertainty-aware drug design

Summary: 
This study introduces a computational inverse design approach for optimizing drug composition to achieve a specific release profile necessary for effective disease treatment. The method, based on topology optimization, considers drug material parameters and shape to determine the desired drug composition. The Noyes-Whitney model is used to govern drug release, with robust topology optimization incorporating random material parameters through the stochastic reduced-order method (SROM). Unlike Monte Carlo methods, SROM reduces computational requirements while accurately predicting release profiles. Application of the method to designing drugs with various target release profiles demonstrates close alignment between designed and target profiles. Moreover, SROM-based drug designs exhibit lower uncertainty in release profiles, indicating the effectiveness of this strategy for uncertainty-aware drug design. <div>
arXiv:2507.16470v1 Announce Type: new 
Abstract: Effective disease treatment often requires precise control of the release of the active pharmaceutical ingredient (API). In this work, we present a computational inverse design approach to determine the optimal drug composition that yields a target release profile. We assume that the drug release is governed by the Noyes-Whitney model, meaning that dissolution occurs at the surface of the drug. Our inverse design method is based on topology optimization. The method optimizes the drug composition based on the target release profile, considering the drug material parameters and the shape of the final drug. Our method is non-parametric and applicable to arbitrary drug shapes. The inverse design method is complemented by robust topology optimization, which accounts for the random drug material parameters. We use the stochastic reduced-order method (SROM) to propagate the uncertainty in the dissolution model. Unlike Monte Carlo methods, SROM requires fewer samples and improves computational performance. We apply our method to designing drugs with several target release profiles. The numerical results indicate that the release profiles of the designed drugs closely resemble the target profiles. The SROM-based drug designs exhibit less uncertainty in their release profiles, suggesting that our method is a convincing approach for uncertainty-aware drug design.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-objective Portfolio Optimization Via Gradient Descent</title>
<link>https://arxiv.org/abs/2507.16717</link>
<guid>https://arxiv.org/abs/2507.16717</guid>
<content:encoded><![CDATA[
<div> Keywords: portfolio optimization, multi-objective, gradient descent, automatic differentiation, constraints<br />
Summary:<br />
The article introduces a new approach to portfolio optimization called multi-objective portfolio optimization (MPO) using gradient descent with automatic differentiation. Traditional methods often struggle with scalability and flexibility in complex scenarios. The MPO framework can handle various optimization objectives, constraints, and scenarios, such as minimizing risk measures or maximizing Sharpe ratio while considering constraints like tracking error limits or asset group restrictions. The authors conducted experiments on six scenarios, showing that their method outperforms standard solvers like CVXPY and SKFOLIO in terms of performance and flexibility. The framework aims to be a practical tool for researchers and practitioners dealing with advanced portfolio optimization problems in real-world conditions.<br /><br />Summary: <div>
arXiv:2507.16717v1 Announce Type: new 
Abstract: Traditional approaches to portfolio optimization, often rooted in Modern Portfolio Theory and solved via quadratic programming or evolutionary algorithms, struggle with scalability or flexibility, especially in scenarios involving complex constraints, large datasets and/or multiple conflicting objectives. To address these challenges, we introduce a benchmark framework for multi-objective portfolio optimization (MPO) using gradient descent with automatic differentiation. Our method supports any optimization objective, such as minimizing risk measures (e.g., CVaR) or maximizing Sharpe ratio, along with realistic constraints, such as tracking error limits, UCITS regulations, or asset group restrictions. We have evaluated our framework across six experimental scenarios, from single-objective setups to complex multi-objective cases, and have compared its performance against standard solvers like CVXPY and SKFOLIO. Our results show that our method achieves competitive performance while offering enhanced flexibility for modeling multiple objectives and constraints. We aim to provide a practical and extensible tool for researchers and practitioners exploring advanced portfolio optimization problems in real-world conditions.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking CO$_2$ Storage Simulations: Results from the 11th Society of Petroleum Engineers Comparative Solution Project</title>
<link>https://arxiv.org/abs/2507.15861</link>
<guid>https://arxiv.org/abs/2507.15861</guid>
<content:encoded><![CDATA[
<div> benchmark, simulation tools, geological carbon dioxide storage, quantitative analysis, grid resolution

Summary:<br />
The 11th Society of Petroleum Engineers Comparative Solution Project (SPE11) benchmarked simulation tools for geological carbon dioxide storage, with 18 valid results included in the study. Qualitative variation in results was related to thermal effects, convective mixing, and facies discontinuities, with grid resolution playing a significant role. The quantitative analysis revealed that unreported variations due to human choices during the simulation process were just as impactful as reported computational choices. The study highlights the need for comprehensive documentation and transparency in reporting simulation results for accurate comparative analysis. <div>
arXiv:2507.15861v1 Announce Type: cross 
Abstract: The 11th Society of Petroleum Engineers Comparative Solution Project (shortened SPE11 herein) benchmarked simulation tools for geological carbon dioxide (CO$_2$) storage. A total of 45 groups from leading research institutions and industry across the globe signed up to participate, with 18 ultimately contributing valid results that were included in the comparative study reported here.
  This paper summarizes the SPE11. A comprehensive introduction and qualitative discussion of the submitted data are provided, together with an overview of online resources for accessing the full depth of data. A global metric for analyzing the relative distance between submissions is proposed and used to conduct a quantitative analysis of the submissions. This analysis attempts to statistically resolve the key aspects influencing the variability between submissions.
  The study shows that the major qualitative variation between the submitted results is related to thermal effects, dissolution-driven convective mixing, and resolution of facies discontinuities. Moreover, a strong dependence on grid resolution is observed across all three versions of the SPE11. However, our quantitative analysis suggests that the observed variations are predominantly influenced by factors not documented in the technical responses provided by the participants. We therefore identify that unreported variations due to human choices within the process of setting up, conducting, and reporting on the simulations underlying each SPE11 submission are at least as impactful as the computational choices reported.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making</title>
<link>https://arxiv.org/abs/2507.14542</link>
<guid>https://arxiv.org/abs/2507.14542</guid>
<content:encoded><![CDATA[
<div> Keywords: High-frequency oscillations, intracranial Electroencephalography, machine learning, variational autoencoder, epilepsy treatment

Summary:<br />
- High-frequency oscillations (HFOs) in intracranial Electroencephalography (iEEG) are important in localizing the epileptogenic zone in epilepsy treatment.
- Traditional rule-based detectors for HFOs have low precision, leading to false positives that require manual review.
- Supervised machine learning approaches for classification depend on labeled datasets, which are hard to acquire and label consistently.
- The Self-Supervised to Label Discovery (SS2LD) framework uses a variational autoencoder (VAE) to refine candidate events from legacy detectors into precise pathological HFOs.
- SS2LD outperforms state-of-the-art methods in identifying pathological HFOs, offering a scalable, label-efficient, and clinically effective strategy using legacy detectors.<br /> 
Summary: <div>
arXiv:2507.14542v1 Announce Type: new 
Abstract: High-frequency oscillations (HFOs) in intracranial Electroencephalography (iEEG) are critical biomarkers for localizing the epileptogenic zone in epilepsy treatment. However, traditional rule-based detectors for HFOs suffer from unsatisfactory precision, producing false positives that require time-consuming manual review. Supervised machine learning approaches have been used to classify the detection results, yet they typically depend on labeled datasets, which are difficult to acquire due to the need for specialized expertise. Moreover, accurate labeling of HFOs is challenging due to low inter-rater reliability and inconsistent annotation practices across institutions. The lack of a clear consensus on what constitutes a pathological HFO further challenges supervised refinement approaches. To address this, we leverage the insight that legacy detectors reliably capture clinically relevant signals despite their relatively high false positive rates. We thus propose the Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of candidate events generated by legacy detectors into a precise set of pathological HFOs. SS2LD employs a variational autoencoder (VAE) for morphological pre-training to learn meaningful latent representation of the detected events. These representations are clustered to derive weak supervision for pathological events. A classifier then uses this supervision to refine detection boundaries, trained on real and VAE-augmented data. Evaluated on large multi-institutional interictal iEEG datasets, SS2LD outperforms state-of-the-art methods. SS2LD offers a scalable, label-efficient, and clinically effective strategy to identify pathological HFOs using legacy detectors.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Generative Models in Condition and Structural Health Monitoring: Opportunities, Limitations and Future Outlook</title>
<link>https://arxiv.org/abs/2507.15026</link>
<guid>https://arxiv.org/abs/2507.15026</guid>
<content:encoded><![CDATA[
<div> Keywords: Condition monitoring, Structural health monitoring, Deep generative models, Fault diagnosis, Anomaly detection

Summary: 
Condition and structural health monitoring (CM/SHM) are crucial for predictive maintenance in various industrial sectors. Conventional deep learning models face challenges such as operational variability, imbalanced datasets, and multimodal sensory data. Deep generative models (DGMs) like autoregressive models and generative adversarial networks offer solutions by generating data samples, reconstructing system states, and handling multimodal data. This review compares DGMs with traditional models in CM/SHM, focusing on tasks like data imbalance, domain adaptation, and fault diagnosis. Limitations of DGMs include explainability issues, computational inefficiencies, and the need for parameter-efficient strategies. Future research can explore zero-shot learning, multimodal generalization, hybrid architectures combining DGMs with physics knowledge, and reinforcement learning with DGMs for industrial scenarios. <div>
arXiv:2507.15026v1 Announce Type: new 
Abstract: Condition and structural health monitoring (CM/SHM) is a pivotal component of predictive maintenance (PdM) strategies across diverse industrial sectors, including mechanical rotating machinery, airplane composite wings, offshore wind turbines, and civil engineering structures. Conventional deep learning models, while effective in fault diagnosis and anomaly detection through supervised feature extraction and rule-based data augmentation, often struggle with operational variability, imbalanced or scarce fault datasets, and multimodal sensory data from complex systems. Deep generative models (DGMs) in this regard, including autoregressive models, variational autoencoders, generative adversarial networks, diffusion-based models, and emerging large language models, offer transformative capabilities by synthesizing high-fidelity data samples, reconstructing latent system states, and modeling complex multimodal data streams. This review systematically examines state-of-the-art DGM applications in CM/SHM systems, emphasizing their role in addressing key challenges: data imbalance and imputation, domain adaptation and generalization, multimodal data fusion, and downstream fault diagnosis and anomaly detection tasks, with rigorous comparison among signal processing, conventional machine learning or deep learning models, and DGMs. We also analyze current limitations of DGMs, including challenges of explainable and trustworthy models, computational inefficiencies for edge deployment, and the need for parameter-efficient fine-tuning strategies. Future research directions can focus on zero-shot and few-shot learning, robust multimodal generalization, hybrid architectures integrating DGMs with physics knowledge, and reinforcement learning with DGMs to enhance robustness and accuracy in industrial scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Configurational-force-driven adaptive refinement and coarsening in topology optimization</title>
<link>https://arxiv.org/abs/2507.15570</link>
<guid>https://arxiv.org/abs/2507.15570</guid>
<content:encoded><![CDATA[
<div> Keywords: topology optimization, configurational forces, mesh adaptivity, Eshelby stress, multi-level refinement

Summary:
Topology optimization often requires solving numerous linear equation systems, leading to high computational costs due to fine mesh requirements. A multi-level adaptive refinement and coarsening strategy based on configurational forces is proposed to address this challenge. Configurational forces, derived from Eshelby stress, predict configurational changes like crack propagation. By utilizing configurational forces for refinement, a high-resolution structure is achieved along design boundaries and stress-critical regions, while multilevel coarsening reduces computational effort. This approach is particularly advantageous in stress-sensitive problems where preventing stress failure is crucial. Ultimately, the use of configurational forces for mesh adaptivity in topology optimization results in geometrically well-defined structures with significantly reduced computational costs. 

<br /><br />Summary: <div>
arXiv:2507.15570v1 Announce Type: new 
Abstract: The iterative nature of topology optimization, especially in combination with nonlinear state problems, often requires the solution of thousands of linear equation systems. Furthermore, due to the pixelated design representation, the use of a fine mesh is essential to obtain geometrically well-defined structures and to accurately compute response quantities such as the von Mises stress. Therefore, the computational cost of solving a fine-mesh topology optimization problem quickly adds up. To address this challenge, we consider a multi-level adaptive refinement and coarsening strategy based on configurational forces. Configurational forces based on the Eshelby stress predict configurational changes such as crack propagation or dislocation motion. Due to a relaxation in the calculation of (Eshelby) stresses with respect to the design variables, discrete configurational forces increase not only in highly stressed regions, but also in grey transition regions (design boundaries). For this reason they are an ideal criterion for mesh adaptivity in topology optimization, especially when avoiding stress failure is a priority. By using configurational forces for refinement, we obtain a high-resolution structure where the refined mesh is present along the design boundaries as well as in stress-critical regions. At the same time, multilevel coarsening using the same criterion drastically minimizes the computational effort.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers</title>
<link>https://arxiv.org/abs/2507.15753</link>
<guid>https://arxiv.org/abs/2507.15753</guid>
<content:encoded><![CDATA[
<div> diffusion transformers, metamaterials, generative framework, 3D geometries, mechanical objectives<br />
<br />
Summary:<br />
A new generative framework called DiffuMeta has been developed to enable the inverse design of three-dimensional metamaterials. It uses diffusion transformers and a novel algebraic language representation to encode complex 3D geometries as mathematical sentences. This approach allows for the precise control of stress-strain responses in shell structures, considering factors like buckling and contact. DiffuMeta can generate diverse solutions to address the one-to-many mapping challenge and simultaneously optimize multiple mechanical objectives, including nonlinear responses. Experimental validation has shown the effectiveness of DiffuMeta in designing metamaterials with tailored properties, demonstrating its potential for accelerating the design process of complex structures. <div>
arXiv:2507.15753v1 Announce Type: new 
Abstract: Generative machine learning models have revolutionized material discovery by capturing complex structure-property relationships, yet extending these approaches to the inverse design of three-dimensional metamaterials remains limited by computational complexity and underexplored design spaces due to the lack of expressive representations. Here, we present DiffuMeta, a generative framework integrating diffusion transformers with a novel algebraic language representation, encoding 3D geometries as mathematical sentences. This compact, unified parameterization spans diverse topologies while enabling direct application of transformers to structural design. DiffuMeta leverages diffusion models to generate novel shell structures with precisely targeted stress-strain responses under large deformations, accounting for buckling and contact while addressing the inherent one-to-many mapping by producing diverse solutions. Uniquely, our approach enables simultaneous control over multiple mechanical objectives, including linear and nonlinear responses beyond training domains. Experimental validation of fabricated structures further confirms the efficacy of our approach for accelerated design of metamaterials and structures with tailored properties.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing Physics Discovery through Fully Differentiable Finite Element-Based Machine Learning</title>
<link>https://arxiv.org/abs/2507.15787</link>
<guid>https://arxiv.org/abs/2507.15787</guid>
<content:encoded><![CDATA[
<div> finite element-based machine learning, surrogate-modelling, PDE solutions, unknown physics, trainable operators <br />
Summary: 
The article introduces a novel approach called fully differentiable finite element-based machine learning (FEBML) to address limitations in existing surrogate-modelling methods for problems involving unknown or incomplete relationships in PDEs. FEBML embeds trainable operators for unknown physics within a state-of-the-art FEM solver, enabling end-to-end differentiation. It represents unknown operators as an encode-process-decode pipeline over finite-element degrees of freedom, ensuring learned physics respects the variational structure. The versatility of FEBML is demonstrated by successfully recovering nonlinear stress-strain laws from laboratory tests, applying the learned model to new mechanical scenarios without retraining, and identifying temperature-dependent conductivity in transient heat flow. This new framework offers promise for improving accuracy and generality in modelling complex scientific and engineering problems. <br /> <div>
arXiv:2507.15787v1 Announce Type: new 
Abstract: Although many problems in science and engineering are modelled by well-established PDEs, they often involve unknown or incomplete relationships, such as material constitutive laws or thermal response, that limit accuracy and generality. Existing surrogate-modelling approaches directly approximate PDE solutions but remain tied to a specific geometry, boundary conditions, and set of physical constraints. To address these limitations, we introduce a fully differentiable finite element-based machine learning (FEBML) framework that embeds trainable operators for unknown physics within a state-of-the-art, general FEM solver, enabling true end-to-end differentiation. At its core, FEBML represents each unknown operator as an encode-process-decode pipeline over finite-element degrees of freedom: field values are projected to nodal coefficients, transformed by a neural network, and then lifted back to a continuous FE function, ensuring the learned physics respects the variational structure. We demonstrate its versatility by recovering nonlinear stress-strain laws from laboratory tests, applying the learned model to a new mechanical scenario without retraining, and identifying temperature-dependent conductivity in transient heat flow.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions</title>
<link>https://arxiv.org/abs/2507.14245</link>
<guid>https://arxiv.org/abs/2507.14245</guid>
<content:encoded><![CDATA[
<div> Keywords: nanomaterials, proteins, AI, NanoPro-3M, multimodal representation learning <br />
Summary: 
NanoPro-3M dataset is introduced, containing over 3.2 million samples and 37,000 unique proteins, aiming to enhance understanding of nanomaterial-protein interactions. NanoProFormer model is proposed to predict these interactions with strong generalization abilities, handling missing features and unseen entities. The model outperforms single-modality approaches by utilizing multimodal representation learning, identifying crucial factors influencing corona formation. It showcases applicability to various tasks via zero-shot inference and fine-tuning. This work lays the groundwork for accurate and generalized predictions of nanomaterial-protein interactions, reducing reliance on experimental data and speeding up in vitro applications. <br /><br />Summary: <div>
arXiv:2507.14245v1 Announce Type: cross 
Abstract: Unlocking the potential of nanomaterials in medicine and environmental science hinges on understanding their interactions with proteins, a complex decision space where AI is poised to make a transformative impact. However, progress has been hindered by limited datasets and the restricted generalizability of existing models. Here, we propose NanoPro-3M, the largest nanomaterial-protein interaction dataset to date, comprising over 3.2 million samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer, a foundational model that predicts nanomaterial-protein affinities through multimodal representation learning, demonstrating strong generalization, handling missing features, and unseen nanomaterials or proteins. We show that multimodal modeling significantly outperforms single-modality approaches and identifies key determinants of corona formation. Furthermore, we demonstrate its applicability to a range of downstream tasks through zero-shot inference and fine-tuning. Together, this work establishes a solid foundation for high-performance and generalized prediction of nanomaterial-protein interaction endpoints, reducing experimental reliance and accelerating various in vitro applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do Large Language Models know about materials?</title>
<link>https://arxiv.org/abs/2507.14586</link>
<guid>https://arxiv.org/abs/2507.14586</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Mechanical Engineering, Materials Science, Periodic Table of Elements, Benchmark 

Summary: 
Large Language Models (LLMs) are being used in the fields of mechanical engineering and materials science to facilitate step-wise reasoning through the Processing-Structure-Property-Performance chain. While current LLMs are trained on a wide range of internet data, much of this data is non-scientific. To ensure LLMs can provide accurate information about materials, it is important to evaluate their intrinsic knowledge. In this study, the researchers focus on the example of the Periodic Table of Elements to assess LLMs' ability to generate factually correct output. By analyzing the vocabulary and tokenization used in different LLM models, the study highlights the uniqueness of material fingerprints and identifies the need for specialized models in various stages of the PSPP chain. This work serves as a benchmark for determining which steps in the material science process LLMs are suitable for and where specialized models may be necessary. 

<br /><br />Summary: <div>
arXiv:2507.14586v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly applied in the fields of mechanical engineering and materials science. As models that establish connections through the interface of language, LLMs can be applied for step-wise reasoning through the Processing-Structure-Property-Performance chain of material science and engineering. Current LLMs are built for adequately representing a dataset, which is the most part of the accessible internet. However, the internet mostly contains non-scientific content. If LLMs should be applied for engineering purposes, it is valuable to investigate models for their intrinsic knowledge -- here: the capacity to generate correct information about materials. In the current work, for the example of the Periodic Table of Elements, we highlight the role of vocabulary and tokenization for the uniqueness of material fingerprints, and the LLMs' capabilities of generating factually correct output of different state-of-the-art open models. This leads to a material knowledge benchmark for an informed choice, for which steps in the PSPP chain LLMs are applicable, and where specialized models are required.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators</title>
<link>https://arxiv.org/abs/2507.14652</link>
<guid>https://arxiv.org/abs/2507.14652</guid>
<content:encoded><![CDATA[
<div> Hamiltonian Monte Carlo, Bayesian neural networks, variational inference, stochastic gradient MCMC, uncertainty estimation <br />
Summary: 
This paper introduces a hybrid approach that combines variational inference and Hamiltonian Monte Carlo (HMC) methods to efficiently estimate uncertainties in neural networks. By initially training with variational inference and identifying non-contributory parameters, the dimension of the parameter space is reduced, allowing for faster and more accurate HMC inference. This approach is demonstrated on deep neural networks and operator networks, showing effectiveness in learning surrogates for complex physical systems. The method enables inference for large networks with tens to hundreds of thousands of parameters, showcasing its efficiency and accuracy in quantifying uncertainties. Additionally, the approach is applied to model an operator mapping in hypersonic flow, illustrating its capability in learning complex relations between input conditions and output data. <br /> <div>
arXiv:2507.14652v1 Announce Type: cross 
Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample from the posterior distribution in Bayesian inference. However, HMC techniques are computationally demanding for Bayesian neural networks due to the high dimensionality of the network's parameter space and the non-convexity of their posterior distributions. Therefore, various approximation techniques, such as variational inference (VI) or stochastic gradient MCMC, are often employed to infer the posterior distribution of the network parameters. Such approximations introduce inaccuracies in the inferred distributions, resulting in unreliable uncertainty estimates. In this work, we propose a hybrid approach that combines inexpensive VI and accurate HMC methods to efficiently and accurately quantify uncertainties in neural networks and neural operators. The proposed approach leverages an initial VI training on the full network. We examine the influence of individual parameters on the prediction uncertainty, which shows that a large proportion of the parameters do not contribute substantially to uncertainty in the network predictions. This information is then used to significantly reduce the dimension of the parameter space, and HMC is performed only for the subset of network parameters that strongly influence prediction uncertainties. This yields a framework for accelerating the full batch HMC for posterior inference in neural networks. We demonstrate the efficiency and accuracy of the proposed framework on deep neural networks and operator networks, showing that inference can be performed for large networks with tens to hundreds of thousands of parameters. We show that this method can effectively learn surrogates for complex physical systems by modeling the operator that maps from upstream conditions to wall-pressure data on a cone in hypersonic flow.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Profiling and Address Role Inference in Tokenized U.S. Treasuries</title>
<link>https://arxiv.org/abs/2507.14808</link>
<guid>https://arxiv.org/abs/2507.14808</guid>
<content:encoded><![CDATA[
<div> Tokenized U.S. Treasuries, real-world assets, blockchain networks, transaction-level behavior, functional dissection<br />
<br />
Summary: Tokenized U.S. Treasuries, a subset of real-world assets, are yield-bearing instruments collateralized by sovereign debt deployed on various blockchain networks. This study delves into the transaction-level behavior of U.S. Treasury-backed RWA tokens (e.g., BUIDL, BENJI, USDY) across multiple chains, identifying core functional primitives like issuance, redemption, transfer, and bridge activity. The analysis reveals a distinction in behavior between institutional and retail users. A curvature-aware representation learning framework utilizing Poincar embeddings and liquidity-based graph features is introduced for address-level economic role modeling. The method surpasses baseline models in role inference and extends to anomaly detection and wallet classification in broader blockchain transaction networks. These findings offer insights into functional diversity and participant roles in tokenized Treasuries on a transaction-specific level, enhancing understanding of on-chain financialization.<br /><br /> <div>
arXiv:2507.14808v1 Announce Type: cross 
Abstract: Tokenized U.S. Treasuries have emerged as a prominent subclass of real-world assets (RWAs), offering cryptographically enforced, yield-bearing instruments collateralized by sovereign debt and deployed across multiple blockchain networks. While the market has expanded rapidly, empirical analyses of transaction-level behaviour remain limited. This paper conducts a quantitative, function-level dissection of U.S. Treasury-backed RWA tokens including BUIDL, BENJI, and USDY, across multi-chain: mostly Ethereum and Layer-2s. We analyze decoded contract calls to isolate core functional primitives such as issuance, redemption, transfer, and bridge activity, revealing segmentation in behaviour between institutional actors and retail users. To model address-level economic roles, we introduce a curvature-aware representation learning framework using Poincar\'e embeddings and liquidity-based graph features. Our method outperforms baseline models on our RWA Treasury dataset in role inference and generalizes to downstream tasks such as anomaly detection and wallet classification in broader blockchain transaction networks. These findings provide a structured understanding of functional heterogeneity and participant roles in tokenized Treasury in a transaction-level perspective, contributing new empirical evidence to the study of on-chain financialization.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering</title>
<link>https://arxiv.org/abs/2507.15003</link>
<guid>https://arxiv.org/abs/2507.15003</guid>
<content:encoded><![CDATA[
<div> Keywords: AI teammates, autonomous coding agents, software engineering, dataset, collaboration

Summary:<br /><br />
The paper introduces AIDev, a large-scale dataset capturing the operation of autonomous coding agents in software development. It includes data from over 456,000 pull requests by five leading agents across 61,000 repositories and 47,000 developers. AIDev provides rich metadata on pull requests, authorship, review timelines, code changes, and integration outcomes. The dataset allows for research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. While autonomous agents show faster code submission compared to humans, their pull requests are accepted less frequently, indicating a trust and utility gap. The dataset highlights that even though agents accelerate the submission process, the code they generate is structurally simpler. AIDev is intended to be a living resource for the software engineering and AI communities, aiming to support research into AI-native workflows and symbiotic human-AI collaboration. The dataset is publicly available for further analysis and exploration.<br /><br />Summary: <div>
arXiv:2507.15003v1 Announce Type: cross 
Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI teammates: autonomous, goal-driven systems collaborating with human developers. Among these, autonomous coding agents are especially transformative, now actively initiating, reviewing, and evolving code at scale. This paper introduces AIDev, the first large-scale dataset capturing how such agents operate in the wild. Spanning over 456,000 pull requests by five leading agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across 61,000 repositories and 47,000 developers, AIDev provides an unprecedented empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software engineering, AIDev offers structured, open data to support research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. The dataset includes rich metadata on PRs, authorship, review timelines, code changes, and integration outcomes--enabling exploration beyond synthetic benchmarks like SWE-bench. For instance, although agents often outperform humans in speed, their PRs are accepted less frequently, revealing a trust and utility gap. Furthermore, while agents accelerate code submission--one developer submitted as many PRs in three days as they had in three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev enables a new generation of research into AI-native workflows and supports building the next wave of symbiotic human-AI collaboration. The dataset is publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperelastic nature of the Hoek-Brown criterion</title>
<link>https://arxiv.org/abs/2507.15813</link>
<guid>https://arxiv.org/abs/2507.15813</guid>
<content:encoded><![CDATA[
<div> hyperbolic elasticity, elasto-plastic model, yield criterion, plasticity, finite element simulations
Summary:
The article presents a new nonlinear elasto-plastic model that incorporates hyperbolic elasticity resulting from an invariant yield criterion on the plasticity level. This model combines nonlinear elastic behavior with plasticity adhering to the associated flow rule. It highlights the connection between a linear yield criterion on the thermodynamic force of plasticity and a quadratic yield criterion in stress space, indicating a relationship between different yield criteria. Comparisons between linear and hyperbolic elasticity in the context of the Drucker-Prager yield criterion show the nonlinear case exhibiting dilatancy saturation in triaxial compression tests. Structural finite element simulations are conducted to demonstrate the practicality of the proposed model, showcasing its numerical applicability in various scenarios. <div>
arXiv:2507.15813v1 Announce Type: cross 
Abstract: We propose a nonlinear elasto-plastic model, for which a specific class of hyperbolic elasticity arises as a straight consequence of the yield criterion invariance on the plasticity level. We superimpose this nonlinear elastic (or hyperelastic) behavior with plasticity obeying the associated flow rule. Interestingly, we find that a linear yield criterion on the thermodynamical force associated with plasticity results in a quadratic yield criterion in the stress space. This suggests a specific hyperelastic connection between Mohr-Coulomb and Hoek-Brown (or alternatively between Drucker-Prager and Pan-Hudson) yield criteria. We compare the elasto-plastic responses of standard tests for the Drucker-Prager yield criterion using either linear or the suggested hyperbolic elasticity. Notably, the nonlinear case stands out due to dilatancy saturation observed during cyclic loading in the triaxial compression test. We conclude this study with structural finite element simulations that clearly demonstrate the numerical applicability of the proposed model.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection</title>
<link>https://arxiv.org/abs/2507.13459</link>
<guid>https://arxiv.org/abs/2507.13459</guid>
<content:encoded><![CDATA[
<div> Graph neural network, surrogate modeling, contact mechanics, soft deformable bodies, computational cost 
Summary: 
The article introduces a graph neural network architecture for surrogate modeling in nonlinear boundary value problems in mechanics, focusing on contact between soft deformable bodies. This approach incorporates continuous collision detection and sufficient conditions for contact, improving generalization of the network. The framework is tested on soft tissue mechanics problems, such as predicting the closed state of a bioprosthetic aortic valve, demonstrating better generalization with additional contact terms in the loss function. The network can handle complex contact scenarios with varying reference geometries but comes with high computational costs during training. Despite this, the implementation results in significant speedups for inference, showing up to a thousand-fold improvement on benchmark problems. Overall, the graph neural network offers promising advancements in efficiently solving contact mechanics problems involving soft deformable bodies. 
<br /><br />Summary: <div>
arXiv:2507.13459v1 Announce Type: new 
Abstract: Surrogate models for the rapid inference of nonlinear boundary value problems in mechanics are helpful in a broad range of engineering applications. However, effective surrogate modeling of applications involving the contact of deformable bodies, especially in the context of varying geometries, is still an open issue. In particular, existing methods are confined to rigid body contact or, at best, contact between rigid and soft objects with well-defined contact planes. Furthermore, they employ contact or collision detection filters that serve as a rapid test but use only the necessary and not sufficient conditions for detection. In this work, we present a graph neural network architecture that utilizes continuous collision detection and, for the first time, incorporates sufficient conditions designed for contact between soft deformable bodies. We test its performance on two benchmarks, including a problem in soft tissue mechanics of predicting the closed state of a bioprosthetic aortic valve. We find a regularizing effect on adding additional contact terms to the loss function, leading to better generalization of the network. These benefits hold for simple contact at similar planes and element normal angles, and complex contact at differing planes and element normal angles. We also demonstrate that the framework can handle varying reference geometries. However, such benefits come with high computational costs during training, resulting in a trade-off that may not always be favorable. We quantify the training cost and the resulting inference speedups on various hardware architectures. Importantly, our graph neural network implementation results in up to a thousand-fold speedup for our benchmark problems at inference.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems</title>
<link>https://arxiv.org/abs/2507.14043</link>
<guid>https://arxiv.org/abs/2507.14043</guid>
<content:encoded><![CDATA[
<div> Algorithm, Snake Optimizer, Multi-strategy, Levy flight, UAV path planning

Summary:
The study introduces the Multi-strategy Improved Snake Optimizer (MISO) to enhance the Snake Optimizer algorithm by addressing issues such as slow convergence and local optima traps. MISO incorporates adaptive random disturbance and Levy flight strategies to prevent local optima trapping and improve global optimum exploration. A unique position update strategy combining elite leadership and Brownian motion accelerates convergence speed while maintaining precision. Experimental validation against 30 CEC2017 test functions and the CEC2022 test suite showcases MISO's effectiveness compared to 11 popular algorithms. Application of MISO to UAV 3D path planning and engineering design problems demonstrates superior solution quality and stability, highlighting its potential for practical use.

<br /><br />Summary: <div>
arXiv:2507.14043v1 Announce Type: cross 
Abstract: Metaheuristic algorithms have gained widespread application across various fields owing to their ability to generate diverse solutions. One such algorithm is the Snake Optimizer (SO), a progressive optimization approach. However, SO suffers from the issues of slow convergence speed and susceptibility to local optima. In light of these shortcomings, we propose a novel Multi-strategy Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random disturbance strategy based on sine function to alleviate the risk of getting trapped in a local optimum. Secondly, we introduce adaptive Levy flight strategy based on scale factor and leader and endow the male snake leader with flight capability, which makes it easier for the algorithm to leap out of the local optimum and find the global optimum. More importantly, we put forward a position update strategy combining elite leadership and Brownian motion, effectively accelerating the convergence speed while ensuring precision. Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test functions and the CEC2022 test suite, comparing it with 11 popular algorithms across different dimensions to validate its effectiveness. Moreover, Unmanned Aerial Vehicle (UAV) has been widely used in various fields due to its advantages of low cost, high mobility and easy operation. However, the UAV path planning problem is crucial for flight safety and efficiency, and there are still challenges in establishing and optimizing the path model. Therefore, we apply MISO to the UAV 3D path planning problem as well as 6 engineering design problems to assess its feasibility in practical applications. The experimental results demonstrate that MISO exceeds other competitive algorithms in terms of solution quality and stability, establishing its strong potential for application.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Paradigm Shift to Assembly-like Finite Element Model Updating</title>
<link>https://arxiv.org/abs/2502.02592</link>
<guid>https://arxiv.org/abs/2502.02592</guid>
<content:encoded><![CDATA[
<div> Validation, Finite Element Model, Aircraft, Computational Efficiency, Assembly-Like Approach
<br />
Summary: 
The article introduces a new assembly-like approach for updating finite element models in aeronautics, crucial for developing aircraft with modern flexible wings. This method updates the model as parts are assembled, offering significant computational efficiency by requiring 20% fewer iterations and fewer parameters compared to the traditional one-shot approach. The proposed approach maintains fidelity to the global method while reducing computational burden, making it a promising technique for complex structures. <div>
arXiv:2502.02592v2 Announce Type: replace 
Abstract: In general, there is a mismatch between a finite element model of a structure and its real behaviour. In aeronautics, this mismatch must be small because finite element models are a fundamental part of the development of an aircraft and of increasing importance with the trend to more flexible wings in modern designs. Finite element model updating can be computationally expensive for complex structures and surrogate models can be employed to reduce the computational burden. A novel approach for finite element model updating, namely assembly-like, is proposed and validated using real experimental data. The assembly-like model updating framework implies that the model is updated as parts are assembled. Benchmarking against the classical global, or one-shot, approach demonstrates that the proposed method is more computationally efficient since it takes 20% fewer iterations to obtain convergence, also using fewer parameters for the model evaluations. Despite the increase in computational performance, the new approach retains the fidelity of the global approach.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vector-level Feedforward Control of LPBF Melt Pool Area Using a Physics-Based Thermal Model</title>
<link>https://arxiv.org/abs/2507.12557</link>
<guid>https://arxiv.org/abs/2507.12557</guid>
<content:encoded><![CDATA[
<div> Keywords: Laser powder bed fusion, additive manufacturing, feedforward control, melt pool area, part quality <br />
Summary: 
A new feedforward control framework for regulating melt pool area in Laser Powder Bed Fusion (LPBF) additive manufacturing is proposed. The framework combines a thermal model and a melt pool model to efficiently predict and optimize melt pool area, reducing geometric inaccuracies and porosity in metal parts. Calibration of the models using minimal experiments allows for accurate control of laser power scheduling. The framework was successfully validated on complex 3D geometries made of Inconel 718 and 316L stainless steel, showing significant improvements in part quality metrics. By proactively compensating for thermal effects, the approach demonstrates enhanced part quality while remaining computationally efficient and adaptable to different materials and machines. Overall, the vector-level feedforward control framework presents a promising method for improving the quality of LPBF-produced parts. <br /><br /> <div>
arXiv:2507.12557v1 Announce Type: new 
Abstract: Laser powder bed fusion (LPBF) is an additive manufacturing technique that has gained popularity thanks to its ability to produce geometrically complex, fully dense metal parts. However, these parts are prone to internal defects and geometric inaccuracies, stemming in part from variations in the melt pool. This paper proposes a novel vector-level feedforward control framework for regulating melt pool area in LPBF. By decoupling part-scale thermal behavior from small-scale melt pool physics, the controller provides a scale-agnostic prediction of melt pool area and efficient optimization over it. This is done by operating on two coupled lightweight models: a finite-difference thermal model that efficiently captures vector-level temperature fields and a reduced-order, analytical melt pool model. Each model is calibrated separately with minimal single-track and 2D experiments, and the framework is validated on a complex 3D geometry in both Inconel 718 and 316L stainless steel. Results showed that feedforward vector-level laser power scheduling reduced geometric inaccuracy in key dimensions by 62%, overall porosity by 16.5%, and photodiode variation by 6.8% on average. Overall, this modular, data-efficient approach demonstrates that proactively compensating for known thermal effects can significantly improve part quality while remaining computationally efficient and readily extensible to other materials and machines.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDS-Net: A novel framework for few-shot photovoltaic power prediction with interpretable dynamic selection and feature information fusion</title>
<link>https://arxiv.org/abs/2507.12745</link>
<guid>https://arxiv.org/abs/2507.12745</guid>
<content:encoded><![CDATA[
<div> transfer learning, PV power stations, feature selection, interpretable dynamic selection network, prediction

Summary:
The article introduces a novel interpretable dynamic selection network (IDS-Net) for accurate few-shot prediction in PV power stations. The framework includes pre-training on a large dataset, feature selection using the ReliefF algorithm, and outlier correction with the Hampel Identifier. The IDS-Net model incorporates interpretable weights and adaptive selection outcomes for accurate predictions. An end-to-end adaptive transfer learning strategy is designed for final prediction results on the target dataset. The framework's effectiveness and generalization are demonstrated using two PV power datasets from Hebei province, China. <div>
arXiv:2507.12745v1 Announce Type: new 
Abstract: With the growing demand for renewable energy, countries are accelerating the construction of photovoltaic (PV) power stations. However, accurately forecasting power data for newly constructed PV stations is extremely challenging due to limited data availability. To this end, we propose a novel interpretable dynamic selection network (IDS-Net) based on feature information fusion to achieve accurate few-shot prediction. This transfer learning framework primarily consists of two parts. In the first stage, we pre-train on the large dataset, utilizing Maximum Mean Discrepancy (MMD) to select the source domain dataset most similar to the target domain data distribution. Subsequently, the ReliefF algorithm is utilized for feature selection, reducing the influence of feature redundancy. Then, the Hampel Identifier (HI) is used for training dataset outlier correction. In the IDS-Net model, we first obtain the initial extracted features from a pool of predictive models. Following this, two separate weighting channels are utilized to determine the interpretable weights for each sub-model and the adaptive selection outcomes, respectively. Subsequently, the extracted feature results from each sub-model are multiplied by their corresponding weights and then summed to obtain the weighted extracted features. Then, we perform cross-embedding on the additional features and fuse them with the extracted weighted features. This fused information is then passed through the MLP (Multi-Layer Perceptron) layer to obtain predictions. In the second stage, we design an end-to-end adaptive transfer learning strategy to obtain the final prediction results on the target dataset. We validate the transfer learning process using two PV power datasets from Hebei province, China, to demonstrate the effectiveness and generalization of our framework and transfer learning strategy.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Reinforcement Learning with LSTM Forecasting Signals for Optimizing Fintech Trading Decisions</title>
<link>https://arxiv.org/abs/2507.12835</link>
<guid>https://arxiv.org/abs/2507.12835</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, quantum circuits, financial systems, LSTM predictions, performance<br />
<br />
Summary: <br />
Financial trading environments are complex and dynamic, posing challenges for traditional reinforcement learning methods. This study introduces a novel approach by integrating quantum circuits into a reinforcement learning framework customized for financial systems. The comparison between classical A3C and quantum A3C algorithms, alongside incorporating LSTM-based predictions of economic trends, reveals that quantum models with predictive signals outperform traditional methods. The experiments conducted in a Gymnasium-compatible trading environment demonstrate the superior performance and stability of quantum models, even with shallow quantum circuit depth, in noisy financial conditions. This research highlights the potential of quantum reinforcement learning in tackling the complexities of financial markets and improving trading strategies. <div>
arXiv:2507.12835v1 Announce Type: new 
Abstract: Financial trading environments are characterized by high volatility, numerous macroeconomic signals, and dynamically shifting market regimes, where traditional reinforcement learning methods often fail to deliver breakthrough performance. In this study, we design a reinforcement learning framework tailored for financial systems by integrating quantum circuits. We compare (1) the performance of classical A3C versus quantum A3C algorithms, and (2) the impact of incorporating LSTM-based predictions of the following week's economic trends on learning outcomes. The experimental framework adopts a custom Gymnasium-compatible trading environment, simulating discrete trading actions and evaluating rewards based on portfolio feedback. Experimental results show that quantum models - especially when combined with predictive signals - demonstrate superior performance and stability under noisy financial conditions, even with shallow quantum circuit depth.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic Chain-of-Thought Synthesis Optimization</title>
<link>https://arxiv.org/abs/2507.12901</link>
<guid>https://arxiv.org/abs/2507.12901</guid>
<content:encoded><![CDATA[
<div> Dataset, financial reasoning, language models, CoT synthesis, knowledge space

Summary:
Agentar-DeepFinance-300K is a new large-scale financial reasoning dataset created with a focus on optimizing chain-of-thought (CoT) synthesis for robust financial reasoning. The dataset is generated using a Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) pipeline to ensure comprehensive and deep financial reasoning trajectories. The researchers also conducted a systematic investigation called CoT Cube to analyze critical factors affecting the effectiveness of CoT, such as necessity, length, and synthesizer. Models trained on Agentar-DeepFinance-300K show significant improvements on financial benchmarks, highlighting the importance of well-designed CoT construction in financial reasoning models. The dataset is publicly available to advance research in financial reasoning models.
<br /><br />Summary: <div>
arXiv:2507.12901v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have demonstrated remarkable general reasoning capabilities, holding significant potential for applications in the financial domain, a field that requires robust and reliable reasoning. It has been demonstrated that distilling high-quality chain-of-thought (CoT) rationales from advanced general reasoning models offers a promising and efficient path to the financial reasoning model. However, existing CoT synthesis methods suffer from shallow CoT sampling, leaving the question of how to construct a well-designed knowledge space for finance reasoning unexplored. In this paper, we present \textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset characterized by its systematic CoT synthesis optimization. We first introduce a comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and deep financial reasoning trajectories. Furthermore, a systematic investigation, termed CoT Cube, is conducted to analyze critical factors that influence CoT effectiveness, such as necessity, length and synthesizer, yielding valuable insights for high-quality financial CoT construction. Experiments demonstrate that models trained on our Agentar-DeepFinance-300K achieve significant improvements on financial benchmarks. We publicly release Agentar-DeepFinance-300K , hoping to advance the research in financial reasoning models.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To What Extent Can Public Equity Indices Statistically Hedge Real Purchasing Power Loss in Compounded Structural Emerging-Market Crises? An Explainable ML-Based Assessment</title>
<link>https://arxiv.org/abs/2507.13055</link>
<guid>https://arxiv.org/abs/2507.13055</guid>
<content:encoded><![CDATA[
<div> Keywords: local public equity indices, real purchasing power loss, macro-financial collapses, emerging markets, tail dependence copula analysis

Summary: 
This study examines the effectiveness of local public equity indices in hedging real purchasing power loss during macro-financial collapses in emerging markets. Using non-linear real return calculations and advanced statistical analysis techniques, the research focuses on collapse episodes in Turkey (2018), Nigeria (2020), and Pakistan (2021). The findings highlight the limitations of using equity-based protection during simultaneous macroeconomic and monetary dislocations. The study challenges traditional notions of equity pricing theory for inflation and devaluation hedge effectiveness and emphasizes the need for context-sensitive strategies in times of compounded macro-financial distress. The analysis underscores the importance of considering tail risk and the potential breakdown of equity-based protection mechanisms during crises. Contextual factors and crisis triggers play a significant role in shaping the effectiveness of equity indices in preserving purchasing power during economic turmoil. <div>
arXiv:2507.13055v1 Announce Type: new 
Abstract: This study investigates the extent to which local public equity indices can statistically hedge real purchasing power loss during compounded structural macro-financial collapses in emerging markets. We employ a non-linear multiplicative real return calculations consistent with Fisher-parity logics for both domestic and foreign investors with a principled quantile regression, tail dependence copula analysis, and Shapley Additive Explanations (SHAP) to assess the explanatory power of macro variables. The analysis focuses on three recent and data-accessible exemplary collapse episodes: Turkey (2018), Nigeria (2020), and Pakistan (2021). Such cases, selected to align with post-2018 improvements in data standardization and crisis comparability, span varied monetary regimes and crisis triggers. Our tail-focused modeling reveals a systematic breakdown in public-equity-based purchasing power protection precisely during simultaneous macroeconomic and monetary dislocations when such protection is most needed. The findings call into question conventional inflation and devaluation hedge presumptions in equity pricing theory, emphasizing the limitations of equity-based protection and the need for context-sensitive strategies during compounded macro-financial distress.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding</title>
<link>https://arxiv.org/abs/2507.12482</link>
<guid>https://arxiv.org/abs/2507.12482</guid>
<content:encoded><![CDATA[
<div> retrieval, code generation, autonomous code understanding, debugging, software maintenance <br />
Summary: 
The article introduces Kodezi Chronos, a new architecture for autonomous code understanding that can operate across ultra-long contexts without fixed window limits. It leverages a multi-level embedding memory engine to efficiently reason over millions of lines of code, supporting tasks like repository-scale comprehension and real-time self-healing actions. A novel benchmark, the Multi Random Retrieval, evaluates the models ability to resolve distant associations across code artifacts, outperforming prior models by 23% in bug detection. Chronos reduces debugging cycles by up to 40% compared to traditional approaches. By integrating with IDEs and CI/CD workflows, Chronos enhances code reliability and productivity, reducing manual effort and advancing towards self-sustaining software ecosystems. <br /><br />Summary: <div>
arXiv:2507.12482v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have advanced code generation and software automation, but are fundamentally constrained by limited inference-time context and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a next-generation architecture for autonomous code understanding, debugging, and maintenance, designed to operate across ultra-long contexts comprising entire codebases, histories, and documentation, all without fixed window limits. Kodezi Chronos leverages a multi-level embedding memory engine, combining vector and graph-based indexing with continuous code-aware retrieval. This enables efficient and accurate reasoning over millions of lines of code, supporting repository-scale comprehension, multi-file refactoring, and real-time self-healing actions. Our evaluation introduces a novel Multi Random Retrieval benchmark, specifically tailored to the software engineering domain. Unlike classical retrieval benchmarks, this method requires the model to resolve arbitrarily distant and obfuscated associations across code artifacts, simulating realistic tasks such as variable tracing, dependency migration, and semantic bug localization. Chronos outperforms prior LLMs and code models, demonstrating a 23% improvement in real-world bug detection and reducing debugging cycles by up to 40% compared to traditional sequence-based approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos enables seamless, autonomous software maintenance, elevating code reliability and productivity while reducing manual effort. These results mark a critical advance toward self-sustaining, continuously optimized software ecosystems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RONOM: Reduced-Order Neural Operator Modeling</title>
<link>https://arxiv.org/abs/2507.12814</link>
<guid>https://arxiv.org/abs/2507.12814</guid>
<content:encoded><![CDATA[
<div> Reduced-order modeling, neural operators, time-dependent partial differential equations, discretization error bound, spatial super-resolution.<br />
Summary:<br />
The article introduces Reduced-Order Neural Operator Modeling (RONOM), combining concepts from reduced-order modeling (ROM) and operator learning. It addresses the computational intensity of time-dependent partial differential equations in many-query scenarios. RONOM bridges the gap between ROM and neural operator approaches, providing insights into discretization convergence and robustness. The framework offers a discretization error bound analogous to ROM for rigorous numerical error estimation. Comparing RONOM to existing neural operators in solving PDEs, results show RONOM's standard vector-to-vector neural networks achieve comparable input generalization and superior performance in spatial super-resolution and discretization robustness. Additionally, RONOM offers novel insights into temporal super-resolution scenarios. <div>
arXiv:2507.12814v1 Announce Type: cross 
Abstract: Time-dependent partial differential equations are ubiquitous in physics-based modeling, but they remain computationally intensive in many-query scenarios, such as real-time forecasting, optimal control, and uncertainty quantification. Reduced-order modeling (ROM) addresses these challenges by constructing a low-dimensional surrogate model but relies on a fixed discretization, which limits flexibility across varying meshes during evaluation. Operator learning approaches, such as neural operators, offer an alternative by parameterizing mappings between infinite-dimensional function spaces, enabling adaptation to data across different resolutions. Whereas ROM provides rigorous numerical error estimates, neural operator learning largely focuses on discretization convergence and invariance without quantifying the error between the infinite-dimensional and the discretized operators. This work introduces the reduced-order neural operator modeling (RONOM) framework, which bridges concepts from ROM and operator learning. We establish a discretization error bound analogous to those in ROM, and get insights into RONOM's discretization convergence and discretization robustness. Moreover, two numerical examples are presented that compare RONOM to existing neural operators for solving partial differential equations. The results demonstrate that RONOM using standard vector-to-vector neural networks achieves comparable performance in input generalization and superior performance in both spatial super-resolution and discretization robustness, while also offering novel insights into temporal super-resolution scenarios.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying data needs in surrogate modeling for flow fields in 2D stirred tanks with physics-informed neural networks (PINNs)</title>
<link>https://arxiv.org/abs/2507.11640</link>
<guid>https://arxiv.org/abs/2507.11640</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Surrogate Models, Stirred Tanks, Computational Fluid Dynamics, Data Requirements<br />
<br />Summary:
Physics-informed neural networks (PINNs) are proposed as a solution to develop efficient surrogate models for flow fields in stirred tanks. This study investigates the data requirements for developing these models and compares them with classical neural networks and boundary-informed neural networks (BINNs). The results show that PINNs can achieve accurate predictions with as few as six data points, demonstrating their effectiveness in reducing data requirements. Surrogate models can achieve prediction errors of around 3% across a range of Reynolds numbers, with an approximation of velocity profile leading to errors of 2.5%. This study highlights the potential of PINNs in efficiently modeling flow fields in stirred tanks, even with limited or approximate datasets. <div>
arXiv:2507.11640v1 Announce Type: new 
Abstract: Stirred tanks are vital in chemical and biotechnological processes, particularly as bioreactors. Although computational fluid dynamics (CFD) is widely used to model the flow in stirred tanks, its high computational cost$-$especially in multi-query scenarios for process design and optimization$-$drives the need for efficient data-driven surrogate models. However, acquiring sufficiently large datasets can be costly. Physics-informed neural networks (PINNs) offer a promising solution to reduce data requirements while maintaining accuracy by embedding underlying physics into neural network (NN) training. This study quantifies the data requirements of vanilla PINNs for developing surrogate models of a flow field in a 2D stirred tank. We compare these requirements with classical supervised neural networks and boundary-informed neural networks (BINNs). Our findings demonstrate that surrogate models can achieve prediction errors around 3% across Reynolds numbers from 50 to 5000 using as few as six datapoints. Moreover, employing an approximation of the velocity profile in place of real data labels leads to prediction errors of around 2.5%. These results indicate that even with limited or approximate datasets, PINNs can be effectively trained to deliver high accuracy comparable to high-fidelity data.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MNO : A Multi-modal Neural Operator for Parametric Nonlinear BVPs</title>
<link>https://arxiv.org/abs/2507.11870</link>
<guid>https://arxiv.org/abs/2507.11870</guid>
<content:encoded><![CDATA[
<div> Multi-parameter, Nonlinear, Boundary value problems, Multimodal Neural Operator, Generalized FMM<br />
Summary:<br />
The article introduces a novel Multimodal Neural Operator (MNO) architecture for learning solution operators for multi-parameter nonlinear boundary value problems (BVPs). Unlike traditional neural operators that map PDE coefficients or source terms independently, the MNO architecture can map multiple parameters, including PDE coefficients, source terms, and boundary conditions, to the solution space in a unified manner. Inspired by the Fast Multipole Method (FMM), the MNO consists of three key components: a Generalized FMM (GFMM) block, a Unimodal Neural Operator (UNO) for single parameter mappings, and a multimodal fusion mechanism. Experiment results demonstrate the MNO's capability to handle variations in PDE coefficients and source or boundary terms simultaneously. <div>
arXiv:2507.11870v1 Announce Type: new 
Abstract: We introduce a novel Multimodal Neural Operator (MNO) architecture designed to learn solution operators for multi-parameter nonlinear boundary value problems (BVPs). Traditional neural operators primarily map either the PDE coefficients or source terms independently to the solution, limiting their flexibility and applicability. In contrast, our proposed MNO architecture generalizes these approaches by mapping multiple parameters including PDE coefficients, source terms, and boundary conditions to the solution space in a unified manner. Our MNO is motivated by the hierarchical nested bases of the Fast Multipole Method (FMM) and is constructed systematically through three key components: a parameter efficient Generalized FMM (GFMM) block, a Unimodal Neural Operator (UNO) built upon GFMM blocks for single parameter mappings, and most importantly, a multimodal fusion mechanism extending these components to learn the joint map. We demonstrate the multimodal generalization capacity of our approach on both linear and nonlinear BVPs. Our experiments show that the network effectively handles simultaneous variations in PDE coefficients and source or boundary terms.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Fourier Neural Operators for Micromechanics</title>
<link>https://arxiv.org/abs/2507.12233</link>
<guid>https://arxiv.org/abs/2507.12233</guid>
<content:encoded><![CDATA[
<div> Fourier Neural Operators, Micromechanics, Homogenization, Deep Learning, Fast Fourier Transform <br />
Summary: <br />
The article discusses the use of Fourier Neural Operators (FNOs) in solving cell problems in micromechanics. Traditional computational frameworks outperform deep-learning frameworks in this area, and the potential of machine-learning approaches for micromechanics is unclear. The study shows that FNOs, empowered by insights from fast Fourier transform (FFT) methods, can accurately predict solutions to cell problems with arbitrary stiffness distribution, subject to a material-contrast constraint. This approach does not require restrictions on material properties, number of phases, or geometry of interfaces between materials. The fidelity provided by FNOs is sharp and uniform, with explicit guarantees of accuracy. Furthermore, an FNO explicitly constructed without training demonstrates the universal approximation property, with memory requirements and runtimes comparable to classical FFT solvers. This work aims to bridge the gap between FFT-based methods and FNOs, potentially facilitating collaboration between the two approaches. <br /> <div>
arXiv:2507.12233v1 Announce Type: new 
Abstract: \noindent Solving cell problems in homogenization is hard, and available deep-learning frameworks fail to match the speed and generality of traditional computational frameworks. More to the point, it is generally unclear what to expect of machine-learning approaches, let alone single out which approaches are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs) for micromechanics, empowering them by insights from computational micromechanics methods based on the fast Fourier transform (FFT). We construct an FNO surrogate mimicking the basic scheme foundational for FFT-based methods and show that the resulting operator predicts solutions to cell problems with \emph{arbitrary} stiffness distribution only subject to a material-contrast constraint up to a desired accuracy. In particular, there are no restrictions on the material symmetry like isotropy, on the number of phases and on the geometry of the interfaces between materials. Also, the provided fidelity is sharp and uniform, providing explicit guarantees leveraging our physical empowerment of FNOs. To show the desired universal approximation property, we construct an FNO explicitly that requires no training to begin with. Still, the obtained neural operator complies with the same memory requirements as the basic scheme and comes with runtimes proportional to classical FFT solvers. In particular, large-scale problems with more than 100 million voxels are readily handled. The goal of this work is to underline the potential of FNOs for solving micromechanical problems, linking FFT-based methods to FNOs. This connection is expected to provide a fruitful exchange between both worlds.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Identification of Nonlinear Dynamics with Conformal Prediction</title>
<link>https://arxiv.org/abs/2507.11739</link>
<guid>https://arxiv.org/abs/2507.11739</guid>
<content:encoded><![CDATA[
<div> Sparse Identification of Nonlinear Dynamics, SINDy, uncertainty quantification, Conformal Prediction, Ensemble-SINDy<br />
<br />
Summary: 
The article investigates the integration of Conformal Prediction with Ensemble-SINDy (E-SINDy) for uncertainty quantification in nonlinear dynamical system models. The study focuses on three key applications: quantifying uncertainty in time series prediction, model selection based on library feature importance, and assessing the uncertainty of identified model coefficients using feature conformal prediction. Results show that the integration of Conformal Prediction with E-SINDy can reliably achieve target coverage for time series forecasting, effectively quantify feature importance in model selection, and produce robust uncertainty intervals for model coefficients, even under non-Gaussian noise conditions. The study demonstrates the versatility and reliability of the approach in various scenarios, including stochastic predator-prey dynamics and chaotic dynamical systems. <div>
arXiv:2507.11739v1 Announce Type: cross 
Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for discovering nonlinear dynamical system models from data. Quantifying uncertainty in SINDy models is essential for assessing their reliability, particularly in safety-critical applications. While various uncertainty quantification methods exist for SINDy, including Bayesian and ensemble approaches, this work explores the integration of Conformal Prediction, a framework that can provide valid prediction intervals with coverage guarantees based on minimal assumptions like data exchangeability. We introduce three applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1) quantifying uncertainty in time series prediction, (2) model selection based on library feature importance, and (3) quantifying the uncertainty of identified model coefficients using feature conformal prediction. We demonstrate the three applications on stochastic predator-prey dynamics and several chaotic dynamical systems. We show that conformal prediction methods integrated with E-SINDy can reliably achieve desired target coverage for time series forecasting, effectively quantify feature importance, and produce more robust uncertainty intervals for model coefficients, even under non-Gaussian noise, compared to standard E-SINDy coefficient estimates.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Purity: Defense Paradigm For Chain-of-Thought Attack</title>
<link>https://arxiv.org/abs/2507.12314</link>
<guid>https://arxiv.org/abs/2507.12314</guid>
<content:encoded><![CDATA[
<div> Large Reasoning Models, Chain-of-Thought Attack, security threats, reinforcement learning, Thought Purity<br />
Summary:<br />
The article discusses the vulnerability of reinforcement learning-trained Large Reasoning Models to Chain-of-Thought Attack (CoTA) due to backdoor prompt attacks. CoTA exploits prompt controllability, compromising both safety and task performance. To address this, the authors propose a defense mechanism called Thought Purity (TP) with three key components: a safety-optimized data processing pipeline, reinforcement learning-enhanced rule constraints, and adaptive monitoring metrics. TP aims to strengthen resistance to malicious content while maintaining operational efficacy. This approach offers the first comprehensive defense against CoTA vulnerabilities in reasoning systems aligned with reinforcement learning, enhancing the security-functionality balance in future AI architectures. <br /><br /> <div>
arXiv:2507.12314v1 Announce Type: cross 
Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g., Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large Language Models (LLMs) domain, their susceptibility to security threats remains a critical vulnerability. This weakness is particularly evident in Chain-of-Thought (CoT) generation processes, where adversarial methods like backdoor prompt attacks can systematically subvert the model's core reasoning mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this vulnerability through exploiting prompt controllability, simultaneously degrading both CoT safety and task performance with low-cost interventions. To address this compounded security-performance vulnerability, we propose Thought Purity (TP): a defense paradigm that systematically strengthens resistance to malicious content while preserving operational efficacy. Our solution achieves this through three synergistic components: (1) a safety-optimized data processing pipeline (2) reinforcement learning-enhanced rule constraints (3) adaptive monitoring metrics. Our approach establishes the first comprehensive defense mechanism against CoTA vulnerabilities in reinforcement learning-aligned reasoning systems, significantly advancing the security-functionality equilibrium for next-generation AI architectures.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data</title>
<link>https://arxiv.org/abs/2507.12425</link>
<guid>https://arxiv.org/abs/2507.12425</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Enterprise Data, Dense Embeddings, Metadata-aware Filtering

Summary: 
The study introduces an advanced Retrieval-Augmented Generation (RAG) framework for enterprise data that combines hybrid retrieval strategies using dense embeddings and BM25, along with metadata-aware filtering and cross-encoder reranking. By applying semantic chunking and retaining tabular data structures, the framework ensures textual coherence and maintains the integrity of data. Experiments on enterprise datasets demonstrate significant improvements in Precision@5, Recall@5, and Mean Reciprocal Rank. Qualitative evaluations also show higher scores in Faithfulness, Completeness, and Relevance. The framework delivers accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work involves extending the framework to handle multimodal data and integrating agent-based retrieval. The source code will be made available on GitHub at the provided link.<br /><br />Summary: <div>
arXiv:2507.12425v1 Announce Type: cross 
Abstract: Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Algorithms and Implementations for Computing the Minimum Distance of Quantum Codes</title>
<link>https://arxiv.org/abs/2408.10743</link>
<guid>https://arxiv.org/abs/2408.10743</guid>
<content:encoded><![CDATA[
<div> stabilizer quantum code, symplectic distance, fast algorithms, computational time, shared-memory parallel architectures  
Summary: The article introduces three new fast algorithms and implementations for computing the symplectic distance of stabilizer quantum codes. The distance of a stabilizer quantum code is crucial for error detection and correction. The new algorithms, based on the Brouwer-Zimmermann algorithm, outperform current state-of-the-art implementations on various processors, showing significant improvements in computational time, especially in highly demanding cases. The study demonstrates superior performance on single-core processors, multicore processors, and shared-memory multiprocessors, with scalability observed on shared-memory parallel architectures. These advancements in computing the symplectic distance offer a substantial leap forward in the efficiency and speed of error detection and correction in stabilizer quantum codes. <br /><br />Summary: <div>
arXiv:2408.10743v2 Announce Type: replace-cross 
Abstract: The distance of a stabilizer quantum code is a very important feature since it determines the number of errors that can be detected and corrected. We present three new fast algorithms and implementations for computing the symplectic distance of the associated classical code. Our new algorithms are based on the Brouwer-Zimmermann algorithm. Our experimental study shows that these new implementations are much faster than current state-of-the-art licensed implementations on single-core processors, multicore processors, and shared-memory multiprocessors. In the most computationally-demanding cases, the performance gain in the computational time can be larger than one order of magnitude. The experimental study also shows a good scalability on shared-memory parallel architectures.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Three-dimensional SPH modeling of brittle fracture under hydrodynamic loading</title>
<link>https://arxiv.org/abs/2507.10553</link>
<guid>https://arxiv.org/abs/2507.10553</guid>
<content:encoded><![CDATA[
<div> Modeling fluid-structure interactions, three-dimensional SPH computational framework, weakly compressible SPH, pseudo-spring-based SPH solver, structural deformation, structural failure<br />
<br />
Summary:<br />
A three-dimensional computational framework using SPH for fluid-structure interactions is introduced. The model integrates weakly compressible SPH with a pseudo-spring-based solver to simulate fluid flow and deformable structures while capturing solid boundaries and fluid-structure interfaces without contact forces. Pressure calculations in the fluid phase are enhanced by the $\delta$-SPH technique, and structural damage is modeled using a pseudo-spring approach with limited particle interactions. The framework accurately simulates detailed fracture patterns without complex crack-tracking algorithms. It has shown effectiveness compared to existing models and experimental data, providing insights into the effects of hydrodynamic events on structural integrity.<br /><br /> <div>
arXiv:2507.10553v1 Announce Type: new 
Abstract: A three-dimensional SPH computational framework is presented for modeling fluid-structure interactions with structural deformation and failure. We combine weakly compressible SPH with a pseudo-spring-based SPH solver to capture the fluid flow and deformable structures. A unified modeling approach captures the solid boundaries and fluid-structure interfaces without penalty-based contact force. The $\delta$-SPH technique improves the pressure calculations in the fluid phase, while structural damage is modeled using a pseudo-spring approach, with particle interactions limited to its neighbors. The present framework can capture the three-dimensional crack surfaces in structures without any computationally intensive crack-tracking algorithm or visibility criteria. The framework has been proven effective against existing models and experimental data, demonstrating high accuracy and robustness in simulating detailed fracture patterns and offering insights into the impact of hydrodynamic events on structural integrity.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Multiple Time-Stepping Method for 3-Body Interactions in High Performance Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2507.11172</link>
<guid>https://arxiv.org/abs/2507.11172</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular dynamics, two-body interactions, three-body interactions, r-RESPA algorithm, High Performance Computing

Summary:
This study focuses on improving the efficiency of molecular dynamics (MD) simulations by incorporating two-body and three-body interactions. Traditional two-body potentials may not fully capture the complexity of molecular systems, necessitating the inclusion of three-body interactions. However, three-body interactions are computationally expensive due to their cubic complexity class. The r-RESPA algorithm is utilized to reduce the number of three-body interaction calculations, enhancing efficiency. The study explores this method in the context of High Performance Computing (HPC) methods for parallelizing calculations. It introduces a communication-reducing distributed-memory parallel method and a novel shared-memory parallel cutoff method implemented in the particle simulation library AutoPas. The results and methods discussed offer insights into potential advancements in MD simulation efficiency. 

<br /><br />Summary: <div>
arXiv:2507.11172v1 Announce Type: new 
Abstract: Understanding the complex behavior of molecular systems is fundamental to fields such as physics, materials science, and biology. Molecular dynamics (MD) simulations are crucial tools for studying atomic-level dynamics. This work focuses on improving the efficiency of MD simulations involving two-body and three-body interactions. Traditional two-body potentials often can not fully capture the complexity of molecular systems, making the inclusion of three-body interactions important. However, these interactions are in a cubic complexity class, compared to a quadratic one for two-body interactions, and therefore are computationally expensive, even when a cutoff distance is applied. One way to improve efficiency is to use the r-RESPA multiple time-stepping algorithm to reduce the number of three-body interaction calculations. In this work, we investigate this method in the context of High Performance Computing (HPC) methods that parallelize the calculations. In particular, we investigate a communication-reducing distributed-memory parallel method from literature and present a novel shared-memory parallel cutoff method, implemented in the particle simulation library AutoPas. The results and methods are discussed, providing insights into potential advancements in MD simulation efficiency.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Differential Evolution in Tire Industry Extrusion: Leveraging Surrogate Models</title>
<link>https://arxiv.org/abs/2507.11191</link>
<guid>https://arxiv.org/abs/2507.11191</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, data-driven optimization, manufacturing systems, machine learning, metaheuristic<br />
<br />Summary: 
This study introduces a data-driven methodology for optimizing complex manufacturing systems using historical process data. By utilizing machine learning models to create surrogate models, the approach, Data-Driven Differential Evolution with Multi-Level Penalty Functions and Surrogate Models, is tailored to the specifics of the industry. The method is applied to an extrusion process in tire manufacturing to optimize initialization parameters and reduce waste and production time. Results show a 65% reduction in setup time and a decrease in material waste compared to historical configurations, demonstrating the superiority of the surrogate-based optimization approach. This research underscores the advantages of combining data-driven modeling with metaheuristic optimization for industrial processes lacking explicit formulations. <br /><br /> <div>
arXiv:2507.11191v1 Announce Type: new 
Abstract: The optimization of industrial processes remains a critical challenge, particularly when no mathematical formulation of objective functions or constraints is available. This study addresses this issue by proposing a surrogate-based, data-driven methodology for optimizing complex real-world manufacturing systems using only historical process data. Machine learning models are employed to approximate system behavior and construct surrogate models, which are integrated into a tailored metaheuristic approach: Data-Driven Differential Evolution with Multi-Level Penalty Functions and Surrogate Models, an adapted version of Differential Evolution suited to the characteristics of the studied process. The methodology is applied to an extrusion process in the tire manufacturing industry, with the goal of optimizing initialization parameters to reduce waste and production time. Results show that the surrogate-based optimization approach outperforms historical best configurations, achieving a 65\% reduction in initialization and setup time, while also significantly minimizing material waste. These findings highlight the potential of combining data-driven modeling and metaheuristic optimization for industrial processes where explicit formulations are unavailable.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tax-Efficient Model Predictive Control Policy for Retirement Funding</title>
<link>https://arxiv.org/abs/2507.10603</link>
<guid>https://arxiv.org/abs/2507.10603</guid>
<content:encoded><![CDATA[
<div> taxes, retirement funding, inflation, investment returns, bequest
<br />
Summary: 
The article presents a retirement funding policy that addresses the challenge of managing a retiree's savings to ensure constant post-tax inflation-adjusted consumption throughout their lifetime. The policy involves two main steps. Firstly, a simplified planning problem is formulated as a convex optimization problem to maximize the bequest while maintaining a constant inflation-adjusted consumption target. This allows for quick and reliable solution of the planning problem. Secondly, a model predictive control (MPC) retirement policy is developed based on the annual update-plan-act cycle. The MPC policy takes into account uncertain factors such as investment returns, inflation, changes in life expectancy, external income, liabilities, and tax rules and rates. The effectiveness of the MPC retirement policy is demonstrated through Monte Carlo simulation, showcasing its ability to adapt to changing circumstances and provide a reliable approach to retirement funding. <div>
arXiv:2507.10603v1 Announce Type: cross 
Abstract: The retirement funding problem addresses the question of how to manage a retiree's savings to provide her with a constant post-tax inflation adjusted consumption throughout her lifetime. This consists of choosing withdrawals and transfers from and between several accounts with different tax treatments, taking into account basic rules such as required minimum distributions and limits on Roth conversions, additional income, liabilities, taxes, and the bequest when the retiree dies. We develop a retirement funding policy in two steps. In the first step, we consider a simplified planning problem in which various future quantities, such as the retiree's remaining lifetime, future investment returns, and future inflation, are known. Using a simplified model of taxes, we pose this planning problem as a convex optimization problem, where we maximize the bequest subject to providing a constant inflation adjusted consumption target. Since this problem is convex, it can be solved quickly and reliably. We leverage this planning method to form a retirement funding policy that determines the actions to take each year, based on information known at that time. Each year the retiree forms a new plan for the future years, using the current account values and life expectancy, and optionally, updated information such as changes in tax rates or rules. The retiree then carries out the actions from the first year of the current plan. This update-plan-act cycle is repeated each year, a general policy called model predictive control (MPC). The MPC retirement policy reacts to the effects of uncertain investment returns and inflation, changes in the retiree's expected lifetime or external income and liabilities, and changes in tax rules and rates. We demonstrate the effectiveness of the MPC retirement policy using Monte Carlo simulation.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong</title>
<link>https://arxiv.org/abs/2507.11502</link>
<guid>https://arxiv.org/abs/2507.11502</guid>
<content:encoded><![CDATA[
<div> foundational sovereign large language model, Hong Kong, multilingual, value-aligned, AI infrastructure<br />
Summary: This paper introduces HKGAI-V1, a large language model tailored for Hong Kong's unique multilingual and socio-legal environment. Developed using the DeepSeek architecture, the model is aligned with regional norms through full parameter fine-tuning. Integrated with a retrieval-augmented generation system, it provides timely and factual information access. The paper showcases two key achievements: the successful development of HKGAI-V1, outperforming general-purpose models in handling culturally sensitive queries, and the creation of the Adversarial HK Value Benchmark for evaluating model alignment with local ethical and legal standards. This work presents a replicable blueprint for developing regionally focused AI systems with a strong emphasis on local identity and values.<br /> <div>
arXiv:2507.11502v1 Announce Type: cross 
Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign large language model (LLM), developed as part of an initiative to establish value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing the region's unique multilingual environment (Cantonese, Mandarin, and English), its distinct socio-legal context under the "one country, two systems" framework, and specific local cultural and value considerations, the model is built upon the DeepSeek architecture and systematically aligned with regional norms through a multifaceted full parameter fine-tuning process. It is further integrated with a retrieval-augmented generation (RAG) system to ensure timely and factually grounded information access. The core contribution lies in the design and implementation of a comprehensive, region-specific AI alignment and safety framework, demonstrated through two key achievements: 1) The successful development of HKGAI-V1 itself - which outper-forms general-purpose models in handling Hong Kong-specific culturally sensitive queries, and embodies a "governance-embedded" approach to digital sovereignty - empowers Hong Kong to exercise control over AI applications in critical sectors including public services, legal systems, and edu-cation. 2) The development of the proprietary Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment with local ethical and legal stand-ards under challenging conditions. By documenting these achievements, the paper provides not only a technological artifact but also a replicable blueprint for developing advanced, regionally focused AI systems deeply rooted in their local identities.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering</title>
<link>https://arxiv.org/abs/2507.11527</link>
<guid>https://arxiv.org/abs/2507.11527</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Model, Civil Engineering, technical drawing revision, automation agents

Summary:
DrafterBench is a comprehensive benchmark designed for evaluating Large Language Model (LLM) agents in the context of technical drawing revision in Civil Engineering. It consists of twelve types of tasks derived from real-world drawing files, encompassing 46 customized functions/tools and a total of 1920 tasks. The benchmark aims to rigorously test AI agents' abilities in interpreting complex instructions, leveraging prior knowledge, and adapting to varying instruction quality. It evaluates capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. Detailed analysis of task accuracy and error statistics is provided to gain deeper insights into agent proficiency and improvement areas for integrating LLMs in engineering applications. The benchmark is open-source and available for access, facilitating the assessment of automation agents from an industrial perspective. 

<br /><br />Summary: <div>
arXiv:2507.11527v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Irrotational Contact Fields</title>
<link>https://arxiv.org/abs/2312.03908</link>
<guid>https://arxiv.org/abs/2312.03908</guid>
<content:encoded><![CDATA[
<div> Framework, convex approximations, complex contact models, Coulomb's law, maximum dissipation

Summary: The article introduces a framework for generating convex approximations of complex contact models, incorporating validated models like Hunt & Crossley and Coulomb's law of friction. The approach is robust across a wide range of stiffness values, suitable for compliant surfaces and rigid approximations. The approximations are evaluated across various test cases, with detailed properties and limitations outlined. The implementation in the open-source robotics toolkit, Drake, provides a fully differentiable solution, allowing for computation of gradients for complex geometric models while reusing contact resolution factorizations. The hybrid approach enables robust simulation of robotic tasks at interactive rates, accurately resolving stiction and contact transitions, thus supporting effective sim-to-real transfer. <div>
arXiv:2312.03908v3 Announce Type: replace-cross 
Abstract: We present a framework for generating convex approximations of complex contact models, incorporating experimentally validated models like Hunt & Crossley coupled with Coulomb's law of friction alongside the principle of maximum dissipation. Our approach is robust across a wide range of stiffness values, making it suitable for both compliant surfaces and rigid approximations. We evaluate these approximations across a wide variety of test cases, detailing properties and limitations. We implement a fully differentiable solution in the open-source robotics toolkit, Drake. Our novel hybrid approach enables computation of gradients for complex geometric models while reusing factorizations from contact resolution. We demonstrate robust simulation of robotic tasks at interactive rates, with accurately resolved stiction and contact transitions, supporting effective sim-to-real transfer.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Parameter Inference and Uncertainty Quantification for a Computational Pulmonary Hemodynamics Model Using Gaussian Processes</title>
<link>https://arxiv.org/abs/2502.14251</link>
<guid>https://arxiv.org/abs/2502.14251</guid>
<content:encoded><![CDATA[
<div> Keywords: subject-specific modeling, cardiovascular research, chronic thromboembolic pulmonary hypertension, microvascular disease, Gaussian process emulators 

Summary: 
Subject-specific modeling in cardiovascular research is crucial for personalized treatment guidance beyond current clinical diagnostics. This study utilized a one-dimensional fluid dynamics model informed by experimental data from a dog model of chronic thromboembolic pulmonary hypertension (CTEPH). The model incorporated measurements from multiple subjects under both baseline and CTEPH conditions, allowing for the assessment of microvascular disease severity. By modeling each lung separately to account for heterogeneity in CTEPH, the study identified distinct parameter shifts reflecting heterogeneous microvascular adaptation. Gaussian process emulators were used to accelerate model calibration, enabling the estimation of microvascular parameters and their uncertainties efficiently. The study demonstrated strong correlations between model parameter changes and disease severity, particularly in the lung with more advanced disease. This framework provides a rapid, uncertainty-aware method for evaluating microvascular dysfunction in CTEPH and may inform targeted treatment strategies with clinical applicability.<br /><br />Summary: <div>
arXiv:2502.14251v2 Announce Type: replace-cross 
Abstract: Subject-specific modeling is a powerful tool in cardiovascular research, providing insights beyond the reach of current clinical diagnostics. Limitations in available clinical data require the incorporation of uncertainty into models to improve guidance for personalized treatments. However, for clinical relevance, such modeling must be computationally efficient. In this study, we used a one-dimensional (1D) fluid dynamics model informed by experimental data from a dog model of chronic thromboembolic pulmonary hypertension (CTEPH), incorporating measurements from multiple subjects under both baseline and CTEPH conditions. Surgical intervention can alleviate CTEPH, yet patients with microvascular disease (e.g., remodeling and narrowing of small vessels) often exhibit persistent pulmonary hypertension, highlighting the importance of assessing microvascular disease severity. Thus, each lung was modeled separately to account for the heterogeneous nature of CTEPH, allowing us to explore lung-specific microvascular narrowing and resistance. We compared inferred parameters between baseline and CTEPH and examined their correlation with clinical markers of disease severity. To accelerate model calibration, we employed Gaussian process (GP) emulators, enabling the estimation of microvascular parameters and their uncertainties within a clinically feasible timeframe. Our results demonstrated that CTEPH leads to heterogeneous microvascular adaptation, reflected in distinct parameter shifts. Notably, the changes in model parameters strongly correlated with disease severity, especially in the lung previously reported to have more advanced disease. This framework provides a rapid, uncertainty-aware method for evaluating microvascular dysfunction in CTEPH and may support more targeted treatment strategies within a timeframe suitable for clinical application.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets</title>
<link>https://arxiv.org/abs/2507.09255</link>
<guid>https://arxiv.org/abs/2507.09255</guid>
<content:encoded><![CDATA[
<div> simulation platform, large language models, financial decision-making, StockSim, open-source
Summary:
StockSim is an open-source simulation platform designed for evaluating large language models (LLMs) in realistic financial decision-making scenarios. It offers a comprehensive system that accurately models market dynamics and supports various simulation modes with different levels of detail. By incorporating real-world factors like latency and order-book microstructure, StockSim allows for more insightful assessment of LLM-based trading agents. Its extensible agent framework supports diverse trading strategies and multi-agent coordination, making it a valuable tool for NLP research on reasoning under uncertainty and sequential decision-making.
<br /><br />Summary: <div>
arXiv:2507.09255v1 Announce Type: new 
Abstract: We present StockSim, an open-source simulation platform for systematic evaluation of large language models (LLMs) in realistic financial decision-making scenarios. Unlike previous toolkits that offer limited scope, StockSim delivers a comprehensive system that fully models market dynamics and supports diverse simulation modes of varying granularity. It incorporates critical real-world factors, such as latency, slippage, and order-book microstructure, that were previously neglected, enabling more faithful and insightful assessment of LLM-based trading agents. An extensible, role-based agent framework supports heterogeneous trading strategies and multi-agent coordination, making StockSim a uniquely capable testbed for NLP research on reasoning under uncertainty and sequential decision-making. We open-source all our code at https: //github.com/harrypapa2002/StockSim.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoWarp: An automatically differentiable and GPU-accelerated implicit MPM framework for geomechanics based on NVIDIA Warp</title>
<link>https://arxiv.org/abs/2507.09435</link>
<guid>https://arxiv.org/abs/2507.09435</guid>
<content:encoded><![CDATA[
<div> Keywords: material point method, implicit formulation, geomechanics, GPU parallelism, automatic differentiation

Summary:
GeoWarp is introduced as an implicit material point method (MPM) framework for geomechanics. It utilizes GPU parallelism and automatic differentiation to compute Jacobian matrices without manual derivation, overcoming the limitations of explicit MPM formulations. The framework includes a sparse Jacobian construction algorithm that takes advantage of localized particle-grid interactions inherent in MPM. GeoWarp is verified through examples in large-deformation elastoplasticity and coupled poromechanics, showcasing its robustness, scalability, and extensibility for differentiable implicit MPM simulation in computational geomechanics. By leveraging GPU parallelism and automatic differentiation, GeoWarp provides a powerful tool for simulating large-deformation and history-dependent behavior in geomechanical systems, making it a valuable asset in the field of computational geomechanics. 

<br /><br />Summary: <div>
arXiv:2507.09435v1 Announce Type: new 
Abstract: The material point method (MPM), a hybrid Lagrangian-Eulerian particle method, is increasingly used to simulate large-deformation and history-dependent behavior of geomaterials. While explicit time integration dominates current MPM implementations due to its algorithmic simplicity, such schemes are unsuitable for quasi-static and long-term processes typical in geomechanics. Implicit MPM formulations are free of these limitations but remain less adopted, largely due to the difficulty of computing the Jacobian matrix required for Newton-type solvers, especially when consistent tangent operators should be derived for complex constitutive models. In this paper, we introduce GeoWarp -- an implicit MPM framework for geomechanics built on NVIDIA Warp -- that exploits GPU parallelism and reverse-mode automatic differentiation to compute Jacobians without manual derivation. To enhance efficiency, we develop a sparse Jacobian construction algorithm that leverages the localized particle-grid interactions intrinsic to MPM. The framework is verified through forward and inverse examples in large-deformation elastoplasticity and coupled poromechanics. Results demonstrate that GeoWarp provides a robust, scalable, and extensible platform for differentiable implicit MPM simulation in computational geomechanics.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EV-STLLM: Electric vehicle charging forecasting based on spatio-temporal large language models with multi-frequency and multi-scale information fusion</title>
<link>https://arxiv.org/abs/2507.09527</link>
<guid>https://arxiv.org/abs/2507.09527</guid>
<content:encoded><![CDATA[
<div> VMD, ICEEMDAN, FIG, ReliefF, EV-STLLM <br />
Summary: <br />
The paper proposes a novel EV spatio-temporal large language model (EV-STLLM) for accurate prediction of electric vehicle (EV) charging demand and station occupancy. The framework consists of two modules: a data processing module using VMD and ICEEMDAN for data denoising and multi-frequency decomposition, FIG for extracting multi-scale information, and ReliefF for feature selection; a forecasting module using EV-STLLM for direct prediction. The model integrates adjacency matrices from regional station networks and spatio-temporal-frequency embedding information to capture data characteristics. The PFGA module maintains sequential feature modeling capabilities and incorporates EV domain knowledge. Experiments on real-world data from Shenzhen show superior accuracy compared to existing methods. <div>
arXiv:2507.09527v1 Announce Type: new 
Abstract: With the proliferation of electric vehicles (EVs), accurate charging demand and station occupancy forecasting are critical for optimizing urban energy and the profit of EVs aggregator. Existing approaches in this field usually struggle to capture the complex spatio-temporal dependencies in EV charging behaviors, and their limited model parameters hinder their ability to learn complex data distribution representations from large datasets. To this end, we propose a novel EV spatio-temporal large language model (EV-STLLM) for accurate prediction. Our proposed framework is divided into two modules. In the data processing module, we utilize variational mode decomposition (VMD) for data denoising, and improved complete ensemble empirical mode decomposition with adaptive noise (ICEEMDAN) for data multi-frequency decomposition. Fuzzy information granulation (FIG) for extracting multi-scale information. Additionally, ReliefF is used for feature selection to mitigate redundancy. In the forecasting module, the EV-STLLM is used to directly achieve EV charging and occupancy forecasting. Firstly, we fully capture the intrinsic spatio-temporal characteristics of the data by integrating adjacency matrices derived from the regional stations network and spatio-temporal-frequency embedding information. Then, the partially frozen graph attention (PFGA) module is utilized to maintain the sequential feature modeling capabilities of the pre-trained large model while incorporating EV domain knowledge. Extensive experiments using real-world data from Shenzhen, China, demonstrate that our proposed framework can achieve superior accuracy and robustness compared to the state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed machine learning surrogate for scalable simulation of thermal histories during wire-arc directed energy deposition</title>
<link>https://arxiv.org/abs/2507.09591</link>
<guid>https://arxiv.org/abs/2507.09591</guid>
<content:encoded><![CDATA[
<div> large-scale structural engineering, wire-arc directed energy deposition, finite element method, physics-informed neural networks, metal additive manufacturing <br /> 
Summary: <br /> 
The study focuses on wire-arc directed energy deposition (DED) for large-scale structural engineering applications. Traditional finite element method (FEM) simulations for thermal history prediction during deposition are computationally intensive. Physics-informed neural networks (PINNs) offer an alternative by combining physical knowledge with machine learning. The study investigates the scalability of PINNs, emphasizing efficient collocation points sampling to reduce computational time. Results show that PINNs can significantly decrease effort by up to 98.6% while maintaining accuracy and providing "super-resolution." The research suggests future enhancements for PINN performance in the context of metal additive manufacturing. <br /> <div>
arXiv:2507.09591v1 Announce Type: new 
Abstract: Wire-arc directed energy deposition (DED) has emerged as a promising additive manufacturing (AM) technology for large-scale structural engineering applications. However, the complex thermal dynamics inherent to the process present challenges in ensuring structural integrity and mechanical properties of fabricated thick walls and plates. While finite element method (FEM) simulations have been conventionally employed to predict thermal history during deposition, their computational demand remains prohibitively high for actual large-scale applications. Given the necessity of multiple repetitive simulations for heat management and the determination of an optimal printing strategy, FEM simulation quickly becomes entirely infeasible. Instead, advancements have been made in using trained neural networks as surrogate models for rapid prediction. However, traditional data-driven approaches necessitate large amounts of relevant and verifiable external data, during the training and validation of the neural network. Regarding large-scale wire-arc DED, none of these data sources are readily available in quantities sufficient for an accurate surrogate. The introduction of physics-informed neural networks (PINNs) has opened up an alternative simulation strategy by leveraging the existing physical knowledge of the phenomena with advanced machine learning methods. Despite their theoretical advantages, PINNs have seen limited application in the context of large-scale wire-arc DED for structural engineering. This study investigates the scalability of PINNs, focusing on efficient collocation points sampling, a critical factor controlling both the training time and model performance. Results show PINNs can reduce computational time and effort by up to 98.6%, while maintaining the desired accuracy and offering "super-resolution". Future directions for enhancing PINN performance in metal AM are discussed.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Matters Most? A Quantitative Meta-Analysis of AI-Based Predictors for Startup Success</title>
<link>https://arxiv.org/abs/2507.09675</link>
<guid>https://arxiv.org/abs/2507.09675</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, startup success, predictor importance, meta-analysis, context

Summary: 
- The study conducts a meta-analysis to synthesize predictor importance in AI-based startup evaluation, analyzing 13 empirical studies and identifying 58 unique predictors.
- The most powerful predictors for startup success are Firm Characteristics, Investor Structure, Digital and Social Traction, and Funding History.
- Predictor importance varies depending on the startup's goals and stage, with context influencing the hierarchy of factors.
- Factors predicting near-term funding milestones focus on immediate deal context, while long-term exits prioritize firm and investor characteristics.
- The study suggests a potential "convenience bias" in the literature, where predictor importance may be linked to data accessibility.
<br /><br />Summary: <div>
arXiv:2507.09675v1 Announce Type: new 
Abstract: Background: Predicting startup success with machine learning is a rapidly growing field, yet findings on key predictors are often fragmented and context-specific. This makes it difficult to discern robust patterns and highlights a need for a systematic synthesis of the evidence.
  Methods: This study conducts a quantitative meta-analysis to synthesize the literature on predictor importance in AI-based startup evaluation. We performed a systematic review to identify a final sample of 13 empirical studies that report rankable feature importance. From these papers, we extracted and categorized 58 unique predictors, synthesizing their importance using a Weighted Importance Score (WIS) that balances a feature's average rank with its frequency of appearance. We also conducted a moderator analysis to investigate how predictor importance changes with context (e.g., success definition).
  Results: Our aggregate analysis reveals that the most consistently powerful predictors are a quartet of foundational attributes: Firm Characteristics (e.g., age, location), Investor Structure (e.g., investor quality), Digital and Social Traction (e.g., online momentum), and Funding History. The moderator analysis further reveals that this hierarchy is highly context-dependent. For instance, predicting near-term funding milestones elevates the importance of the deal's immediate context, while predicting long-term exits prioritizes fundamental firm and investor characteristics.
  Conclusion: The factors that best predict startup success are not universal but are contingent on the startup's goals, stage, and the data used for evaluation. Our findings point to a potential "convenience bias" in the literature, where predictor importance may be tied to data accessibility. We conclude by underscoring the need for standardized reporting practices to enable more robust, cumulative knowledge building in the field.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Legendre Polynomials and Their Use for Karhunen-Lo\`eve Expansion</title>
<link>https://arxiv.org/abs/2507.09825</link>
<guid>https://arxiv.org/abs/2507.09825</guid>
<content:encoded><![CDATA[
<div> Legendre polynomials, recurrence relation, Gaussian random fields, Karhunen-Love expansions, computational framework<br />
<br />
Summary:<br />
This paper presents a pedagogical review of the derivation of the three-term recurrence relation for Legendre polynomials, aimed at undergraduate students. It also introduces a computational framework for Karhunen-Love expansions of isotropic Gaussian random fields on hyper-rectangular domains. The framework utilizes Legendre polynomials and associated Gaussian quadrature techniques, ensuring efficiency in higher spatial dimensions. The approach approximates a covariance kernel using a non-negative mixture of squared-exponentials and employs a separable kernel for efficient Legendre-Galerkin discretization. Structural properties like even/odd parity structure in submatrices and a Duffy-type transformation for assembly reduce memory usage and arithmetic cost. The paper includes algorithms and numerical experiments in an open-source repository, reproducing all figures and tables provided in the work. <div>
arXiv:2507.09825v1 Announce Type: new 
Abstract: This paper makes two main contributions. First, we present a pedagogical review of the derivation of the three-term recurrence relation for Legendre polynomials, without relying on the classical Legendre differential equation, Rodrigues' formula, or generating functions. This exposition is designed to be accessible to undergraduate students.
  Second, we develop a computational framework for Karhunen-Lo\`eve expansions of isotropic Gaussian random fields on hyper-rectangular domains. The framework leverages Legendre polynomials and their associated Gaussian quadrature, and it remains efficient even in higher spatial dimensions.
  A covariance kernel is first approximated by a non-negative mixture of squared-exponentials, obtained via a Newton-optimized fit with a theoretically informed initialization. The resulting separable kernel enables a Legendre-Galerkin discretization in the form of a Kronecker product over single dimensions, with submatrices that exhibit even/odd parity structure. For assembly, we introduce a Duffy-type transformation followed by quadrature. These structural properties significantly reduce both memory usage and arithmetic cost compared to naive approaches. All algorithms and numerical experiments are provided in an open-source repository that reproduces every figure and table in this work.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-smooth optimization meets automated material model discovery</title>
<link>https://arxiv.org/abs/2507.10196</link>
<guid>https://arxiv.org/abs/2507.10196</guid>
<content:encoded><![CDATA[
<div> Automated material model discovery, Non-smooth L1-norm regularization, Minimization algorithms, Sparse regression problem, Regularization path computation<br />
<br />
Summary: 
This study explores the minimization of functions with non-smooth L1-norm regularization for automated material model discovery. It investigates the minimization of functions involving a metric quantifying the model-data mismatch and a regularization parameter determining solution sparsity. The study covers cases where the metric function is quadratic or non-quadratic, proposing efficient algorithms for solving the minimization problem and computing the entire regularization path. Algorithms discussed include coordinate descent, LARS for determining critical regularization values, proximal gradient method ISTA for non-quadratic scenarios, and a pathwise extension of ISTA. These algorithms are applied to discover hyperelastic material models from tension and shear data, showcasing their effectiveness in automated material model discovery in mechanics.
<br /> <div>
arXiv:2507.10196v1 Announce Type: new 
Abstract: Automated material model discovery disrupts the tedious and time-consuming cycle of iteratively calibrating and modifying manually designed models. Non-smooth L1-norm regularization is the backbone of automated model discovery; however, the current literature on automated material model discovery offers limited insights into the robust and efficient minimization of non-smooth objective functions. In this work, we examine the minimization of functions of the form f(w) + a ||w||_1, where w are the material model parameters, f is a metric that quantifies the mismatch between the material model and the observed data, and a is a regularization parameter that determines the sparsity of the solution. We investigate both the straightforward case where f is quadratic and the more complex scenario where it is non-quadratic or even non-convex. Importantly, we do not only focus on methods that solve the sparse regression problem for a given value of the regularization parameter a, but propose methods to efficiently compute the entire regularization path, facilitating the selection of a suitable a. Specifically, we present four algorithms and discuss their roles for automated material model discovery in mechanics: First, we recapitulate a well-known coordinate descent algorithm that solves the minimization problem assuming that f is quadratic for a given value of a, also known as the LASSO. Second, we discuss the algorithm LARS, which automatically determines the critical values of a, at which material parameters in w are set to zero. Third, we propose to use the proximal gradient method ISTA for automated material model discovery if f is not quadratic, and fourth, we suggest a pathwise extension of ISTA for computing the regularization path. We demonstrate the applicability of all algorithms for the discovery of hyperelastic material models from uniaxial tension and simple shear data.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios</title>
<link>https://arxiv.org/abs/2507.10448</link>
<guid>https://arxiv.org/abs/2507.10448</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial report generation, LLM models, multi-agent collaborative system, real financial scenarios, human evaluation

Summary:
Financial report generation tasks are complex and require extensive data analysis across various areas of finance. Existing Language Model models are limited in their ability to comprehensively analyze real financial scenarios. To address this, the FinTeam system was developed with a collaborative workflow involving four specialized agents: document analyzer, analyst, accountant, and consultant. Trained on specific financial expertise datasets, these agents work together to produce comprehensive financial reports. Evaluation results show that FinTeam outperformed baseline models like GPT-4o and Xuanyuan, achieving a 62.00% acceptance rate. The system also demonstrated an average improvement of 7.43% on FinCUGE and a 2.06% accuracy boost on FinEval. The project code is available on GitHub for further exploration. 

<br /><br />Summary: 
- Financial report generation tasks are complex and require extensive data analysis.
- The FinTeam system utilizes a collaborative workflow with specialized agents trained on specific financial expertise.
- Evaluation results show that FinTeam outperformed baseline models and achieved a high acceptance rate.
- The system demonstrated improvements on key financial evaluation metrics.
- The project code is available on GitHub for further exploration. <div>
arXiv:2507.10448v1 Announce Type: new 
Abstract: Financial report generation tasks range from macro- to micro-economics analysis, also requiring extensive data analysis. Existing LLM models are usually fine-tuned on simple QA tasks and cannot comprehensively analyze real financial scenarios. Given the complexity, financial companies often distribute tasks among departments. Inspired by this, we propose FinTeam, a financial multi-agent collaborative system, with a workflow with four LLM agents: document analyzer, analyst, accountant, and consultant. We train these agents with specific financial expertise using constructed datasets. We evaluate FinTeam on comprehensive financial tasks constructed from real online investment forums, including macroeconomic, industry, and company analysis. The human evaluation shows that by combining agents, the financial reports generate from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43% average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project is available at https://github.com/FudanDISC/DISC-FinLLM/.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Central Bank Digital Currencies: A Survey</title>
<link>https://arxiv.org/abs/2507.08880</link>
<guid>https://arxiv.org/abs/2507.08880</guid>
<content:encoded><![CDATA[
<div> Keywords: Central Bank Digital Currencies, CBDC design taxonomy, ledger technology, consensus mechanisms, CBDC ecosystem

Summary:
Central banks are exploring the implementation of Central Bank Digital Currencies (CBDCs) due to advancements in digital payment technologies. A review of 135 research papers from 2018 to 2025 examines CBDC design taxonomy and ecosystem frameworks. The study refines key architectural elements, investigates ledger technologies, consensus mechanisms, offline payments, and digital wallet integration. A comparative analysis of 26 existing CBDC systems across system architecture, ledger technology, access model, and application domain reveals trends like a two-tier architecture, distributed ledger technology (DLT), and token-based access model. There is a growing focus on using CBDCs for cross-border payments to improve efficiency. Recommendations for future research are provided. 

<br /><br />Summary: <div>
arXiv:2507.08880v1 Announce Type: cross 
Abstract: With the advancement of digital payment technologies, central banks worldwide have increasingly begun to explore the implementation of Central Bank Digital Currencies (CBDCs). This paper presents a comprehensive review of the latest developments in CBDC system design and implementation. By analyzing 135 research papers published between 2018 and 2025, the study provides an in-depth examination of CBDC design taxonomy and ecosystem frameworks. Grounded in the CBDC Design Pyramid, the paper refines and expands key architectural elements by thoroughly investigating innovations in ledger technologies, the selection of consensus mechanisms, and challenges associated with offline payments and digital wallet integration. Furthermore, it conceptualizes a CBDC ecosystem. A detailed comparative analysis of 26 existing CBDC systems is conducted across four dimensions: system architecture, ledger technology, access model, and application domain. The findings reveal that the most common configuration consists of a two-tier architecture, distributed ledger technology (DLT), and a token-based access model. However, no dominant trend has emerged regarding application domains. Notably, recent research shows a growing focus on leveraging CBDCs for cross-border payments to resolve inefficiencies and structural delays in current systems. Finally, the paper offers several forward-looking recommendations for future research.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components</title>
<link>https://arxiv.org/abs/2507.09443</link>
<guid>https://arxiv.org/abs/2507.09443</guid>
<content:encoded><![CDATA[
<div> Predictive Maintenance, Nuclear Power Plants, Convolutional Neural Network, Computational Thermomechanical Model, Fuel Rod<br />
Summary:<br />
Proactive maintenance strategies, like Predictive Maintenance, are crucial for Nuclear Power Plants as they help in reducing downtime caused by unexpected component failures. This study explores the use of a Convolutional Neural Network combined with a computational thermomechanical model to estimate the temperature, stress, and strain of a Pressurized Water Reactor fuel rod during operation using limited temperature measurements. The datasets for training, validation, and testing were generated through simulations involving a nuclear fuel performance code and a Thermal-Hydraulics Module. The CNN was trained for over 1,000 epochs and showed accurate temperature distribution predictions, further used in a thermomechanical model to determine stress and strain distribution within the fuel rod. This methodology has the potential to aid in the development of Predictive Maintenance tools for real-time monitoring of nuclear reactors. <br /> <div>
arXiv:2507.09443v1 Announce Type: cross 
Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play an important role in the operation of Nuclear Power Plants (NPPs), particularly due to their capacity to reduce offline time by preventing unexpected shutdowns caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN) architecture combined with a computational thermomechanical model to calculate the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel rod during operation. This estimation relies on a limited number of temperature measurements from the cladding's outer surface. This methodology can potentially aid in developing PdM tools for nuclear reactors by enabling real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled simulations involving BISON, a finite element-based nuclear fuel performance code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven simulations, varying the peak linear heat generation rates. Of these, eight were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting, achieving highly accurate temperature distribution predictions. These were then used in a thermomechanical model to determine the stress and strain distribution within the fuel rod.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When the Weak Becomes Strong: Effective Observables via Time-Symmetric Quantum Selection</title>
<link>https://arxiv.org/abs/2507.09716</link>
<guid>https://arxiv.org/abs/2507.09716</guid>
<content:encoded><![CDATA[
<div> weak values, time-symmetric quantum mechanics, sequential composition, state-conditioned observable, interference information

Summary: In this study, the authors explore the sequential composition of weak values within time-symmetric quantum mechanics. They analyze the combination of forward and reverse weak measurements, showing that their product corresponds to the normalized expectation value of a state-conditioned observable. This observable encodes interference information, especially when the initial state is a superposition. The concept extends to mixed states by using a density matrix, connecting to generalized quantum measurements. Practical applications in quantum computing include error detection and the inference of weak value phases through strong measurements in the case of pure states. <div>
arXiv:2507.09716v1 Announce Type: cross 
Abstract: We investigate the sequential composition of weak values in the framework of time-symmetric quantum mechanics. Specifically, we consider a forward'' weak measurement from a preselected state $\ket{\psi}$ to a post-selected state $\ket{\phi}$, followed by a reverse'' weak measurement. We show that the product of these two weak values corresponds to the normalized expectation value of a strong, state-conditioned observable $B = A P_\psi A$, where $P_\psi = \ket{\psi}\bra{\psi}$ is the projector onto the preselected state. Analyzing the structure of $B$, we demonstrate how it encodes interference information, particularly when $\ket{\psi}$ is a superposition rather than an eigenstate of $A$. This formulation extends naturally to mixed states by replacing $P_\psi$ with a generic density matrix $\rho$, linking the construction to the formalism of generalized quantum measurements. We illustrate practical applications in quantum information, including state-specific error witnessing in quantum computing, and show how the phase of a weak value can be inferred via strong measurements in the pure-state case.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coincidence of Wants Mechanism for Swap Trade Execution in Decentralized Exchanges</title>
<link>https://arxiv.org/abs/2507.10149</link>
<guid>https://arxiv.org/abs/2507.10149</guid>
<content:encoded><![CDATA[
<div> cycle, Coincidence of Wants, decentralized exchange, asset matrix, liquidity providing

Summary:
The article presents a novel framework for identifying and completing Coincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators. Unlike existing auction based systems like CoWSwap, this approach uses an asset matrix formulation to verify feasibility, utilize oracle prices, and adhere to formal conservation laws. It also introduces bridging orders to execute slippage-free and capital preserving swap orders, offering a delta-neutral strategy for liquidity providing market makers. By leveraging graph traversal and imbalance correction, the algorithm efficiently discovers CoW cycles in real-world Arbitrum swap data and can seamlessly insert synthetic orders for atomic cycle closure. This structured CoW cycle execution demonstrates the potential for enhanced liquidity management and improved order processing in decentralized exchange ecosystems. <div>
arXiv:2507.10149v1 Announce Type: cross 
Abstract: We propose a mathematically rigorous framework for identifying and completing Coincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators. Unlike existing auction based systems such as CoWSwap, our approach introduces an asset matrix formulation that not only verifies feasibility using oracle prices and formal conservation laws but also completes partial CoW cycles of swap orders that are discovered using graph traversal and are settled using imbalance correction. We define bridging orders and show that the resulting execution is slippage free and capital preserving for LPs. Applied to real world Arbitrum swap data, our algorithm demonstrates efficient discovery of CoW cycles and supports the insertion of synthetic orders for atomic cycle closure. This work can be thought of as the detailing of a potential delta-neutral strategy by liquidity providing market makers: a structured CoW cycle execution.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Model for Composite Microstructures: Reconstruction, Stiffness, and Nonlinear Behavior Prediction</title>
<link>https://arxiv.org/abs/2411.06565</link>
<guid>https://arxiv.org/abs/2411.06565</guid>
<content:encoded><![CDATA[
<div> Keywords: Material Masked Autoencoder, self-supervised learning, Vision Transformer, microstructural features, composite images <br />
Summary: 
The Material Masked Autoencoder (MMAE) is introduced as a self-supervised Vision Transformer pre-trained on a large dataset of short-fiber composite images. The MMAE captures essential microstructural features and has broad applicability across tasks. Through fine-tuning on limited data, the MMAE can predict homogenized stiffness components effectively. The MMAE combined with an interaction-based material network (IMN) allows for inferring physically interpretable parameters and enables the extrapolation of nonlinear stress-strain responses. These results demonstrate the potential of microstructure foundation models in dealing with complex systems like 3D composites and experimental datasets. The MMAE represents a promising approach for advancing research in material science and lays the foundation for future extensions to more intricate systems. <br /><br />Summary: <div>
arXiv:2411.06565v4 Announce Type: replace 
Abstract: We present the Material Masked Autoencoder (MMAE), a self-supervised Vision Transformer pretrained on a large corpus of short-fiber composite images via masked image reconstruction. The pretrained MMAE learns latent representations that capture essential microstructural features and are broadly transferable across tasks. We demonstrate two key applications: (i) predicting homogenized stiffness components through fine-tuning on limited data, and (ii) inferring physically interpretable parameters by coupling MMAE with an interaction-based material network (IMN), thereby enabling extrapolation of nonlinear stress-strain responses. These results highlight the promise of microstructure foundation models and lay the groundwork for future extensions to more complex systems, such as 3D composites and experimental datasets.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EP-GAT: Energy-based Parallel Graph Attention Neural Network for Stock Trend Classification</title>
<link>https://arxiv.org/abs/2507.08184</link>
<guid>https://arxiv.org/abs/2507.08184</guid>
<content:encoded><![CDATA[
arXiv:2507.08184v1 Announce Type: new 
Abstract: Graph neural networks have shown remarkable performance in forecasting stock movements, which arises from learning complex inter-dependencies between stocks and intra-dynamics of stocks. Existing approaches based on graph neural networks typically rely on static or manually defined factors to model changing inter-dependencies between stocks. Furthermore, these works often struggle to preserve hierarchical features within stocks. To bridge these gaps, this work presents the Energy-based Parallel Graph Attention Neural Network, a novel approach for predicting future movements for multiple stocks. First, it generates a dynamic stock graph with the energy difference between stocks and Boltzmann distribution, capturing evolving inter-dependencies between stocks. Then, a parallel graph attention mechanism is proposed to preserve the hierarchical intra-stock dynamics. Extensive experiments on five real-world datasets are conducted to validate the proposed approach, spanning from the US stock markets (NASDAQ, NYSE, SP) and UK stock markets (FTSE, LSE). The experimental results demonstrate that EP-GAT consistently outperforms competitive five baselines on test periods across various metrics. The ablation studies and hyperparameter sensitivity analysis further validate the effectiveness of each module in the proposed method.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models</title>
<link>https://arxiv.org/abs/2507.08030</link>
<guid>https://arxiv.org/abs/2507.08030</guid>
<content:encoded><![CDATA[
arXiv:2507.08030v1 Announce Type: cross 
Abstract: Generative AI models, including large language models (LLMs) and vision-language models (VLMs), are increasingly used to interpret medical images and answer clinical questions. Their responses often include inaccuracies; therefore, safety measures like medical disclaimers are critical to remind users that AI outputs are not professionally vetted or a substitute for medical advice. This study evaluated the presence of disclaimers in LLM and VLM outputs across model generations from 2022 to 2025. Using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs were screened for disclaimer phrases. Medical disclaimer presence in LLM and VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023 to 1.05% in 2025, respectively. By 2025, the majority of models displayed no disclaimers. As public models become more capable and authoritative, disclaimers must be implemented as a safeguard adapting to the clinical context of each output.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk Improves Trading Decisions</title>
<link>https://arxiv.org/abs/2507.08584</link>
<guid>https://arxiv.org/abs/2507.08584</guid>
<content:encoded><![CDATA[
arXiv:2507.08584v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in agentic frameworks, in which prompts trigger complex tool-based analysis in pursuit of a goal. While these frameworks have shown promise across multiple domains including in finance, they typically lack a principled model-building step, relying instead on sentiment- or trend-based analysis. We address this gap by developing an agentic system that uses LLMs to iteratively discover stochastic differential equations for financial time series. These models generate risk metrics which inform daily trading decisions. We evaluate our system in both traditional backtests and using a market simulator, which introduces synthetic but causally plausible price paths and news events. We find that model-informed trading strategies outperform standard LLM-based agents, improving Sharpe ratios across multiple equities. Our results show that combining LLMs with agentic model discovery enhances market risk estimation and enables more profitable trading decisions.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating diversion and treatment policies for opioid use disorder</title>
<link>https://arxiv.org/abs/2311.05076</link>
<guid>https://arxiv.org/abs/2311.05076</guid>
<content:encoded><![CDATA[
arXiv:2311.05076v4 Announce Type: replace 
Abstract: The United States (US) opioid crisis contributed to 81,806 fatalities in 2022. It has strained hospitals, treatment facilities, and law enforcement agencies due to the enormous resources and procedures needed to respond to the crisis. As a result, many individuals who use opioids never receive or finish the treatment they need and instead have many interactions with hospitals or the criminal justice system. This paper introduces a discrete event simulation model that evaluates three opioid use disorder treatment policies: arrest diversion, re-entry case management, and overdose diversion. Publicly available data from 2011 to 2019 in Dane County, Wisconsin, was used to forecast opioid-related outcomes through 2032. Through analyzing a variety of policy-mix implementations, the study offers a versatile framework for evaluating policies at various implementation levels. The results demonstrate that treatment policies that create new pathways and programming by utilizing treatment services and successfully divert at least 20% of eligible individuals can lead to more opioid-resilient communities. The benefits increase when more policies are enacted and/or offered to more individuals, with the largest impact from overdose diversion, followed by re-entry case management, and the smallest impact from arrest diversion. The statistically significant 10-year cumulative total reduction in societal costs from 2023 through 2032 ranges from $39 M (USD) to $584 M (USD), excluding implementation costs of policies. To reverse the opioid crisis within a community, treatment policies may need to be combined with other strategies, such as harm reduction, supply reduction, and use prevention.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing wheel loader performance -- an end-to-end approach</title>
<link>https://arxiv.org/abs/2501.06583</link>
<guid>https://arxiv.org/abs/2501.06583</guid>
<content:encoded><![CDATA[
arXiv:2501.06583v3 Announce Type: replace 
Abstract: Wheel loaders in mines and construction sites repeatedly load soil from a pile to load receivers. Automating this task presents a challenging planning problem since each loading's performance depends on the pile state, which depends on previous loadings. We investigate an end-to-end optimization approach considering future loading outcomes and transportation costs between the pile and load receivers. To predict the evolution of the pile state and the loading performance, we use world models that leverage deep neural networks trained on numerous simulated loading cycles. A look-ahead tree search optimizes the sequence of loading actions by evaluating the performance of thousands of action candidates, which expand into subsequent action candidates under the predicted pile states recursively. Test results demonstrate that, over a horizon of 15 sequential loadings, the look-ahead tree search is 6% more efficient than a greedy strategy, which always selects the action that maximizes the current single loading performance, and 14% more efficient than using a fixed loading controller optimized for the nominal case.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pandora's Box Problem with Sequential Inspections</title>
<link>https://arxiv.org/abs/2507.07508</link>
<guid>https://arxiv.org/abs/2507.07508</guid>
<content:encoded><![CDATA[
<div> optimization, Pandora's box, tradeoff, information acquisition, cost efficiency

Summary:
The study explores a generalization of the Pandora's box problem in economic theory, where an agent can choose to fully open boxes at a certain fee or partially open them at a reduced cost. The research establishes a hardness result and utilizes stochastic optimization techniques to analyze the model comprehensively. Structural properties of the optimal policy are identified, providing insights into decision-making. Problem relaxations and near-optimal solutions are derived, and the optimal policy is characterized in special cases. An extensive numerical study comparing various policies reveals that threshold-based policies extending the Pandora's box optimal solution can effectively guide search decisions. The study contributes to understanding the tradeoff between information acquisition and cost efficiency in decision-making scenarios. <br /><br />Summary: <div>
arXiv:2507.07508v1 Announce Type: new 
Abstract: The Pandora's box problem (Weitzman 1979) is a core model in economic theory that captures an agent's (Pandora's) search for the best alternative (box). We study an important generalization of the problem where the agent can either fully open boxes for a certain fee to reveal their exact values or partially open them at a reduced cost. This introduces a new tradeoff between information acquisition and cost efficiency. We establish a hardness result and employ an array of techniques in stochastic optimization to provide a comprehensive analysis of this model. This includes (1) the identification of structural properties of the optimal policy that provide insights about optimal decisions; (2) the derivation of problem relaxations and provably near-optimal solutions; (3) the characterization of the optimal policy in special yet non-trivial cases; and (4) an extensive numerical study that compares the performance of various policies, and which provides additional insights about the optimal policy. Throughout, we show that intuitive threshold-based policies that extend the Pandora's box optimal solution can effectively guide search decisions.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics</title>
<link>https://arxiv.org/abs/2507.07830</link>
<guid>https://arxiv.org/abs/2507.07830</guid>
<content:encoded><![CDATA[
<div> modal reference spaces, model-order reduction framework, meshless SPH method, proper orthogonal decomposition, Galerkin POD, Adjoint Petrov-Galerkin

Summary:
The paper introduces a model-order reduction framework for meshless weakly compressible smoothed particle hydrodynamics (SPH). It proposes modal reference spaces to handle the challenges of discovering low-dimensional subspaces in SPH simulations. By projecting SPH snapshot data onto a reference space, low dimensionality of field quantities can be identified through modal decomposition techniques like proper orthogonal decomposition (POD). These modal quantities are then mapped back to the meshless SPH space during online prediction using scattered data interpolation. The framework is based on the meshless Galerkin POD and Adjoint Petrov-Galerkin projection model-order reduction formulations. Testing on various numerical experiments demonstrates good agreement in reconstructed and predictive velocity fields. However, the pressure field shows sensitivity to projection error due to weakly-compressible assumptions in SPH, which can be mitigated using nonlinear approximations like the APG approach. Overall, the proposed meshless model-order reduction framework shows promise in reducing computational costs for SPH simulations. 

<br /><br />Summary: <div>
arXiv:2507.07830v1 Announce Type: new 
Abstract: This work proposes a model-order reduction framework for the meshless weakly compressible smoothed particle hydrodynamics (SPH) method. The proposed framework introduces the concept of modal reference spaces to overcome the challenges of discovering low-dimensional subspaces from unstructured, dynamic, and mixing numerical topology that is often seen in SPH simulations. The proposed modal reference spaces enable a low-dimensional representation of the SPH field equations while maintaining their inherent meshless qualities. Modal reference spaces are constructed by projecting SPH snapshot data onto a reference space where low-dimensionality of field quantities can be discovered via traditional modal decomposition techniques (e.g., the proper orthogonal decomposition (POD)). Modal quantities are mapped back to the meshless SPH space via scattered data interpolation during the online predictive stage. The proposed model-order reduction framework is cast into the \emph{meshless} Galerkin POD (GPOD) and the Adjoint Petrov--Galerkin (APG) projection model-order reduction (PMOR) formulation. The PMORs are tested on three numerical experiments: 1) the Taylor--Green vortex; 2) lid-driven cavity; and 3) flow past an open cavity. Results show good agreement in reconstructed and predictive velocity fields, which showcase the ability of the proposed framework to evolve the unstructured, dynamic, and mixing SPH field equations in a low-dimensional subspace. Results also show that the pressure field is sensitive to the projection error due to the stiff weakly-compressible assumption made in the current SPH framework, but can be alleviated through nonlinear approximations, such as the APG approach. Ultimately, the presented meshless model-order reduction framework marks a step toward enabling drastic cost savings of SPH simulations.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Enhanced Multi-Factor Quantitative Trading: A Cross-Sectional Portfolio Optimization Approach with Bias Correction</title>
<link>https://arxiv.org/abs/2507.07107</link>
<guid>https://arxiv.org/abs/2507.07107</guid>
<content:encoded><![CDATA[
<div> alpha discovery, bias correction, factor computation, portfolio optimization, machine learning

Summary:
This paper introduces a machine learning framework for quantitative trading that emphasizes factor engineering, real-time computation optimization, and portfolio construction. The framework combines multi-factor alpha discovery with bias correction techniques using PyTorch-accelerated factor computation. It processes 500-1000 factors from open-source alpha101 extensions and market microstructure signals. Key innovations include tensor-based factor computation acceleration, data augmentation using geometric Brownian motion, and cross-sectional neutralization strategies. Empirical validation on Chinese A-share markets from 2010-2024 showcases annualized returns of 20% with Sharpe ratios exceeding 2.0, surpassing traditional approaches. The study underscores the significance of bias correction in factor construction and the considerable impact of cross-sectional portfolio optimization on strategy performance. Experimental implementations and code are accessible on GitHub at: https://github.com/initial-d/ml-quant-trading

<br /><br />Summary: <div>
arXiv:2507.07107v1 Announce Type: cross 
Abstract: This paper presents a comprehensive machine learning framework for quantitative trading that achieves superior risk-adjusted returns through systematic factor engineering, real-time computation optimization, and cross-sectional portfolio construction. Our approach integrates multi-factor alpha discovery with bias correction techniques, leveraging PyTorch-accelerated factor computation and advanced portfolio optimization. The system processes 500-1000 factors derived from open-source alpha101 extensions and proprietary market microstructure signals. Key innovations include tensor-based factor computation acceleration, geometric Brownian motion data augmentation, and cross-sectional neutralization strategies. Empirical validation on Chinese A-share markets (2010-2024) demonstrates annualized returns of $20\%$ with Sharpe ratios exceeding 2.0, significantly outperforming traditional approaches. Our analysis reveals the critical importance of bias correction in factor construction and the substantial impact of cross-sectional portfolio optimization on strategy performance. Code and experimental implementations are available at: https://github.com/initial-d/ml-quant-trading
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable ADER-DG Transport Method with Polynomial Order Independent CFL Limit</title>
<link>https://arxiv.org/abs/2507.07304</link>
<guid>https://arxiv.org/abs/2507.07304</guid>
<content:encoded><![CDATA[
<div> Discontinuous Galerkin methods, Locally Implicit, Globally Explicit, ADER-DG scheme, Element-width based CFL condition, Transport-dominated problems <br />
Summary:<br />
This paper introduces a novel locally implicit, globally explicit ADER-DG scheme for transport-dominated problems. The method overcomes the restrictive time step constraints seen in high-order DG methods by using an element-width based CFL condition, allowing for a maximum stable time step independent of polynomial degree. By solving element-local implicit problems at each time step, the method effectively captures the domain of dependence and remains stable for CFL numbers up to $1/\sqrt{d}$ in $d$ spatial dimensions. Rigorous stability proofs in one dimension and von Neumann stability analysis in higher dimensions validate the method's accuracy and convergence. Numerical experiments on linear and nonlinear test cases further demonstrate the effectiveness of the proposed approach. <br /> <div>
arXiv:2507.07304v1 Announce Type: cross 
Abstract: Discontinuous Galerkin (DG) methods are known to suffer from increasingly restrictive time step constraints as the polynomial order increases, limiting their efficiency at high orders. In this paper, we introduce a novel locally implicit, but globally explicit ADER-DG scheme designed for transport-dominated problems. The method achieves a maximum stable time step governed by an element-width based CFL condition that is independent of the polynomial degree. By solving a set of element-local implicit problems at each time step, our approach more effectively captures the domain of dependence. As a result, our method remains stable for CFL numbers up to $1/\sqrt{d}$ in $d$ spatial dimensions. We provide a rigorous stability proof in one dimension, and extend the analysis to two and three dimensions using a semi-analytical von Neumann stability analysis. The accuracy and convergence of the method are demonstrated through numerical experiments on both linear and nonlinear test cases.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Plausibility-Validity Gap by Fine-Tuning a Reasoning-Enhanced LLM for Chemical Synthesis and Discovery</title>
<link>https://arxiv.org/abs/2507.07328</link>
<guid>https://arxiv.org/abs/2507.07328</guid>
<content:encoded><![CDATA[
<div> chemistry, language models, plausibility-validity gap, Low-Rank Adaptation, dual-domain dataset

Summary:
The paper addresses the challenge of factually invalid information generated by Large Language Models (LLMs) in specialized domains like chemistry, known as the "plausibility-validity gap." A specialized scientific assistant was developed by fine-tuning the Magistral Small model using Low-Rank Adaptation (LoRA) and a dual-domain dataset curated from various sources. The evaluation showed improved format adherence, chemical validity of generated molecules, and feasibility of proposed synthesis routes. The model exhibited a hierarchical learning pattern, with syntactic correctness learned more easily than chemical possibility and synthesis feasibility. While competitive with human experts in areas like chemical creativity and reasoning, limitations such as errors in stereochemistry, static knowledge cutoff, and occasional reference hallucination were identified. This work establishes a framework for transforming generalist LLMs into specialized tools for chemical research, while also highlighting areas for future enhancement. 

<br /><br />Summary: <div>
arXiv:2507.07328v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often generate scientifically plausible but factually invalid information, a challenge we term the "plausibility-validity gap," particularly in specialized domains like chemistry. This paper presents a systematic methodology to bridge this gap by developing a specialized scientific assistant. We utilized the Magistral Small model, noted for its integrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation (LoRA). A key component of our approach was the creation of a "dual-domain dataset," a comprehensive corpus curated from various sources encompassing both molecular properties and chemical reactions, which was standardized to ensure quality. Our evaluation demonstrates that the fine-tuned model achieves significant improvements over the baseline model in format adherence, chemical validity of generated molecules, and the feasibility of proposed synthesis routes. The results indicate a hierarchical learning pattern, where syntactic correctness is learned more readily than chemical possibility and synthesis feasibility. While a comparative analysis with human experts revealed competitive performance in areas like chemical creativity and reasoning, it also highlighted key limitations, including persistent errors in stereochemistry, a static knowledge cutoff, and occasional reference hallucination. This work establishes a viable framework for adapting generalist LLMs into reliable, specialized tools for chemical research, while also delineating critical areas for future improvement.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computationally Efficient Information-Driven Optical Design with Interchanging Optimization</title>
<link>https://arxiv.org/abs/2507.07789</link>
<guid>https://arxiv.org/abs/2507.07789</guid>
<content:encoded><![CDATA[
<div> IDEAL-IO, imaging systems, information content, optical design, computational decoding<br />
<br />
Summary:<br />
Recent work has shown that evaluating imaging systems based on the information content of their measurements alone can simplify optical design. However, the IDEAL method for automating this process faces challenges such as high memory usage, long runtimes, and a potentially mismatched objective function. To address these issues, IDEAL-IO was introduced, which separates density estimation from optical parameter optimization. By alternating between fitting models to measurements and updating optical parameters using fixed models, IDEAL-IO reduces runtime and memory usage while allowing for more expressive density models. This approach was successfully validated on various imaging applications, demonstrating the practicality and scalability of information-driven optimization for real-world imaging system design. <br /> <div>
arXiv:2507.07789v1 Announce Type: cross 
Abstract: Recent work has demonstrated that imaging systems can be evaluated through the information content of their measurements alone, enabling application-agnostic optical design that avoids computational decoding challenges. Information-Driven Encoder Analysis Learning (IDEAL) was proposed to automate this process through gradient-based. In this work, we study IDEAL across diverse imaging systems and find that it suffers from high memory usage, long runtimes, and a potentially mismatched objective function due to end-to-end differentiability requirements. We introduce IDEAL with Interchanging Optimization (IDEAL-IO), a method that decouples density estimation from optical parameter optimization by alternating between fitting models to current measurements and updating optical parameters using fixed models for information estimation. This approach reduces runtime and memory usage by up to 6x while enabling more expressive density models that guide optimization toward superior designs. We validate our method on diffractive optics, lensless imaging, and snapshot 3D microscopy applications, establishing information-theoretic optimization as a practical, scalable strategy for real-world imaging system design.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development and Real-World Application of Commercial Motor Vehicle Safety Enforcement Dashboards</title>
<link>https://arxiv.org/abs/2507.06351</link>
<guid>https://arxiv.org/abs/2507.06351</guid>
<content:encoded><![CDATA[
<div> Keywords: Commercial Motor Vehicle Safety, Dashboards, Performance Measures, Enforcement Initiatives, Maryland State Police

Summary: 
This study introduces CMV safety dashboards developed with input from CMV enforcement professionals. The dashboards enhance analysis of CMV safety performance measures based on probe vehicle speeds, inspection/citation data, and enforcement activities. Collaboration with Maryland State Police identified a section of I-81 for targeted CMV enforcement, with a post-enforcement evaluation revealing mixed results. The dashboards aim to facilitate efficient monitoring of CMV safety and enforcement initiatives, with a focus on improving highway safety. Further refinement of the dashboards and citation data is needed to enhance the effectiveness of targeted enforcement efforts.<br /><br /> <div>
arXiv:2507.06351v1 Announce Type: new 
Abstract: Commercial Motor Vehicle (CMV) safety is crucial in traffic management and public safety. CMVs account for numerous traffic incidents, so monitoring CMV safety and safety inspections is essential for ensuring safe and efficient highway movement. This paper presents the development and real-world application of CMV dashboards designed under the guidance of CMV safety enforcement professionals from the Maryland State Police (MSP), the Maryland Department of Transportation - State Highway Administration (MDOT - SHA), and the Federal Motor Carrier Safety Administration (FMCSA) to enable intuitive and efficient analysis of CMV safety performance measures. First, three CMV safety dashboards enable CMV safety professionals to identify sites with a history of safety performance issues. A supplemental dashboard automates the analysis of CMV enforcement initiatives using the same performance measures. These performance measures are based on CMV probe vehicle speeds, inspection/citation data from Truck Weigh and Inspection Stations (TWIS), patrolling enforcement, and Virtual Weigh Stations (VWS). The authors collaborated with MSP to identify a portion of I-81 in Maryland, susceptible to improvement from targeted CMV enforcement. The supplemental enforcement assessment dashboard was employed to evaluate the impact of enforcement, including the post-enforcement halo effect. The results of the post-enforcement evaluation were mixed, indicating a need for more fine-grained citation data.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eyes on the Road, Mind Beyond Vision: Context-Aware Multi-modal Enhanced Risk Anticipation</title>
<link>https://arxiv.org/abs/2507.06444</link>
<guid>https://arxiv.org/abs/2507.06444</guid>
<content:encoded><![CDATA[
<div> Keywords: accident anticipation, multi-modal framework, driver attention maps, adaptive mechanism, spatio-temporal dependencies <br />
Summary:<br />
The paper introduces CAMERA, a multi-modal framework for accident anticipation that integrates dashcam video, textual annotations, and driver attention maps. This model utilizes an adaptive mechanism based on scene complexity and gaze entropy to reduce false alarms while maintaining high recall in dynamic traffic scenarios. By employing a hierarchical fusion pipeline with Bi-GRU and a Geo-Context Vision-Language module, CAMERA captures spatio-temporal dependencies and translates spatial relationships into human-centric alerts. Evaluations on the DADA-2000 dataset and benchmarks show that CAMERA outperforms existing methods in accuracy and lead time, demonstrating the effectiveness of incorporating driver cognition and contextual information in accident anticipation models. <div>
arXiv:2507.06444v1 Announce Type: new 
Abstract: Accurate accident anticipation remains challenging when driver cognition and dynamic road conditions are underrepresented in predictive models. In this paper, we propose CAMERA (Context-Aware Multi-modal Enhanced Risk Anticipation), a multi-modal framework integrating dashcam video, textual annotations, and driver attention maps for robust accident anticipation. Unlike existing methods that rely on static or environment-centric thresholds, CAMERA employs an adaptive mechanism guided by scene complexity and gaze entropy, reducing false alarms while maintaining high recall in dynamic, multi-agent traffic scenarios. A hierarchical fusion pipeline with Bi-GRU (Bidirectional GRU) captures spatio-temporal dependencies, while a Geo-Context Vision-Language module translates 3D spatial relationships into interpretable, human-centric alerts. Evaluations on the DADA-2000 and benchmarks show that CAMERA achieves state-of-the-art performance, improving accuracy and lead time. These results demonstrate the effectiveness of modeling driver attention, contextual description, and adaptive risk thresholds to enable more reliable accident anticipation.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forex Trading Robot Using Fuzzy Logic</title>
<link>https://arxiv.org/abs/2507.06383</link>
<guid>https://arxiv.org/abs/2507.06383</guid>
<content:encoded><![CDATA[
<div> Proposed fuzzy system, short-term transactions, forex market, fuzzy logic, improved accuracy<br />Summary:<br /> This study introduces a fuzzy system for short-term forex transactions, enhancing traditional strategies by utilizing fuzzy logic. Unlike conventional approaches using predefined ranges for technical indicators like RSI and CCI, this system employs fuzzy Mamdani systems for each indicator, with results combined via voting to create a trading robot. Compared to other methods, the proposed approach shows a significant increase in profitability factor, as demonstrated by calculations of net profit, gross profit, and maximum capital reduction. <div>
arXiv:2507.06383v1 Announce Type: cross 
Abstract: In this study, we propose a fuzzy system for conducting short-term transactions in the forex market. The system is designed to enhance common strategies in the forex market using fuzzy logic, thereby improving the accuracy of transactions. Traditionally, technical strategies based on oscillator indicators have relied on predefined ranges for indicators such as Relative Strength Index (RSI), Commodity Channel Indicator (CCI), and Stochastic to determine entry points for trades. However, the use of these classic indicators has yielded suboptimal results due to the changing nature of the market over time. In our proposed approach, instead of employing classical indicators, we introduce a fuzzy Mamdani system for each indicator. The results obtained from these systems are then combined through voting to design a trading robot. Our findings demonstrate a considerable increase in the profitability factor compared to three other methods. Additionally, net profit, gross profit, and maximum capital reduction are calculated and compared across all approaches.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rugsafe: A multichain protocol for recovering from and defending against Rug Pulls</title>
<link>https://arxiv.org/abs/2507.06423</link>
<guid>https://arxiv.org/abs/2507.06423</guid>
<content:encoded><![CDATA[
<div> Keywords: Rugsafe, cryptocurrency, protocol, vaults, anticoins

Summary: 
Rugsafe introduces a protocol to address rug pulls in the cryptocurrency ecosystem by using cryptographic security and economic incentives. The protocol includes specialized vaults where rugged tokens can be securely deposited, and anticoins are issued as receipts. These anticoins are pegged to the price of rugged tokens and can be used within the ecosystem or burned for additional rewards. The supply of Rugsafe tokens is adjusted based on activity, ensuring stability. Users deposit rugged tokens into vaults on multiple chains and burn anticoins to receive incentives on the RugSafe chain. The protocol's vaults work across different blockchains, offering a practical solution to cryptocurrency market challenges.<br /><br />Summary: <div>
arXiv:2507.06423v1 Announce Type: cross 
Abstract: Rugsafe introduces a comprehensive protocol aimed at mitigating the risks of rug pulls in the cryptocurrency ecosystem. By utilizing cryptographic security measures and economic incentives, the protocol provides a secure multichain system for recovering assets and transforming rugged tokens into opportunities and rewards. Foundational to Rugsafe are specialized vaults where rugged tokens can be securely deposited, and anticoin tokens are issued as receipts. These anticoins are designed to be inversely pegged to the price movement of the underlying rugged token. Users can utilize these anticoins within the ecosystem or choose to burn them, further securing the protocol and earning additional rewards. The supply of the native Rugsafe token is dynamically adjusted based on the volume, value, and activity of rugged tokens, ensuring stability and resilience. By depositing rugged tokens into a vault on several chains, and by burning anticoins, users receive incentives on the RugSafe chain. This protocol's vaults are designed to work in heterogenous blockchain ecosystems, offering a practical and effective solution to one of the most significant challenges in the cryptocurrency market.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams</title>
<link>https://arxiv.org/abs/2507.06803</link>
<guid>https://arxiv.org/abs/2507.06803</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamical systems, domain knowledge, expert knowledge, SysML diagrams, natural language processing

Summary:
This paper presents a strategy for automating the generation of computational models for engineering dynamical systems. The approach leverages domain and expert knowledge to extract information from relevant documents using System Modeling Language (SysML) diagrams. Natural Language Processing (NLP) techniques and Large Language Models (LLMs) are used to enhance the accuracy of the generated diagrams. The process involves several steps, including extracting key nouns and relationships, generating block attributes and relationships, and creating Block Definition Diagrams (BDDs). Case studies demonstrate the application of automated SysML diagram generation. The computational models are then derived from the SysML diagrams via code generation and computational model generation steps. NLP aids in summarization during code generation, while LLMs are used for validation. The proposed approach is flexible across systems, domains, and software, as shown through an example with a simple pendulum model. Improved performance is achieved compared to using LLMs alone.<br /><br />Summary: <div>
arXiv:2507.06803v1 Announce Type: cross 
Abstract: This paper contributes to speeding up the design and deployment of engineering dynamical systems by proposing a strategy for exploiting domain and expert knowledge for the automated generation of dynamical system computational model starting from a corpus of document relevant to the dynamical system of interest and an input document describing the specific system. This strategy is implemented in five steps and, crucially, it uses system modeling language diagrams (SysML) to extract accurate information about the dependencies, attributes, and operations of components. Natural Language Processing (NLP) strategies and Large Language Models (LLMs) are employed in specific tasks to improve intermediate outputs of the SySML diagrams automated generation, such as: list of key nouns; list of extracted relationships; list of key phrases and key relationships; block attribute values; block relationships; and BDD diagram generation. The applicability of automated SysML diagram generation is illustrated with different case studies. The computational models of complex dynamical systems from SysML diagrams are then obtained via code generation and computational model generation steps. In the code generation step, NLP strategies are used for summarization, while LLMs are used for validation only. The proposed approach is not limited to a specific system, domain, or computational software. The applicability of the proposed approach is shown via an end-to-end example from text to model of a simple pendulum, showing improved performance compared to results yielded by LLMs only.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models</title>
<link>https://arxiv.org/abs/2507.06853</link>
<guid>https://arxiv.org/abs/2507.06853</guid>
<content:encoded><![CDATA[
<div> diffusion models, structure elucidation, generative modeling, molecular spectra, machine learning

Summary: 
DiffSpectra is a novel generative framework for molecular structure elucidation from spectral data. It combines diffusion models with SE(3)-equivariant architectures to infer both 2D and 3D molecular structures. The model integrates topological and geometric information to accurately predict molecular structures. SpecFormer, a transformer-based spectral encoder, captures spectral dependencies for conditioning the generation process. DiffSpectra achieves high accuracy in structure elucidation, with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model's effectiveness is attributed to its 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. This work advances the field by unifying multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation. <div>
arXiv:2507.06853v1 Announce Type: cross 
Abstract: Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Load and Information Processing in Financial Markets: Theory and Evidence from Disclosure Complexity</title>
<link>https://arxiv.org/abs/2507.07037</link>
<guid>https://arxiv.org/abs/2507.07037</guid>
<content:encoded><![CDATA[
<div> complexity, cognitive load, financial markets, price discovery, information processing 

Summary: 
The article presents a theoretical framework for understanding how cognitive load affects information processing in financial markets. It distinguishes between attention allocation and cognitive processing capacity, demonstrating that complex information has varying effects on different types of investors. Using a dataset of corporate disclosures and regulatory changes, the study finds that cognitive load significantly impairs price discovery, particularly among less sophisticated investors. A one-standard-deviation increase in cognitive complexity leads to an 18% reduction in information incorporation speed and a 23% increase in mispricing duration. The research supports three theoretical mechanisms: selective attention, processing errors, and strategic complexity, indicating that cognitive constraints create inefficiencies in financial markets. These findings have implications for disclosure regulation and market design. 

Summary: <div>
arXiv:2507.07037v1 Announce Type: cross 
Abstract: We develop a theoretical framework for understanding how cognitive load affects information processing in financial markets and test it using exogenous variation in disclosure complexity. Our model distinguishes between attention allocation and cognitive processing capacity, showing that complex information creates differential effects across investor types. Using a comprehensive dataset of corporate disclosures and a novel identification strategy based on regulatory changes, we find that cognitive load significantly impairs price discovery, with effects concentrated among less sophisticated investors. A one-standard-deviation increase in cognitive complexity reduces information incorporation speed by 18\% and increases mispricing duration by 23\%. We provide evidence for three theoretical mechanisms: selective attention, processing errors, and strategic complexity. Our findings suggest that cognitive constraints create systematic inefficiencies in financial markets, with important implications for disclosure regulation and market design.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Bounded Rationality: Formal Verification of Simon's Satisficing Through Flexible Stochastic Dominance</title>
<link>https://arxiv.org/abs/2507.07052</link>
<guid>https://arxiv.org/abs/2507.07052</guid>
<content:encoded><![CDATA[
<div> formalization, bounded rationality, Lean 4, machine-verified proofs, decision-making

Summary: 
The paper introduces Flexible First-Order Stochastic Dominance (FFSD), a formal framework that bridges classical expected utility theory with Herbert Simon's theory of bounded rationality. Machine-verified proofs in Lean 4 demonstrate how FFSD incorporates parameterized tolerance thresholds to capture satisficing behavior. The paper identifies a critical threshold for unique reference points, establishes an equivalence theorem between FFSD and expected utility maximization for approximate indicator functions, and extends the framework to multi-dimensional decision settings. By encoding these concepts in Lean 4's dependent type theory, the paper presents the first machine-checked formalization of bounded rationality, enabling mechanized reasoning about economic decision-making under uncertainty and cognitive limitations. This work showcases the synergy between formal mathematics and economic theory, illustrating how interactive theorem proving can enhance the understanding of behavioral economics concepts traditionally expressed qualitatively.<br /><br />Summary: <div>
arXiv:2507.07052v1 Announce Type: cross 
Abstract: This paper introduces Flexible First-Order Stochastic Dominance (FFSD), a mathematically rigorous framework that formalizes Herbert Simon's concept of bounded rationality using the Lean 4 theorem prover. We develop machine-verified proofs demonstrating that FFSD bridges classical expected utility theory with Simon's satisficing behavior through parameterized tolerance thresholds. Our approach yields several key results: (1) a critical threshold $\varepsilon < 1/2$ that guarantees uniqueness of reference points, (2) an equivalence theorem linking FFSD to expected utility maximization for approximate indicator functions, and (3) extensions to multi-dimensional decision settings. By encoding these concepts in Lean 4's dependent type theory, we provide the first machine-checked formalization of Simon's bounded rationality, creating a foundation for mechanized reasoning about economic decision-making under uncertainty with cognitive limitations. This work contributes to the growing intersection between formal mathematics and economic theory, demonstrating how interactive theorem proving can advance our understanding of behavioral economics concepts that have traditionally been expressed only qualitatively.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolFORM: Multi-modal Flow Matching for Structure-Based Drug Design</title>
<link>https://arxiv.org/abs/2507.05503</link>
<guid>https://arxiv.org/abs/2507.05503</guid>
<content:encoded><![CDATA[
<div> drug design, structure-based, generative models, molecular modalities, multi-modal flow

Summary:
This article introduces MolFORM, a novel generative framework for structure-based drug design that incorporates both discrete (atom types) and continuous (3D coordinates) molecular modalities using multi-flow matching. The framework also includes a preference-guided fine-tuning stage based on Direct Preference Optimization (DPO) using the Vina score as a reward signal. The proposed multi-modal flow DPO co-modeling strategy aligns discrete and continuous modalities, leading to consistent improvements in generation quality as measured by various evaluation metrics. This approach provides a promising alternative to diffusion-based generative models in structure-based drug design, offering a new direction for improving the effectiveness of drug discovery efforts. <div>
arXiv:2507.05503v1 Announce Type: new 
Abstract: Structure-based drug design (SBDD) seeks to generate molecules that bind effectively to protein targets by leveraging their 3D structural information. While diffusion-based generative models have become the predominant approach for SBDD, alternative non-autoregressive frameworks remain relatively underexplored. In this work, we introduce MolFORM, a novel generative framework that jointly models discrete (atom types) and continuous (3D coordinates) molecular modalities using multi-flow matching. To further enhance generation quality, we incorporate a preference-guided fine-tuning stage based on \textit{Direct Preference Optimization} (DPO), using Vina score as a reward signal. We propose a multi-modal flow DPO co-modeling strategy that simultaneously aligns discrete and continuous modalities, leading to consistent improvements across multiple evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCNP-GO: A python package for assembling MCNP input files with a systems engineering approach</title>
<link>https://arxiv.org/abs/2507.05659</link>
<guid>https://arxiv.org/abs/2507.05659</guid>
<content:encoded><![CDATA[
<div> MCNP-GO, Python package, MCNP input files, assembly, precise modeling<br />
<br />
Summary: 
MCNP-GO is a Python package designed to manipulate and assemble MCNP input files, making it easier for users to assemble multiple independent objects into a cohesive file. It addresses challenges in managing large databases of input files by providing various functionalities such as renumbering, extracting subsets, transforming, and assembling files while handling collisions and materials. The tool ensures reliability and traceability through configuration management systems, keeping track of operations performed on files for easy modification. It is especially useful for applications where precise modeling and positioning of equipment are crucial. The package is user-friendly and efficient, showcased through a practical example of assembling an MCNP input file for a tomographic experiment. MCNP-GO is suitable for users with minimal Python knowledge. <br /><br /> <div>
arXiv:2507.05659v1 Announce Type: new 
Abstract: This article introduces MCNP-GO (https://github.com/afriou/mcnpgo), a Python package designed to manipulate and assemble MCNP input files, allowing users to assemble a set of independent objects, each described by a valid MCNP file, into a single cohesive file. This tool is particularly useful for applications where precise modeling and positioning of equipment are crucial. The package addresses the challenges of managing large databases of MCNP input files, ensuring reliability and traceability through configuration management systems. MCNP-GO provides functionalities such as renumbering, extracting subsets of files, transforming files, and assembling files while managing collisions and materials. It also keeps track of the operations performed on files, enhancing traceability and ease of modification. The article demonstrates the package's capabilities through a practical example of assembling an MCNP input file for a tomographic experiment, highlighting its efficiency and user-friendliness. MCNP-GO is designed for users with minimal Python knowledge.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions</title>
<link>https://arxiv.org/abs/2507.06133</link>
<guid>https://arxiv.org/abs/2507.06133</guid>
<content:encoded><![CDATA[
<div> video diffusion, physics surrogate, partial differential equations, Sequential Deep Operator Network, conditional video diffusion

Summary:
- The article introduces a physics surrogate using conditional video diffusion models.
- A two-stage surrogate is proposed, consisting of an S-DeepONet for producing a physics-consistent prior and a conditional video diffusion model for learning the residual.
- By focusing on the residual space, the model can sharpen high-frequency structures while maintaining global coherence.
- The hybrid surrogate outperforms single-stage counterparts in vortex-dominated flow and plastic deformation benchmarks.
- The approach allows for faithful reconstruction of localized features, accelerates convergence, and transfers seamlessly between different types of nonlinear, time-dependent continua. 

<br /><br />Summary: <div>
arXiv:2507.06133v1 Announce Type: new 
Abstract: Video-diffusion models have recently set the standard in video generation, inpainting, and domain translation thanks to their training stability and high perceptual fidelity. Building on these strengths, we repurpose conditional video diffusion as a physics surrogate for spatio-temporal fields governed by partial differential equations (PDEs). Our two-stage surrogate first applies a Sequential Deep Operator Network (S-DeepONet) to produce a coarse, physics-consistent prior from the prescribed boundary or loading conditions. The prior is then passed to a conditional video diffusion model that learns only the residual: the point-wise difference between the ground truth and the S-DeepONet prediction. By shifting the learning burden from the full solution to its much smaller residual space, diffusion can focus on sharpening high-frequency structures without sacrificing global coherence. The framework is assessed on two disparate benchmarks: (i) vortex-dominated lid-driven cavity flow and (ii) tensile plastic deformation of dogbone specimens. Across these data sets the hybrid surrogate consistently outperforms its single-stage counterpart, cutting the mean relative L2 error from 4.57% to 0.83% for the flow problem and from 4.42% to 2.94% for plasticity, a relative improvements of 81.8% and 33.5% respectively. The hybrid approach not only lowers quantitative errors but also improves visual quality, visibly recovering fine spatial details. These results show that (i) conditioning diffusion on a physics-aware prior enables faithful reconstruction of localized features, (ii) residual learning reduces the problem, accelerating convergence and enhancing accuracy, and (iii) the same architecture transfers seamlessly from incompressible flow to nonlinear elasto-plasticity without problem-specific architectural modifications, highlighting its broad applicability to nonlinear, time-dependent continua.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new engineering theory describing oblique free surface impact by flexible plates</title>
<link>https://arxiv.org/abs/2103.08012</link>
<guid>https://arxiv.org/abs/2103.08012</guid>
<content:encoded><![CDATA[
<div> fluid-structural interaction, slamming loads, water entry, flexible plate, impact force <br />
Summary: 
This paper addresses the challenge of incorporating slamming loads into the structural design of planning hulls by proposing a new engineering theory using a specialized fluid-structural interaction simulation approach. The researchers validated their simulation approach through water entry experiments with flexible plates. They then conducted numerical analyses to understand the impact force and plate deformations based on different parameters. Using their simulation as a "microscope," the study observed the evolution of fluid flows and plate deformations during slamming events. From these observations, a new engineering theory was proposed for flexible plates obliquely impacting the water surface, such as high-speed water craft reentry characterized by porpoising. The research contributes to advancing the understanding of slamming phenomena and offers insight into designing structures to withstand slamming loads.<br /><br />Summary: <div>
arXiv:2103.08012v3 Announce Type: cross 
Abstract: Consideration of slamming loads within the structural design of planning hulls is of critical importance in ensuring adequate structural performance in order to avoid potential catastrophic consequences. However, because of the intricacy in the interplay between complex fluid flows and nonlinear structural deformations that accompany the phenomenology of slamming, a general engineering theory in slamming has yet to be uncovered, and so design relies on specialized theories. In this paper, we propose one such theory for a design case that has, until now, eluded a proper description. In pursuit of this theory, we employ a specialized implicit, partitioned fluid-structural interaction (FSI) simulation approach, in order to study the underlying physical mechanisms accompanying the oblique impact of a flexible plate during water entry. In the present work, we first present validation results from flexible plate water entry experiments, to confirm the veracity of the developed FSI solver. Subsequent to validation, we carry out a series of numerical analyses, in an effort to characterize the regimes in impact force and plate out-of-plane deformations, as a function of impact velocities and plate flexural rigidity. Finally, we use our FSI solver, as a kind of "microscope", to study the mechanistic evolution of fluid flows and elastic plate deformations that occur during slamming. Based on these observations, we propose a novel, but simple engineering theory for flexible plates obliquely impacting the water free surface (e.g. high speed porpoising water craft reentry).
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying heterogeneous micromechanical properties of biological tissues via physics-informed neural networks</title>
<link>https://arxiv.org/abs/2402.10741</link>
<guid>https://arxiv.org/abs/2402.10741</guid>
<content:encoded><![CDATA[
<div> machine learning, elastic properties, soft materials, hyperelastic materials, physics-informed

Summary:
- The study focuses on identifying heterogeneous elastic properties in soft materials using a physics-informed machine learning approach.
- Traditional methods struggle with estimating local stress fields making it challenging to determine full-field mechanical responses.
- The proposed method utilizes physics-informed neural networks (PINNs) to infer elasticity maps in large deformation hyperelastic materials.
- The accuracy and computational efficiency of the approach were evaluated across various materials with structural complexity resembling real tissue patterns.
- The network architecture consistently produced highly accurate estimations of heterogeneous elasticity maps, even in the presence of up to 10% noise in the training data. <div>
arXiv:2402.10741v3 Announce Type: cross 
Abstract: The heterogeneous micromechanical properties of biological tissues have profound implications across diverse medical and engineering domains. However, identifying full-field heterogeneous elastic properties of soft materials using traditional engineering approaches is fundamentally challenging due to difficulties in estimating local stress fields. Recently, there has been a growing interest in using data-driven models to learn full-field mechanical responses such as displacement and strain from experimental or synthetic data. However, research studies on inferring full-field elastic properties of materials, a more challenging problem, are scarce, particularly for large deformation, hyperelastic materials. Here, we propose a physics-informed machine learning approach to identify the elasticity map in nonlinear, large deformation hyperelastic materials. We evaluate the prediction accuracies and computational efficiency of physics-informed neural networks (PINNs) by inferring the heterogeneous elasticity maps across three materials with structural complexity that closely resemble real tissue patterns, such as brain tissue and tricuspid valve tissue. We further applied our improved architecture to three additional examples of breast cancer tissue and extended our analysis to three hyperelastic constitutive models: Neo-Hookean, Mooney Rivlin, and Gent. Our selected network architecture consistently produced highly accurate estimations of heterogeneous elasticity maps, even when there was up to 10% noise present in the training data.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADEPT: A Noninvasive Method for Determining Elastic Parameters of Valve Tissue</title>
<link>https://arxiv.org/abs/2409.19081</link>
<guid>https://arxiv.org/abs/2409.19081</guid>
<content:encoded><![CDATA[
<div> Keywords: valve repair, computer simulation, noninvasive method, mechanical parameters, tricuspid valve

Summary: 
The study introduces a novel noninvasive method, ADEPT, for determining elastic parameters of valve tissue, focusing on the tricuspid valve in a child. By tracking valve displacements in 3D echocardiogram sequences and employing physics-informed neural networks, patient-specific mechanical properties were estimated and applied to a simulated model. The method significantly improved accuracy compared to generic parameters from literature, with the simulated model closely aligning with reference image segmentation. This approach enhances the feasibility of computer simulations for predicting optimal valve repair outcomes before intervention, addressing the current limitation of insufficient noninvasive methods to assess in vivo mechanical parameters of valves. The study demonstrates the potential of ADEPT in personalized medicine for valve interventions, paving the way for more precise and tailored treatment strategies in clinical practice.<br /><br />Summary: <div>
arXiv:2409.19081v2 Announce Type: cross 
Abstract: Computer simulation of "virtual interventions" may inform optimal valve repair for a given patient prior to intervention. However, the paucity of noninvasive methods to determine in vivo mechanical parameters of valves limits the accuracy of computer prediction and their clinical application. To address this, we propose ADEPT: A noninvasive method for Determining Elastic Parameters of valve Tissue. In this work, we demonstrated its application to the tricuspid valve of a child. We first tracked valve displacements from open to closed frames within a 3D echocardiogram time sequence using image registration. Physics-informed neural networks were subsequently applied to estimate the nonlinear mechanical properties from first principles and reference displacements. The simulated model using these patient-specific parameters closely aligned with the reference image segmentation, achieving a mean symmetric distance of less than 1 mm. Our approach doubled the accuracy of the simulated model compared to the generic parameters reported in the literature.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity</title>
<link>https://arxiv.org/abs/2507.05816</link>
<guid>https://arxiv.org/abs/2507.05816</guid>
<content:encoded><![CDATA[
<div> Keywords: ROP risk prediction, large language models, affective biases, Chinese benchmark dataset, Affective-ROPTester <br />
<br />
Summary: 
The study introduces a new Chinese benchmark dataset, CROP, for predicting retinopathy of prematurity (ROP) risk using large language models (LLMs). The Affective-ROPTester framework is proposed to evaluate LLMs' predictive capabilities and affective biases in ROP risk stratification. Results show that LLMs perform better with structured external inputs than with intrinsic knowledge alone in predicting ROP risk. Affective biases are observed in model outputs, with a tendency to overestimate medium- and high-risk cases. Positive emotional framing helps mitigate predictive bias compared to negative emotions. The study underscores the importance of affect-sensitive prompt engineering in enhancing diagnostic reliability and proposes Affective-ROPTester as a tool for evaluating and mitigating affective bias in clinical language modeling systems. <div>
arXiv:2507.05816v1 Announce Type: cross 
Abstract: Despite the remarkable progress of large language models (LLMs) across various domains, their capacity to predict retinopathy of prematurity (ROP) risk remains largely unexplored. To address this gap, we introduce a novel Chinese benchmark dataset, termed CROP, comprising 993 admission records annotated with low, medium, and high-risk labels. To systematically examine the predictive capabilities and affective biases of LLMs in ROP risk stratification, we propose Affective-ROPTester, an automated evaluation framework incorporating three prompting strategies: Instruction-based, Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and ICL schemes leverage external medical knowledge to enhance predictive accuracy. Crucially, we integrate emotional elements at the prompt level to investigate how different affective framings influence the model's ability to predict ROP and its bias patterns. Empirical results derived from the CROP dataset yield two principal observations. First, LLMs demonstrate limited efficacy in ROP risk prediction when operating solely on intrinsic knowledge, yet exhibit marked performance gains when augmented with structured external inputs. Second, affective biases are evident in the model outputs, with a consistent inclination toward overestimating medium- and high-risk cases. Third, compared to negative emotions, positive emotional framing contributes to mitigating predictive bias in model outputs. These findings highlight the critical role of affect-sensitive prompt engineering in enhancing diagnostic reliability and emphasize the utility of Affective-ROPTester as a framework for evaluating and mitigating affective bias in clinical language modeling systems.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topic Modeling and Link-Prediction for Material Property Discovery</title>
<link>https://arxiv.org/abs/2507.06139</link>
<guid>https://arxiv.org/abs/2507.06139</guid>
<content:encoded><![CDATA[
<div> Hierarchical Link Prediction, Matrix Factorization, Material Discovery, Topic Tree, Transition-Metal Dichalcogenides <br />
Summary:<br />
The study introduces a hierarchical link prediction framework utilizing matrix factorization to uncover hidden connections within scientific literature networks and knowledge graphs. By combining Hierarchical Nonnegative Matrix Factorization (HNMFk), Boolean matrix factorization (BNMFk), and Logistic matrix factorization (LMF), a three-level topic tree is constructed from a vast document corpus focusing on transition-metal dichalcogenides (TMDs). Through an ensemble approach of BNMFk + LMF, the method provides both interpretable clusters and probabilistic scoring, revealing coherent topics related to TMDs such as superconductivity and energy storage. Missing or weakly connected links between topics and materials are highlighted, sparking new hypotheses for cross-disciplinary exploration. Validation shows the model accurately predicts associations within TMD clusters, showcasing its ability to uncover hidden connections in a diverse scientific document corpus. Interactive tools like the Streamlit dashboard facilitate human-in-the-loop scientific discovery by presenting the inferred links and enabling further exploration. <br /> <div>
arXiv:2507.06139v1 Announce Type: cross 
Abstract: Link prediction infers missing or future relations between graph nodes, based on connection patterns. Scientific literature networks and knowledge graphs are typically large, sparse, and noisy, and often contain missing links between entities. We present an AI-driven hierarchical link prediction framework that integrates matrix factorization to infer hidden associations and steer discovery in complex material domains. Our method combines Hierarchical Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization (BNMFk) with automatic model selection, as well as Logistic matrix factorization (LMF), we use to construct a three-level topic tree from a 46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs). These materials are studied in a variety of physics fields with many current and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with probabilistic scoring. The resulting HNMFk clusters map each material onto coherent topics like superconductivity, energy storage, and tribology. Also, missing or weakly connected links are highlight between topics and materials, suggesting novel hypotheses for cross-disciplinary exploration. We validate our method by removing publications about superconductivity in well-known superconductors, and show the model predicts associations with the superconducting TMD clusters. This shows the method finds hidden connections in a graph of material to latent topic associations built from scientific literature, especially useful when examining a diverse corpus of scientific documents covering the same class of phenomena or materials but originating from distinct communities and perspectives. The inferred links generating new hypotheses, produced by our method, are exposed through an interactive Streamlit dashboard, designed for human-in-the-loop scientific discovery.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Membrane Degradation in PEM Electrolyzers with Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2507.02887</link>
<guid>https://arxiv.org/abs/2507.02887</guid>
<content:encoded><![CDATA[
<div> Keywords: Proton exchange membrane electrolyzers, degradation modeling, Physics-Informed Neural Networks, membrane thinning, predictive tools 

Summary: 
Proton exchange membrane (PEM) electrolyzers play a crucial role in sustainable hydrogen production but face challenges due to membrane degradation, impacting long-term performance. Traditional physics-based models require numerous parameters, while data-driven approaches like machine learning may lack physical consistency. This study introduces Physics-Informed Neural Networks (PINNs) to model membrane degradation in PEM electrolyzers accurately. By coupling two differential equations, one modeling membrane thinning and another governing cell voltage evolution, the PINN captures system degradation dynamics with limited noisy data. The hybrid approach offers a balance between interpretability and flexibility, providing a foundation for more robust predictive tools in electrochemical system diagnostics.<br /><br />Summary: <div>
arXiv:2507.02887v1 Announce Type: new 
Abstract: Proton exchange membrane (PEM) electrolyzers are pivotal for sustainable hydrogen production, yet their long-term performance is hindered by membrane degradation, which poses reliability and safety challenges. Therefore, accurate modeling of this degradation is essential for optimizing durability and performance. To address these concerns, traditional physics-based models have been developed, offering interpretability but requiring numerous parameters that are often difficult to measure and calibrate. Conversely, data-driven approaches, such as machine learning, offer flexibility but may lack physical consistency and generalizability. To address these limitations, this study presents the first application of Physics-Informed Neural Networks (PINNs) to model membrane degradation in PEM electrolyzers. The proposed PINN framework couples two ordinary differential equations, one modeling membrane thinning via a first-order degradation law and another governing the time evolution of the cell voltage under membrane degradation. Results demonstrate that the PINN accurately captures the long-term system's degradation dynamics while preserving physical interpretability with limited noisy data. Consequently, this work introduces a novel hybrid modeling approach for estimating and understanding membrane degradation mechanisms in PEM electrolyzers, offering a foundation for more robust predictive tools in electrochemical system diagnostics.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanics Simulation with Implicit Neural Representations of Complex Geometries</title>
<link>https://arxiv.org/abs/2507.03087</link>
<guid>https://arxiv.org/abs/2507.03087</guid>
<content:encoded><![CDATA[
<div> Neural Implicit Representations, Shifted Boundary Method, Linear Elasticity Simulations, Computational Framework, Meshless Simulations <br />
Summary: <br />
This study introduces a novel computational framework that seamlessly integrates Implicit Neural Representations (INRs) with the Shifted Boundary Method (SBM) for linear elasticity simulations, eliminating the need for explicit geometry transformations and meshing. By directly accessing neural implicit geometry, the method acquires essential boundary information and distance vectors for SBM, enhancing efficiency and accuracy. The approach is tested on complex geometries (Stanford Bunny, Eiffel Tower, gyroids) sourced from triangle soups and point clouds, demonstrating significant computational advantages. The method's robustness and accuracy make it suitable for various applications, including biomedical, geophysical, and advanced manufacturing fields. <div>
arXiv:2507.03087v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs), characterized by neural network-encoded signed distance fields, provide a powerful means to represent complex geometries continuously and efficiently. While successful in computer vision and generative modeling, integrating INRs into computational analysis workflows, such as finite element simulations, remains underdeveloped. In this work, we propose a computational framework that seamlessly combines INRs with the Shifted Boundary Method (SBM) for high-fidelity linear elasticity simulations without explicit geometry transformations. By directly querying the neural implicit geometry, we obtain the surrogate boundaries and distance vectors essential for SBM, effectively eliminating the meshing step. We demonstrate the efficacy and robustness of our approach through elasticity simulations on complex geometries (Stanford Bunny, Eiffel Tower, gyroids) sourced from triangle soups and point clouds. Our method showcases significant computational advantages and accuracy, underscoring its potential in biomedical, geophysical, and advanced manufacturing applications.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Cross-Attention Interaction in Transformers for Interpreting TCR-pMHC Binding</title>
<link>https://arxiv.org/abs/2507.03197</link>
<guid>https://arxiv.org/abs/2507.03197</guid>
<content:encoded><![CDATA[
<div> Keywords: CD8+ killer T cells, CD4+ helper T cells, T cell receptors, transformer-based models, post-hoc explainable AI

Summary:
CD8+ "killer" T cells and CD4+ "helper" T cells are crucial in the immune system, recognizing antigens presented by pMHC via T cell receptors. Transformer models like TULIP have shown great performance in this area but lack interpretability. The new QCAI method aims to interpret cross-attention mechanisms in transformer decoders for TCR-pMHC modeling. A benchmark, TCR-XAI, with 274 TCR-pMHC structures is used to evaluate the method's performance. By computing physical distances between amino acid residues and assessing residue importance, QCAI demonstrates state-of-the-art interpretability and prediction accuracy. The study highlights the significance of understanding T cell response mechanisms and the potential of QCAI in enhancing comprehensibility in TCR-pMHC modeling. 

<br /><br />Summary: <div>
arXiv:2507.03197v1 Announce Type: new 
Abstract: CD8+ "killer" T cells and CD4+ "helper" T cells play a central role in the adaptive immune system by recognizing antigens presented by Major Histocompatibility Complex (pMHC) molecules via T Cell Receptors (TCRs). Modeling binding between T cells and the pMHC complex is fundamental to understanding basic mechanisms of human immune response as well as in developing therapies. While transformer-based models such as TULIP have achieved impressive performance in this domain, their black-box nature precludes interpretability and thus limits a deeper mechanistic understanding of T cell response. Most existing post-hoc explainable AI (XAI) methods are confined to encoder-only, co-attention, or model-specific architectures and cannot handle encoder-decoder transformers used in TCR-pMHC modeling. To address this gap, we propose Quantifying Cross-Attention Interaction (QCAI), a new post-hoc method designed to interpret the cross-attention mechanisms in transformer decoders. Quantitative evaluation is a challenge for XAI methods; we have compiled TCR-XAI, a benchmark consisting of 274 experimentally determined TCR-pMHC structures to serve as ground truth for binding. Using these structures we compute physical distances between relevant amino acid residues in the TCR-pMHC interaction region and evaluate how well our method and others estimate the importance of residues in this region across the dataset. We show that QCAI achieves state-of-the-art performance on both interpretability and prediction accuracy under the TCR-XAI benchmark.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElliottAgents: A Natural Language-Driven Multi-Agent System for Stock Market Analysis and Prediction</title>
<link>https://arxiv.org/abs/2507.03435</link>
<guid>https://arxiv.org/abs/2507.03435</guid>
<content:encoded><![CDATA[
<div> Keywords: ElliottAgents, multi-agent system, natural language processing, stock market data, AI-driven analysis

Summary:
ElliottAgents is a multi-agent system that utilizes natural language processing and large language models to analyze complex stock market data. The system integrates AI-driven analysis with the Elliott Wave Principle to create predictions and explanations that are easily understandable by humans. One of its key features is the natural language dialogue between agents, allowing for collaborative analysis refinement. The architecture, enhanced by large language models, enables advanced language understanding, reasoning, and autonomous decision-making. Through experiments, it has been demonstrated that ElliottAgents is effective in pattern recognition and generating natural language descriptions of market trends. This research contributes to the field of natural language processing applications in specialized domains, showcasing how AI-driven dialogue systems can enhance collaborative analysis in data-intensive fields. By bridging the gap between complex financial data and human comprehension, ElliottAgents addresses the need for interpretable and adaptive prediction systems in finance. 

<br /><br />Summary: <div>
arXiv:2507.03435v1 Announce Type: new 
Abstract: This paper presents ElliottAgents, a multi-agent system leveraging natural language processing (NLP) and large language models (LLMs) to analyze complex stock market data. The system combines AI-driven analysis with the Elliott Wave Principle to generate human-comprehensible predictions and explanations. A key feature is the natural language dialogue between agents, enabling collaborative analysis refinement. The LLM-enhanced architecture facilitates advanced language understanding, reasoning, and autonomous decision-making. Experiments demonstrate the system's effectiveness in pattern recognition and generating natural language descriptions of market trends. ElliottAgents contributes to NLP applications in specialized domains, showcasing how AI-driven dialogue systems can enhance collaborative analysis in data-intensive fields. This research bridges the gap between complex financial data and human understanding, addressing the need for interpretable and adaptive prediction systems in finance.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Concept for Autonomous Problem-Solving in Intralogistics Scenarios</title>
<link>https://arxiv.org/abs/2507.03534</link>
<guid>https://arxiv.org/abs/2507.03534</guid>
<content:encoded><![CDATA[
<div> autonomy, automation systems, enabling technologies, Large Language Models, problem-solving capabilities  
Summary:  
- The paper discusses the importance of achieving greater autonomy in automation systems to effectively handle unforeseen situations in complex real-world environments.  
- It outlines a structured concept consisting of context enrichment, situation analysis, and solution strategy generation as key steps towards increasing autonomy in automation systems.  
- The proposed approach aims to reduce the need for human intervention by enabling automation systems to make more independent decisions.  
- Possible realizations of the concept, including the use of Large Language Models, are discussed as ways to enhance autonomy in automation systems.  
- While some tasks may still require human assistance, the approach significantly improves the adaptive and intelligent problem-solving capabilities of automation systems.  
<br /><br />Summary: <div>
arXiv:2507.03534v1 Announce Type: new 
Abstract: Achieving greater autonomy in automation systems is crucial for handling unforeseen situations effectively. However, this remains challenging due to technological limitations and the complexity of real-world environments. This paper examines the need for increased autonomy, defines the problem, and outlines key enabling technologies. A structured concept is proposed, consisting of three main steps: context enrichment, situation analysis, and generation of solution strategies. By following this approach, automation systems can make more independent decisions, reducing the need for human intervention. Additionally, possible realizations of the concept are discussed, especially the use of Large Language Models. While certain tasks may still require human assistance, the proposed approach significantly enhances the autonomy of automation systems, enabling more adaptive and intelligent problem-solving capabilities.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Based Control for Power-to-X Platforms: Knowledge Integration for Digital Twins</title>
<link>https://arxiv.org/abs/2507.03553</link>
<guid>https://arxiv.org/abs/2507.03553</guid>
<content:encoded><![CDATA[
<div> Keywords: Offshore Power-to-X platforms, Digital Twins, adaptive process control, semantic technologies, Neo4j <br />
Summary: <br />
Offshore Power-to-X platforms face challenges in adaptive process control due to volatile operating conditions. To address this issue, utilizing Digital Twins in these platforms is seen as a promising solution. The integration of heterogeneous models and structured representation of model information is crucial for comprehensive knowledge integration in Digital Twins. The proposed approach utilizes a standardized description of behavior models, semantic technologies, and a graph-based model understanding for automatic adaptation and selection of suitable models. This approach is implemented using a graph-based knowledge representation with Neo4j, automatic data extraction from Asset Administration Shells, and port matching to ensure compatible model configurations. By combining these elements, the approach aims to enhance the flexibility and efficiency of Power-to-X platforms. <div>
arXiv:2507.03553v1 Announce Type: new 
Abstract: Offshore Power-to-X platforms enable flexible conversion of renewable energy, but place high demands on adaptive process control due to volatile operating conditions. To face this challenge, using Digital Twins in Power-to-X platforms is a promising approach. Comprehensive knowledge integration in Digital Twins requires the combination of heterogeneous models and a structured representation of model information. The proposed approach uses a standardized description of behavior models, semantic technologies and a graph-based model understanding to enable automatic adaption and selection of suitable models. It is implemented using a graph-based knowledge representation with Neo4j, automatic data extraction from Asset Administration Shells and port matching to ensure compatible model configurations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operator-based machine learning framework for generalizable prediction of unsteady treatment dynamics in stormwater infrastructure</title>
<link>https://arxiv.org/abs/2507.04682</link>
<guid>https://arxiv.org/abs/2507.04682</guid>
<content:encoded><![CDATA[
<div> neural network, stormwater treatment, urban water management, computational fluid dynamics, pollutant transport

Summary: 
The study introduces a composite operator-based neural network (CPNN) framework for predicting hydraulics and pollutant dynamics in stormwater treatment systems. Traditional models lack accuracy due to oversimplified processes, while computational fluid dynamics is too computationally expensive. The CPNN framework achieves high accuracy in predicting hydraulic behavior and particulate matter concentration in stormwater treatment devices. Challenges are found in capturing dynamics under extreme low-flow conditions, as they contribute less to the training process. Sensitivity analyses using the CPNN highlight the impact of storm event loading on pollutant transport. The CPNN framework shows promise for continuous, long-term evaluation of stormwater infrastructure performance, aiding in climate-aware planning and implementation. <div>
arXiv:2507.04682v1 Announce Type: new 
Abstract: Stormwater infrastructures are decentralized urban water-management systems that face highly unsteady hydraulic and pollutant loadings from episodic rainfall-runoff events. Accurately evaluating their in-situ treatment performance is essential for cost-effective design and planning. Traditional lumped dynamic models (e.g., continuously stirred tank reactor, CSTR) are computationally efficient but oversimplify transport and reaction processes, limiting predictive accuracy and insight. Computational fluid dynamics (CFD) resolves detailed turbulent transport and pollutant fate physics but incurs prohibitive computational cost for unsteady and long-term simulations. To address these limitations, this study develops a composite operator-based neural network (CPNN) framework that leverages state-of-the-art operator learning to predict the spatial and temporal dynamics of hydraulics and particulate matter (PM) in stormwater treatment. The framework is demonstrated on a hydrodynamic separator (HS), a common urban treatment device. Results indicate that the CPNN achieves R2 > 0.8 for hydraulic predictions in 95.2% of test cases; for PM concentration predictions, R2 > 0.8 in 72.6% of cases and 0.4 < R2 < 0.8 in 22.6%. The analysis identifies challenges in capturing dynamics under extreme low-flow conditions, owing to their lower contribution to the training loss. Exploiting the automatic-differentiation capability of the CPNN, sensitivity analyses quantify the influence of storm event loading on PM transport. Finally, the potential of the CPNN framework for continuous, long-term evaluation of stormwater infrastructure performance is discussed, marking a step toward robust, climate-aware planning and implementation.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Gradient Low-Rank Projection Fine-Tuning for LLMs</title>
<link>https://arxiv.org/abs/2507.02503</link>
<guid>https://arxiv.org/abs/2507.02503</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Continual Learning, Low-Rank Adaptation, GORP, Gradient Projection

Summary:
Continual fine-tuning of Large Language Models (LLMs) often faces a trade-off between efficiency and expressiveness. The Low-Rank Adaptation (LoRA) method, while efficient, restricts the model's learning capabilities and knowledge transfer due to its low-rank nature and reliance on explicit parameter constraints. In response, GORP (Gradient LOw Rank Projection) for Continual Learning introduces a novel training strategy that combines full and low-rank parameters to update within a unified low-rank gradient subspace. This approach expands the optimization space while maintaining efficiency and reducing catastrophic forgetting. Through extensive experiments on continual learning benchmarks, GORP outperforms existing state-of-the-art methods. The code for GORP is publicly available on GitHub, providing a framework for implementing and testing this innovative training strategy.<br /><br />Summary: <div>
arXiv:2507.02503v1 Announce Type: cross 
Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable diffusion-based generation for multi-channel biological data</title>
<link>https://arxiv.org/abs/2507.02902</link>
<guid>https://arxiv.org/abs/2507.02902</guid>
<content:encoded><![CDATA[
<div> diffusion-based models; spatial profiling technologies; generative modeling; high-dimensional data; spatial alignment <br />
Summary: This article introduces a unified diffusion framework for generating structured and spatial biological data, such as imaging mass cytometry and spatial transcriptomics. The model incorporates a hierarchical feature injection mechanism for multi-resolution conditioning and a combination of latent-space and output-space channel-wise attention to capture inter-channel relationships. It is trained using a random masking strategy to support flexible conditioning and generalization to arbitrary subsets of observed channels. The model demonstrates superior performance in tasks such as protein imputation in IMC and gene-to-protein prediction in single-cell datasets, as well as strong generalization to unseen conditional configurations. <div>
arXiv:2507.02902v1 Announce Type: cross 
Abstract: Spatial profiling technologies in biology, such as imaging mass cytometry (IMC) and spatial transcriptomics (ST), generate high-dimensional, multi-channel data with strong spatial alignment and complex inter-channel relationships. Generative modeling of such data requires jointly capturing intra- and inter-channel structure, while also generalizing across arbitrary combinations of observed and missing channels for practical application. Existing diffusion-based models generally assume low-dimensional inputs (e.g., RGB images) and rely on simple conditioning mechanisms that break spatial correspondence and ignore inter-channel dependencies. This work proposes a unified diffusion framework for controllable generation over structured and spatial biological data. Our model contains two key innovations: (1) a hierarchical feature injection mechanism that enables multi-resolution conditioning on spatially aligned channels, and (2) a combination of latent-space and output-space channel-wise attention to capture inter-channel relationships. To support flexible conditioning and generalization to arbitrary subsets of observed channels, we train the model using a random masking strategy, enabling it to reconstruct missing channels from any combination of inputs. We demonstrate state-of-the-art performance across both spatial and non-spatial prediction tasks, including protein imputation in IMC and gene-to-protein prediction in single-cell datasets, and show strong generalization to unseen conditional configurations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolProphecy: Bridging Medicinal Chemists' Knowledge and Molecular Pre-Trained Models via a Multi-Modal Framework</title>
<link>https://arxiv.org/abs/2507.02932</link>
<guid>https://arxiv.org/abs/2507.02932</guid>
<content:encoded><![CDATA[
<div> framework, molecular property prediction, human-in-the-loop, ChatGPT, chemist knowledge <br />
Summary: 
The MolProphecy framework integrates chemists' domain knowledge into molecular property prediction models using a human-in-the-loop approach. By leveraging ChatGPT as a virtual chemist, it simulates expert reasoning and decision-making to enhance model accuracy. MolProphecy outperforms state-of-the-art models on benchmark datasets, showing improvements in RMSE and AUROC metrics. The framework combines chemist knowledge with structural features through a gated cross-attention mechanism, improving both accuracy and interpretability. It offers a collaborative approach to drug discovery, with the flexibility to incorporate real chemist input without requiring retraining. The implementation of MolProphecy is publicly available for use. <br /> <div>
arXiv:2507.02932v1 Announce Type: cross 
Abstract: MolProphecy is a human-in-the-loop (HITL) multi-modal framework designed to integrate chemists' domain knowledge into molecular property prediction models. While molecular pre-trained models have enabled significant gains in predictive accuracy, they often fail to capture the tacit, interpretive reasoning central to expert-driven molecular design. To address this, MolProphecy employs ChatGPT as a virtual chemist to simulate expert-level reasoning and decision-making. The generated chemist knowledge is embedded by the large language model (LLM) as a dedicated knowledge representation and then fused with graph-based molecular features through a gated cross-attention mechanism, enabling joint reasoning over human-derived and structural features. Evaluated on four benchmark datasets (FreeSolv, BACE, SIDER, and ClinTox), MolProphecy outperforms state-of-the-art (SOTA) models, achieving a 15.0 percent reduction in RMSE on FreeSolv and a 5.39 percent improvement in AUROC on BACE. Analysis reveals that chemist knowledge and structural features provide complementary contributions, improving both accuracy and interpretability. MolProphecy offers a practical and generalizable approach for collaborative drug discovery, with the flexibility to incorporate real chemist input in place of the current simulated proxy--without the need for model retraining. The implementation is publicly available at https://github.com/zhangruochi/MolProphecy.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-Through Tensors: A Unified Computational Graph Architecture for Multi-Layer Transportation Network Optimization</title>
<link>https://arxiv.org/abs/2507.02961</link>
<guid>https://arxiv.org/abs/2507.02961</guid>
<content:encoded><![CDATA[
<div> Keywords: Flow Through Tensors, transportation network modeling, tensor decomposition, gradient-based optimization, integrated mobility systems<br />
Summary:<br />
Flow Through Tensors (FTT) is introduced as a unified computational graph architecture for transportation network modeling. It connects origin destination flows, path probabilities, and link travel times as interconnected tensors, allowing for multidimensional analysis of traffic patterns. The framework enables gradient-based optimization across different modeling elements, supports efficient system efficiency quantification over time, space, and user groups, and implements tensor decomposition techniques for large-scale applications. These innovations enable real-time control strategies, coordination between transportation modes/operators, and enforcement of network constraints. FTT bridges the gap between theoretical models and practical deployment needs, providing a foundation for next-generation integrated mobility systems. <div>
arXiv:2507.02961v1 Announce Type: cross 
Abstract: Modern transportation network modeling increasingly involves the integration of diverse methodologies including sensor-based forecasting, reinforcement learning, classical flow optimization, and demand modeling that have traditionally been developed in isolation. This paper introduces Flow Through Tensors (FTT), a unified computational graph architecture that connects origin destination flows, path probabilities, and link travel times as interconnected tensors. Our framework makes three key contributions: first, it establishes a consistent mathematical structure that enables gradient-based optimization across previously separate modeling elements; second, it supports multidimensional analysis of traffic patterns over time, space, and user groups with precise quantification of system efficiency; third, it implements tensor decomposition techniques that maintain computational tractability for large scale applications. These innovations collectively enable real time control strategies, efficient coordination between multiple transportation modes and operators, and rigorous enforcement of physical network constraints. The FTT framework bridges the gap between theoretical transportation models and practical deployment needs, providing a foundation for next generation integrated mobility systems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LANTERN: A Machine Learning Framework for Lipid Nanoparticle Transfection Efficiency Prediction</title>
<link>https://arxiv.org/abs/2507.03209</link>
<guid>https://arxiv.org/abs/2507.03209</guid>
<content:encoded><![CDATA[
<div> ionizable lipids, lipid nanoparticle, RNA delivery, machine learning, transfection efficiency

Summary:
LANTERN is a machine learning framework designed to predict transfection efficiency for lipid nanoparticles used in RNA delivery. By utilizing chemically informative features like Morgan fingerprints and Expert descriptors, LANTERN outperformed previous models, including AGILE, in predicting transfection efficiency. The model achieved a high performance with an R-squared value of 0.8161 and a correlation coefficient of 0.9053. LANTERN showed consistent strong performance across multiple evaluation metrics, making it a valuable tool for accelerating the design of lipid-based RNA delivery systems. The discovery of new ionizable lipids for efficient RNA delivery is crucial for the development of RNA-based therapeutics. The use of machine learning in predicting transfection efficiency from molecular structure has the potential to streamline the identification of lead compounds and advance the field of RNA-based therapeutics. <div>
arXiv:2507.03209v1 Announce Type: cross 
Abstract: The discovery of new ionizable lipids for efficient lipid nanoparticle (LNP)-mediated RNA delivery remains a critical bottleneck for RNA-based therapeutics development. Recent advances have highlighted the potential of machine learning (ML) to predict transfection efficiency from molecular structure, enabling high-throughput virtual screening and accelerating lead identification. However, existing approaches are hindered by inadequate data quality, ineffective feature representations, low predictive accuracy, and poor generalizability. Here, we present LANTERN (Lipid nANoparticle Transfection Efficiency pRedictioN), a robust ML framework for predicting transfection efficiency based on ionizable lipid representation. We benchmarked a diverse set of ML models against AGILE, a previously published model developed for transfection prediction. Our results show that combining simpler models with chemically informative features, particularly count-based Morgan fingerprints, outperforms more complex models that rely on internally learned embeddings, such as AGILE. We also show that a multi-layer perceptron trained on a combination of Morgan fingerprints and Expert descriptors achieved the highest performance ($\text{R}^2$ = 0.8161, r = 0.9053), significantly exceeding AGILE ($\text{R}^2$ = 0.2655, r = 0.5488). We show that the models in LANTERN consistently have strong performance across multiple evaluation metrics. Thus, LANTERN offers a robust benchmarking framework for LNP transfection prediction and serves as a valuable tool for accelerating lipid-based RNA delivery systems design.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time prediction of plasma instabilities with sparse-grid-accelerated optimized dynamic mode decomposition</title>
<link>https://arxiv.org/abs/2507.03245</link>
<guid>https://arxiv.org/abs/2507.03245</guid>
<content:encoded><![CDATA[
<div> sparse grid interpolation, parametric reduced-order models, gyrokinetic simulations, fusion experiments, dynamic mode decomposition <br />
Summary: 
This paper explores the efficient training of parametric reduced-order models (ROMs) using sparse grid interpolation with (L)-Leja points for scenarios with high-dimensional input spaces. By focusing on gyrokinetic simulations of plasma micro-instabilities in fusion experiments, the study constructs parametric ROMs for the full 5D gyrokinetic distribution function using optimized dynamic mode decomposition and sparse grids based on (L)-Leja points. The research assesses the ROM prediction capabilities in different scenarios, including the Cyclone Base Case benchmark and a real-world micro-instability simulation with six input parameters. The results demonstrate that accurate parametric ROMs can be achieved at a low cost of high-fidelity simulations using sparse grids, showing the potential of these models in enabling complex many-query tasks in fusion research. <br /><br />Summary: <div>
arXiv:2507.03245v1 Announce Type: cross 
Abstract: Parametric data-driven reduced-order models (ROMs) that embed dependencies in a large number of input parameters are crucial for enabling many-query tasks in large-scale problems. These tasks, including design optimization, control, and uncertainty quantification, are essential for developing digital twins in real-world applications. However, standard training data generation methods are computationally prohibitive due to the curse of dimensionality, as their cost scales exponentially with the number of inputs.This paper investigates efficient training of parametric data-driven ROMs using sparse grid interpolation with (L)-Leja points, specifically targeting scenarios with higher-dimensional input parameter spaces. (L)-Leja points are nested and exhibit slow growth, resulting in sparse grids with low cardinality in low-to-medium dimensional settings, making them ideal for large-scale, computationally expensive problems. Focusing on gyrokinetic simulations of plasma micro-instabilities in fusion experiments as a representative real-world application, we construct parametric ROMs for the full 5D gyrokinetic distribution function via optimized dynamic mode decomposition (optDMD) and sparse grids based on (L)-Leja points. We perform detailed experiments in two scenarios: First, the Cyclone Base Case benchmark assesses optDMD ROM prediction capabilities beyond training time horizons and across variations in the binormal wave number. Second, for a real-world electron temperature gradient driven micro-instability simulation featuring six input parameters, we demonstrate that an accurate parametric optDMD ROM can be constructed at a cost of only $28$ high-fidelity gyrokinetic simulations thanks to sparse grids. In the broader context of fusion research, these results demonstrate the potential of sparse grid-based parametric ROMs to enable otherwise intractable many-query tasks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-robust multi-fidelity surrogate modelling for parametric partial differential equations</title>
<link>https://arxiv.org/abs/2507.03691</link>
<guid>https://arxiv.org/abs/2507.03691</guid>
<content:encoded><![CDATA[
<div> surrogate models, multi-fidelity, stochastic collocation, numerical noise, PDEs <br />
Summary:
The article addresses the challenge of constructing noise-robust surrogate models for quantities of interest (QoIs) from parametric partial differential equations (PDEs) using multi-fidelity collocation techniques, specifically the Multi-Index Stochastic Collocation (MISC). In scenarios where PDE evaluations are corrupted by numerical noise, an improved version of MISC is proposed to automatically detect and ignore noisy fidelities, mitigating overfitting and degradation of surrogate quality. The approach monitors the spectral decay of the surrogate at each iteration to identify noise onset and selectively halt the use of noisy fidelities. Numerical validation on parabolic advection-diffusion PDE and parametric turbulent incompressible Navier-Stokes problem demonstrates the accuracy and robustness of the resulting multi-fidelity surrogate even on under-resolved meshes unsuitable for single-fidelity computations. This method enhances the utility of multi-fidelity surrogates for downstream tasks like uncertainty quantification, optimization, and control. <br /><br /> <div>
arXiv:2507.03691v1 Announce Type: cross 
Abstract: We address the challenge of constructing noise-robust surrogate models for quantities of interest (QoIs) arising from parametric partial differential equations (PDEs), using multi-fidelity collocation techniques; specifically, the Multi-Index Stochastic Collocation (MISC). In practical scenarios, the PDE evaluations used to build a response surface are often corrupted by numerical noise, especially for the low-fidelity models. This noise, which may originate from loose solver tolerances, coarse discretisations, or transient effects, can lead to overfitting in MISC, degrading surrogate quality through nonphysical oscillations and loss of convergence, thereby limiting its utility in downstream tasks like uncertainty quantification, optimisation, and control. To correct this behaviour, we propose an improved version of MISC that can automatically detect the presence of solver noise during the surrogate model construction and then ignore the exhausted fidelities. Our approach monitors the spectral decay of the surrogate at each iteration, identifying stagnation in the coefficient spectrum that signals the onset of noise. Once detected, the algorithm selectively halts the use of noisy fidelities, focusing computational resources on those fidelities that still provide meaningful information. The effectiveness of this approach is numerically validated on two challenging test cases: a parabolic advection--diffusion PDE with uncertain coefficients, and a parametric turbulent incompressible Navier--Stokes problem. The results showcase the accuracy and robustness of the resulting multi-fidelity surrogate and its capability to extract relevant information, even from under-resolved meshes not suitable for reliable single-fidelity computations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital Wills</title>
<link>https://arxiv.org/abs/2507.03694</link>
<guid>https://arxiv.org/abs/2507.03694</guid>
<content:encoded><![CDATA[
<div> Decentralized protocol, digital estate planning, cryptography, distributed computing, blockchain technology <br />
<br />
Summary: This work introduces a decentralized protocol for digital estate planning that leverages advanced distributed computing and cryptography. The protocol is implemented as a layer-1 solution using modern interchain communication to bridge different types of blockchains. It incorporates modern cryptographic primitives to support various forms of claims for information validation, ensuring enhanced privacy for digital inheritance. The protocol features heterogeneous smart contracts on multiple chains, facilitating secure asset distribution according to the decedent's wishes without the need to transfer funds. A user interaction model with a check-in system and account abstraction process improves flexibility and security. By utilizing a permissionless blockchain secured by validators and interchain relayers, the protocol revolutionizes digital estate planning, demonstrating the potential of blockchain technology in legal and personal spheres. The incorporation of a cryptoeconomic network enables unique incentive-compatible economic mechanisms in inheritance planning. <br /><br /> <div>
arXiv:2507.03694v1 Announce Type: cross 
Abstract: This work presents a novel decentralized protocol for digital estate planning that integrates advances distributed computing, and cryptography. The original proof-of-concept was constructed using purely solidity contracts. Since then, we have enhanced the implementation into a layer-1 protocol that uses modern interchain communication to connect several heterogeneous chain types. A key contribution of this research is the implementation of several modern cryptographic primitives to support various forms of claims for information validation. These primitives introduce an unmatched level of privacy to the process of digital inheritance. We also demonstrate on a set of heterogeneous smart contracts, following the same spec, on each chain to serve as entry points, gateways, or bridge contracts that are invoked via a path from the will module on our protocol, to the contract. This ensures a fair and secure distribution of digital assets in accordance with the wishes of the decedent without the requirement of moving their funds. This research further extends its innovations with a user interaction model, featuring a check-in system and account abstraction process, which enhances flexibility and user-friendliness without compromising on security. By developing a dedicated permissionless blockchain that is secured by a network of validators, and interchain relayers, the proposed protocol signifies a transformation in the digital estate planning industry and illustrates the potential of blockchain technology in revolutionizing traditional legal and personal spheres. Implementing a cryptoeconomic network at the core of inheritance planning allows for unique incentive compatible economic mechanisms to be constructed.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM</title>
<link>https://arxiv.org/abs/2507.03868</link>
<guid>https://arxiv.org/abs/2507.03868</guid>
<content:encoded><![CDATA[
<div> Query-style prototypes, Prompt Bank, Mixture-of-Expert Low-Rank Adaptation, Uni-Retrieval, Uni-RAG <br />
Summary: 
The article introduces Uni-Retrieval, a multi-modal retrieval module that dynamically matches query-style prototypes with tokens from a Prompt Bank, enhancing its capability to handle diverse educational scenarios. By incorporating a language model, Uni-Retrieval becomes Uni-RAG, which retrieves educational materials and generates human-readable content aligned with learning objectives. Experimental results indicate Uni-RAG's superior performance in retrieval accuracy and generation quality compared to baseline systems, all while being computationally efficient. The framework offers a scalable and pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to provide personalized, explainable, and efficient learning assistance across STEM scenarios. <br /> <br />Summary: <div>
arXiv:2507.03868v1 Announce Type: cross 
Abstract: In AI-facilitated teaching, leveraging various query styles to interpret abstract educational content is crucial for delivering effective and accessible learning experiences. However, existing retrieval systems predominantly focus on natural text-image matching and lack the capacity to address the diversity and ambiguity inherent in real-world educational scenarios. To address this limitation, we develop a lightweight and efficient multi-modal retrieval module, named Uni-Retrieval, which extracts query-style prototypes and dynamically matches them with tokens from a continually updated Prompt Bank. This Prompt Bank encodes and stores domain-specific knowledge by leveraging a Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to enhance Uni-Retrieval's capability to accommodate unseen query types at test time. To enable natural language educational content generation, we integrate the original Uni-Retrieval with a compact instruction-tuned language model, forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given a style-conditioned query, Uni-RAG first retrieves relevant educational materials and then generates human-readable explanations, feedback, or instructional content aligned with the learning objective. Experimental results on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline retrieval and RAG systems in both retrieval accuracy and generation quality, while maintaining low computational cost. Our framework provides a scalable, pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to support personalized, explainable, and efficient learning assistance across diverse STEM scenarios.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency through Evolution, A Darwinian Approach to Agent-Based Economic Forecast Modeling</title>
<link>https://arxiv.org/abs/2507.04074</link>
<guid>https://arxiv.org/abs/2507.04074</guid>
<content:encoded><![CDATA[
<div> methodology, macroeconomic forecasting, agent-based modeling, evolutionary principles, computational efficiency

Summary:
- The paper introduces a novel Darwinian Agent-Based Modeling (ABM) methodology for macroeconomic forecasting.
- It leverages evolutionary principles to achieve computational efficiency and emergent realism.
- The approach uses simple "common sense" rules representative of small firms serving final consumers.
- The methodology treats households as primary drivers of economic dynamics, with firms adapting through market-based natural selection.
- The model, constrained by Input-Output table structures, generates realistic economic patterns without extensive parameter calibration.
- Using FIGARO Input-Output tables for 46 countries, the model reproduces empirical regularities for Austria with minimal country-specific parameter calibration.
- Key findings include realistic firm and employment distributions, reproduction of initial Social Accounting Matrix values, successful calibration with a few parameters, and computational efficiency on standard laptops.
- Evolutionary ABM approaches offer robust policy insights by capturing decentralized market adaptations while avoiding the complexity of traditional DSGE and comprehensive ABM models.

<br /><br />Summary: <div>
arXiv:2507.04074v1 Announce Type: cross 
Abstract: This paper presents a novel Darwinian Agent-Based Modeling (ABM) methodology formacroeconomic forecasting that leverages evolutionary principles to achieve remarkablecomputational efficiency and emergent realism. Unlike conventional DSGE and ABM approachesthat rely on complex behavioral rules derived from large firm analysis, our framework employssimple "common sense" rules representative of small firms directly serving final consumers. Themethodology treats households as the primary drivers of economic dynamics, with firms adaptingthrough market-based natural selection within limited interaction neighborhoods. We demonstrate that this approach, when constrained by Input-Output table structures,generates realistic economic patterns including wealth distributions, firm size distributions, andsectoral employment patterns without extensive parameter calibration. Using FIGARO Input-Output tables for 46 countries and focusing on Austria as a case study, we show that the modelreproduces empirical regularities while maintaining computational efficiency on standard laptopsrather than requiring supercomputing clusters. Key findings include: (1) emergence of realistic firm and employment distributions fromminimal behavioral assumptions, (2) accurate reproduction of the initial Social Accounting Matrixvalues through evolutionary dynamics, (3) successful calibration using only 5-6 country-specificparameters to complement the FIGARO data, and (4) computational performance enabling fullsimulations on consumer hardware. These results suggest that evolutionary ABM approaches canprovide robust policy insights by capturing decentralized market adaptations while avoiding thecomputational complexity of traditional DSGE and comprehensive ABM models.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems</title>
<link>https://arxiv.org/abs/2507.04996</link>
<guid>https://arxiv.org/abs/2507.04996</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomy, autonomous vehicles, agentic AI, human-machine interaction, mobility ecosystems

Summary: 
This paper introduces the concept of agentic vehicles (AgVs) as vehicles integrated with agentic AI to interact and reason in complex environments. It highlights the gap between technical autonomy and the cognitive and social capabilities required for future mobility systems. The paper presents a framework to characterize AgVs, focusing on their cognitive and communicative layers, differentiating them from conventional autonomous vehicles (AuVs). It discusses the integration of agentic AI, robotics, and human-machine interaction in AgVs, emphasizing their role as interactive agents within mobility ecosystems. Key challenges in developing and governing AgVs are identified, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.<br /><br />Summary: <div>
arXiv:2507.04996v1 Announce Type: cross 
Abstract: Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are defined as systems capable of perceiving their environment and executing preprogrammed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 1 to 6), such as interaction with humans and machines, goal adaptation, contextual reasoning, external tool use, and long-term planning, particularly with the integration of large language models (LLMs) and agentic AI systems. These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this, we introduce the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and interact within complex environments. This paper presents a systems-level framework to characterize AgVs, focusing on their cognitive and communicative layers and differentiating them from conventional AuVs. It synthesizes relevant advances in agentic AI, robotics, multi-agent systems, and human-machine interaction, and highlights how agentic AI, through high-level reasoning and tool use, can function not merely as computational tools but as interactive agents embedded in mobility ecosystems. The paper concludes by identifying key challenges in the development and governance of AgVs, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Tweet Posting Behavior on Citizen Security: A Hawkes Point Process Analysis</title>
<link>https://arxiv.org/abs/2402.03378</link>
<guid>https://arxiv.org/abs/2402.03378</guid>
<content:encoded><![CDATA[
<div> Keywords: Perception of Security, social network data, predictive insights, external factors, proactive security planning

Summary: 
This article presents a novel approach to measuring the Perception of Security (PoS) using social network data to provide real-time monitoring and predictive insights. By analyzing social network content related to security perceptions, the model incorporates external factors that influence the publication and reposting of such content. The results show that the proposed model achieves competitive predictive performance and offers a high level of interpretability regarding the factors influencing security perceptions. The research highlights the importance of understanding temporal patterns and external factors in anticipating security perceptions, providing valuable insights for proactive security planning. <div>
arXiv:2402.03378v2 Announce Type: replace-cross 
Abstract: The Perception of Security (PoS) refers to people's opinions about security or insecurity in a place or situation. While surveys have traditionally been the primary means to capture such perceptions, they need to be improved in their ability to offer real-time monitoring or predictive insights into future security perceptions. Recent evidence suggests that social network content can provide complementary insights into quantifying these perceptions. However, the challenge of accurately predicting these perceptions, with the capacity to anticipate them, still needs to be explored. This article introduces an innovative approach to PoS within short time frames using social network data. Our model incorporates external factors that influence the publication and reposting of content related to security perceptions. Our results demonstrate that this proposed model achieves competitive predictive performance and maintains a high degree of interpretability regarding the factors influencing security perceptions. This research contributes to understanding how temporal patterns and external factors impact the anticipation of security perceptions, providing valuable insights for proactive security planning.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative and parametric insurance on the Ethereum blockchain</title>
<link>https://arxiv.org/abs/2412.05321</link>
<guid>https://arxiv.org/abs/2412.05321</guid>
<content:encoded><![CDATA[
<div> Blockchain, insurance, parametric, collaborative, smart contract
Summary:
This paper presents a novel blockchain-based insurance scheme that combines parametric and collaborative elements. It involves a group of investors, called surplus providers, who lock funds in a smart contract to allow blockchain users to underwrite parametric insurance contracts. These contracts automatically trigger compensation upon meeting specific conditions. The collaborative aspect is reflected in the generation of tokens distributed to surplus providers, representing their share of surplus and granting voting rights for management decisions. The smart contract is coded in Solidity for the Ethereum blockchain and deployed on the Sepolia testnet. Data processing and analysis are done using Python. Open-source code is provided for transparency, and key research challenges are outlined for potential improvements. Overall, this paper sets the foundation for future research and development in blockchain-based insurance mechanisms.'<br /><br />Summary: <div>
arXiv:2412.05321v2 Announce Type: replace-cross 
Abstract: This paper introduces a blockchain-based insurance scheme that integrates parametric and collaborative elements. A pool of investors, referred to as surplus providers, locks funds in a smart contract, enabling blockchain users to underwrite parametric insurance contracts. These contracts automatically trigger compensation when predefined conditions are met. The collaborative aspect is embodied in the generation of tokens, which are distributed to surplus providers. These tokens represent each participant's share of the surplus and grant voting rights for management decisions. The smart contract is developed in Solidity, a high-level programming language for the Ethereum blockchain, and deployed on the Sepolia testnet, with data processing and analysis conducted using Python. In addition, open-source code is provided and main research challenges are identified, so that further research can be carried out to overcome limitations of this first proof of concept.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener</title>
<link>https://arxiv.org/abs/2507.02005</link>
<guid>https://arxiv.org/abs/2507.02005</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Explainable AI, Fatigue Strength, Welded Steel Structures, Feature Engineering<br />
Summary:<br />
This research presents a unified approach that combines Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. By incorporating expert-driven feature engineering with algorithmic feature creation, the accuracy and explainability of the models are enhanced. The study utilizes regression models such as gradient boosting, random forests, and neural networks trained using AutoML under three feature schemes. Ensemble methods, including CatBoost and LightGBM, demonstrated top performance. The domain-informed model achieved the best balance of test RMSE and $R^2 values. XAI methods identified key predictors such as stress ratio, stress range, yield strength, and post-weld treatment. Secondary geometric factors also significantly influenced fatigue life. The integration of AutoML with XAI leads to accurate, interpretable, and robust fatigue strength models for welded steel structures, facilitating AI-assisted design and assessment. Future research will focus on probabilistic fatigue life modeling and integration into digital twin environments. <br />Summary: <div>
arXiv:2507.02005v1 Announce Type: new 
Abstract: This research introduces a unified approach combining Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. It integrates expert-driven feature engineering with algorithmic feature creation to enhance accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient boosting, random forests, and neural networks - were trained using AutoML under three feature schemes: domain-informed, algorithmic, and combined. This allowed a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE $\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta \sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527% within the engineering-relevant 0 - 150 MPa domain. The denser-feature model ($\mathcal M_3$) showed minor gains during training but poorer generalization, while the simpler base-feature model ($\mathcal M_1$) performed comparably, confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG dressing vs. as-welded) as dominant predictors. Secondary geometric factors - plate width, throat thickness, stiffener height - also significantly affected fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate, interpretable, and robust fatigue strength models for welded steel structures. It bridges data-driven modeling with engineering validation, enabling AI-assisted design and assessment. Future work will explore probabilistic fatigue life modeling and integration into digital twin environments.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Resolution Independent Operator Learning</title>
<link>https://arxiv.org/abs/2507.02524</link>
<guid>https://arxiv.org/abs/2507.02524</guid>
<content:encoded><![CDATA[
<div> Keywords: time-dependent partial differential equations, recurrent DeepONet, Neural Controlled Differential Equation, transient mechanics, operator learning

Summary:
Accurately learning solution operators for time-dependent partial differential equations (PDEs) from sparse and irregular data is a challenging task. Existing methods like recurrent DeepONet extensions and neural-ODE surrogates have limitations in handling discrete-time and new inputs after initialization. To address these issues, NCDE-DeepONet is introduced, which combines a Neural Controlled Differential Equation (NCDE) with explicit space-time coordinates to create a continuous-time operator network. The NCDE encodes the entire load history as a solution of a controlled ODE driven by a spline-interpolated input path, allowing for input-resolution-independent representation. The trunk of the network can probe this latent path at arbitrary spatial locations and times, enabling output-resolution independence. Benchmarks on various transient mechanics problems demonstrate the robustness and accuracy of the framework, showcasing almost instant solution prediction. The approach of using controlled dynamics proves to be a principled and efficient method for high-fidelity operator learning in transient mechanics. 

<br /><br />Summary: <div>
arXiv:2507.02524v1 Announce Type: new 
Abstract: Accurately learning solution operators for time-dependent partial differential equations (PDEs) from sparse and irregular data remains a challenging task. Recurrent DeepONet extensions inherit the discrete-time limitations of sequence-to-sequence (seq2seq) RNN architectures, while neural-ODE surrogates cannot incorporate new inputs after initialization. We introduce NCDE-DeepONet, a continuous-time operator network that embeds a Neural Controlled Differential Equation (NCDE) in the branch and augments the trunk with explicit space-time coordinates. The NCDE encodes an entire load history as the solution of a controlled ODE driven by a spline-interpolated input path, making the representation input-resolution-independent: it encodes different input signal discretizations of the observed samples. The trunk then probes this latent path at arbitrary spatial locations and times, rendering the overall map output-resolution independent: predictions can be queried on meshes and time steps unseen during training without retraining or interpolation. Benchmarks on transient Poisson, elastodynamic, and thermoelastic problems confirm the robustness and accuracy of the framework, achieving almost instant solution prediction. These findings suggest that controlled dynamics provide a principled and efficient foundation for high-fidelity operator learning in transient mechanics.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation and Heterogeneity Shape the Resilience of Community Currency Networks</title>
<link>https://arxiv.org/abs/2507.02678</link>
<guid>https://arxiv.org/abs/2507.02678</guid>
<content:encoded><![CDATA[
<div> Keywords: community currency, mutual credit systems, graph theory, behavioral connectivity, network evolution

Summary:
This paper examines community currency networks, focusing on the case study of Sardex in Sardinia, Italy. The analysis is done through a graph theoretic framework, studying strongly connected components, condensed representations, and behavioral connectivity patterns. The evolution of the network over three years is analyzed, revealing temporal contraction, flow imbalances, and structural fragmentation based on user types. The findings show deviations from degree-based models, indicating behavioral imitation among users and a preference for more active peers. The impact of heterogeneous connections between user types is also evaluated, highlighting their role in strengthening the network topology and enhancing resilience.<br /><br />Summary: <div>
arXiv:2507.02678v1 Announce Type: new 
Abstract: Community currency networks are made up of individuals and or companies that share some physical or social characteristics and engage in economic transactions using a virtual currency. This paper investigates the structural and dynamic properties of such mutual credit systems through a case study of Sardex, a community currency initiated and mainly operating in Sardinia, Italy. The transaction network is modeled as a directed weighted graph and analyzed through a graph theoretic framework focused on the analysis of strongly connected components, condensed representations, and behavioral connectivity patterns. Emphasis is placed on understanding the evolution of the network's core and peripheral structures over a three year period, with attention to temporal contraction, flow asymmetries, and structural fragmentation depending on different user types. Our findings reveal persistent deviations from degree based null models and suggest the presence of behavioral imitation, specifically, a user preference for more active peers. We further assess the impact of heterogeneous connections between different type of users, which strengthen the network topology and enhance its resilience.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery</title>
<link>https://arxiv.org/abs/2507.02730</link>
<guid>https://arxiv.org/abs/2507.02730</guid>
<content:encoded><![CDATA[
<div> Physics-Informed Automated Discovery of Kinetics, catalytic processes, kinetic models, symbolic regression, Metropolis-Hastings algorithm <br />
<br />
Summary: The article introduces the Physics-Informed Automated Discovery of Kinetics (PI-ADoK) framework for industrial catalytic processes. Traditional mechanistic models for kinetics demand expertise, while data-driven approaches lack interpretability. PI-ADoK integrates physical constraints into symbolic regression to reduce the search space and experiments needed for model convergence. It includes a Metropolis-Hastings algorithm for uncertainty quantification, providing credible prediction intervals. Benchmarking against conventional methods in catalytic case studies shows PI-ADoK enhances model fidelity and decreases the experimental burden, offering efficient and reliable kinetic model discovery for chemical reaction engineering. <div>
arXiv:2507.02730v1 Announce Type: new 
Abstract: The industrialization of catalytic processes hinges on the availability of reliable kinetic models for design, optimization, and control. Traditional mechanistic models demand extensive domain expertise, while many data-driven approaches often lack interpretability and fail to enforce physical consistency. To overcome these limitations, we propose the Physics-Informed Automated Discovery of Kinetics (PI-ADoK) framework. By integrating physical constraints directly into a symbolic regression approach, PI-ADoK narrows the search space and substantially reduces the number of experiments required for model convergence. Additionally, the framework incorporates a robust uncertainty quantification strategy via the Metropolis-Hastings algorithm, which propagates parameter uncertainty to yield credible prediction intervals. Benchmarking our method against conventional approaches across several catalytic case studies demonstrates that PI-ADoK not only enhances model fidelity but also lowers the experimental burden, highlighting its potential for efficient and reliable kinetic model discovery in chemical reaction engineering.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification</title>
<link>https://arxiv.org/abs/2507.01971</link>
<guid>https://arxiv.org/abs/2507.01971</guid>
<content:encoded><![CDATA[
<div> Keywords: support and resistance levels, deep learning, financial analysis, market microstructure, attention-based architecture

Summary:
DeepSupp is a new deep learning approach designed to detect financial support levels using multi-head attention mechanisms and advanced feature engineering. It leverages dynamic correlation matrices to capture evolving market relationships and uses an attention-based autoencoder for robust representation learning. The final support levels are identified through unsupervised clustering with DBSCAN, resulting in state-of-the-art performance across six financial metrics on S&amp;P 500 tickers. DeepSupp outperforms six baseline methods in essential support accuracy and market regime sensitivity, demonstrating consistent results across diverse market conditions. This approach fills critical gaps in support and resistance level detection, providing a scalable and reliable solution for modern financial analysis. The use of attention-based architectures in DeepSupp uncovers nuanced market patterns and enhances technical trading strategies. <br /><br />Summary: <div>
arXiv:2507.01971v1 Announce Type: cross 
Abstract: Support and resistance (SR) levels are central to technical analysis, guiding traders in entry, exit, and risk management. Despite widespread use, traditional SR identification methods often fail to adapt to the complexities of modern, volatile markets. Recent research has introduced machine learning techniques to address the following challenges, yet most focus on price prediction rather than structural level identification. This paper presents DeepSupp, a new deep learning approach for detecting financial support levels using multi-head attention mechanisms to analyze spatial correlations and market microstructure relationships. DeepSupp integrates advanced feature engineering, constructing dynamic correlation matrices that capture evolving market relationships, and employs an attention-based autoencoder for robust representation learning. The final support levels are extracted through unsupervised clustering, leveraging DBSCAN to identify significant price thresholds. Comprehensive evaluations on S&amp;P 500 tickers demonstrate that DeepSupp outperforms six baseline methods, achieving state-of-the-art performance across six financial metrics, including essential support accuracy and market regime sensitivity. With consistent results across diverse market conditions, DeepSupp addresses critical gaps in SR level detection, offering a scalable and reliable solution for modern financial analysis. Our approach highlights the potential of attention-based architectures to uncover nuanced market patterns and improve technical trading strategies.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design of Corrugated Boards: A New FEM Modeling and Experimental Validation</title>
<link>https://arxiv.org/abs/2507.02189</link>
<guid>https://arxiv.org/abs/2507.02189</guid>
<content:encoded><![CDATA[
<div> FEM modeling, corrugated boards, homogenization method, Weibull distributions, packaging design<br />
Summary:<br />
The study presents a simplified Finite Element Method (FEM) modeling approach for large structures made of corrugated boards, focusing on customized packages. It utilizes a homogenization method to transform flute geometries into equivalent elastic models, reducing computational time. Correction factors are introduced for internal mechanisms, adjusting the effective elastic modulus and thickness in the presence of large deformations and contact. Two statistical Weibull distributions representing contact and buckling mechanisms in corrugated boards are derived experimentally and validated for computational efficiency. The statistical parameters obtained ($\beta_1 = 0.14$, $\beta_2 = 1.31) can be effectively used for simplistic representation. The research contributes to optimizing corrugated packaging design by simplifying FEM models for faster yet accurate simulations. <br /><br />Summary: <div>
arXiv:2507.02189v1 Announce Type: cross 
Abstract: This study presents a simplified FEM modeling approach suitable for large structures made of corrugated boards, such as customized packages, based on a homogenization method, which is combined with correction factors for internal mechanisms. The homogenization process reduces computational time by transforming flute geometries into equivalent elastic models. In large deformations and in the presence of contact for a given geometry, the effective elastic modulus in the thickness direction, as well as the effective thickness of the structure, are corrected by two statistical Weibull distributions representing the contact and buckling mechanisms in a corrugated board. The Weibull parameters are obtained via experimental analysis, and such a process is then validated. The results demonstrate that the statistical parameters ($\beta_1 = 0.14$, $\beta_2 = 1.31$) can be used for the simplistic representation of corrugated boards, being computationally efficient. This research contributes to the optimization of corrugated packaging design, specifically by simplifying FEM models for faster yet equally accurate simulations.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Scale Finite Element Method for Investigating Fiber Remodeling in Hypertrophic Cardiomyopathy</title>
<link>https://arxiv.org/abs/2507.02193</link>
<guid>https://arxiv.org/abs/2507.02193</guid>
<content:encoded><![CDATA[
<div> fiber disarray, hypertrophic cardiomyopathy, cellular abnormalities, cardiac pumping function, myocardium mechanics <br />
Summary: <br />
- Fiber disarray is a significant hallmark of hypertrophic cardiomyopathy (HCM) and is associated with various cardiac events such as heart failure.
- Heterogeneous distributions of hypercontractility, hypocontractility, and fibrosis contribute to the development of fiber disarray in the myocardium.
- The pattern of fiber disarray varies depending on the specific perturbation, providing insights into the progression of HCM.
- Higher fiber disarray near the epicardium compared to the endocardium suggests the role of regional myocardial mechanics in HCM development.
- Remodeled left ventricles (LVs) with fibrosis and hypocontractility exhibit declined cardiac performance, highlighting the structural and functional consequences of HCM. <br /> <div>
arXiv:2507.02193v1 Announce Type: cross 
Abstract: A significant hallmark of hypertrophic cardiomyopathy (HCM) is fiber disarray, which is associated with various cardiac events such as heart failure. Quantifying fiber disarray remains critical for understanding the disease s complex pathophysiology. This study investigates the role of heterogeneous HCM-induced cellular abnormalities in the development of fiber disarray and their subsequent impact on cardiac pumping function. Fiber disarray is predicted using a stress-based law to reorient myofibers and collagen within a multiscale finite element cardiac modeling framework, MyoFE. Specifically, the model is used to quantify the distinct impacts of heterogeneous distributions of hypercontractility, hypocontractility, and fibrosis on fiber disarray development and examines their effect on functional characteristics of the heart. Our results show that heterogenous cell level abnormalities highly disrupt the normal mechanics of myocardium and lead to significant fiber disarray. The pattern of disarray varies depending on the specific perturbation, offering valuable insights into the progression of HCM. Despite the random distribution of perturbed regions within the cardiac muscle, significantly higher fiber disarray is observed near the epicardium compared to the endocardium across all perturbed left ventricle (LV) models. This regional difference in fiber disarray, irrespective of perturbation severity, aligns with previous DT-MRI studies, highlighting the role of regional myocardial mechanics in the development of fiber disarray. Furthermore, cardiac performance declined in the remodeled LVs, particularly in those with fibrosis and hypocontractility. These findings provide important insights into the structural and functional consequences of HCM and offer a framework for future investigations into therapeutic interventions targeting cardiac remodeling.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Effective Elastic Modulus and Thickness of Corrugated Boards Using Gaussian Process Regression and Expected Hypervolume Improvement</title>
<link>https://arxiv.org/abs/2507.02208</link>
<guid>https://arxiv.org/abs/2507.02208</guid>
<content:encoded><![CDATA[
<div> Keywords: hypersurface modeling, effective elastic modulus, thickness, corrugated boards, Gaussian Process Regression 

Summary: 
Latin Hypercube Sampling (LHS) combined with Gaussian Process Regression (GP) and Enhanced Expected Hypervolume Improvement (EHVI) were used to model the hypersurface of the effective elastic modulus and thickness in corrugated boards. Accurate modeling of these properties is crucial for optimizing the mechanical properties of corrugated materials. LHS efficiently samples the input space, while GP adapts to the complexity of response surfaces by incorporating prediction and uncertainty. Points are generated and evaluated based on the complexity of hypersurfaces, with emphasis on points with higher variance. The performance evaluation showed that GP with EHVI had improved accuracy, indicated by lower Mean Squared Error (MSE) values for the effective elastic modulus and thickness predictions. This approach demonstrates potential for future applications in structural optimization. 

Summary: <div>
arXiv:2507.02208v1 Announce Type: cross 
Abstract: This work aims to model the hypersurface of the effective elastic modulus, \( E_{z, \text{eff}} \), and thickness, \( th_{\text{eff}} \), in corrugated boards. A Latin Hypercube Sampling (LHS) is followed by Gaussian Process Regression (GP), enhanced by EHVI as a multi-objective acquisition function. Accurate modeling of \( E_{z, \text{eff}} \) and \( th_{\text{eff}} \) is critical for optimizing the mechanical properties of corrugated materials in engineering applications. LHS provides an efficient and straightforward approach for an initial sampling of the input space; GP is expected to be able to adapt to the complexity of the response surfaces by incorporating both prediction and uncertainty. Therefore, the next points being generated and evaluated are based on the complexity of the hypersurfaces, and some points, especially those with higher variance, are more exploited and carry more importance. The performance of GP with EHVI is measured by Mean Squared Error (MSE). Prediction of GP resulted in \( \text{MSE}(E_{z, \text{eff}}) = 5.24 \, \text{kPa}^2 \) and \( \text{MSE}(th_{\text{eff}}) = 1 \, \text{mm}^2 \). GP possesses then improved accuracy and adaptability for future applications in structural optimization.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in Particle Flow Filters with Taylor Expansion Series</title>
<link>https://arxiv.org/abs/2505.01597</link>
<guid>https://arxiv.org/abs/2505.01597</guid>
<content:encoded><![CDATA[
<div> Particle Flow Filters, Measurement Update, Drift Term, Diffusion Term, High-Order Polynomial Expansions<br />
<br />
Summary:<br />
Particle Flow Filters update measurements by moving particles rather than adjusting weights based on likelihood. This study introduces a new derivation method using high-order polynomial expansions, improving upon linearization techniques. The technique utilizes differential algebra to derive high-order particle flows directly onto polynomial representations of distributions. Two new particle flow filters are proposed, differing in the selection of the expansion center for Taylor polynomial evaluations. Numerical experiments demonstrate enhanced performance, particularly compared to Gromov flow and "exact" flow. <div>
arXiv:2505.01597v2 Announce Type: replace 
Abstract: Particle Flow Filters perform the measurement update by moving particles to a different location rather than modifying the particles' weight based on the likelihood. Their movement (flow) is dictated by a drift term, which continuously pushes the particle toward the posterior distribution, and a diffusion term, which guarantees the spread of particles. This work presents a novel derivation of these terms based on high-order polynomial expansions, where the common techniques based on linearization reduce to a simpler version of the new methodology. Thanks to differential algebra, the high-order particle flow is derived directly onto the polynomials representation of the distribution, embedded with differentiation and evaluation. The resulting technique proposes two new particle flow filters, whose difference relies on the selection of the expansion center for the Taylor polynomial evaluation. Numerical applications show the improvement gained by the inclusion of high-order terms, especially when comparing performance with the Gromov flow and the "exact" flow.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPC-AI Coupling Methodology for Scientific Applications</title>
<link>https://arxiv.org/abs/2507.01025</link>
<guid>https://arxiv.org/abs/2507.01025</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, High-performance computing, Coupling, Materials science, Scientific discovery

Summary: 
This study explores the integration of high-performance computing (HPC) and artificial intelligence (AI) in scientific applications, focusing on three coupling patterns: surrogate, directive, and coordinate. Through case studies in materials science, the effectiveness of these patterns is demonstrated, highlighting technical challenges, performance improvements, and implementation details. The proposed coupling patterns offer valuable guidance for future HPC-AI ensembles in various scientific domains, not limited to materials science. The study emphasizes the transformation of numerical-based HPC applications with data-driven AI approaches to address computational intensity challenges. This research provides insight into promising perspectives for HPC-AI coupling and its potential impact on scientific discovery. <div>
arXiv:2507.01025v1 Announce Type: new 
Abstract: Artificial intelligence (AI) technologies have fundamentally transformed numerical-based high-performance computing (HPC) applications with data-driven approaches and endeavored to address existing challenges, e.g. high computational intensity, in various scientific domains. In this study, we explore the scenarios of coupling HPC and AI (HPC-AI) in the context of emerging scientific applications, presenting a novel methodology that incorporates three patterns of coupling: surrogate, directive, and coordinate. Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite, and typical HPC-AI ensembles. Through case studies in materials science, we demonstrate the application and effectiveness of these patterns. The study highlights technical challenges, performance improvements, and implementation details, providing insight into promising perspectives of HPC-AI coupling. The proposed coupling patterns are applicable not only to materials science but also to other scientific domains, offering valuable guidance for future HPC-AI ensembles in scientific discovery.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI in Product Management: A Co-Evolutionary Model</title>
<link>https://arxiv.org/abs/2507.01069</link>
<guid>https://arxiv.org/abs/2507.01069</guid>
<content:encoded><![CDATA[
<div> agentic AI, product management, co-evolutionary framework, systems theory, human-AI interaction theory

Summary: 
This study delves into the transformative impact of agentic AI on product management, proposing a conceptual framework for its integration throughout the product lifecycle. Agentic AI, known for its autonomy and goal-driven behavior, reshapes the role of product managers (PMs) as orchestrators within socio-technical ecosystems. The framework, drawing on systems theory, co-evolutionary theory, and human-AI interaction theory, outlines the capabilities of agentic AI in various stages of product development. Through an integrative review of 70+ sources, the study showcases the evolving roles of PMs in overseeing AI operations, aligning strategies, and ensuring effective integration. A key finding emphasizes the need for PMs to enhance their skills in AI literacy, governance, and systems thinking to facilitate mutual adaptation between PMs and AI. This study lays the groundwork for future research and practical implementation to promote responsible and efficient integration of agentic AI in software organizations. 

<br /><br />Summary: <div>
arXiv:2507.01069v1 Announce Type: new 
Abstract: This study explores agentic AI's transformative role in product management, proposing a conceptual co-evolutionary framework to guide its integration across the product lifecycle. Agentic AI, characterized by autonomy, goal-driven behavior, and multi-agent collaboration, redefines product managers (PMs) as orchestrators of socio-technical ecosystems. Using systems theory, co-evolutionary theory, and human-AI interaction theory, the framework maps agentic AI capabilities in discovery, scoping, business case development, development, testing, and launch. An integrative review of 70+ sources, including case studies from leading tech firms, highlights PMs' evolving roles in AI orchestration, supervision, and strategic alignment. Findings emphasize mutual adaptation between PMs and AI, requiring skills in AI literacy, governance, and systems thinking. Addressing gaps in traditional frameworks, this study provides a foundation for future research and practical implementation to ensure responsible, effective agentic AI integration in software organizations.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatially Distributed Wettability Characterization in Porous Media</title>
<link>https://arxiv.org/abs/2507.01617</link>
<guid>https://arxiv.org/abs/2507.01617</guid>
<content:encoded><![CDATA[
<div> contact angle measurement, micro-CT images, wettability heterogeneity, spatially distributed, open-source tools<br />
Summary:
An enhanced geometric algorithm for automated contact angle measurement from micro-CT images is introduced, offering improved accuracy through robust interface extrapolation. The method generates contact angle maps revealing wettability heterogeneity in mixed-wet systems. Analysis shows that averaged metrics can mask significant variability within samples; a seemingly uniformly weakly water-wet sample may exhibit substantial intermediate-wetting regions. This variation impacts pore-filling mechanisms and interface structure. The study's open-source tools enable precise spatial wettability characterization, enhancing predictions of multiphase flow behavior in porous materials. Such insights are crucial for optimizing subsurface energy processes. <div>
arXiv:2507.01617v1 Announce Type: new 
Abstract: An enhanced geometric algorithm for automated pore-by-pore contact angle measurement from micro-CT images, is presented that achieves superior accuracy compared to existing methods through robust fluid-fluid and solid-fluid interface extrapolation. Using this high resolution data, we generate spatially distributed contact angle maps that reveal previously hidden wettability heterogeneity. Our analysis of mixed-wet systems demonstrates the severe limitations of averaged metrics: a sample with a mean contact angle of 64.7 degrees, conventionally classified as uniformly weakly water-wet, exhibits 40% of its pore space in the intermediate-wetting regime (70-110 degrees). This heterogeneity explains the presence of minimal surface interfaces and fundamentally different pore-filling mechanisms operating within the same sample. By providing open-source tools for spatially-resolved wettability characterization, this work enables more accurate predictions of multiphase flow behavior in heterogeneous porous materials, essential for optimizing subsurface energy storage and recovery processes.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified Levenberg-Marquardt method for estimating the elastic material parameters of polymer waveguides using residuals between autocorrelated frequency responses</title>
<link>https://arxiv.org/abs/2507.01706</link>
<guid>https://arxiv.org/abs/2507.01706</guid>
<content:encoded><![CDATA[
<div> optimization, ultrasound, polymers, elasticity, material parameters
Summary:
This article addresses the estimation of frequency-dependent elastic parameters of polymers in the ultrasound range as an inverse problem. The approach involves fitting simulation signals to measurement signals of displacement responses in hollow cylindrical waveguides for efficiency. Two novel methods are proposed to accelerate optimization: an adapted Levenberg-Marquardt method and an improved objective function based on autocorrelated envelopes of signals. Realistic material parameter ranges are considered for reproducibility. The study focuses on isotropic materials, showing that the proposed methods reduce the total number of model evaluations, speeding up parameter identification. <div>
arXiv:2507.01706v1 Announce Type: new 
Abstract: In this contribution, we address the estimation of the frequency-dependent elastic parameters of polymers in the ultrasound range, which is formulated as an inverse problem. This inverse problem is implemented as a nonlinear regression-type optimization problem, in which the simulation signals are fitted to the measurement signals. These signals consist of displacement responses in waveguides, focusing on hollow cylindrical geometries to enhance the simulation efficiency. To accelerate the optimization and reduce the number of model evaluations and wait times, we propose two novel methods. First, we introduce an adaptation of the Levenberg-Marquardt method derived from a geometrical interpretation of the least-squares optimization problem. Second, we introduce an improved objective function based on the autocorrelated envelopes of the measurement and simulation signals. Given that this study primarily relies on simulation data to quantify optimization convergence, we aggregate the expected ranges of realistic material parameters and derive their distributions to ensure the reproducibility of optimizations with proper measurements. We demonstrate the effectiveness of our objective function modification and step adaptation for various materials with isotropic material symmetry by comparing them with a state-of-the-art optimization method. In all cases, our method reduces the total number of model evaluations, thereby shortening the time to identify the material parameters.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relevance of the Basset history term for Lagrangian particle dynamics</title>
<link>https://arxiv.org/abs/2407.01041</link>
<guid>https://arxiv.org/abs/2407.01041</guid>
<content:encoded><![CDATA[
<div> Keywords: Maxey-Riley equation, fluid dynamics, Lagrangian dynamics, clustering patterns, turbulent flow <br />
Summary: <br />
The study focuses on the impact of the integral "history term" in the Maxey-Riley equation (MRE) on the movement of finite spherical particles in fluid dynamics. Numerical computations were carried out to compare trajectories with and without the history term for a large number of particles in various flow fields. The findings reveal that neglecting the history term significantly affects clustering patterns, especially for moderate to large Stokes numbers. Additionally, the computation of finite-time Lyapunov exponents demonstrates that even for small particles, disregarding the history term leads to notable differences in the resulting scalar field, particularly in turbulent flows. This highlights the importance of considering the history term in the MRE for accurately predicting the Lagrangian dynamics of particles in fluid systems. <br /> <div>
arXiv:2407.01041v3 Announce Type: replace-cross 
Abstract: The movement of small but finite spherical particles in a fluid can be described by the Maxey-Riley equation (MRE) if they are too large to be considered passive tracers. The MRE contains an integral "history term" modeling wake effects, which causes the force acting on a particle at some given time to depend on its full past trajectory. The history term causes complications in the numerical solution of the MRE and is therefore often neglected, despite both numerical and experimental evidence that its effects are generally not negligible. By numerically computing trajectories with and without the history term of a large number of particles in different flow fields, we investigate its impact on the large-scale Lagrangian dynamics of simulated particles. We show that for moderate to large Stokes numbers, ignoring the history term leads to significant differences in clustering patterns. Furthermore, we compute finite-time Lyapunov exponents and show that, even for small particles, the differences in the resulting scalar field from ignoring the BHT can be significant, in particular if the underlying flow is turbulent.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-design of magnetic soft robots with large deformation and contacts via material point method and topology optimization</title>
<link>https://arxiv.org/abs/2503.22767</link>
<guid>https://arxiv.org/abs/2503.22767</guid>
<content:encoded><![CDATA[
<div> soft robots, magnetic particles, topology optimization, magneto-elastic dynamics, autonomous design

Summary:
The article introduces a topology optimization framework for magnetic soft robots embedded with hard magnetic particles. This framework simultaneously designs structures, material magnetization, and time-varying magnetic stimuli to achieve target behaviors such as shape morphing and locomotion. It integrates generalized topology optimization with the magneto-elastic material point method, allowing for dynamic motion and solid contacts. The framework is computationally efficient, completing all design cases within minutes. It enables the autonomous co-design of active soft materials for various tasks, including metasurfaces, drug delivery, and minimally invasive procedures. <div>
arXiv:2503.22767v2 Announce Type: replace-cross 
Abstract: Magnetic soft robots embedded with hard magnetic particles enable untethered actuation via external magnetic fields, offering remote, rapid, and precise control, which is highly promising for biomedical applications. However, designing such systems is challenging due to the complex interplay of magneto-elastic dynamics, large deformation, solid contacts, time-varying stimuli, and posture-dependent loading. As a result, most existing research relies on heuristics and trial-and-error methods or focuses on the independent design of stimuli or structures under static conditions. We propose a topology optimization framework for magnetic soft robots that simultaneously designs structures, location-specific material magnetization and time-varying magnetic stimuli, accounting for large deformations, dynamic motion, and solid contacts. This is achieved by integrating generalized topology optimization with the magneto-elastic material point method, which supports GPU-accelerated parallel simulations and auto-differentiation for sensitivity analysis. We applied this framework to design magnetic robots for various tasks, including multi-task shape morphing and locomotion, in both 2D and 3D. The method autonomously generates optimized robotic systems to achieve target behaviors without requiring human intervention. Despite the nonlinear physics and large design space, it demonstrates high computational efficiency, completing all cases within minutes. The framework provides a computational foundation for the autonomous co-design of active soft materials in applications such as metasurfaces, drug delivery, and minimally invasive procedures.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Kalman Filter for Data Assimilation coupled with low-resolution computations techniques applied in Fluid Dynamics</title>
<link>https://arxiv.org/abs/2507.00539</link>
<guid>https://arxiv.org/abs/2507.00539</guid>
<content:encoded><![CDATA[
<div> Ensemble Kalman Filter, Reduced-Order Model, Data Assimilation, Fluid Dynamics, Computational Efficiency <br />
<br />
Summary: 
This paper introduces an innovative Reduced-Order Model (ROM) that merges experimental and simulation data using Data Assimilation (DA) to estimate the "True" state of fluid dynamics systems. The methodology incorporates the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework to improve prediction accuracy. To address the computational demands, the ROM employs low-resolution techniques, such as downsampling datasets and utilizing low-cost Singular Value Decomposition (lcSVD) for advanced reconstruction. Results show significant reductions in computation time and RAM usage without sacrificing accuracy. The EnKF is effective in estimating and predicting fluid flow systems based on limited observations and low-fidelity data. The proposed DA method shows promise in improving computational efficiency in CFD and related fields, making it suitable for large-scale and real-time applications like environmental monitoring and aerospace. <div>
arXiv:2507.00539v1 Announce Type: new 
Abstract: This paper presents an innovative Reduced-Order Model (ROM) for merging experimental and simulation data using Data Assimilation (DA) to estimate the "True" state of a fluid dynamics system, leading to more accurate predictions. Our methodology introduces a novel approach implementing the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework, grounded in a robust theoretical foundation and applied to fluid dynamics. To address the substantial computational demands of DA, the proposed ROM employs low-resolution (LR) techniques to drastically reduce computational costs. This approach involves downsampling datasets for DA computations, followed by an advanced reconstruction technique based on low-cost Singular Value Decomposition (lcSVD). The lcSVD method, a key innovation in this paper, has never been applied to DA before and offers a highly efficient way to enhance resolution with minimal computational resources. Our results demonstrate significant reductions in both computation time and RAM usage through the LR techniques without compromising the accuracy of the estimations. For instance, in a turbulent test case, the LR approach with a compression rate of 15.9 can achieve a speed-up of 13.7 and a RAM compression of 90.9% while maintaining a low Relative Root Mean Square Error (RRMSE) of 2.6%, compared to 0.8% in the high-resolution (HR) reference. Furthermore, we highlight the effectiveness of the EnKF in estimating and predicting the state of fluid flow systems based on limited observations and low-fidelity numerical data. This paper highlights the potential of the proposed DA method in fluid dynamics applications, particularly for improving computational efficiency in CFD and related fields. Its ability to balance accuracy with low computational and memory costs makes it suitable for large-scale and real-time applications, such as environmental monitoring or aerospace.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark</title>
<link>https://arxiv.org/abs/2507.00034</link>
<guid>https://arxiv.org/abs/2507.00034</guid>
<content:encoded><![CDATA[
<div> neural network, critical heat flux, non-uniform heating, spatial power profiles, transfer-learning

Summary:
- The study compiles a comprehensive dataset on critical heat flux (CHF) under both uniform and non-uniform axial heating conditions to support Phase II of the OECD/NEA AI/ML CHF benchmark.
- Classical CHF correlations show errors under uniform heating and struggle with non-uniform profiles, while modern tabular methods offer improved but imperfect predictions.
- A neural network trained on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, emphasizing the need for models that consider axial power distributions.
- The curated datasets and modeling results provided in this study set the stage for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark. <br /><br />Summary: <div>
arXiv:2507.00034v1 Announce Type: cross 
Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water reactors, defining safe thermal-hydraulic operating limits. To support Phase II of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power profiles, this work compiles and digitizes a broad CHF dataset covering both uniform and non-uniform axial heating conditions. Heating profiles were extracted from technical reports, interpolated onto a consistent axial mesh, validated via energy-balance checks, and encoded in machine-readable formats for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating and degrade markedly when applied to non-uniform profiles, while modern tabular methods offer improved but still imperfect predictions. A neural network trained solely on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, underscoring the need for models that explicitly incorporate axial power distributions. By providing these curated datasets and baseline modeling results, this study lays the groundwork for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process</title>
<link>https://arxiv.org/abs/2507.00046</link>
<guid>https://arxiv.org/abs/2507.00046</guid>
<content:encoded><![CDATA[
<div> evolutionary computing, image segmentation, Additive Friction Stir Deposition, Particle Swarm Optimization, defect detection

Summary:
Evolutionary computing-based image segmentation using Particle Swarm Optimization (PSO) was proposed for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. The methodology integrates gradient magnitude analysis with distance transforms to create attention-weighted visualizations highlighting critical interface regions. Multiple visualization techniques, including self-attention maps and multi-channel visualization, were applied to five AFSD samples to detect material transitions and potential defects. PSO algorithm determined optimal threshold values for precise segmentation. The multi-channel visualization technique combined boundary information, spatial relationships, and material density data for cohesive representations. Attention-based analysis successfully identified incomplete bonding regions and inhomogeneities in AFSD joints, offering quantitative metrics for process optimization and quality assessment of additively manufactured components. <br /><br />Summary: <div>
arXiv:2507.00046v1 Announce Type: cross 
Abstract: This work proposes an evolutionary computing-based image segmentation approach for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. Particle Swarm Optimization (PSO) was employed to determine optimal segmentation thresholds for detecting defects and features in multilayer AFSD builds. The methodology integrates gradient magnitude analysis with distance transforms to create novel attention-weighted visualizations that highlight critical interface regions. Five AFSD samples processed under different conditions were analyzed using multiple visualization techniques i.e. self-attention maps, and multi-channel visualization. These complementary approaches reveal subtle material transition zones and potential defect regions which were not readily observable through conventional imaging. The PSO algorithm automatically identified optimal threshold values (ranging from 156-173) for each sample, enabling precise segmentation of material interfaces. The multi-channel visualization technique effectively combines boundary information (red channel), spatial relationships (green channel), and material density data (blue channel) into cohesive representations that quantify interface quality. The results demonstrate that attention-based analysis successfully identifies regions of incomplete bonding and inhomogeneities in AFSD joints, providing quantitative metrics for process optimization and quality assessment of additively manufactured components.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A collaborative digital twin built on FAIR data and compute infrastructure</title>
<link>https://arxiv.org/abs/2507.00048</link>
<guid>https://arxiv.org/abs/2507.00048</guid>
<content:encoded><![CDATA[
<div> machine learning, automated experimentation, self-driving laboratories, FAIR data infrastructure, optimization

Summary:
The article discusses the integration of machine learning and automated experimentation in self-driving laboratories (SDL) to accelerate discovery and optimization tasks in science and engineering. By utilizing findable, accessible, interoperable, and reusable (FAIR) data infrastructure, geographically dispersed researchers can collaborate effectively within a distributed SDL implementation on nanoHUB services. The framework allows for the sharing of raw experimental data in a central database, enabling researchers to benefit from analysis tools and machine learning models that update automatically with new data. A separate workflow on nanoHUB facilitates sequential optimization through active learning, where researchers define the optimization objective and machine learning models guide future experiment selection. The article presents the application of these concepts in an optimization task involving food dyes, showing how researchers and students can conduct experiments, share data, and explore the combination of FAIR data, predictive ML models, and sequential optimization in a cost-effective manner. <div>
arXiv:2507.00048v1 Announce Type: cross 
Abstract: The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively. This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management. In this framework, geographically dispersed collaborators conducting independent optimization tasks contribute raw experimental data to a shared central database. These researchers can then benefit from analysis tools and machine learning models that automatically update as additional data become available. New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments. Inspired by the concept of ``frugal twin", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color. With easily accessible and inexpensive materials, researchers and students can set up their own experiments, share data with collaborators, and explore the combination of FAIR data, predictive ML models, and sequential optimization. The tools introduced are generally applicable and can easily be extended to other optimization problems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The gradual transformation of inland countries -- human plowing, horse plowing and equity incentives</title>
<link>https://arxiv.org/abs/2507.00067</link>
<guid>https://arxiv.org/abs/2507.00067</guid>
<content:encoded><![CDATA[
<div> civilization, history, governance, conflict, economic development
<br />Summary:
In the article, the author emphasizes the importance of learning from history to upgrade civilization and improve its strength and survival ability. By studying the long-term stability of countries in conflict, including economic benefits and means of suppression, and utilizing mathematical methods to find optimal solutions, it is suggested that civilizations can enhance their ability to handle conflicts and reduce internal strife. The transition from human plowing to horse plowing is mentioned as a means to suppress resistance and provide resistance ability to the people. The selection of rulers should consider various institutional aspects like exams, elections, and drawing lots. Economic development, following a lognormal distribution, can be adjusted using expected value and variance to address wealth inequality. <div>
arXiv:2507.00067v1 Announce Type: cross 
Abstract: Many modern countries have not learned their lessons and often hope for the wisdom of later generations, resulting in them only possessing modern technology and difficult to iterate ancient civilizations. At present, there is no way to tell how we should learn from history and promote the gradual upgrading of civilization. Therefore, we must tell the history of civilization's progress and the means of governance, learn from experience to improve the comprehensive strength and survival ability of civilization, and achieve an optimal solution for the tempering brought by conflicts and the reduction of internal conflicts. Firstly, we must follow the footsteps of history and explore the reasons for the long-term stability of each country in conflict, including providing economic benefits to the people and means of suppressing them; then, use mathematical methods to demonstrate how we can achieve the optimal solution at the current stage. After analysis, we can conclude that the civilization transformed from human plowing to horse plowing can easily suppress the resistance of the people and provide them with the ability to resist; The selection of rulers should consider multiple institutional aspects, such as exams, elections, and drawing lots; Economic development follows a lognormal distribution and can be adjusted by expected value and variance. Using a lognormal distribution with the maximum value to divide equity can adjust the wealth gap.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPGD: Steepest Perturbed Gradient Descent Optimization</title>
<link>https://arxiv.org/abs/2411.04946</link>
<guid>https://arxiv.org/abs/2411.04946</guid>
<content:encoded><![CDATA[
<div> algorithm, optimization, gradient descent, perturbation sampling, local minima<br />
<br />
Summary:
The paper introduces the Steepest Perturbed Gradient Descent (SPGD) algorithm, which combines gradient descent with periodic uniform perturbation sampling to overcome challenges like local minima, saddle points, and plateaus in optimization problems. SPGD generates candidate solutions and selects the one with the steepest loss difference, incorporating exploration to escape sub-optimal minima. It outperforms four established methods in solving the NP-hard 3D component packing problem and shows improvement in complex non-convex optimization problems. Comparative analyses with 2D benchmark functions demonstrate SPGD's superior performance in navigating intricate optimization landscapes. The algorithm's ability to effectively search for global optima across diverse problem spaces highlights its versatility in addressing various optimization challenges.<br /> 
<br />Summary: <div>
arXiv:2411.04946v2 Announce Type: replace-cross 
Abstract: Optimization algorithms are pivotal in advancing various scientific and industrial fields but often encounter obstacles such as trapping in local minima, saddle points, and plateaus (flat regions), which makes the convergence to reasonable or near-optimal solutions particularly challenging. This paper presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that innovatively combines the principles of the gradient descent method with periodic uniform perturbation sampling to effectively circumvent these impediments and lead to better solutions whenever possible. SPGD is distinctively designed to generate a set of candidate solutions and select the one exhibiting the steepest loss difference relative to the current solution. It enhances the traditional gradient descent approach by integrating a strategic exploration mechanism that significantly increases the likelihood of escaping sub-optimal local minima and navigating complex optimization landscapes effectively. Our approach not only retains the directed efficiency of gradient descent but also leverages the exploratory benefits of stochastic perturbations, thus enabling a more comprehensive search for global optima across diverse problem spaces. We demonstrate the efficacy of SPGD in solving the 3D component packing problem, an NP-hard challenge. Preliminary results show a substantial improvement over four established methods, particularly on response surfaces with complex topographies and in multidimensional non-convex continuous optimization problems. Comparative analyses with established 2D benchmark functions highlight SPGD's superior performance, showcasing its ability to navigate complex optimization landscapes. These results emphasize SPGD's potential as a versatile tool for a wide range of optimization problems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STONet: A neural operator for modeling solute transport in micro-cracked reservoirs</title>
<link>https://arxiv.org/abs/2412.05576</link>
<guid>https://arxiv.org/abs/2412.05576</guid>
<content:encoded><![CDATA[
<div> Neural Operator, Solute Transport Operator Network, Contaminant Transport, Micro-Cracked Porous Media, Deep Learning<br />
Summary: <br />
In this work, the Solute Transport Operator Network (STONet) is introduced as a novel neural operator to model contaminant transport in micro-cracked porous media efficiently. The model architecture of STONet combines a DeepONet structure with a transformer-based multi-head attention mechanism to enhance performance without additional computational overhead. Training data obtained from finite element simulations captures diverse scenarios, enabling accurate predictions of concentration field changes. STONet achieves high accuracy with relative errors below 1% compared to FEM simulations and reduces runtime significantly. This computational efficiency enables the development of digital twins for rapid assessment of subsurface contamination risks and optimization of environmental remediation strategies. Data and code for the paper will be available on GitHub for further research and application. <br /> <div>
arXiv:2412.05576v2 Announce Type: replace-cross 
Abstract: In this work, we introduce a novel neural operator, the Solute Transport Operator Network (STONet), to efficiently model contaminant transport in micro-cracked porous media. STONet's model architecture is specifically designed for this problem and uniquely integrates an enriched DeepONet structure with a transformer-based multi-head attention mechanism, enhancing performance without incurring additional computational overhead compared to existing neural operators. The model combines different networks to encode heterogeneous properties effectively and predict the rate of change of the concentration field to accurately model the transport process. The training data is obtained using finite element (FEM) simulations by random sampling of micro-fracture distributions and applied pressure boundary conditions, which capture diverse scenarios of fracture densities, orientations, apertures, lengths, and balance of pressure-driven to density-driven flow. Our numerical experiments demonstrate that, once trained, STONet achieves accurate predictions, with relative errors typically below 1% compared with FEM simulations while reducing runtime by approximately two orders of magnitude. This type of computational efficiency facilitates building digital twins for rapid assessment of subsurface contamination risks and optimization of environmental remediation strategies. The data and code for the paper will be published at https://github.com/ehsanhaghighat/STONet.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Parametric State Estimation in Circulating Fuel Reactors with Shallow Recurrent Decoder Networks</title>
<link>https://arxiv.org/abs/2503.08904</link>
<guid>https://arxiv.org/abs/2503.08904</guid>
<content:encoded><![CDATA[
<div> data-driven methods, state reconstruction, nuclear reactors, Shallow Recurrent Decoder, Generation-IV reactors

Summary:
The article discusses the application of data-driven methods for accurate state reconstruction in nuclear reactors, focusing on the challenging environment of Generation-IV reactors like the Molten Salt Fast Reactor (MSFR). By leveraging the Shallow Recurrent Decoder architecture, the study efficiently estimates the reactor's state vector, including neutron fluxes, precursors concentrations, and thermal parameters, using only three out-of-core time-series neutron flux measurements. The proposed approach extends the architecture to handle parametric time-series data, enabling robust state estimation under various operating conditions. Additionally, the methodology allows for quantifying uncertainty in state estimation with low training costs. The study's successful results highlight its potential for real-time monitoring, control, and the development of a reactor digital twin. <div>
arXiv:2503.08904v2 Announce Type: replace-cross 
Abstract: The recent developments in data-driven methods have paved the way to new methodologies to provide accurate state reconstruction of engineering systems; nuclear reactors represent particularly challenging applications for this task due to the complexity of the strongly coupled physics involved and the extremely harsh and hostile environments, especially for new technologies such as Generation-IV reactors. Data-driven techniques can combine different sources of information, including computational proxy models and local noisy measurements on the system, to robustly estimate the state. This work leverages the novel Shallow Recurrent Decoder architecture to infer the entire state vector (including neutron fluxes, precursors concentrations, temperature, pressure and velocity) of a reactor from three out-of-core time-series neutron flux measurements alone. In particular, this work extends the standard architecture to treat parametric time-series data, ensuring the possibility of investigating different accidental scenarios and showing the capabilities of this approach to provide an accurate state estimation in various operating conditions. This paper considers as a test case the Molten Salt Fast Reactor (MSFR), a Generation-IV reactor concept, characterised by strong coupling between the neutronics and the thermal hydraulics due to the liquid nature of the fuel. The promising results of this work are further strengthened by the possibility of quantifying the uncertainty associated with the state estimation, due to the considerably low training cost. The accurate reconstruction of every characteristic field in real-time makes this approach suitable for monitoring and control purposes in the framework of a reactor digital twin.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</title>
<link>https://arxiv.org/abs/2503.21248</link>
<guid>https://arxiv.org/abs/2503.21248</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, scientific research, benchmark, hypothesis generation, automated discovery

Summary:
Large language models (LLMs) are being evaluated for their potential in aiding scientific research through a new benchmark that focuses on inspiration retrieval, hypothesis composition, and ranking. An automated framework extracts key components from scientific papers across various disciplines with expert validation to ensure accuracy. By exclusively analyzing papers from 2024, the benchmark avoids data contamination. LLMs show promise in retrieving inspirations, indicating their ability to uncover novel knowledge associations. This positions LLMs as valuable tools for automated scientific discovery, capable of generating innovative hypotheses at scale with minimal human input. <div>
arXiv:2503.21248v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analogical Learning for Cross-Scenario Generalization: Framework and Application to Intelligent Localization</title>
<link>https://arxiv.org/abs/2504.08811</link>
<guid>https://arxiv.org/abs/2504.08811</guid>
<content:encoded><![CDATA[
<div> learning models, generalization, deep learning framework, analogical learning, intelligent wireless localization

Summary:
The article introduces an analogical learning (AL) framework, specifically a bipartite neural network called Mateformer, for multi-scenario learning in intelligent wireless localization. The AL approach leverages the understanding that data from different scenarios follow common underlying physical rules despite having distinct reference frames. The Mateformer network captures relativity within multiple latent feature spaces and uses this relativity to guide nonlinear analogy for accurate predictions. Experimental results show that AL outperforms existing models in accuracy in single-scenario benchmarks, exhibits stable transferability between scenarios without catastrophic forgetting, and robustly adapts to new, unseen scenarios such as dynamic weather and traffic conditions without any fine-tuning. The data and code for the framework are also made available for further research and implementation. <div>
arXiv:2504.08811v2 Announce Type: replace-cross 
Abstract: Existing learning models often exhibit poor generalization when deployed across diverse scenarios. It is primarily due to that the underlying reference frame of the data varies with the deployment environment and settings. However, despite that data of each scenario has a distinct reference frame, its generation generally follows common underlying physical rules. Based on this understanding, this article proposes a deep learning framework named analogical learning (AL), which implicitly retrieves the reference frame information associated with a scenario and then to make accurate prediction by relative analogy with other scenarios. Specifically, we design a bipartite neural network called Mateformer. Its first part captures the relativity within multiple latent feature spaces between the input data and a small amount of embedded data from the studied scenario, while its second part uses this relativity to guide the nonlinear analogy. We apply AL to the typical multi-scenario learning problem of intelligent wireless localization in cellular networks. Extensive experiments validate AL's superiority across three key dimensions. First, it achieves state-of-the-art accuracy in single-scenario benchmarks. Second, it demonstrates stable transferability between different scenarios, avoiding catastrophic forgetting. Finally, and most importantly, it robustly adapts to new, unseen scenarios--including dynamic weather and traffic conditions--without any tuning. All data and code are available at https://github.com/ziruichen-research/ALLoc.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Integrated Electrostatic Boundary Element Simulations with Non-Conforming Higher-Order Meshes</title>
<link>https://arxiv.org/abs/2506.22676</link>
<guid>https://arxiv.org/abs/2506.22676</guid>
<content:encoded><![CDATA[
<div> CAD plugin, design analysis, virtual prototyping, electric devices, boundary element method
Summary:
The article introduces a design through analysis workflow for virtual prototyping of electric devices. A CAD plugin facilitates the interaction between design and analysis, enabling the creation of analysis models and visualization of results within the design environment. Simulations employ a fast boundary element method (BEM) capable of handling non-conforming and higher-order meshes. Numerical experiments are conducted to assess the accuracy of the approach and its sensitivity to the initial CAD representation. The workflow facilitates a close link between design and analysis, with the non-conforming higher-order BEM technique producing precise results while simplifying the interaction between the two processes. <div>
arXiv:2506.22676v1 Announce Type: new 
Abstract: We present a design through analysis workflow that enables virtual prototyping of electric devices. A CAD plugin establishes the interaction between design and analysis, allowing the preparation of analysis models and the visualization of its results within the design environment. The simulations utilize a fast boundary element method (BEM) that allows for non-conforming and higher-order meshes. Our numerical experiments investigate the accuracy of the approach and its sensitivity to the initial CAD representation. Overall, the workflow enables a close link between design and analysis, where the non-conforming higher-order BEM approach provides accurate results and significantly simplifies the interaction.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved design of an active landing gear for a passenger aircraft using multi-objective optimization technique</title>
<link>https://arxiv.org/abs/2506.22870</link>
<guid>https://arxiv.org/abs/2506.22870</guid>
<content:encoded><![CDATA[
<div> Controller coefficients, hydraulic actuator parameters, vibration absorber, landing gear system, optimization algorithm<br />
<br />
Summary:<br />
This study focuses on optimizing the landing gear system of aircraft using a bee-inspired multi-objective algorithm. The research addresses the need for better performance under varying landing and runway conditions by optimizing controller coefficients, hydraulic actuator parameters, and vibration absorber simultaneously. Sensitivity analysis for three-point landings and robustness analysis for emergency wind conditions are conducted. By applying the active shock absorber system, optimized through bee-based algorithms, improvements are seen in reducing bounce and pitch displacements, suspension travel, and impact force in both time and frequency domains. The results show enhanced passenger comfort and potentially extended structural fatigue life, showcasing practical industrial application. <div>
arXiv:2506.22870v1 Announce Type: new 
Abstract: The landing gear system is a major aircraft subsystem that must withstand extreme forces during ground maneuvers and absorb vibrations. While traditional systems perform well under normal conditions, their efficiency drops under varying landing and runway scenarios. This study addresses this issue by simultaneously optimizing controller coefficients, parameters of a nonlinear hydraulic actuator integrated into the traditional shock absorber, and a vibration absorber using a bee-inspired multi-objective algorithm. To demonstrate adaptability, the paper includes sensitivity analysis for three-point landings affected by added payload and touchdown speed, and robustness analysis for one- and two-point landings under emergency wind conditions. The dynamic flight equations of an Airbus A320-200 during landing are derived and solved numerically. Results show that the active shock absorber system, optimized via two bee-based algorithms, outperforms the passive system in reducing bounce and pitch displacements and momenta, suspension travel, and impact force in both time and frequency domains. This leads to significantly improved passenger comfort and potentially longer structural fatigue life, demonstrating industrial applicability.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feasibility of spectral-element modeling of wave propagation through the anatomy of marine mammals</title>
<link>https://arxiv.org/abs/2506.22944</link>
<guid>https://arxiv.org/abs/2506.22944</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D spectral-element method, ultrasonic wave propagation, bottlenose dolphin, marine mammal bioacoustics, environmental challenges <br />
Summary: <br />
This study presents a novel 3D spectral-element method (SEM) simulation of ultrasonic wave propagation in a bottlenose dolphin's head, addressing limitations of traditional finite-element methods (FEM). The SEM approach offers exponential convergence and efficient parallel computation, enabling high-frequency simulations with complex anatomical features accurately represented in the mesh. Utilizing Computed Tomography (CT) scan data, detailed simulations of plane and spherical waves validate the efficacy of SEM for ultrasonic time-domain modeling in marine mammal bioacoustics research. This advancement in computational modeling opens up opportunities in studying dolphin echolocation, the effects of anthropogenic marine noise pollution, and the biophysics of hearing and click generation. By overcoming FEM's challenges, SEM emerges as a powerful tool to investigate hypotheses related to dolphin bioacoustics, with implications for conservation efforts and understanding marine mammal auditory systems in the face of increasing environmental challenges. <div>
arXiv:2506.22944v1 Announce Type: new 
Abstract: This study introduces the first 3D spectral-element method (SEM) simulation of ultrasonic wave propagation in a bottlenose dolphin (Tursiops truncatus) head. Unlike traditional finite-element methods (FEM), which struggle with high-frequency simulations due to costly linear-system inversions and slower convergence, SEM offers exponential convergence and efficient parallel computation. Using Computed Tomography (CT) scan data, we developed a detailed hexahedral mesh capturing complex anatomical features, such as acoustic fats and jaws. Our simulations of plane and spherical waves confirm SEM's effectiveness for ultrasonic time-domain modeling. This approach opens new avenues for marine biology, contributing to research in echolocation, the impacts of anthropogenic marine noise pollution and the biophysics of hearing and click generation in marine mammals. By overcoming FEM's limitations, SEM provides a powerful scalable tool to test hypotheses about dolphin bioacoustics, with significant implications for conservation and understanding marine mammal auditory systems under increasing environmental challenges.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparStencil: Retargeting Sparse Tensor Cores to Scientific Stencil Computations via Structured Sparsity Transformation</title>
<link>https://arxiv.org/abs/2506.22969</link>
<guid>https://arxiv.org/abs/2506.22969</guid>
<content:encoded><![CDATA[
<div> Sparse Tensor Core, scientific stencil computations, SparStencil, structured sparsity, adaptive layout morphing.<br />
<br />
Summary: <br />
Sparse Tensor Cores (TCUs) are efficient for AI workloads with 2:4 sparsity, but not utilized for scientific stencil computations. SparStencil transforms sparse TCUs for stencil computations by restructuring patterns into sparse matrices, ensuring compatibility with 2:4 sparsity constraints. It includes Adaptive Layout Morphing to align patterns, Structured Sparsity Conversion for graph matching, and Automatic Kernel Generation for optimized kernels. Evaluated on 79 kernels, SparStencil achieves up to 7.1x speedup, reduces complexity, and matches expert-tuned performance in compute throughput and memory efficiency. <div>
arXiv:2506.22969v1 Announce Type: new 
Abstract: Sparse Tensor Cores offer exceptional performance gains for AI workloads by exploiting structured 2:4 sparsity. However, their potential remains untapped for core scientific workloads such as stencil computations, which exhibit irregular sparsity patterns.This paper presents SparStencil, the first system to retarget sparse TCUs for scientific stencil computations through structured sparsity transformation. SparStencil introduces three key techniques: (1) Adaptive Layout Morphing, which restructures stencil patterns into staircase-aligned sparse matrices via a flatten-and-crush pipeline; (2) Structured Sparsity Conversion, which formulates transformation as a graph matching problem to ensure compatibility with 2:4 sparsity constraints; (3) Automatic Kernel Generation, which compiles transformed stencils into optimized sparse MMA kernels via layout search and table-driven memory mapping. Evaluated on 79 stencil kernels spanning diverse scientific domains, SparStencil achieves up to 7.1x speedup (3.1x on average) over state-of-the-art framework while reducing code complexity and matching or exceeding expert-tuned performance in both compute throughput and memory efficiency.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a better approach to the Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2506.23028</link>
<guid>https://arxiv.org/abs/2506.23028</guid>
<content:encoded><![CDATA[
<div> Keywords: Vehicle Routing Problem, Logistics, Combinatorial Optimization, Constraints, Extensions

Summary:<br /><br />
The Vehicle Routing Problem (VRP) is a critical issue in logistics management, impacting transportation efficiency, cost reduction, and service quality. As a combinatorial optimization problem, it is widely studied in the fields of transportation, logistics, and delivery systems due to its numerous formulations and extensions. This article provides a detailed overview of VRP, exploring its theoretical foundations, limitations of the classical model, and key extensions. By reviewing various constraints, objectives, and variants in recent literature, it aims to enhance understanding of VRP and its ongoing evolution in modern optimization and decision-making processes. <div>
arXiv:2506.23028v1 Announce Type: new 
Abstract: The Vehicle Routing Problem (VRP) is a fundamental challenge in logistics management research, given its substantial influence on transportation efficiency, cost minimization, and service quality. As a combinatorial optimization problem, VRP plays a crucial role in a wide range of real world applications, particularly in transportation, logistics, and delivery systems, due to its diverse formulations and numerous extensions. Over the years, researchers have introduced various VRP variants to address specific operational constraints, emerging industry requirements and optimize specific objectives, making it one of the most extensively studied problems in operations research. This article provides a comprehensive overview of VRP by exploring its theoretical foundations, discussing the limitations of its classical model, and introducing its key extensions. By systematically reviewing the diverse constraints, objectives, and variants examined in recent literature, this study aims to contribute to a deeper understanding of VRP while highlighting its ongoing evolution and relevance in modern optimization and decision making processes.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Multiscale Topology Optimization of Spinodoid Architected Materials with Controllable Anisotropy</title>
<link>https://arxiv.org/abs/2506.23420</link>
<guid>https://arxiv.org/abs/2506.23420</guid>
<content:encoded><![CDATA[
<div> neural networks, topology optimization, spinodoid materials, data-driven design, Gaussian Process

Summary:
The article introduces a new approach to design optimization for spinodoid architected materials, which have unique properties such as stochasticity, aperiodicity, and bi-continuity. Traditional design methods face challenges due to the complexity of spinodoid design parameters. The proposed framework utilizes neural networks to automate the computation of topological gradients, making it more efficient and scalable. Additionally, a Gaussian Process surrogate is integrated to enhance the accuracy of spinodoid constitutive models. This framework provides physical insights into material distribution, showing why anisotropic spinodoids with tailored orientations are preferred in certain regions. The interpretability of the framework bridges the gap between data-driven design and mechanistic understanding, offering a clearer understanding of material behavior. <div>
arXiv:2506.23420v1 Announce Type: new 
Abstract: Spinodoid architected materials have drawn significant attention due to their unique nature in stochasticity, aperiodicity, and bi-continuity. Compared to classic periodic truss-, beam- and plate-based lattice architectures, spinodoids are insensitive to manufacturing defects, scalable for high throughput production, functionally graded by tunable local properties, and material failure resistant due to low-curvature morphology. However, the design of spinodoids is often hindered by the curse of dimensionality with extremely large design space of spinodoid types, material density, orientation, continuity, and anisotropy. From a design optimization perspective, while genetic algorithms are often beyond the reach of computing capacity, gradient-based topology optimization is challenged by the intricate mathematical derivation of gradient fields with respect to various spinodoid parameters. To address such challenges, we propose a data-driven multiscale topology optimization framework. Our framework reformulates the design variables of spinodoid materials as the parameters of neural networks, enabling automated computation of topological gradients. Additionally, it incorporates a Gaussian Process surrogate for spinodoid constitutive models, eliminating the need for repeated computational homogenization and enhancing the scalability of multiscale topology optimization. Compared to 'black-box' deep learning approaches, the proposed framework provides clear physical insights into material distribution. It explicitly reveals why anisotropic spinodoids with tailored orientations are favored in certain regions, while isotropic spinodoids are more suitable elsewhere. This interpretability helps to bridge the gap between data-driven design with mechanistic understanding.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Multiscale Topology Optimization of Soft Functionally Graded Materials with Large Deformations</title>
<link>https://arxiv.org/abs/2506.23422</link>
<guid>https://arxiv.org/abs/2506.23422</guid>
<content:encoded><![CDATA[
<div> Keywords: Functionally Graded Materials, Topology Optimization, Multiscale Architecture, Nonlinear Material Behavior, Neural Network

Summary:
Functionally Graded Materials (FGMs), particularly soft FGMs, are increasingly important in various engineering applications. Designing these complex systems poses challenges due to their multiscale nature, multiple material phases, and nonlinear behaviors. This paper presents a novel topology optimization framework for soft FGMs under large deformations. Key innovations include a microstructure reconstruction algorithm, material homogenization approach, neural network-based optimization, and a nonlinear sensitivity analysis technique. The framework generates unique topological designs with spatially varying microstructures, not achievable using linear elasticity. To ensure efficient convergence, an energy interpolation scheme and a Newton-Raphson solver with adaptive steps are employed. Numerical experiments demonstrate the effectiveness of the proposed approach in automating the design innovation of soft FGMs. 

<br /><br />Summary: <div>
arXiv:2506.23422v1 Announce Type: new 
Abstract: Functionally Graded Materials (FGMs) made of soft constituents have emerged as promising material-structure systems in potential applications across many engineering disciplines, such as soft robots, actuators, energy harvesting, and tissue engineering. Designing such systems remains challenging due to their multiscale architectures, multiple material phases, and inherent material and geometric nonlinearities. The focus of this paper is to propose a general topology optimization framework that automates the design innovation of multiscale soft FGMs exhibiting nonlinear material behaviors under large deformations. Our proposed topology optimization framework integrates several key innovations: (1) a novel microstructure reconstruction algorithm that generates composite architecture materials from a reduced design space using physically interpretable parameters; (2) a new material homogenization approach that estimates effective properties by combining the stored energy functions of multiple soft constituents; (3) a neural network-based topology optimization that incorporates data-driven material surrogates to enable bottom-up, simultaneous optimization of material and structure; and (4) a generic nonlinear sensitivity analysis technique that computes design sensitivities numerically without requiring explicit gradient derivation. To enhance the convergence of the nonlinear equilibrium equations amid topology optimization, we introduce an energy interpolation scheme and employ a Newton-Raphson solver with adaptive step sizes and convergence criteria. Numerical experiments show that the proposed framework produces distinct topological designs, different from those obtained under linear elasticity, with spatially varying microstructures.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics</title>
<link>https://arxiv.org/abs/2506.22520</link>
<guid>https://arxiv.org/abs/2506.22520</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Curiosity, Engagement, Interactive Molecular Dynamics, Team Performance 

Summary: 
The study investigates the impact of an Artificial Intelligence tutor teammate on student curiosity-driven engagement and learning effectiveness in Interactive Molecular Dynamics tasks. Through a Wizard-of-Oz paradigm, the AI's behaviors are adjusted by a human experimenter to stimulate and sustain student curiosity. Results show that high-performing teams exhibit superior task completion, deeper understanding, and increased engagement. Advanced student questions are linked to AI curiosity-triggering, indicating heightened engagement and cognitive complexity. Cross Recurrence Quantification Analysis metrics demonstrate dynamic synchronization in student-AI interactions, promoting structured yet adaptive engagement to foster curiosity. The findings suggest that the AI's dual role as a teammate and educator can provide adaptive feedback, sustaining engagement and epistemic curiosity. <div>
arXiv:2506.22520v1 Announce Type: cross 
Abstract: This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research</title>
<link>https://arxiv.org/abs/2506.23545</link>
<guid>https://arxiv.org/abs/2506.23545</guid>
<content:encoded><![CDATA[
<div> training, diagnostics, psychological research, XR technologies, human performance <br />
<br />
In this panel discussion, the potential of Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies in various domains is highlighted. These immersive systems are being used to enhance human performance in clinical psychology, space exploration, and medical education. In psychological research and training, XR provides a controlled yet realistic environment for measuring cognitive and emotional processes. For space exploration, VR-based astronaut training and diagnostic systems allow for real-time health assessments. In the field of medical education and rehabilitation, immersive environments are utilized for procedural training and patient engagement. Whether through virtual surgical simulations or gamified rehabilitation exercises, XR technologies improve learning outcomes and encourage patient adherence to treatment plans. Overall, VR/AR/XR technologies are proving to be valuable tools in enhancing performance and advancing research in high-risk and regulated environments. <br /><br />Summary: <div>
arXiv:2506.23545v1 Announce Type: cross 
Abstract: Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are increasingly recognized for their applications in training, diagnostics, and psychological research, particularly in high-risk and highly regulated environments. In this panel we discuss how immersive systems enhance human performance across multiple domains, including clinical psychology, space exploration, and medical education. In psychological research and training, XR can offer a controlled yet ecologically valid setting for measuring cognitive and affective processes. In space exploration, we discuss the development of VR-based astronaut training and diagnostic systems, allowing astronauts to perform real-time health assessments. In medical education and rehabilitation, we cover procedural training and patient engagement. From virtual surgical simulations to gamified rehabilitation exercises, immersive environments enhance both learning outcomes and treatment adherence.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment</title>
<link>https://arxiv.org/abs/2506.23739</link>
<guid>https://arxiv.org/abs/2506.23739</guid>
<content:encoded><![CDATA[
<div> Vehicle-in-the-Loop test bench, vulnerable road users, cyber-physical testing, human pose estimation, urban environments <br />
<br />Summary: <br />
This paper discusses a test environment that combines a Vehicle-in-the-Loop test bench with a motion laboratory to test vehicle interactions with pedestrians and cyclists. The study validates a human pose estimation approach using real-world and virtual representations of vulnerable road users. Results show good alignment in human pose estimation between real-world and cyber-physical test conditions for stable motion patterns but inaccuracies persist under dynamic movements and occlusions, especially for complex cyclist postures. The research aims to enhance testing methodologies for evaluating AI-based vehicle perception and improving interaction models between automated vehicles and vulnerable road users in cyber-physical environments. <div>
arXiv:2506.23739v1 Announce Type: cross 
Abstract: Ensuring safe and realistic interactions between automated driving systems and vulnerable road users (VRUs) in urban environments requires advanced testing methodologies. This paper presents a test environment that combines a Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the feasibility of cyber-physical (CP) testing of vehicle-pedestrian and vehicle-cyclist interactions. Building upon previous work focused on pedestrian localization, we further validate a human pose estimation (HPE) approach through a comparative analysis of real-world (RW) and virtual representations of VRUs. The study examines the perception of full-body motion using a commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is generated in Unreal Engine 5, where VRUs are animated in real time and projected onto a screen to stimulate the camera. The proposed stimulation technique ensures the correct perspective, enabling realistic vehicle perception. To assess the accuracy and consistency of HPE across RW and CP domains, we analyze the reliability of detections as well as variations in movement trajectories and joint estimation stability. The validation includes dynamic test scenarios where human avatars, both walking and cycling, are monitored under controlled conditions. Our results show a strong alignment in HPE between RW and CP test conditions for stable motion patterns, while notable inaccuracies persist under dynamic movements and occlusions, particularly for complex cyclist postures. These findings contribute to refining CP testing approaches for evaluating next-generation AI-based vehicle perception and to enhancing interaction models of automated vehicles and VRUs in CP environments.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A topology optimisation framework to design test specimens for one-shot identification or discovery of material models</title>
<link>https://arxiv.org/abs/2501.12756</link>
<guid>https://arxiv.org/abs/2501.12756</guid>
<content:encoded><![CDATA[
<div> Topology Optimization, Geometry Design, Material Model Calibration, Experimental Mechanics, Digital Image Correlation  
Summary:  
- The shift in material model calibration towards using complex geometry tests requires rich displacement data.  
- The paper proposes a density-based topology optimization approach to design specimen geometry for anisotropic material model calibration.  
- High-resolution specimen design aims to maximize the robustness of inverse problem solutions with noisy displacement measurements.  
- The study discusses cost function selection and topology optimization framework design for optimal specimen geometry.  
- Various optimized topologies are analyzed for identifying isotropic and anisotropic elastic responses.  

<br /><br />Summary: <div>
arXiv:2501.12756v2 Announce Type: replace 
Abstract: The increasing availability of full-field displacement data from imaging techniques in experimental mechanics is determining a gradual shift in the paradigm of material model calibration and discovery, from using several simple-geometry tests towards a few, or even one single test with complicated geometry. The feasibility of such a "one-shot" calibration or discovery heavily relies upon the richness of the measured displacement data, i.e., their ability to probe the space of the state variables and the stress space (whereby the stresses depend on the constitutive law being sought) to an extent sufficient for an accurate and robust calibration or discovery process. The richness of the displacement data is in turn directly governed by the specimen geometry. In this paper, we propose a density-based topology optimisation framework to optimally design the geometry of the target specimen for calibration of an anisotropic elastic material model. To this end, we perform automatic, high-resolution specimen design by maximising the robustness of the solution of the inverse problem, i.e., the identified material parameters, given noisy displacement measurements from digital image correlation. We discuss the choice of the cost function and the design of the topology optimisation framework, and we analyse a range of optimised topologies generated for the identification of isotropic and anisotropic elastic responses.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Evaluation Standards: A Unified Framework for Evaluating the Korean Capabilities of Language Models</title>
<link>https://arxiv.org/abs/2503.22968</link>
<guid>https://arxiv.org/abs/2503.22968</guid>
<content:encoded><![CDATA[
<div> framework, Korean large language models, evaluation, benchmarks, HRET  
Summary:  
- Recent advancements in Korean large language models have led to performance variations across institutions due to inconsistent evaluation protocols.  
- HRET (Haerae Evaluation Toolkit) is introduced as an open-source framework to unify Korean LLM assessment by integrating major benchmarks, inference backends, and diverse evaluation methods.  
- The modular registry design of HRET allows for rapid incorporation of new datasets, methods, and backends to adapt to evolving research needs.  
- HRET includes morphological-aware Type-Token Ratio and systematic keyword-omission detection for diagnostic insights into language-specific behaviors, aiding in identifying shortcomings and guiding improvements in Korean LLM development.  
- The framework promotes diverse experimental approaches and language consistency enforcement to ensure genuine Korean outputs.  
<br /><br />Summary: <div>
arXiv:2503.22968v3 Announce Type: replace 
Abstract: Recent advancements in Korean large language models (LLMs) have driven numerous benchmarks and evaluation methods, yet inconsistent protocols cause up to 10 p.p performance gaps across institutions. Overcoming these reproducibility gaps does not mean enforcing a one-size-fits-all evaluation. Rather, effective benchmarking requires diverse experimental approaches and a framework robust enough to support them. To this end, we introduce HRET (Haerae Evaluation Toolkit), an open-source, registry-based framework that unifies Korean LLM assessment. HRET integrates major Korean benchmarks, multiple inference backends, and multi-method evaluation, with language consistency enforcement to ensure genuine Korean outputs. Its modular registry design also enables rapid incorporation of new datasets, methods, and backends, ensuring the toolkit adapts to evolving research needs. Beyond standard accuracy metrics, HRET incorporates Korean-focused output analyses-morphology-aware Type-Token Ratio (TTR) for evaluating lexical diversity and systematic keyword-omission detection for identifying missing concepts-to provide diagnostic insights into language-specific behaviors. These targeted analyses help researchers pinpoint morphological and semantic shortcomings in model outputs, guiding focused improvements in Korean LLM development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calculation of Photocarrier Generation from Optical Absorption for Time-domain Simulation of Optoelectronic Devices</title>
<link>https://arxiv.org/abs/2102.06702</link>
<guid>https://arxiv.org/abs/2102.06702</guid>
<content:encoded><![CDATA[
<div> optoelectronic materials, photocarrier generation rate, Poynting vector, time-domain simulations, optical absorption <br />
Summary: 
The study addresses the inaccurate calculation of photocarrier generation rates in optoelectronic materials using the Poynting vector in time-domain simulations. It proposes an optical absorption-based model that considers material dispersion near the optical frequency corresponding to the bandgap energy. By calculating instantaneous optical absorption from the polarization current density associated with the dispersion model, the proposed approach offers more accurate results compared to the Poynting vector-based method. Numerical simulations demonstrate the efficacy of the new model, particularly when dealing with strong low-frequency fields that can lead to divergent carrier densities in the Poynting vector approach. The method is further validated through simulations of a photoconductive device, highlighting its improved accuracy in determining photocarrier generation rates. <br /> <div>
arXiv:2102.06702v3 Announce Type: replace-cross 
Abstract: Photocarrier generation rate in optoelectronic materials is often calculated using the Poynting vector in the frequency domain. However, this approach is not accurate in time-domain simulations of photoconductive devices because the instantaneous Poynting vector does not distinguish between power flux densities of optical and low-frequency electromagnetic fields. The latter is generated by photocurrents and is not supposed to contribute to the photocarrier generation since the corresponding photon energy is smaller than the bandgap energy of the optoelectronic material. This work proposes an optical absorption-based model to accurately calculate the generation rate in time-domain simulations. The proposed approach considers the material dispersion near the optical frequency corresponding to the bandgap energy of the optoelectronic material and calculates the instantaneous optical absorption from the polarization current density associated with this dispersion model. Numerical examples show that the proposed method is more accurate than the Poynting vector-based approach in calculating the instantaneous optical absorption. The method is further validated against experimental results via simulations of a photoconductive device, where the Poynting vector-based approach results in divergent carrier densities when the low-frequency fields are strong.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Self-Amplifying Hypergraph Structures through Mathematical Optimization</title>
<link>https://arxiv.org/abs/2412.15776</link>
<guid>https://arxiv.org/abs/2412.15776</guid>
<content:encoded><![CDATA[
<div> amplification factor, self-amplifying structures, hypergraphs, optimization, chemical reaction networks

Summary:
The paper introduces self-amplifying structures for hypergraphs, crucial for understanding propagation and internal reinforcement in complex systems. It defines the maximal amplification factor to quantify this phenomenon and develops an optimization-based methodology to compute it efficiently. The problem of identifying the subhypergraph maximizing the amplification factor is addressed as a mixed-integer nonlinear programming (MINLP) problem, solved with an exact iterative algorithm. Extensive computational experiments on synthetic instances demonstrate the relevance and effectiveness of the proposed approach. A case study on chemical reaction networks, including the Formose reaction and E. coli core metabolism, showcases the framework's ability to identify known and novel autocatalytic subnetworks, emphasizing its practical relevance in systems chemistry and biology.<br /><br />Summary: <div>
arXiv:2412.15776v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce the concept of self-amplifying structures for hypergraphs, positioning it as a key element for understanding propagation and internal reinforcement in complex systems. To quantify this phenomenon, we define the maximal amplification factor, a metric that captures how effectively a subhypergraph contributes to its own amplification. We then develop an optimization-based methodology to compute this measure. Building on this foundation, we tackle the problem of identifying the subhypergraph maximizing the amplification factor, formulating it as a mixed-integer nonlinear programming (MINLP) problem. To solve it efficiently, we propose an exact iterative algorithm with proven convergence guarantees. In addition, we report the results of extensive computational experiments on realistic synthetic instances, demonstrating both the relevance and effectiveness of the proposed approach. Finally, we present a case study on chemical reaction networks, including the Formose reaction and E. coli core metabolism, where our framework successfully identifies known and novel autocatalytic subnetworks, highlighting its practical relevance to systems chemistry and biology.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drivetrain simulation using variational autoencoders</title>
<link>https://arxiv.org/abs/2501.17653</link>
<guid>https://arxiv.org/abs/2501.17653</guid>
<content:encoded><![CDATA[
<div> Variational Autoencoders, Vehicle Jerk Prediction, Torque Demand, Drivetrain, Data Efficiency<br />
Summary:<br />
This work explores the use of variational autoencoders (VAEs) for predicting vehicle jerk signals based on torque demand in drivetrain applications with limited real-world datasets. The study involves training VAEs on experimental data from two electric SUV variants to synthesize jerk signals that capture characteristics of different drivetrain scenarios. Comparisons with physics-based and hybrid models demonstrate the effectiveness of VAEs in generating realistic jerk signals without extensive system parametrization. Unconditional VAEs successfully produce realistic signals without prior system knowledge, while conditional VAEs can tailor signals to specific torque inputs. By reducing the reliance on costly experiments and manual modeling, VAEs offer a data-efficient approach for exploring complex operational scenarios in drivetrain simulations. This integration of generative models like VAEs shows potential for enhancing validation procedures and accelerating vehicle development processes. <br /><br /> <div>
arXiv:2501.17653v2 Announce Type: replace-cross 
Abstract: This work proposes variational autoencoders (VAEs) to predict a vehicle's jerk signals from torque demand in the context of limited real-world drivetrain datasets. We implement both unconditional and conditional VAEs, trained on experimental data from two variants of a fully electric SUV with differing torque and drivetrain configurations. The VAEs synthesize jerk signals that capture characteristics from multiple drivetrain scenarios by leveraging the learned latent space. A performance comparison with baseline physics-based and hybrid models confirms the effectiveness of the VAEs, without requiring detailed system parametrization. Unconditional VAEs generate realistic jerk signals without prior system knowledge, while conditional VAEs enable the generation of signals tailored to specific torque inputs. This approach reduces the dependence on costly and time-intensive real-world experiments and extensive manual modeling. The results support the integration of generative models such as VAEs into drivetrain simulation pipelines, both for data augmentation and for efficient exploration of complex operational scenarios, with the potential to streamline validation and accelerate vehicle development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting</title>
<link>https://arxiv.org/abs/2506.21743</link>
<guid>https://arxiv.org/abs/2506.21743</guid>
<content:encoded><![CDATA[
<div> forecasting, storm surge, machine learning, ConvLSTM networks, coastal disaster

Summary:
This study introduces a novel approach to storm surge forecasting by utilizing ConvLSTM networks on structured RGB-encoded image representations of water elevation fields. The model incorporates ground-truth wind fields as dynamic conditioning signals and topo-bathymetry as a static input to capture surge evolution drivers. Evaluation on a dataset of synthetic storms in the Gulf of Mexico shows robust 48-hour forecasting performance across multiple regions along the Texas coast and spatial extensibility to other coastal areas. By combining structured representation, physically grounded forcings, and scalable deep learning, this approach advances storm surge forecasting in usability, adaptability, and interpretability. <div>
arXiv:2506.21743v1 Announce Type: new 
Abstract: Storm surge forecasting plays a crucial role in coastal disaster preparedness, yet existing machine learning approaches often suffer from limited spatial resolution, reliance on coastal station data, and poor generalization. Moreover, many prior models operate directly on unstructured spatial data, making them incompatible with modern deep learning architectures. In this work, we introduce a novel approach that projects unstructured water elevation fields onto structured Red Green Blue (RGB)-encoded image representations, enabling the application of Convolutional Long Short Term Memory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our model further integrates ground-truth wind fields as dynamic conditioning signals and topo-bathymetry as a static input, capturing physically meaningful drivers of surge evolution. Evaluated on a large-scale dataset of synthetic storms in the Gulf of Mexico, our method demonstrates robust 48-hour forecasting performance across multiple regions along the Texas coast and exhibits strong spatial extensibility to other coastal areas. By combining structured representation, physically grounded forcings, and scalable deep learning, this study advances the frontier of storm surge forecasting in usability, adaptability, and interpretability.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.21815</link>
<guid>https://arxiv.org/abs/2506.21815</guid>
<content:encoded><![CDATA[
<div> machine learning, laser powder bed fusion, microstructure, phase-field method, deep reinforcement learning 

Summary:
This research focuses on optimizing laser powder bed fusion (L-PBF) processes to control microstructure outcomes like equiaxed grains. By combining physics-guided modeling with machine learning, particularly a 3D U-Net convolutional neural network, the study aimed to accelerate the prediction of crystalline grain orientations. Three scanning strategies were explored, leading to a significant speedup in computational efficiency. Deep reinforcement learning (DRL) was then employed to generate optimized scan paths for target microstructures, showcasing its effectiveness in enhancing control over microstructure outcomes. The integration of the surrogate 3D U-Net model into the DRL environment further streamlined the training process. Ultimately, the study demonstrated the potential of machine learning methods to not only improve microstructure control but also enhance computational efficiency in optimizing L-PBF processes. <div>
arXiv:2506.21815v1 Announce Type: new 
Abstract: Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing technology for producing intricate metal components with exceptional accuracy. A key challenge in L-PBF is the formation of complex microstructures affecting product quality. We propose a physics-guided, machine-learning approach to optimize scan paths for desired microstructure outcomes, such as equiaxed grains. We utilized a phase-field method (PFM) to model crystalline grain structure evolution. To reduce computational costs, we trained a surrogate machine learning model, a 3D U-Net convolutional neural network, using single-track phase-field simulations with various laser powers to predict crystalline grain orientations based on initial microstructure and thermal history. We investigated three scanning strategies across various hatch spacings within a square domain, achieving a two-orders-of-magnitude speedup using the surrogate model. To reduce trial and error in designing laser scan toolpaths, we used deep reinforcement learning (DRL) to generate optimized scan paths for target microstructure. Results from three cases demonstrate the DRL approach's effectiveness. We integrated the surrogate 3D U-Net model into our DRL environment to accelerate the reinforcement learning training process. The reward function minimizes both aspect ratio and grain volume of the predicted microstructure from the agent's scan path. The reinforcement learning algorithm was benchmarked against conventional zigzag approach for smaller and larger domains, showing machine learning methods' potential to enhance microstructure control and computational efficiency in L-PBF optimization.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-free Forecasting of Rogue Waves using Reservoir Computing</title>
<link>https://arxiv.org/abs/2506.21918</link>
<guid>https://arxiv.org/abs/2506.21918</guid>
<content:encoded><![CDATA[
<div> Reservoir Computing, Hamiltonian systems, rogue waves, nonlinear Schrdinger equation, prediction horizon <br />
Summary: <br />
This paper explores the application of Reservoir Computing in modeling rogue wave dynamics from the nonlinear Schrdinger equation, a Hamiltonian system with modulation instability. The study demonstrates the effectiveness of Reservoir Computing in capturing these dynamics from breather simulations with unstable modes. The Echo State Network successfully predicts dynamics from two distinct testing datasets, including a higher-order breather, with remarkable agreement. The reservoir is able to forecast rogue wave propagation over a long prediction horizon, even when facing unseen dynamics. Additionally, a method is introduced to enhance the Reservoir Computing prediction in autonomous mode, improving its long-term forecasting ability. These findings contribute to advancing the use of Reservoir Computing in spatio-temporal Hamiltonian systems and underline the significance of phase space coverage in training data design. <br /> <div>
arXiv:2506.21918v1 Announce Type: new 
Abstract: Recent research has demonstrated Reservoir Computing's capability to model various chaotic dynamical systems, yet its application to Hamiltonian systems remains relatively unexplored. This paper investigates the effectiveness of Reservoir Computing in capturing rogue wave dynamics from the nonlinear Schr\"{o}dinger equation, a challenging Hamiltonian system with modulation instability. The model-free approach learns from breather simulations with five unstable modes. A properly tuned parallel Echo State Network can predict dynamics from two distinct testing datasets. The first set is a continuation of the training data, whereas the second set involves a higher-order breather. An investigation of the one-step prediction capability shows remarkable agreement between the testing data and the models. Furthermore, we show that the trained reservoir can predict the propagation of rogue waves over a relatively long prediction horizon, despite facing unseen dynamics. Finally, we introduce a method to significantly improve the Reservoir Computing prediction in autonomous mode, enhancing its long-term forecasting ability. These results advance the application of Reservoir Computing to spatio-temporal Hamiltonian systems and highlight the critical importance of phase space coverage in the design of training data.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Algorithm Based on CNN-LSTM Framework for Predicting Cancer Drug Sales Volume</title>
<link>https://arxiv.org/abs/2506.21927</link>
<guid>https://arxiv.org/abs/2506.21927</guid>
<content:encoded><![CDATA[
<div> deep learning, CNN-LSTM framework, cancer drug sales, forecasting, time series data

Summary:
This study explores the application potential of a deep learning model based on the CNN-LSTM framework in forecasting cancer drug sales. The research utilizes sales records of a specific cancer drug in Egypt from 2015 to 2024 to predict sales volume. A hybrid deep learning model combining CNN and LSTM networks is employed to improve prediction accuracy. The CNN component extracts local temporal features, while the LSTM component captures long-term dependencies. Model performance is evaluated using Mean Squared Error (MSE) and Root Mean Squared Error (RMSE), showing effectiveness in handling nonlinear and volatile sales data. The results demonstrate the model's success on the test set, with an MSE of 1.150 and an RMSE of 1.072. This research provides valuable insights for data-driven decision-making in pharmaceutical marketing and healthcare resource planning. 

Summary: <div>
arXiv:2506.21927v1 Announce Type: new 
Abstract: This study explores the application potential of a deep learning model based on the CNN-LSTM framework in forecasting the sales volume of cancer drugs, with a focus on modeling complex time series data. As advancements in medical technology and cancer treatment continue, the demand for oncology medications is steadily increasing. Accurate forecasting of cancer drug sales plays a critical role in optimizing production planning, supply chain management, and healthcare policy formulation. The dataset used in this research comprises quarterly sales records of a specific cancer drug in Egypt from 2015 to 2024, including multidimensional information such as date, drug type, pharmaceutical company, price, sales volume, effectiveness, and drug classification. To improve prediction accuracy, a hybrid deep learning model combining Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks is employed. The CNN component is responsible for extracting local temporal features from the sales data, while the LSTM component captures long-term dependencies and trends. Model performance is evaluated using two widely adopted metrics: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). The results demonstrate that the CNN-LSTM model performs well on the test set, achieving an MSE of 1.150 and an RMSE of 1.072, indicating its effectiveness in handling nonlinear and volatile sales data. This research provides theoretical and technical support for data-driven decision-making in pharmaceutical marketing and healthcare resource planning.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</title>
<link>https://arxiv.org/abs/2501.06184</link>
<guid>https://arxiv.org/abs/2501.06184</guid>
<content:encoded><![CDATA[
<div> geologic map, MLLMs, GeoMap-Bench, GeoMap-Agent, AI applications <br />
Summary: 
Geologic maps play a crucial role in understanding Earth's subsurface and surface. Current Multimodal Large Language Models (MLLMs) often struggle with geologic map understanding due to cartographic generalization challenges. To address this gap, GeoMap-Bench, a benchmark for evaluating MLLMs in geologic map understanding, was established. The GeoMap-Agent, designed for geologic map understanding, consists of Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA) modules. The AI expert group acts as consultants, achieving a high overall score on GeoMap-Bench. This work, known as PEACE, empowers geologic map holistic understanding with MLLMs, paving the way for advanced AI applications in geology. <div>
arXiv:2501.06184v1 Announce Type: cross 
Abstract: Geologic map, as a fundamental diagram in geology science, provides critical insights into the structure and composition of Earth's subsurface and surface. These maps are indispensable in various fields, including disaster detection, resource exploration, and civil engineering. Despite their significance, current Multimodal Large Language Models (MLLMs) often fall short in geologic map understanding. This gap is primarily due to the challenging nature of cartographic generalization, which involves handling high-resolution map, managing multiple associated components, and requiring domain-specific knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever benchmark for evaluating MLLMs in geologic map understanding, which assesses the full-scale abilities in extracting, referring, grounding, reasoning, and analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent designed for geologic map understanding, which features three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA). Inspired by the interdisciplinary collaboration among human scientists, an AI expert group acts as consultants, utilizing a diverse tool pool to comprehensively analyze questions. Through comprehensive experiments, GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o. Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs, paves the way for advanced AI applications in geology, enhancing the efficiency and accuracy of geological investigations.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Simulations of Turbulent Flows using Lattice Boltzmann Methods on Heterogeneous High Performance Computers</title>
<link>https://arxiv.org/abs/2506.21804</link>
<guid>https://arxiv.org/abs/2506.21804</guid>
<content:encoded><![CDATA[
<div> Keywords: GPU-accelerated supercomputers, turbulent flows, Lattice Boltzmann Methods, wall-modeled LES, scalability<br />
Summary:<br />
This paper focuses on the potential of GPU-accelerated supercomputers for large-scale simulations of turbulent flows. It introduces a novel Lattice Boltzmann Method (LBM) scheme tailored for wall-modeled Large Eddy Simulations (LES) in complex geometries. The scheme is specifically designed for efficient implementation within the open source LBM framework OpenLB. The study provides detailed scalability results for different HoreKa partitions, utilizing up to 128 nodes and covering problem sizes up to 18 billion cells. The research highlights the compatibility of LBM with highly parallel execution on both CPUs and GPUs, making it a promising tool for simulating turbulent flows in various applications. <div>
arXiv:2506.21804v1 Announce Type: cross 
Abstract: Current GPU-accelerated supercomputers promise to enable large-scale simulations of turbulent flows. Lattice Boltzmann Methods (LBM) are particularly well-suited to fulfilling this promise due to their intrinsic compatibility with highly parallel execution on both SIMD CPUs and GPUs. A novel LBM scheme for wall-modeled LES in complex geometries is described with a special focus on the efficient implementation in the open source LBM framework OpenLB. Detailed scalability results are provided for all HoreKa partitions, utilizing up to 128 nodes and covering problem sizes up to 18 billion cells.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StructMG: A Fast and Scalable Structured Algebraic Multigrid</title>
<link>https://arxiv.org/abs/2506.21932</link>
<guid>https://arxiv.org/abs/2506.21932</guid>
<content:encoded><![CDATA[
<div> derive, multigrid, structured grid, algebraic multigrid, parallel efficiency
Summary:
StructMG is a novel algebraic multigrid preconditioner designed for solving large-scale sparse linear systems on structured grids. Three key principles derived from the classical 'multigrid seesaw' guide its efficient implementation. StructMG automatically constructs hierarchical grids, achieving low cost per iteration and good convergence in parallel. By using a stencil-based triple-matrix product for multi-dimensional Galerkin coarsening, grid complexity and implementation effort are reduced. A unified parallel framework for sparse triangular solvers ensures fast convergence and high parallel efficiency in smoothers. StructMG outperforms existing multigrid preconditioners like \textit{hypre}'s in radiation hydrodynamics, petroleum reservoir simulation, numerical weather prediction, and solid mechanics applications on ARM and X86 platforms, with average speedups of 15.5x, 5.5x, 6.7x, and 7.3x, respectively. Additionally, it improves strong and weak scaling efficiencies significantly.<br /><br />Summary: <div>
arXiv:2506.21932v1 Announce Type: cross 
Abstract: Parallel multigrid is widely used as preconditioners in solving large-scale sparse linear systems. However, the current multigrid library still needs more satisfactory performance for structured grid problems regarding speed and scalability. Based on the classical 'multigrid seesaw', we derive three necessary principles for an efficient structured multigrid, which instructs our design and implementation of StructMG, a fast and scalable algebraic multigrid that constructs hierarchical grids automatically. As a preconditioner, StructMG can achieve both low cost per iteration and good convergence when solving large-scale linear systems with iterative methods in parallel. A stencil-based triple-matrix product via symbolic derivation and code generation is proposed for multi-dimensional Galerkin coarsening to reduce grid complexity, operator complexity, and implementation effort. A unified parallel framework of sparse triangular solver is presented to achieve fast convergence and high parallel efficiency for smoothers, including dependence-preserving Gauss-Seidel and incomplete LU methods. Idealized and real-world problems from radiation hydrodynamics, petroleum reservoir simulation, numerical weather prediction, and solid mechanics, are evaluated on ARM and X86 platforms to show StructMG's effectiveness. In comparison to \textit{hypre}'s structured and general multigrid preconditioners, StructMG achieves the fastest time-to-solutions in all cases with average speedups of 15.5x, 5.5x, 6.7x, 7.3x over SMG, PFMG, SysPFMG, and BoomerAMG, respectively. StructMG also significantly improves strong and weak scaling efficiencies.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EUR/USD Exchange Rate Forecasting incorporating Text Mining Based on Pre-trained Language Models and Deep Learning Methods</title>
<link>https://arxiv.org/abs/2411.07560</link>
<guid>https://arxiv.org/abs/2411.07560</guid>
<content:encoded><![CDATA[
<div> Keywords: EUR/USD, exchange rate forecasting, deep learning, textual analysis, particle swarm optimization

Summary: 

This study presents a new approach for forecasting the EUR/USD exchange rate by combining deep learning, textual analysis, and particle swarm optimization. By utilizing online news and analysis texts as qualitative data, the proposed PSO-LSTM model demonstrates superior accuracy compared to traditional models. The research incorporates advanced text mining techniques such as sentiment analysis and topic modeling. Empirical results show that integrating textual data significantly improves forecasting performance, with the PSO-LSTM model outperforming benchmark models. Ablation experiments highlight the contribution of each textual data category to the overall accuracy. The study emphasizes the potential of artificial intelligence in finance and suggests future research avenues for real-time forecasting and alternative data integration. 

<br /><br />Summary: <div>
arXiv:2411.07560v2 Announce Type: replace 
Abstract: This study introduces a novel approach for EUR/USD exchange rate forecasting that integrates deep learning, textual analysis, and particle swarm optimization (PSO). By incorporating online news and analysis texts as qualitative data, the proposed PSO-LSTM model demonstrates superior performance compared to traditional econometric and machine learning models. The research employs advanced text mining techniques, including sentiment analysis using the RoBERTa-Large model and topic modeling with LDA. Empirical findings underscore the significant advantage of incorporating textual data, with the PSO-LSTM model outperforming benchmark models such as SVM, SVR, ARIMA, and GARCH. Ablation experiments reveal the contribution of each textual data category to the overall forecasting performance. The study highlights the transformative potential of artificial intelligence in finance and paves the way for future research in real-time forecasting and the integration of alternative data sources.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Data Quality for AI to AI for Data Quality: A Systematic Review of Tools for AI-Augmented Data Quality Management in Data Warehouses</title>
<link>https://arxiv.org/abs/2406.10940</link>
<guid>https://arxiv.org/abs/2406.10940</guid>
<content:encoded><![CDATA[
<div> tools, AI-augmented data quality management, data warehouses, automation capabilities, DQ rules<br />
Summary:<br />
The study evaluates the automation capabilities of 151 data quality tools for AI-augmented data quality management in data warehouse environments. Only 10 tools were found suitable, focusing more on data cleansing and preparation for AI rather than improving DQ using AI. Features like SQL-based rule specification, reconciliation logic, and explainability of AI-driven recommendations are lacking. The study provides guidance for tool selection and highlights design requirements for next-generation AI-driven DQ solutions. It advocates a shift from "data quality for AI" to "AI for data quality management".<br /> <div>
arXiv:2406.10940v3 Announce Type: replace-cross 
Abstract: While high data quality (DQ) is critical for analytics, compliance, and AI performance, data quality management (DQM) remains a complex, resource-intensive, and often manual process. This study investigates the extent to which existing tools support AI-augmented data quality management (DQM) in data warehouse environments. To this end, we conduct a systematic review of 151 DQ tools to evaluate their automation capabilities, particularly in detecting and recommending DQ rules in data warehouses -- a key component of modern data ecosystems. Using a multi-phase screening process based on functionality, trialability, regulatory compliance (e.g., GDPR), and architectural compatibility with data warehouses, only 10 tools met the criteria for AI-augmented DQM. The analysis reveals that most tools emphasize data cleansing and preparation for AI, rather than leveraging AI to improve DQ itself. Although metadata- and ML-based rule detection techniques are present, features such as SQL-based rule specification, reconciliation logic, and explainability of AI-driven recommendations remain scarce. This study offers practical guidance for tool selection and outlines critical design requirements for next-generation AI-driven DQ solutions -- advocating a paradigm shift from ``data quality for AI'' to ``AI for data quality management''.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EUR-USD Exchange Rate Forecasting Based on Information Fusion with Large Language Models and Deep Learning Methods</title>
<link>https://arxiv.org/abs/2408.13214</link>
<guid>https://arxiv.org/abs/2408.13214</guid>
<content:encoded><![CDATA[
<div> Keywords: EUR/USD exchange rate, forecasting, textual data, structured data, Optuna-Bi-LSTM model 

Summary: 
The paper introduces the IUS framework, a novel approach for enhancing EUR/USD exchange rate prediction by integrating unstructured textual data with structured data. This framework utilizes large language models for sentiment analysis and exchange rate movement classification, combined with quantitative features in a Causality-Driven Feature Generator. An Optuna-optimized Bi-LSTM model is then employed for forecasting. Experimental results show that the proposed method outperforms benchmark models, achieving a 10.69% reduction in MAE and a 9.56% decrease in RMSE. The fusion of unstructured and structured data proves beneficial, with the combined approach yielding higher accuracy than using structured data alone. Feature selection highlights the importance of the top 12 quantitative features combined with textual features. Overall, the IUS framework and Optuna-Bi-LSTM model offer a robust and effective method for EUR/USD exchange rate forecasting through the integration of multi-source data. 

<br /><br />Summary: <div>
arXiv:2408.13214v2 Announce Type: replace-cross 
Abstract: Accurate forecasting of the EUR/USD exchange rate is crucial for investors, businesses, and policymakers. This paper proposes a novel framework, IUS, that integrates unstructured textual data from news and analysis with structured data on exchange rates and financial indicators to enhance exchange rate prediction. The IUS framework employs large language models for sentiment polarity scoring and exchange rate movement classification of texts. These textual features are combined with quantitative features and input into a Causality-Driven Feature Generator. An Optuna-optimized Bi-LSTM model is then used to forecast the EUR/USD exchange rate. Experiments demonstrate that the proposed method outperforms benchmark models, reducing MAE by 10.69% and RMSE by 9.56% compared to the best performing baseline. Results also show the benefits of data fusion, with the combination of unstructured and structured data yielding higher accuracy than structured data alone. Furthermore, feature selection using the top 12 important quantitative features combined with the textual features proves most effective. The proposed IUS framework and Optuna-Bi-LSTM model provide a powerful new approach for exchange rate forecasting through multi-source data integration.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion</title>
<link>https://arxiv.org/abs/2506.20763</link>
<guid>https://arxiv.org/abs/2506.20763</guid>
<content:encoded><![CDATA[
<div> formulation, phase field, multi-physics, finite element, engineering

Summary: 
This article introduces a new formulation to address coupled structural integrity problems by combining phase field and multi-physics modeling techniques. The approach leverages the flexibility of the heat transfer equation, making it suitable for integration into commercial finite element packages with only integration point-level implementation. The methodology is demonstrated through the implementation of coupled phenomena using user subroutines in the Abaqus finite element package. The theoretical and computational framework is applied to four engineering and scientific problems: thermo-mechanical fracture, hydraulic fracture, hydrogen-assisted cracking, and metallic corrosion, in both 2D and 3D scenarios. The results obtained show excellent agreement with experimental, numerical, and analytical solutions. The user subroutines developed for implementation are freely accessible online, enhancing the accessibility and usability of the proposed approach. <div>
arXiv:2506.20763v1 Announce Type: new 
Abstract: We present a novel, generalised formulation to treat coupled structural integrity problems by combining phase field and multi-physics modelling. The approach exploits the versatility of the heat transfer equation and is therefore well suited to be adopted in commercial finite element packages, requiring only integration point-level implementation. This aspect is demonstrated here by implementing coupled, multi-variable phenomena through simple \texttt{UMAT} and \texttt{UMATHT} subroutines in the finite element package \texttt{Abaqus}. The generalised theoretical and computational framework presented is particularised to four problems of engineering and scientific relevance: thermo-mechanical fracture, hydraulic fracture, hydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are considered. The results reveal a very good agreement with experimental data, and existing numerical and analytical solutions.The user subroutines developed are made freely available at https://mechmat.web.ox.ac.uk/codes.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers</title>
<link>https://arxiv.org/abs/2506.20773</link>
<guid>https://arxiv.org/abs/2506.20773</guid>
<content:encoded><![CDATA[
<div> Keywords: viscoelasticity, permanent set, polymers, transient network theory, numerical framework

Summary:
An efficient numerical framework for modeling viscoelasticity and permanent set of polymers is presented. The framework is based on the hereditary integral form of transient network theory, where polymer chains belong to distinct networks with different natural equilibrium states. Chains detach from previous networks and attach to new networks in a state of zero stress. The free energy of these networks is defined in terms of the deformation gradient relative to the network's birth configuration. A decomposition of the kernel for various free energies allows for a recurrence relationship without the need for full-time history integration. The technique applies to both highly compressible and nearly incompressible materials using various material models. The framework can handle rate-dependent response and residual strains under complex loading histories, as demonstrated through multiple examples. <div>
arXiv:2506.20773v1 Announce Type: new 
Abstract: An efficient numerical framework is presented for modeling viscoelasticity and permanent set of polymers. It is based on the hereditary integral form of transient network theory, in which polymer chains belong to distinct networks each with different natural equilibrium states. Chains continually detach from previously formed networks and reattach to new networks in a state of zero stress. The free energy of these networks is given in terms of the deformation gradient relative to the configuration at which the network was born. A decomposition of the kernel for various free energies allows for a recurrence relationship to be established, bypassing the need to integrate over all time history. The technique is established for both highly compressible and nearly incompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and Ogden-Hill material models. Multiple examples are presented showing the ability to handle rate-dependent response and residual strains under complex loading histories.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation</title>
<link>https://arxiv.org/abs/2506.21362</link>
<guid>https://arxiv.org/abs/2506.21362</guid>
<content:encoded><![CDATA[
<div> Counterfactual Voting Adjustment, online platforms, information quality, helpfulness voting, content ranking  
Summary:  
The article introduces the Counterfactual Voting Adjustment (CVA) framework for fairer assessment of information quality on online platforms. CVA addresses biases in aggregated votes by considering the context of individual votes, effectively modeling position and herding biases. Preliminary experiments demonstrate that CVA accurately recovers predefined content quality. In a real experiment, reranking content based on CVA-learned quality aligns better with user sentiment and quality evaluation by GPT-4o compared to rankings based on aggregated votes. The article also offers insights into the behavioral dynamics of expert user groups across 120 StackExchange communities using CVA embeddings. <div>
arXiv:2506.21362v1 Announce Type: new 
Abstract: Efficient access to high-quality information is vital for online platforms. To promote more useful information, users not only create new content but also evaluate existing content, often through helpfulness voting. Although aggregated votes help service providers rank their user content, these votes are often biased by disparate accessibility per position and the cascaded influence of prior votes. For a fairer assessment of information quality, we propose the Counterfactual Voting Adjustment (CVA), a causal framework that accounts for the context in which individual votes are cast. Through preliminary and semi-synthetic experiments, we show that CVA effectively models the position and herding biases, accurately recovering the predefined content quality. In a real experiment, we demonstrate that reranking content based on the learned quality by CVA exhibits stronger alignment with both user sentiment and quality evaluation assessed by GPT-4o, outperforming system rankings based on aggregated votes and model-based rerankings without causal inference. Beyond the individual quality inference, our embeddings offer comparative insights into the behavioral dynamics of expert user groups across 120 major StackExchange communities.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools</title>
<link>https://arxiv.org/abs/2506.20743</link>
<guid>https://arxiv.org/abs/2506.20743</guid>
<content:encoded><![CDATA[
<div> Foundation models, materials science, AI systems, scientific discovery, multimodal. 

Summary:
Foundation models are revolutionizing materials science by enabling scalable, versatile AI systems for scientific research. These models offer cross-domain generalization and emergent capabilities, making them ideal for the diverse challenges in materials science. The survey covers various application areas, including data extraction, atomistic simulation, property prediction, materials design, process planning, and multiscale modeling. It discusses advancements in unimodal and multimodal foundation models, as well as large language model agents. Standardized datasets, open-source tools, and autonomous experimental platforms are also reviewed. While foundation models have shown early success, challenges remain in generalizability, interpretability, data imbalance, safety, and multimodal fusion. Future research directions focus on scalable pretraining, continual learning, data governance, and trustworthiness.<br /><br />Summary: <div>
arXiv:2506.20743v1 Announce Type: cross 
Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\&amp;A atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pull-off strength of mushroom-shaped fibrils adhered to rigid substrates</title>
<link>https://arxiv.org/abs/2506.20745</link>
<guid>https://arxiv.org/abs/2506.20745</guid>
<content:encoded><![CDATA[
<div> Keywords: adhesion, fibrillar structures, mushroom-shaped fibrils, computational analysis, cohesive zone model

Summary: 
- The study focuses on analyzing the detachment behavior of mushroom-shaped fibrils adhered to a rigid substrate using a computational approach based on a Dugdale cohesive zone model.
- The results indicate that the separation process during detachment is inherently unstable under load control, regardless of whether detachment initiates at the edge or center of the fibril.
- Fibrils with wide, thin mushroom caps demonstrate enhanced adhesion by reducing stress concentrations and promoting central detachment, leading to improved adhesion strength.
- While central detachment is not observed in all geometries, edge detachment can occur under specific conditions in all cases.
- Adhesion defects at the fibril center can greatly reduce pull-off strength, especially at high values of the dimensionless parameter \c{hi}.

<br /><br />Summary: <div>
arXiv:2506.20745v1 Announce Type: cross 
Abstract: The exceptional adhesion properties of biological fibrillar structures -- such as those found in geckos -- have inspired the development of synthetic adhesive surfaces. Among these, mushroom-shaped fibrils have demonstrated superior pull-off strength compared to other geometries. In this study, we employ a computational approach based on a Dugdale cohesive zone model to analyze the detachment behavior of these fibrils when adhered to a rigid substrate. The results provide complete pull-off curves, revealing that the separation process is inherently unstable under load control, regardless of whether detachment initiates at the fibril edge or center. Our findings show that fibrils with a wide, thin mushroom cap effectively reduce stress concentrations and promote central detachment, leading to enhanced adhesion. However, detachment from the center is not observed in all geometries, whereas edge detachment can occur under certain conditions in all cases. Additionally, we investigate the impact of adhesion defects at the fibril center, showing that they can significantly reduce pull-off strength, particularly at high values of the dimensionless parameter \c{hi}. These insights contribute to the optimization of bio-inspired adhesives and microstructured surfaces for various engineering applications.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering</title>
<link>https://arxiv.org/abs/2506.20821</link>
<guid>https://arxiv.org/abs/2506.20821</guid>
<content:encoded><![CDATA[
<div> Keywords: financial documents, multimodal extraction, retrieval-augmented generation, cross-modal reasoning, MultiFinRAG

Summary: 
- Financial documents, such as 10-Ks and investor presentations, are complex and require joint reasoning across modalities.
- MultiFinRAG is a framework designed for financial question answering that integrates multimodal extraction and retrieval-augmented generation.
- The framework utilizes a lightweight multimodal LLM for extracting structured outputs and textual summaries from tables and figures.
- MultiFinRAG indexes these outputs with modality-aware similarity thresholds for precise retrieval and employs a tiered fallback strategy for cross-modal reasoning.
- Despite running on basic hardware, MultiFinRAG outperforms ChatGPT-4o in accuracy by 19 percentage points on financial QA tasks involving multiple modalities. 

<br /><br />Summary: <div>
arXiv:2506.20821v1 Announce Type: cross 
Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span hundreds of pages and combine diverse modalities, including dense narrative text, structured tables, and complex figures. Answering questions over such content often requires joint reasoning across modalities, which strains traditional large language models (LLMs) and retrieval-augmented generation (RAG) pipelines due to token limitations, layout loss, and fragmented cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation framework purpose-built for financial QA. MultiFinRAG first performs multimodal extraction by grouping table and figure images into batches and sending them to a lightweight, quantized open-source multimodal LLM, which produces both structured JSON outputs and concise textual summaries. These outputs, along with narrative text, are embedded and indexed with modality-aware similarity thresholds for precise retrieval. A tiered fallback strategy then dynamically escalates from text-only to text+table+image contexts when necessary, enabling cross-modal reasoning while reducing irrelevant context. Despite running on commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy than ChatGPT-4o (free-tier) on complex financial QA tasks involving text, tables, images, and combined multimodal reasoning.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multicontinuum Homogenization for Poroelasticity Model</title>
<link>https://arxiv.org/abs/2506.20890</link>
<guid>https://arxiv.org/abs/2506.20890</guid>
<content:encoded><![CDATA[
<div> homogenization, poroelasticity, multicontinuum, porous media, computational challenges  
Summary:
In this paper, the authors derive multicontinuum poroelasticity models using the multicontinuum homogenization method. Poroelasticity models are essential in various fields to describe coupled flow and mechanics in porous media. The high contrast properties of poroelastic media create computational challenges, which standard homogenization approaches struggle to address. By employing multicontinuum methods, multiple average states called continua are defined, allowing for accurate solutions in cases of high property contrast. The authors extend previous research by deriving a generalized multicontinuum poroelasticity model using a rigorous approach. They formulate coupled constraint cell problems in oversampled regions, obtain a multicontinuum expansion of fine-scale fields, and derive the multicontinuum model while assuming the smoothness of macroscopic variables. Numerical experiments confirm the accuracy of the proposed multicontinuum models for various heterogeneous media cases. <div>
arXiv:2506.20890v1 Announce Type: cross 
Abstract: In this paper, we derive multicontinuum poroelasticity models using the multicontinuum homogenization method. Poroelasticity models are widely used in many areas of science and engineering to describe coupled flow and mechanics processes in porous media. However, in many applications, the properties of poroelastic media possess high contrast, presenting serious computational challenges. It is well known that standard homogenization approaches often fail to give an accurate solution due to the lack of macroscopic parameters. Multicontinuum approaches allow us to consider such cases by defining several average states known as continua. In the field of poroelasticity, multiple-network models arising from the multiple porous media theory are representatives of these approaches. In this work, we extend previous findings by deriving the generalized multicontinuum poroelasticity model. We apply the recently developed multicontinuum homogenization method and provide a rigorous derivation of multicontinuum equations. For this purpose, we formulate coupled constraint cell problems in oversampled regions to consider different homogenized effects. Then, we obtain a multicontinuum expansion of the fine-scale fields and derive the multicontinuum model supposing the smoothness of macroscopic variables. We present the most general version of equations and the simplified ones based on our numerical experiments. Numerical results are presented for different heterogeneous media cases and demonstrate the high accuracy of our proposed multicontinuum models.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-guided Chemical Process Optimization with a Multi-Agent Approach</title>
<link>https://arxiv.org/abs/2506.20921</link>
<guid>https://arxiv.org/abs/2506.20921</guid>
<content:encoded><![CDATA[
<div> framework, optimization, constraints, autonomous, process <br />
Summary: <br />
The article presents a novel approach to chemical process optimization using a multi-agent framework of large language model agents. This framework autonomously infers operating constraints from minimal process descriptions and collaboratively guides optimization. By employing OpenAI's o3 model, the AutoGen-based framework eliminates the need for predefined operational bounds, achieving better computational efficiency and competitive performance with conventional methods. Validated on the hydrodealkylation process, the framework demonstrated a 31-fold speedup over grid search, converging in under 20 minutes. The reasoning-guided search showcases sophisticated process understanding, correctly identifying utility trade-offs and applying domain-informed heuristics. This approach shows significant potential for optimization scenarios with poorly characterized or unavailable constraints, particularly in emerging processes and retrofit applications. <br /> <div>
arXiv:2506.20921v1 Announce Type: cross 
Abstract: Chemical process optimization is crucial to maximize production efficiency and economic performance. Traditional methods, including gradient-based solvers, evolutionary algorithms, and parameter grid searches, become impractical when operating constraints are ill-defined or unavailable, requiring engineers to rely on subjective heuristics to estimate feasible parameter ranges. To address this constraint definition bottleneck, we present a multi-agent framework of large language model (LLM) agents that autonomously infer operating constraints from minimal process descriptions, then collaboratively guide optimization using the inferred constraints. Our AutoGen-based agentic framework employs OpenAI's o3 model, with specialized agents for constraint generation, parameter validation, simulation execution, and optimization guidance. Through two phases - autonomous constraint generation using embedded domain knowledge, followed by iterative multi-agent optimization - the framework eliminates the need for predefined operational bounds. Validated on the hydrodealkylation process across cost, yield, and yield-to-cost ratio metrics, the framework demonstrated competitive performance with conventional optimization methods while achieving better computational efficiency, requiring fewer iterations to converge. Our approach converged in under 20 minutes, achieving a 31-fold speedup over grid search. Beyond computational efficiency, the framework's reasoning-guided search demonstrates sophisticated process understanding, correctly identifying utility trade-offs, and applying domain-informed heuristics. This approach shows significant potential for optimization scenarios where operational constraints are poorly characterized or unavailable, particularly for emerging processes and retrofit applications.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Institutional Noise, Strategic Deviation, and Intertemporal Collapse: A Formal Model of Miner Behaviour under Protocol Uncertainty</title>
<link>https://arxiv.org/abs/2506.20992</link>
<guid>https://arxiv.org/abs/2506.20992</guid>
<content:encoded><![CDATA[
<div> blockchain, game theory, protocol mutability, institutional rules, cooperative mining

Summary:
Using a game-theoretic model, this paper examines the impact of protocol mutability on cooperative mining behavior in blockchain systems. It finds that uncertainty in institutional rules leads to higher discounting and strategic deviation among miners. Stable protocols support long-term investment and equilibrium strategies, while mutable protocols promote short-termism and collapse of cooperation. Simulation results highlight zones of instability where rational mining transitions to extractive practices. The study suggests that stable rules are crucial for productive action and sustainable cooperation in decentralized systems. Protocol design should be viewed as a fundamental economic constraint rather than a discretionary variable to foster sustainable cooperation. <div>
arXiv:2506.20992v1 Announce Type: cross 
Abstract: This paper develops a formal game-theoretic model to examine how protocol mutability disrupts cooperative mining behaviour in blockchain systems. Using a repeated game framework with stochastic rule shocks, we show that even minor uncertainty in institutional rules increases time preference and induces strategic deviation. Fixed-rule environments support long-term investment and stable equilibrium strategies; in contrast, mutable protocols lead to short-termism, higher discounting, and collapse of coordinated engagement. Simulation results identify instability zones in the parameter space where rational mining gives way to extractive or arbitrage conduct. These findings support an Austrian economic interpretation: calculability requires rule stability. Institutional noise undermines the informational basis for productive action. We conclude that protocol design must be treated as a constitutional economic constraint, not a discretionary variable, if sustainable cooperation is to emerge in decentralised systems.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Inverse Problem for Multi-armed Bandits via Convex Optimization</title>
<link>https://arxiv.org/abs/2501.18945</link>
<guid>https://arxiv.org/abs/2501.18945</guid>
<content:encoded><![CDATA[
<div> Keywords: IMAB, multi-armed bandits, convex optimization, heuristic method, neuroscience<br />
<br />
Summary: 
The article focuses on the inverse problem of multi-armed bandits (IMAB) and its application in neuroscience and psychology research. It demonstrates that the IMAB problem is not convex but can be transformed into a convex problem for easier solution. A two-step sequential heuristic method is proposed to approximately solve the IMAB problem efficiently. The method is shown to provide global solutions with a certificate under certain conditions and offers approximations to reduce computational time. Numerical experiments indicate that the proposed heuristic method outperforms traditional local optimization approaches and matches the performance of Monte Carlo methods in a shorter time frame. The method is implemented using CVXPY, making it accessible to users without expertise in convex optimization. <div>
arXiv:2501.18945v3 Announce Type: replace 
Abstract: We consider the inverse problem of multi-armed bandits (IMAB) that are widely used in neuroscience and psychology research for behavior modelling. We first show that the IMAB problem is not convex in general, but can be relaxed to a convex problem via variable transformation. Based on this result, we propose a two-step sequential heuristic for (approximately) solving the IMAB problem. We discuss a condition where our method provides global solution to the IMAB problem with certificate, as well as approximations to further save computing time. Numerical experiments indicate that our heuristic method is more robust than directly solving the IMAB problem via repeated local optimization, and can achieve the performance of Monte Carlo methods within a significantly decreased running time. We provide the implementation of our method based on CVXPY, which allows straightforward application by users not well versed in convex optimization.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Reinforcement Learning via Convex Optimization</title>
<link>https://arxiv.org/abs/2501.15957</link>
<guid>https://arxiv.org/abs/2501.15957</guid>
<content:encoded><![CDATA[
<div> Convex Optimization, Inverse Reinforcement Learning, Expert Demonstrations, Markov Decision Process, Auto-selection<br />
Summary: 
This article discusses the Inverse Reinforcement Learning (IRL) problem and introduces a convex formulation of the problem known as CIRL. The authors present a method to apply the domain-specific language CVXPY to solve the convex IRL problem, making it more accessible for users without a background in convex optimization. They also address scenarios where the expert policy is provided as state-action pairs rather than analytically, extending the CIRL problem to handle such cases. Theoretical analysis and practical implementation for hyperparameter auto-selection are discussed, allowing users to easily apply CIRL to their specific problems. This approach aims to overcome the challenges posed by traditional nonconvex IRL formulations, offering a more robust and reproducible solution for estimating unknown reward functions in Markov decision processes based on expert demonstrations. <div>
arXiv:2501.15957v2 Announce Type: replace-cross 
Abstract: We consider the inverse reinforcement learning (IRL) problem, where an unknown reward function of some Markov decision process is estimated based on observed expert demonstrations. In most existing approaches, IRL is formulated and solved as a nonconvex optimization problem, posing challenges in scenarios where robustness and reproducibility are critical. We discuss a convex formulation of the IRL problem (CIRL) initially proposed by Ng and Russel, and reformulate the problem such that the domain-specific language CVXPY can be applied directly to specify and solve the convex problem. We also extend the CIRL problem to scenarios where the expert policy is not given analytically but by trajectory as state-action pairs, which can be strongly inconsistent with optimality, by augmenting some of the constraints. Theoretical analysis and practical implementation for hyperparameter auto-selection are introduced. This note helps the users to easily apply CIRL for their problems, without background knowledge on convex optimization.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena Perspective</title>
<link>https://arxiv.org/abs/2503.18313</link>
<guid>https://arxiv.org/abs/2503.18313</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial decision-making, DeepFund, multi-agent framework, live environment

Summary:
DeepFund addresses the inadequately evaluated effectiveness of Large Language Models (LLMs) in financial decision-making by introducing a comprehensive arena platform for evaluating LLM-based trading strategies. The platform implements a multi-agent framework that simulates real-world investment decision processes, allowing LLMs to take on different key roles in managing assets and uncovering trading opportunities. DeepFund offers a web interface for visualizing LLMs' performance with fund investment metrics across various market conditions, enabling detailed comparative analysis. The platform aims to provide a more realistic and fair assessment of LLM capabilities in fund investment, addressing issues such as data leakage, navel-gazing, over-intervention, and maintenance-hardness identified in existing benchmarks. By offering diversified insights and potential applications in real-world financial markets, DeepFund contributes to advancing LLM research in financial decision-making. The code for DeepFund is publicly available on GitHub at https://github.com/HKUSTDial/DeepFund. 

<br /><br />Summary: <div>
arXiv:2503.18313v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision-making remains inadequately evaluated. Current benchmarks primarily assess LLMs' understanding on financial documents rather than the ability to manage assets or dig out trading opportunities in dynamic market conditions. Despite the release of new benchmarks for evaluating diversified tasks on the financial domain, we identified four major problems in these benchmarks, which are data leakage, navel-gazing, over-intervention, and maintenance-hard. To pave the research gap, we introduce DeepFund, a comprehensive arena platform for evaluating LLM-based trading strategies in a live environment. Our approach implements a multi-agent framework where they serve as multiple key roles that realize the real-world investment decision processes. Moreover, we provide a web interface that visualizes LLMs' performance with fund investment metrics across different market conditions, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more realistic and fair assessment on LLM's capabilities in fund investment, offering diversified insights and revealing their potential applications in real-world financial markets. Our code is publicly available at https://github.com/HKUSTDial/DeepFund.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-convex Programming for Discrete Latent Factor Models Prototyping</title>
<link>https://arxiv.org/abs/2504.01431</link>
<guid>https://arxiv.org/abs/2504.01431</guid>
<content:encoded><![CDATA[
<div> CVXPY, discrete latent factor models, fitting problem, regularization terms, constraints
Summary: 
Discrete latent factor models (DLFMs) are commonly used in various fields but require custom solvers for fitting, limiting their flexibility. This paper introduces a generic framework based on CVXPY that enables users to easily specify and solve the fitting problem of a range of DLFMs through a concise script. The framework supports both regression and classification models, integrates regularization terms, and allows for constraints on DLFM parameters and latent factors. This versatility allows users to prototype DLFM structures tailored to their dataset and application needs. The open-source Python implementation of the framework is demonstrated through several examples, showcasing its utility and effectiveness in practice. <div>
arXiv:2504.01431v2 Announce Type: replace-cross 
Abstract: Discrete latent factor models (DLFMs) are widely used in various domains such as machine learning, economics, neuroscience, psychology, etc. Currently, fitting a DLFM to some dataset relies on a customized solver for individual models, which requires lots of effort to implement and is limited to the targeted specific instance of DLFMs. In this paper, we propose a generic framework based on CVXPY, which allows users to specify and solve the fitting problem of a wide range of DLFMs, including both regression and classification models, within a very short script. Our framework is flexible and inherently supports the integration of regularization terms and constraints on the DLFM parameters and latent factors, such that the users can easily prototype the DLFM structure according to their dataset and application scenario. We introduce our open-source Python implementation and illustrate the framework in several examples.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiT-SGCR: Directed Temporal Structural Representation with Global-Cluster Awareness for Ethereum Malicious Account Detection</title>
<link>https://arxiv.org/abs/2506.20123</link>
<guid>https://arxiv.org/abs/2506.20123</guid>
<content:encoded><![CDATA[
<div> Keywords: Ethereum, DeFi, malicious account detection, temporal transaction evolution, unsupervised graph encoder

Summary: 
The article proposes DiT-SGCR, an unsupervised graph encoder designed to detect malicious accounts on the Ethereum platform, focusing on the dynamic transaction behaviors of users. By incorporating directional temporal aggregation, differentiable clustering, and graph Laplacian regularization, DiT-SGCR effectively captures money movement trajectories and organized collective behavior. This approach enhances the quality and dimensionality of account embeddings while maintaining scalability advantages. Through extensive experiments on three datasets, DiT-SGCR consistently outperforms existing methods in malicious account detection, showcasing significant improvements in F1-score accuracy ranging from 3.62% to 10.83%. The research highlights the importance of considering temporal dynamics, global topology, and cluster-specific behavioral patterns in identifying malicious actors within decentralized finance ecosystems. <br /><br />Summary: <div>
arXiv:2506.20123v1 Announce Type: new 
Abstract: The detection of malicious accounts on Ethereum - the preeminent DeFi platform - is critical for protecting digital assets and maintaining trust in decentralized finance. Recent advances highlight that temporal transaction evolution reveals more attack signatures than static graphs. However, current methods either fail to model continuous transaction dynamics or incur high computational costs that limit scalability to large-scale transaction networks. Furthermore, current methods fail to consider two higher-order behavioral fingerprints: (1) direction in temporal transaction flows, which encodes money movement trajectories, and (2) account clustering, which reveals coordinated behavior of organized malicious collectives. To address these challenges, we propose DiT-SGCR, an unsupervised graph encoder for malicious account detection. Specifically, DiT-SGCR employs directional temporal aggregation to capture dynamic account interactions, then coupled with differentiable clustering and graph Laplacian regularization to generate high-quality, low-dimensional embeddings. Our approach simultaneously encodes directional temporal dynamics, global topology, and cluster-specific behavioral patterns, thereby enhancing the discriminability and robustness of account representations. Furthermore, DiT-SGCR bypasses conventional graph propagation mechanisms, yielding significant scalability advantages. Extensive experiments on three datasets demonstrate that DiT-SGCR consistently outperforms state-of-the-art methods across all benchmarks, achieving F1-score improvements ranging from 3.62% to 10.83%.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing Artificial Mechanics Intuitions from Extremely Small Data</title>
<link>https://arxiv.org/abs/2506.20148</link>
<guid>https://arxiv.org/abs/2506.20148</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, mechanics intuitions, sample-switchable training, brachistochrone problem, nonlinear deformation problem

Summary:
The article discusses the development of artificial mechanics intuitions through a sample-switchable training method that allows learning from limited examples. The method enables artificial intelligence to master complex problems like the brachistochrone problem, catenary problem, and large nonlinear deformation problem of an elastic plate using only three samples. The model's intuitive prediction capacity improves non-linearly with the number of training samples, highlighting the possibility of achieving excellent mechanics intuitions with a finite set of examples. This approach offers a new perspective on educating AI systems to intuitively understand and predict material behavior, resembling scenarios often depicted in Science-Fiction movies.
<br /><br />Summary: The article explores the development of artificial mechanics intuitions using sample-switchable training, enabling AI to master complex problems with minimal data. The model's intuitive prediction improves with the number of training samples, demonstrating the potential to achieve exceptional mechanics intuition with a limited dataset. This work presents a novel approach to educating AI systems to intuitively predict material behavior, akin to scenarios in Science-Fiction films. <div>
arXiv:2506.20148v1 Announce Type: new 
Abstract: Humans can possess good mechanics intuitions by learning from a few examples, which leads to the question of how to develop artificial mechanics intuitions that can be learned from small data, as we are eagerly entering the era of artificial intelligence. We propose in this Letter the sample-switchable training method, which successfully develops highly-accurate artificial mechanics intuitions that can master brachistochrone problem, catenary problem, and large nonlinear deformation problem of elastic plate by learning from no more than three samples. The model's intuitive prediction ability increases nonlinearly with respect to the number of training samples, suggesting that superb mechanics intuitions can be in-principle achieved based on a finite number of samples, reflecting how human brains form good mechanics intuitions just by learning a few cases. Our current work presents an alternative perspective for educating artificial intelligence capable of intuitively understand and predict how materials deform and move, a scenario that has been frequently seen in Science-Fiction movies.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion Dynamics with Highly Oscillating Opinions</title>
<link>https://arxiv.org/abs/2506.20472</link>
<guid>https://arxiv.org/abs/2506.20472</guid>
<content:encoded><![CDATA[
<div> Opinion Dynamics; Agent-Based Models; Evolutionary Algorithms; Immigration dataset; ATBCR <br />
Summary: <br />
Opinion Dynamics (OD) models focus on understanding how opinions evolve within a population through interactions between agents. This study addresses the limitations of existing OD models in analyzing scenarios of highly oscillating opinions, such as those found in real-world situations. The research formulates an optimization problem and applies Evolutionary Algorithms to evaluate the performance of various OD models in capturing highly oscillating dynamics. Using a real-world opinion dataset on immigration, the study finds that the ATBCR model, incorporating both rational and emotional opinion update mechanisms, is the most accurate for representing highly oscillating opinions. This research contributes to enhancing the understanding of opinion evolution in complex social contexts. <br /> <div>
arXiv:2506.20472v1 Announce Type: new 
Abstract: Opinion Dynamics (OD) models are a particular case of Agent-Based Models in which the evolution of opinions within a population is studied. In most OD models, opinions evolve as a consequence of interactions between agents, and the opinion fusion rule defines how those opinions are updated. In consequence, despite being simplistic, OD models provide an explainable and interpretable mechanism for understanding the underlying dynamics of opinion evolution. Unfortunately, existing OD models mainly focus on explaining the evolution of (usually synthetic) opinions towards consensus, fragmentation, or polarization, but they usually fail to analyze scenarios of (real-world) highly oscillating opinions. This work overcomes this limitation by studying the ability of several OD models to reproduce highly oscillating dynamics. To this end, we formulate an optimization problem which is further solved using Evolutionary Algorithms, providing both quantitative results on the performance of the optimization and qualitative interpretations on the obtained results. Our experiments on a real-world opinion dataset about immigration from the monthly barometer of the Spanish Sociological Research Center show that the ATBCR, based on both rational and emotional mechanisms of opinion update, is the most accurate OD model for capturing highly oscillating opinions.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ten simple rules for PIs to integrate Research Software Engineering into their research group</title>
<link>https://arxiv.org/abs/2506.20217</link>
<guid>https://arxiv.org/abs/2506.20217</guid>
<content:encoded><![CDATA[
<div> Research Software Engineering, high-quality research software, accessibility, PIs, reproducibility <br />
Summary:
Research Software Engineering (RSEng) plays a crucial role in producing high-quality research software, enhancing research outcomes. However, many principal investigators (PIs) and research group leaders may lack knowledge on RSEng and its benefits. The technical complexities of RSEng can also hinder accessibility for some researchers. To address these challenges, this paper presents ten simple rules to help PIs and leaders integrate RSEng into their research groups more effectively. By following these rules, researchers can improve the quality, reproducibility, and trustworthiness of their research software, ultimately leading to better, more reproducible, and trustworthy research outcomes. <div>
arXiv:2506.20217v1 Announce Type: cross 
Abstract: Research Software Engineering (RSEng) is a key success factor in producing high-quality research software, which in turn enables and improves research outcomes. However, as a principal investigator or leader of a research group you may not know what RSEng is, where to get started with it, or how to use it to maximize its benefit for your research. RSEng also often comes with technical complexity, and therefore reduced accessibility to some researchers. The ten simple rules presented in this paper aim to improve the accessibility of RSEng, and provide practical and actionable advice to PIs and leaders for integrating RSEng into their research group. By following these rules, readers can improve the quality, reproducibility, and trustworthiness of their research software, ultimately leading to better, more reproducible and more trustworthy research outcomes.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem</title>
<link>https://arxiv.org/abs/2506.20400</link>
<guid>https://arxiv.org/abs/2506.20400</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent-based simulations, electric vehicle home charging ecosystems, Python-based dashboard framework, emergent behavior, root-cause analysis

Summary:
This paper introduces a Python-based dashboard framework using Dash by Plotly for analyzing multi-agent-based simulations of electric vehicle home charging ecosystems. The framework consists of three coordinated views for exploring emergent behavior in the simulation outputs, including time-series plots, spatial heatmaps, and drill-down tools. A case study in a Danish residential network with full EV adoption and smart charging showcases the dashboard's ability to identify anomalies like transformer overloads and charging failures. The system enables quick detection and contextual explanation of system-level events that are difficult to detect through traditional post-processing methods. The dashboard provides actionable insights for researchers and distribution system operators, and its adaptable architecture can be used for analyzing other complex energy systems and distributed energy resources. 

<br /><br />Summary: <div>
arXiv:2506.20400v1 Announce Type: cross 
Abstract: Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging ecosystems generate large, complex, and stochastic time-series datasets that capture interactions between households, grid infrastructure, and energy markets. These interactions can lead to unexpected system-level events, such as transformer overloads or consumer dissatisfaction, that are difficult to detect and explain through static post-processing. This paper presents a modular, Python-based dashboard framework, built using Dash by Plotly, that enables efficient, multi-level exploration and root-cause analysis of emergent behavior in MABS outputs. The system features three coordinated views (System Overview, System Analysis, and Consumer Analysis), each offering high-resolution visualizations such as time-series plots, spatial heatmaps, and agent-specific drill-down tools. A case study simulating full EV adoption with smart charging in a Danish residential network demonstrates how the dashboard supports rapid identification and contextual explanation of anomalies, including clustered transformer overloads and time-dependent charging failures. The framework facilitates actionable insight generation for researchers and distribution system operators, and its architecture is adaptable to other distributed energy resources and complex energy systems.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Company Adjustment Matter? Insights from Uplift Modeling on Financial Health</title>
<link>https://arxiv.org/abs/2506.19049</link>
<guid>https://arxiv.org/abs/2506.19049</guid>
<content:encoded><![CDATA[
<div> Keywords: uplift modeling, company adjustment, financial status, time-dependent actions, MTDnet

Summary:
Uplift modeling, using machine learning and deep learning, analyzes the impact of company adjustments on financial status by treating adjustments as interventions. Unlike simpler scenarios, company adjustments involve a series of time-dependent actions, necessitating the consideration of both individual treatment traits and temporal order. Real-world data from Luxembourg is used for experiments, comparing two meta-learners and three existing uplift models with a new framework called MTDnet. Results highlight the importance of timing in estimating the effects of company adjustments, demonstrating the need for models that can account for the time-dependent nature of these interventions.<br /><br />Summary: <div>
arXiv:2506.19049v1 Announce Type: new 
Abstract: Uplift modeling has achieved significant success in various fields, particularly in online marketing. It is a method that primarily utilizes machine learning and deep learning to estimate individual treatment effects. This paper we apply uplift modeling to analyze the effect of company adjustment on their financial status, and we treat these adjustment as treatments or interventions in this study. Although there have been extensive studies and application regarding binary treatments, multiple treatments, and continuous treatments, company adjustment are often more complex than these scenarios, as they constitute a series of multiple time-dependent actions. The effect estimation of company adjustment needs to take into account not only individual treatment traits but also the temporal order of this series of treatments. This study collects a real-world data set about company financial statements and reported behavior in Luxembourg for the experiments. First, we use two meta-learners and three other well-known uplift models to analyze different company adjustment by simplifying the adjustment as binary treatments. Furthermore, we propose a new uplift modeling framework (MTDnet) to address the time-dependent nature of these adjustment, and the experimental result shows the necessity of considering the timing of these adjustment.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LKA: Large Kernel Adapter for Enhanced Medical Image Classification</title>
<link>https://arxiv.org/abs/2506.19118</link>
<guid>https://arxiv.org/abs/2506.19118</guid>
<content:encoded><![CDATA[
<div> Keyword: Parameter-Efficient Fine-Tuning, Medical Image Analysis, Large Kernel Adapter, Receptive Field, State-of-the-art <br />
Summary:
A new approach called the Large Kernel Adapter (LKA) is introduced to improve Parameter-Efficient Fine-Tuning (PEFT) methods for medical image analysis. The challenge in medical datasets lies in the anatomical variation and low contrast, necessitating larger receptive fields. The LKA comprises down-projection, channel-wise large kernel convolution, and up-projection to enhance adaptation. It outperforms 11 existing PEFT methods and achieves a 3.5% higher top-1 accuracy across five medical datasets, surpassing the state-of-the-art. The key factors tackled include the need for larger receptive fields in medical images and the lack of explicit enhancement in existing PEFT methods. The incorporation of a larger kernel size proves pivotal in improving pre-trained model adaptation for medical image analysis. The proposed LKA demonstrates significant advancements in parameter efficiency and performance metrics for medical image datasets. <br /><br />Summary: <div>
arXiv:2506.19118v1 Announce Type: new 
Abstract: Despite the notable success of current Parameter-Efficient Fine-Tuning (PEFT) methods across various domains, their effectiveness on medical datasets falls short of expectations. This limitation arises from two key factors: (1) medical images exhibit extensive anatomical variation and low contrast, necessitating a large receptive field to capture critical features, and (2) existing PEFT methods do not explicitly address the enhancement of receptive fields. To overcome these challenges, we propose the Large Kernel Adapter (LKA), designed to expand the receptive field while maintaining parameter efficiency. The proposed LKA consists of three key components: down-projection, channel-wise large kernel convolution, and up-projection. Through extensive experiments on various datasets and pre-trained models, we demonstrate that the incorporation of a larger kernel size is pivotal in enhancing the adaptation of pre-trained models for medical image analysis. Our proposed LKA outperforms 11 commonly used PEFT methods, surpassing the state-of-the-art by 3.5% in top-1 accuracy across five medical datasets.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Industrial Gas Turbines: Recent Trends, Advancements and Challenges</title>
<link>https://arxiv.org/abs/2506.19503</link>
<guid>https://arxiv.org/abs/2506.19503</guid>
<content:encoded><![CDATA[
<div> PINNs, Industrial Gas Turbines, aerodynamics, aeromechanical, deep learning<br />
<br />
Summary: Physics-Informed Neural Networks (PINNs) are a computational framework combining deep learning with physical constraints to solve differential equations. This survey specifically focuses on their application in Industrial Gas Turbines (IGTs). PINNs have shown promise in analyzing aerodynamic and aeromechanical phenomena in gas turbines, as well as in tasks such as flow field reconstruction, fatigue evaluation, and flutter prediction. Recent advancements in accuracy, computational efficiency, and hybrid modelling strategies have been reviewed. The survey also discusses challenges in implementation and future research directions aimed at enhancing the robustness and scalability of PINNs in the context of IGTs. <div>
arXiv:2506.19503v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising computational framework for solving differential equations by integrating deep learning with physical constraints. However, their application in gas turbines is still in its early stages, requiring further refinement and standardization for wider adoption. This survey provides a comprehensive review of PINNs in Industrial Gas Turbines (IGTs) research, highlighting their contributions to the analysis of aerodynamic and aeromechanical phenomena, as well as their applications in flow field reconstruction, fatigue evaluation, and flutter prediction, and reviews recent advancements in accuracy, computational efficiency, and hybrid modelling strategies. In addition, it explores key research efforts, implementation challenges, and future directions aimed at improving the robustness and scalability of PINNs.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spline-Based Stress Function Approach for the Principle of Minimum Complementary Energy</title>
<link>https://arxiv.org/abs/2506.19534</link>
<guid>https://arxiv.org/abs/2506.19534</guid>
<content:encoded><![CDATA[
<div> spline-based stress function, computational engineering, finite element methods, stress prediction, structural analysis <br />
Summary:<br />
This article introduces a novel numerical approach for accurate stress prediction in computational engineering applications. The current methods have limitations in predicting stress accurately and efficiently for complex geometries and boundary conditions due to the high number of unknowns required. The proposed method is based on a spline-based stress function formulation for the principle of minimum complementary energy, applied to plane, linear elastostatics. It is validated against analytical solutions and tested on challenging test cases showing promising results. The method offers improved flexibility, accuracy, and efficiency in stress prediction for various geometries and boundary conditions. It achieves comparable stress accuracy to displacement-based finite element methods while requiring fewer degrees of freedom, making it a valuable tool for structural analysis and numerical design.<br /> <div>
arXiv:2506.19534v1 Announce Type: new 
Abstract: In computational engineering, ensuring the integrity and safety of structures in fields such as aerospace and civil engineering relies on accurate stress prediction. However, analytical methods are limited to simple test cases, and displacement-based finite element methods (FEMs), while commonly used, require a large number of unknowns to achieve high accuracy; stress-based numerical methods have so far failed to provide a simple and effective alternative. This work aims to develop a novel numerical approach that overcomes these limitations by enabling accurate stress prediction with improved flexibility for complex geometries and boundary conditions and fewer degrees of freedom (DOFs). The proposed method is based on a spline-based stress function formulation for the principle of minimum complementary energy, which we apply to plane, linear elastostatics. The method is first validated against an analytical power series solution and then tested on two test cases challenging for current state-of-the-art numerical schemes, a bi-layer cantilever with anisotropic material behavior, and a cantilever with a non-prismatic, parabolic-shaped beam geometry. Results demonstrate that our approach, unlike analytical methods, can be easily applied to general geometries and boundary conditions, and achieves stress accuracy comparable to that reported in the literature for displacement-based FEMs, while requiring significantly fewer DOFs. This novel spline-based stress function approach thus provides an efficient and flexible tool for accurate stress prediction, with promising applications in structural analysis and numerical design.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and Diagnosis</title>
<link>https://arxiv.org/abs/2506.19610</link>
<guid>https://arxiv.org/abs/2506.19610</guid>
<content:encoded><![CDATA[
<div> localization, multimodal techniques, medical visual question answering, vision to text chain-of-thought, reasoning pathways

Summary:
From Vision to Text Chain-of-Thought (V2T-CoT) is a new method for Medical Visual Question Answering (Med-VQA) that focuses on localizing disease-specific regions in biomedical images for more accurate diagnosis. V2T-CoT automates this localization process and integrates it into the reasoning pathway, balancing answer accuracy and clinical decision-making. By fine-tuning on the R-Med 39K dataset, V2T-CoT provides precise medical reasoning paths, combining visual grounding with textual rationale generation for explainable diagnostic results. Experimental results across four Med-VQA benchmarks show that V2T-CoT achieves state-of-the-art performance, improving both performance and interpretability significantly. <div>
arXiv:2506.19610v1 Announce Type: new 
Abstract: Recent advances in multimodal techniques have led to significant progress in Medical Visual Question Answering (Med-VQA). However, most existing models focus on global image features rather than localizing disease-specific regions crucial for diagnosis. Additionally, current research tends to emphasize answer accuracy at the expense of the reasoning pathway, yet both are crucial for clinical decision-making. To address these challenges, we propose From Vision to Text Chain-of-Thought (V2T-CoT), a novel approach that automates the localization of preference areas within biomedical images and incorporates this localization into region-level pixel attention as knowledge for Vision CoT. By fine-tuning the vision language model on constructed R-Med 39K dataset, V2T-CoT provides definitive medical reasoning paths. V2T-CoT integrates visual grounding with textual rationale generation to establish precise and explainable diagnostic results. Experimental results across four Med-VQA benchmarks demonstrate state-of-the-art performance, achieving substantial improvements in both performance and interpretability.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReLink: Computational Circular Design of Planar Linkage Mechanisms Using Available Standard Parts</title>
<link>https://arxiv.org/abs/2506.19657</link>
<guid>https://arxiv.org/abs/2506.19657</guid>
<content:encoded><![CDATA[
<div> Circular Economy, sustainability, planar linkage mechanisms, generative design algorithm, CO2 footprint

Summary:
ReLink is a computational framework focusing on circular design for planar linkage mechanisms, promoting sustainability by reusing standardized parts. It consists of design generation and inverse design components, aiming to minimize the need for new parts and prioritize reuse. Trade-offs between kinematic performance and CO2 footprint are analyzed when incorporating new parts. The framework addresses challenges like the combinatorial nature of the design problem and ensuring valid solutions. By combining sustainability principles with kinematic synthesis, ReLink lays the foundation for further research in computational circular design, supporting the integration of reused components into mechanical products.<br /><br />Summary: <div>
arXiv:2506.19657v1 Announce Type: new 
Abstract: The Circular Economy framework emphasizes sustainability by reducing resource consumption and waste through the reuse of components and materials. This paper presents ReLink, a computational framework for the circular design of planar linkage mechanisms using available standard parts. Unlike most mechanism design methods, which assume the ability to create custom parts and infinite part availability, ReLink prioritizes the reuse of discrete, standardized components, thus minimizing the need for new parts. The framework consists of two main components: design generation, where a generative design algorithm generates mechanisms from an inventory of available parts, and inverse design, which uses optimization methods to identify designs that match a user-defined trajectory curve. The paper also examines the trade-offs between kinematic performance and CO2 footprint when incorporating new parts. Challenges such as the combinatorial nature of the design problem and the enforcement of valid solutions are addressed. By combining sustainability principles with kinematic synthesis, ReLink lays the groundwork for further research into computational circular design to support the development of systems that integrate reused components into mechanical products.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modular and extensible library for parameterized terrain generation</title>
<link>https://arxiv.org/abs/2506.19751</link>
<guid>https://arxiv.org/abs/2506.19751</guid>
<content:encoded><![CDATA[
<div> terrain generation, procedural, Python, simulation-driven, reproducibility <br />
<br />
The article introduces a modular Python library for procedural terrain generation, focusing on controllable and well-defined artificial terrains for simulation-driven development of intelligent machines. The library allows users to create complex, parameterized terrains by combining simple modules, supporting both structured and noise-based terrain elements. It integrates with Blender for rendering and object placement, making it suitable for applications like training machine learning models or perception tasks. The system prioritizes reproducibility, variation, and easy integration with automated pipelines by offering fine-grained control over terrain features like slope, roughness, and object placement. Overall, the library's minimal yet extensible set of modules enables users to achieve high flexibility in terrain generation while maintaining simplicity in configuration and expansion.<br /><br />Summary: <div>
arXiv:2506.19751v1 Announce Type: new 
Abstract: Simulation-driven development of intelligent machines benefits from artificial terrains with controllable, well-defined characteristics. However, most existing tools for terrain generation focus on artist-driven workflows and visual realism, with limited support for parameterization, reproducibility, or scripting. We present a modular, Python-based library for procedural terrain generation that enables users to construct complex, parameterized terrains by chaining together simple modules. The system supports both structured and noise-based terrain elements, and integrates with Blender for rendering and object placement. The framework is designed to support applications such as generating synthetic terrains for training machine learning models or producing ground truth for perception tasks. By using a minimal but extensible set of modules, the system achieves high flexibility while remaining easy to configure and expand. We demonstrate that this enables fine-grained control over features such as slope, roughness, and the number of rocks, as well as extension to additional measures. This makes it well suited for workflows that demand reproducibility, variation, and integration with automated pipelines.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate identification of communication between multiple interacting neural populations</title>
<link>https://arxiv.org/abs/2506.19094</link>
<guid>https://arxiv.org/abs/2506.19094</guid>
<content:encoded><![CDATA[
<div> variational autoencoder, neural recording, brain regions, inter-regional communication, dynamical systems
Summary:
Variational autoencoder MR-LFADS is introduced for disentangling communication between brain regions, unobserved inputs, and local neural dynamics. It outperforms existing models in identifying inter-regional communication in simulations and predicts brain-wide effects of circuit perturbations in electrophysiology data. MR-LFADS shows promise in uncovering brain-wide information processing principles. <div>
arXiv:2506.19094v1 Announce Type: cross 
Abstract: Neural recording technologies now enable simultaneous recording of population activity across many brain regions, motivating the development of data-driven models of communication between brain regions. However, existing models can struggle to disentangle the sources that influence recorded neural populations, leading to inaccurate portraits of inter-regional communication. Here, we introduce Multi-Region Latent Factor Analysis via Dynamical Systems (MR-LFADS), a sequential variational autoencoder designed to disentangle inter-regional communication, inputs from unobserved regions, and local neural population dynamics. We show that MR-LFADS outperforms existing approaches at identifying communication across dozens of simulations of task-trained multi-region networks. When applied to large-scale electrophysiology, MR-LFADS predicts brain-wide effects of circuit perturbations that were held out during model fitting. These validations on synthetic and real neural data position MR-LFADS as a promising tool for discovering principles of brain-wide information processing.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models to Democratize Access to Costly Datasets for Academic Research</title>
<link>https://arxiv.org/abs/2412.02065</link>
<guid>https://arxiv.org/abs/2412.02065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data access, democratization, data collection, corporate disclosures

Summary: 
Large Language Models (LLMs) have the potential to democratize access to costly datasets essential for research by automating data collection from corporate disclosures. A novel methodology using GPT-4o-mini in a Retrieval-Augmented Generation (RAG) framework successfully collected CEO pay ratios from 10,000 proxy statements and Critical Audit Matters (CAMs) from over 12,000 10-K filings with human-level accuracy. The LLM processing times were notably efficient at 9 and 40 minutes respectively, with minimal costs under $10. This approach contrasts significantly with manual collection methods requiring hundreds of hours or expensive commercial database subscriptions. By sharing their methodology and resulting datasets, the researchers aim to empower those with limited resources to engage in research and foster a more inclusive research community. 

<br /><br />Summary: <div>
arXiv:2412.02065v2 Announce Type: replace-cross 
Abstract: Unequal access to costly datasets essential for empirical research has long hindered researchers from disadvantaged institutions, limiting their ability to contribute to their fields and advance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential to democratize data access by automating data collection from unstructured sources. We develop and evaluate a novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation (RAG) framework to collect data from corporate disclosures. Our approach achieves human-level accuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and 40 minutes respectively, each at a cost under $10. This stands in stark contrast to the hundreds of hours needed for manual collection or the thousands of dollars required for commercial database subscriptions. To foster a more inclusive research community by empowering researchers with limited resources to explore new avenues of inquiry, we share our methodology and the resulting datasets.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear optimals and their role in sustaining turbulence in channel flow</title>
<link>https://arxiv.org/abs/2503.08283</link>
<guid>https://arxiv.org/abs/2503.08283</guid>
<content:encoded><![CDATA[
<div> investigation, energy transfer, channel flow, optimal disturbances, turbulence

Summary:
The study focuses on the energy transfer in channel flow by analyzing nonlinear optimal disturbances that maximize energy growth over a fixed time period. Results indicate that these disturbances exhibit streak spacing and amplitude consistent with Direct Numerical Simulation (DNS) data at Re_tau = 180, suggesting they capture key mechanisms sustaining turbulence. Additionally, the time horizon for nonlinear disturbances to surpass linear optimal disturbances aligns with estimates based on eddy turnover time from previous DNS studies. This finding sheds light on the determination of turbulent time scales and highlights the potential of nonlinear disturbances in understanding turbulence dynamics. Overall, the research provides valuable insights into the energy dynamics and mechanisms underlying turbulence in channel flow. 

<br /><br />Summary: <div>
arXiv:2503.08283v2 Announce Type: replace-cross 
Abstract: We investigate the energy transfer from the mean profile to velocity fluctuations in channel flow by calculating nonlinear optimal disturbances,i.e. the initial condition of a given finite energy that achieves the highest possible energy growth during a given fixed time horizon. It is found that for a large range of time horizons and initial disturbance energies, the nonlinear optimal exhibits streak spacing and amplitude consistent with DNS at least at Re_tau = 180, which suggests that they isolate the relevant physical mechanisms that sustain turbulence. Moreover, the time horizon necessary for a nonlinear disturbance to outperform a linear optimal is consistent with previous DNS-based estimates using eddy turnover time, which offers a new perspective on how some turbulent time scales are determined.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Equilibrium and Kinetics Prediction with a Data-Weighted Neural Network Model of Methane Steam Reforming</title>
<link>https://arxiv.org/abs/2506.17224</link>
<guid>https://arxiv.org/abs/2506.17224</guid>
<content:encoded><![CDATA[
<div> neural network, methane steam reforming, process optimization, surrogate model, energy carrier  
Summary:  
- A surrogate model was developed using an artificial neural network to simulate methane steam reforming for hydrogen production.  
- The model successfully unified kinetic and equilibrium regimes by training on a comprehensive dataset including experimental, interpolated, and theoretical data.  
- Data augmentation and weighted training improved the model's accuracy in predicting the composition of post-reaction mixtures.  
- The surrogate model demonstrated a mean squared error of 0.000498 and strong Pearson correlation coefficients of 0.927, indicating high predictive accuracy.  
- The model's capability to provide continuous derivatives of its predictions makes it useful for process modeling and optimization.  
- Overall, the surrogate model proves robust for simulating methane steam reforming, offering a valuable tool for design and process optimization.  

<br /><br />Summary: <div>
arXiv:2506.17224v1 Announce Type: new 
Abstract: Hydrogen's role is growing as an energy carrier, increasing the need for efficient production, with methane steam reforming being the most widely used technique. This process is crucial for applications like fuel cells, where hydrogen is converted into electricity, pushing for reactor miniaturization and optimized process control through numerical simulations. Existing models typically address either kinetic or equilibrium regimes, limiting their applicability. Here we show a surrogate model capable of unifying both regimes. An artificial neural network trained on a comprehensive dataset that includes experimental data from kinetic and equilibrium experiments, interpolated data, and theoretical data derived from theoretical models for each regime. Data augmentation and assigning appropriate weights to each data type enhanced training. After evaluating Bayesian Optimization and Random Sampling, the optimal model demonstrated high predictive accuracy for the composition of the post-reaction mixture under varying operating parameters, indicated by a mean squared error of 0.000498 and strong Pearson correlation coefficients of 0.927. The network's ability to provide continuous derivatives of its predictions makes it particularly useful for process modeling and optimization. The results confirm the surrogate model's robustness for simulating methane steam reforming in both kinetic and equilibrium regimes, making it a valuable tool for design and process optimization.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Quantum Latent Encoding for Topology Optimization</title>
<link>https://arxiv.org/abs/2506.17487</link>
<guid>https://arxiv.org/abs/2506.17487</guid>
<content:encoded><![CDATA[
<div> neural decoding, quantum encoding, classical encoding, variational framework, topology optimization

Summary:<br />
The article introduces a variational framework for structural topology optimization that combines quantum and classical latent encoding strategies within a coordinate-based neural decoding architecture. A low-dimensional latent vector is generated through either a variational quantum circuit or Gaussian distribution and mapped to a higher-dimensional latent space via a learnable projection layer. The enriched representation is decoded into a high-resolution material distribution using a neural network that takes both the latent vector and spatial coordinates as input. Optimization is performed on the latent parameters guided by physics-based objectives such as compliance minimization and volume constraints evaluated through finite element analysis. Quantum latent vectors constructed from measured Pauli observables provide an entangled encoding. The variational formulation enables the generation of diverse and valid topologies by exploring the latent space through sampling or perturbation. Numerical experiments show that both quantum and classical encodings produce high-quality structural designs, with quantum encoding showing advantages in compliance and design diversity. This suggests the potential of quantum circuits in physics-constrained topology optimization using near-term quantum hardware.<br /> <div>
arXiv:2506.17487v1 Announce Type: new 
Abstract: A variational framework for structural topology optimization is developed, integrating quantum and classical latent encoding strategies within a coordinate-based neural decoding architecture. In this approach, a low-dimensional latent vector, generated either by a variational quantum circuit or sampled from a Gaussian distribution, is mapped to a higher-dimensional latent space via a learnable projection layer. This enriched representation is then decoded into a high-resolution material distribution using a neural network that takes both the latent vector and Fourier-mapped spatial coordinates as input. The optimization is performed directly on the latent parameters, guided solely by physics-based objectives such as compliance minimization and volume constraints evaluated through finite element analysis, without requiring any precomputed datasets or supervised training. Quantum latent vectors are constructed from the expectation values of Pauli observables measured on parameterized quantum circuits, providing a structured and entangled encoding of information. The classical baseline uses Gaussian-sampled latent vectors projected in the same manner. The proposed variational formulation enables the generation of diverse and physically valid topologies by exploring the latent space through sampling or perturbation, in contrast to traditional optimization methods that yield a single deterministic solution. Numerical experiments show that both classical and quantum encodings produce high-quality structural designs. However, quantum encodings demonstrate advantages in several benchmark cases in terms of compliance and design diversity. These results highlight the potential of quantum circuits as an effective and scalable tool for physics-constrained topology optimization and suggest promising directions for applying near-term quantum hardware in structural design.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A predictor-corrector scheme for approximating signed distances using finite element methods</title>
<link>https://arxiv.org/abs/2506.17830</link>
<guid>https://arxiv.org/abs/2506.17830</guid>
<content:encoded><![CDATA[
<div> prediction-correction approach, signed distance functions, finite element method, Eikonal equation, interfaces

Summary:
The article introduces a finite element method for computing approximate signed distance functions to arbitrary boundaries in 2D and 3D. The method utilizes a prediction-correction approach involving a linear diffusion-based prediction step and a nonlinear minimization-based correction step related to the Eikonal equation. This approach efficiently generates an initial guess for complex level set functions and facilitates convergence. The method can handle complex interfaces and level set functions with steep or flat regions, overcoming challenges faced by existing techniques. Through various examples, including classical geometries, star domains, and 3D tori, the method demonstrates accuracy, efficiency, and robustness in reinitializing diverse level set functions. <div>
arXiv:2506.17830v1 Announce Type: new 
Abstract: In this article, we introduce a finite element method designed for the robust computation of approximate signed distance functions to arbitrary boundaries in two and three dimensions. Our method employs a novel prediction-correction approach, involving first the solution of a linear diffusion-based prediction problem, followed by a nonlinear minimization-based correction problem associated with the Eikonal equation. The prediction step efficiently generates a suitable initial guess, significantly facilitating convergence of the nonlinear correction step. A key strength of our approach is its ability to handle complex interfaces and initial level set functions with arbitrary steep or flat regions, a notable challenge for existing techniques. Through several representative examples, including classical geometries and more complex shapes such as star domains and three-dimensional tori, we demonstrate the accuracy, efficiency, and robustness of the method, validating its broad applicability for reinitializing diverse level set functions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from the Storm: A Multivariate Machine Learning Approach to Predicting Hurricane-Induced Economic Losses</title>
<link>https://arxiv.org/abs/2506.17964</link>
<guid>https://arxiv.org/abs/2506.17964</guid>
<content:encoded><![CDATA[
<div> Keywords: hurricanes, economic loss, modeling framework, machine learning, insurance claims 

Summary:
Hurricanes in Florida result in substantial economic losses, prompting the need for a comprehensive framework to assess contributing factors. This study introduces a modeling framework categorizing factors into hurricane characteristics, water-related environmental factors, and socioeconomic factors at the ZIP Code Tabulation Area level. By utilizing machine learning models and insurance claims as indicators, the approach accurately predicts economic loss and evaluates the importance of each component. The findings offer valuable insights for disaster mitigation, risk assessment, and urban strategies in storm-exposed areas. The code for this research is now publicly available to facilitate further study and application in disaster management.  <br /><br />Summary: <div>
arXiv:2506.17964v1 Announce Type: new 
Abstract: Florida is particularly vulnerable to hurricanes, which frequently cause substantial economic losses. While prior studies have explored specific contributors to hurricane-induced damage, few have developed a unified framework capable of integrating a broader range of influencing factors to comprehensively assess the sources of economic loss. In this study, we propose a comprehensive modeling framework that categorizes contributing factors into three key components: (1) hurricane characteristics, (2) water-related environmental factors, and (3) socioeconomic factors of affected areas. By integrating multi-source data and aggregating all variables at the finer spatial granularity of the ZIP Code Tabulation Area (ZCTA) level, we employ machine learning models to predict economic loss, using insurance claims as indicators of incurred damage. Beyond accurate loss prediction, our approach facilitates a systematic assessment of the relative importance of each component, providing practical guidance for disaster mitigation, risk assessment, and the development of adaptive urban strategies in coastal and storm-exposed areas. Our code is now available at: https://github.com/LabRAI/Hurricane-Induced-Economic-Loss-Prediction
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A phase field model for hydraulic fracture: Drucker-Prager driving force and a hybrid coupling strategy</title>
<link>https://arxiv.org/abs/2506.18161</link>
<guid>https://arxiv.org/abs/2506.18161</guid>
<content:encoded><![CDATA[
<div> phase field approach, hydraulic fracture, coupling, Drucker-Prager, simulation

Summary: 
This study introduces a novel phase field framework for simulating hydraulic fracture, with a focus on improving the coupling between fluid flow and the phase field and incorporating a universal fracture driving force. Two key innovations are presented: a hybrid coupling approach for enhanced accuracy and flexibility in handling the fracture-fluid flow interplay, and a Drucker-Prager-based strain energy decomposition to model materials with asymmetric tension-compression fracture behavior. The framework is applied to four case studies to demonstrate its capabilities in addressing permeability coupling, cracking behavior, and multiaxial conditions in hydraulic fracturing simulations. The developed codes are freely available for download, offering valuable resources to the community for further research in this field. <div>
arXiv:2506.18161v1 Announce Type: new 
Abstract: Recent years have seen a significant interest in using phase field approaches to model hydraulic fracture, so as to optimise a process that is key to industries such as petroleum engineering, mining and geothermal energy extraction. Here, we present a novel theoretical and computational phase field framework to simulate hydraulic fracture. The framework is general and versatile, in that it allows for improved treatments of the coupling between fluid flow and the phase field, and encompasses a universal description of the fracture driving force. Among others, this allows us to bring two innovations to the phase field hydraulic fracture community: (i) a new hybrid coupling approach to handle the fracture-fluid flow interplay, offering enhanced accuracy and flexibility; and (ii) a Drucker-Prager-based strain energy decomposition, extending the simulation of hydraulic fracture to materials exhibiting asymmetric tension-compression fracture behaviour (such as shale rocks) and enabling the prediction of geomechanical phenomena such as fault reactivation and stick-slip behaviour. Four case studies are addressed to illustrate these additional modelling capabilities and bring insight into permeability coupling, cracking behaviour, and multiaxial conditions in hydraulic fracturing simulations. The codes developed are made freely available to the community and can be downloaded from {https://mechmat.web.ox.ac.uk/
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Fractal Dimension using Discrete Global Grid Systems</title>
<link>https://arxiv.org/abs/2506.18175</link>
<guid>https://arxiv.org/abs/2506.18175</guid>
<content:encoded><![CDATA[
<div> fractal dimension, Discrete Global Grid System, Minkowski-Bouligand dimension, geospatial vector data, satellite images <br />
Summary: This study explores the relationship between fractal dimension and Discrete Global Grid Systems (DGGS), used for geospatial vector data analysis. By applying the method to synthetic data and opaque cloud fields from satellite images, the results closely match theoretical fractal dimensions. The use of DGGSs addresses issues such as arbitrary grid placement, orientation, and cell size progression in geospatial data analysis. DGGSs also account for the curvature of the Earth when calculating intersections with large geographic extents. The paper validates the use of DGGSs as covering sets and highlights desirable properties for effective application in geospatial analysis. <br /> <div>
arXiv:2506.18175v1 Announce Type: new 
Abstract: This study builds a bridge between two well-studied but distant topics: fractal dimension and Discrete Global Grid System (DGGS). DGGSs are used as covering sets for geospatial vector data to calculate the Minkowski-Bouligand dimension. Using the method on synthetic data yields results within 1% of their theoretical fractal dimensions. A case study on opaque cloud fields obtained from satellite images gives fractal dimension in agreement with that available in the literature. The proposed method alleviates the problems of arbitrary grid placement and orientation, as well as the progression of cell sizes of the covering sets for geospatial data. Using DGGSs further ensure that intersections of the covering sets with the geospatial vector having large geographic extents are calculated by taking the curvature of the earth into account. This paper establishes the validity of DGGSs as covering sets theoretically and discusses desirable properties of DGGSs suitable for this purpose.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conservative data-driven finite element formulation</title>
<link>https://arxiv.org/abs/2506.18206</link>
<guid>https://arxiv.org/abs/2506.18206</guid>
<content:encoded><![CDATA[
<div> finite element framework, mixed finite element formulation, data-driven approach, conservation law, numerical simulations<br />
<br />
Summary:<br />
This paper introduces a data-driven finite element framework using a mixed finite element formulation for diffusion problems. Unlike traditional methods that rely on fitting experimental data to material models, this approach directly utilizes experimental data in the simulations without the need for parameter fitting. By satisfying conservation laws through the finite element method and enforcing continuity of flux components, a mixed formulation is introduced to handle uncertainties in datasets and predict solution non-uniqueness. Error indicators tailored for data-driven approaches allow for adaptive refinement. An example of nonlinear heat transfer in nuclear graphite demonstrates the formulation's capabilities using synthetically generated datasets to showcase its effectiveness in predicting uncertainties and providing accurate results. <div>
arXiv:2506.18206v1 Announce Type: new 
Abstract: This paper presents a new data-driven finite element framework derived with mixed finite element formulation. The standard approach to diffusion problems requires the solution of the mathematical equations that describe both the conservation law and the constitutive relations, where the latter is traditionally obtained after fitting experimental data to simplified material models. To exploit all available information and avoid bias in the material model, we follow a data-driven approach. While the conservation laws and boundary conditions are satisfied by means of the finite element method, the experimental data is used directly in the numerical simulations, avoiding the need of fitting material model parameters. In order to satisfy the conservation law a priori in the strong sense, we introduce a mixed finite element formulation. This relaxes the regularity requirements on approximation spaces while enforcing continuity of the normal flux component across all of the inner boundaries. This weaker mixed formulation provides a posteriori error indicators tailored for this data-driven approach, enabling adaptive hp-refinement. The relaxed regularity of the approximation spaces makes it easier to observe how the variation in the datasets results in the non-uniqueness of the solution, which can be quantified to predict the uncertainty of the results. The capabilities of the formulation are demonstrated in an example of the nonlinear heat transfer in nuclear graphite using synthetically generated material datasets.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Conditional Score-Guided Generative Modeling for Amortized Inference in Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2506.18227</link>
<guid>https://arxiv.org/abs/2506.18227</guid>
<content:encoded><![CDATA[
<div> conditional inference, amortized, diffusion models, generative model, neural network

Summary:
The article introduces a framework for efficient conditional inference by utilizing exact conditional score-guided diffusion models to train a non-reversible neural network as a generative model. Traditional normalizing flow methods typically require reversible architectures, limiting their expressiveness and efficiency. Leveraging a two-stage method, the authors first create a training-free conditional diffusion model with an exact score function derived under a Gaussian mixture prior. This allows for the generation of noise-labeled data efficiently. This data is then used to train a feedforward neural network that directly maps noise and observations to posterior samples, eliminating the need for reversibility or iterative sampling during inference. The resulting model offers fast, accurate, and scalable conditional sampling for high-dimensional and multi-modal posterior distributions, making it suitable for uncertainty quantification tasks such as parameter estimation for complex physical systems. The effectiveness of the approach is demonstrated through numerical experiments. 

<br /><br />Summary: <div>
arXiv:2506.18227v1 Announce Type: new 
Abstract: We propose an efficient framework for amortized conditional inference by leveraging exact conditional score-guided diffusion models to train a non-reversible neural network as a conditional generative model. Traditional normalizing flow methods require reversible architectures, which can limit their expressiveness and efficiency. Although diffusion models offer greater flexibility, they often suffer from high computational costs during inference. To combine the strengths of both approaches, we introduce a two-stage method. First, we construct a training-free conditional diffusion model by analytically deriving an exact score function under a Gaussian mixture prior formed from samples of the underlying joint distribution. This exact conditional score model allows us to efficiently generate noise-labeled data, consisting of initial diffusion Gaussian noise and posterior samples conditioned on various observation values, by solving a reverse-time ordinary differential equation. Second, we use this noise-labeled data to train a feedforward neural network that maps noise and observations directly to posterior samples, eliminating the need for reversibility or iterative sampling at inference time. The resulting model provides fast, accurate, and scalable conditional sampling for high-dimensional and multi-modal posterior distributions, making it well-suited for uncertainty quantification tasks, e.g., parameter estimation of complex physical systems. We demonstrate the effectiveness of our approach through a series of numerical experiments.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural-operator element method: Efficient and scalable finite element method enabled by reusable neural operators</title>
<link>https://arxiv.org/abs/2506.18427</link>
<guid>https://arxiv.org/abs/2506.18427</guid>
<content:encoded><![CDATA[
<div> neural-operator element method, finite element method, machine learning, PDEs, multiscale simulations 
Summary: 
The neural-operator element method (NOEM) combines the finite element method (FEM) with neural operators to efficiently solve partial differential equations (PDEs) without the need for dense meshing. By using neural operators to simulate subdomains where FEM would require many elements, NOEM reduces computational costs. Each subdomain is represented by a neural-operator element (NOE), which is integrated with standard finite elements to achieve accurate and scalable solutions. This approach addresses the challenges of high training costs and low model reusability associated with machine learning-based methods for PDEs. Extensive numerical experiments demonstrate the accuracy, efficiency, and scalability of NOEM across various scenarios, including nonlinear PDEs, multiscale problems, complex geometries, and discontinuous coefficient fields. <br /><br />Summary: <div>
arXiv:2506.18427v1 Announce Type: new 
Abstract: The finite element method (FEM) is a well-established numerical method for solving partial differential equations (PDEs). However, its mesh-based nature gives rise to substantial computational costs, especially for complex multiscale simulations. Emerging machine learning-based methods (e.g., neural operators) provide data-driven solutions to PDEs, yet they present challenges, including high training cost and low model reusability. Here, we propose the neural-operator element method (NOEM) by synergistically combining FEM with operator learning to address these challenges. NOEM leverages neural operators (NOs) to simulate subdomains where a large number of finite elements would be required if FEM was used. In each subdomain, an NO is used to build a single element, namely a neural-operator element (NOE). NOEs are then integrated with standard finite elements to represent the entire solution through the variational framework. Thereby, NOEM does not necessitate dense meshing and offers efficient simulations. We demonstrate the accuracy, efficiency, and scalability of NOEM by performing extensive and systematic numerical experiments, including nonlinear PDEs, multiscale problems, PDEs on complex geometries, and discontinuous coefficient fields.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual failure assessment diagrams for hydrogen transmission pipelines</title>
<link>https://arxiv.org/abs/2506.18554</link>
<guid>https://arxiv.org/abs/2506.18554</guid>
<content:encoded><![CDATA[
<div> Thermo-metallurgical welding, diffusion-elastic-plastic phase field fracture, hydrogen transport pipelines, residual stress, Virtual Failure Assessment Diagrams <br />
Summary: <br />
The study combines advanced welding process modeling with fracture simulations to predict failure states in hydrogen transport pipelines. By considering factors such as residual stress, material properties, and hydrogen purity, failure pressures can be efficiently quantified. Virtual Failure Assessment Diagrams are created to facilitate the assessment of asset fitness under various conditions. The model's predictions align well with industry standards but highlight the need for a more mechanistic approach to account for the heterogeneous weld microstructure. Safety factors are established to address residual stresses and brittle weld regions, ensuring a more accurate assessment of pipeline integrity and safety. <div>
arXiv:2506.18554v1 Announce Type: new 
Abstract: We combine state-of-the-art thermo-metallurgical welding process modelling with coupled diffusion-elastic-plastic phase field fracture simulations to predict the failure states of hydrogen transport pipelines. This enables quantitatively resolving residual stress states and the role of brittle, hard regions of the weld such as the heat affected zone (HAZ). Failure pressures can be efficiently quantified as a function of asset state (existing defects), materials and weld procedures adopted, and hydrogen purity. Importantly, simulations spanning numerous relevant conditions (defect size and orientations) are used to build \emph{Virtual} Failure Assessment Diagrams (FADs), enabling a straightforward uptake of this mechanistic approach in fitness-for-service assessment. Model predictions are in very good agreement with FAD approaches from the standards but show that the latter are not conservative when resolving the heterogeneous nature of the weld microstructure. Appropriate, \emph{mechanistic} FAD safety factors are established that account for the role of residual stresses and hard, brittle weld regions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-Informed Neural Network Framework for Simulating Creep Buckling in Growing Viscoelastic Biological Tissues</title>
<link>https://arxiv.org/abs/2506.18565</link>
<guid>https://arxiv.org/abs/2506.18565</guid>
<content:encoded><![CDATA[
<div> framework, viscoelastic behavior, physics-informed neural network, stress relaxation, morphogenesis <br />
Summary: <br />
This study presents a novel energy-based physics-informed neural network (PINN) framework for modeling viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. The framework eliminates the need for explicit meshing or custom programs, simplifying computational complexity. By training neural networks to minimize potential energy functional, the framework ensures physics consistency, capturing creep buckling and predicting tissue growth with high accuracy. Results show the framework can predict viscoelastic instabilities, post-buckling evolution, and tissue morphological changes effectively. This versatile tool offers a promising alternative to traditional methods and can find applications in structural engineering, soft materials, and tissue development. <div>
arXiv:2506.18565v1 Announce Type: new 
Abstract: Modeling viscoelastic behavior is crucial in engineering and biomechanics, where materials undergo time-dependent deformations, including stress relaxation, creep buckling and biological tissue development. Traditional numerical methods, like the finite element method, often require explicit meshing, artificial perturbations or embedding customised programs to capture these phenomena, adding computational complexity. In this study, we develop an energy-based physics-informed neural network (PINN) framework using an incremental approach to model viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. Physics consistency is ensured by training neural networks to minimize the systems potential energy functional, implicitly satisfying equilibrium and constitutive laws. We demonstrate that this framework can naturally capture creep buckling without pre-imposed imperfections, leveraging inherent training dynamics to trigger instabilities. Furthermore, we extend our framework to biological tissue growth and morphogenesis, predicting both uniform expansion and differential growth-induced buckling in cylindrical structures. Results show that the energy-based PINN effectively predicts viscoelastic instabilities, post-buckling evolution and tissue morphological evolution, offering a promising alternative to traditional methods. This study demonstrates that PINN can be a flexible robust tool for modeling complex, time-dependent material behavior, opening possible applications in structural engineering, soft materials, and tissue development.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication Architecture for Autonomous Power-to-X Platforms: Enhancing Inspection and Operation With Legged Robots and 5G</title>
<link>https://arxiv.org/abs/2506.18572</link>
<guid>https://arxiv.org/abs/2506.18572</guid>
<content:encoded><![CDATA[
<div> Keywords: Power to X platforms, communication architecture, robotic system, inspection, 5G network

Summary: 
Power to X platforms are classified, and a communication architecture is proposed for monitoring, control, and teleoperation. A robotic system using a quadruped robot is integrated to autonomously perform inspection and maintenance tasks, reducing the need for human labor. The implementation considers a 5G standalone network for remote monitoring, control, and teleoperation of the robot. Evaluation includes recording and comparison of aspects such as availability and latency within this network. <div>
arXiv:2506.18572v1 Announce Type: new 
Abstract: Inspection and maintenance of offshore platforms are associated with high costs, primarily due to the significant personnel requirements and challenging operational conditions. This paper first presents a classification of Power to X platforms. Building upon this foundation, a communication architecture is proposed to enable monitoring, control, and teleoperation for a Power to X platform. To reduce the demand for human labor, a robotic system is integrated to autonomously perform inspection and maintenance tasks. The implementation utilizes a quadruped robot. Remote monitoring, control, and teleoperation of the robot are analyzed within the context of a 5G standalone network. As part of the evaluation, aspects such as availability and latency are recorded, compared, and critically assessed.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Dynamic Stock Relationship Modeling and S&amp;P500 Price Forecasting Based on Differential Graph Transformer</title>
<link>https://arxiv.org/abs/2506.18717</link>
<guid>https://arxiv.org/abs/2506.18717</guid>
<content:encoded><![CDATA[
<div> Keywords: Stock price prediction, Differential Graph Transformer, Dynamic relationship modeling, Temporal attention, Correlation metrics

Summary:
The study introduces the Differential Graph Transformer (DGT) framework for dynamic stock price prediction by capturing evolving stock relationships. DGT utilizes differential graph mechanisms and causal temporal attention to model global and local dependencies in price sequences. The framework incorporates correlation metrics (Pearson, Mutual Information, Spearman, Kendall's Tau) across different scopes as spatial-attention priors, improving prediction accuracy over traditional models. Using 10 years of S&amp;P 500 closing prices, DGT with spatial priors outperformed GRU baselines, with optimal results obtained using Kendall's Tau global matrices. Clustering analysis identified distinct stock clusters such as "high-volatility growth" and "defensive blue-chip," with each exhibiting varying prediction errors based on correlation stability. The study highlights the importance of dynamic modeling and optimal correlation metrics in enhancing financial time-series prediction and quantitative investment strategies.

Summary: <br />Keywords: Stock price prediction, Differential Graph Transformer, Dynamic relationship modeling, Temporal attention, Correlation metrics <div>
arXiv:2506.18717v1 Announce Type: new 
Abstract: Stock price prediction is vital for investment decisions and risk management, yet remains challenging due to markets' nonlinear dynamics and time-varying inter-stock correlations. Traditional static-correlation models fail to capture evolving stock relationships. To address this, we propose a Differential Graph Transformer (DGT) framework for dynamic relationship modeling and price prediction. Our DGT integrates sequential graph structure changes into multi-head self-attention via a differential graph mechanism, adaptively preserving high-value connections while suppressing noise. Causal temporal attention captures global/local dependencies in price sequences. We further evaluate correlation metrics (Pearson, Mutual Information, Spearman, Kendall's Tau) across global/local/dual scopes as spatial-attention priors. Using 10 years of S&amp;P 500 closing prices (z-score normalized; 64-day sliding windows), DGT with spatial priors outperformed GRU baselines (RMSE: 0.24 vs. 0.87). Kendall's Tau global matrices yielded optimal results (MAE: 0.11). K-means clustering revealed "high-volatility growth" and "defensive blue-chip" stocks, with the latter showing lower errors (RMSE: 0.13) due to stable correlations. Kendall's Tau and Mutual Information excelled in volatile sectors. This study innovatively combines differential graph structures with Transformers, validating dynamic relationship modeling and identifying optimal correlation metrics/scopes. Clustering analysis supports tailored quantitative strategies. Our framework advances financial time-series prediction through dynamic modeling and cross-asset interaction analysis.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Real-time Structural Dynamics Simulation with Graph-based Digital Twin Modelling</title>
<link>https://arxiv.org/abs/2506.18724</link>
<guid>https://arxiv.org/abs/2506.18724</guid>
<content:encoded><![CDATA[
<div> Graph-Based Digital Twin Modelling, Structural Dynamics, Computational Efficiency, Health Monitoring, Spatial Topologies
<br />
Summary:
<br />
This study introduces a graph-based digital twin modelling (GDTM) framework for simulating structural dynamic behavior. The framework utilizes adjacency matrices to capture spatial relationships between structural elements, enhancing physical interpretability. Results from numerical and experimental validation show high accuracy in simulating dynamics across various topologies, with low Normalized Mean-Squared Error values. The framework significantly enhances computational efficiency, outperforming traditional finite element methods (FEM) by over 80-fold. By addressing challenges in data-driven methods, this research paves the way for practical applications in structural performance evaluation and health monitoring. <div>
arXiv:2506.18724v1 Announce Type: new 
Abstract: Precise and timely simulation of a structure's dynamic behavior is crucial for evaluating its performance and assessing its health status. Traditional numerical methods are often limited by high computational costs and low efficiency, while deep learning approaches offer a promising alternative. However, these data-driven methods still face challenges, such as limited physical interpretability and difficulty in adapting to diverse structural configurations. To address these issues, this study proposes a graph-based digital twin modelling (GDTM) framework to simulate structural dynamic responses across various spatial topologies. In this framework, the adjacency matrix explicitly represents the spatial relationships between structural vertices, enhancing the model's physical interpretability. The effectiveness of the proposed framework was validated through comprehensive numerical and experimental studies. The results demonstrate that the framework accurately simulated structural dynamics across different topological configurations, with Normalized Mean-Squared Error (NMSE) values consistently below 0.005 in numerical simulations and 0.0015 in experimental validations. Furthermore, the framework achieved over 80-fold improvements in computational efficiency compared to traditional finite element methods (FEM). This research promotes the practical application of graph-based structural dynamics modelling, which has the potential to significantly advance structural performance evaluation and health monitoring.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeletal Reaction Models for Gasoline Surrogate Combustion</title>
<link>https://arxiv.org/abs/2506.18853</link>
<guid>https://arxiv.org/abs/2506.18853</guid>
<content:encoded><![CDATA[
<div> Gasoline surrogate model, skeletal reaction models, sensitivity analysis, reduced-order modeling, CUR matrix decomposition<br />
<br />
Summary: <br />
Skeletal reaction models for a four-component gasoline surrogate model were developed using an implicit time-dependent basis CUR methodology. The sensitivities of species mass fractions and temperature to reaction rates were estimated using reduced-order modeling. These sensitivities were then used to create skeletal reaction models automatically. The developed models, with 679 and 494 species, accurately reproduced detailed kinetics model results, with errors of less than 1% and less than 10%, respectively. This automated approach provides a valuable tool for reducing complex detailed models while maintaining high predictive accuracy. <div>
arXiv:2506.18853v1 Announce Type: new 
Abstract: Skeletal reaction models are derived for a four-component gasoline surrogate model via an instantaneous local sensitivity analysis technique. The sensitivities of the species mass fractions and the temperature with respect to the reaction rates are estimated by a reduced-order modeling (ROM) methodology. Termed "implicit time-dependent basis CUR (implicit TDB-CUR)," this methodology is based on the CUR matrix decomposition and incorporates implicit time integration for evolving the bases. The estimated sensitivities are subsequently analyzed to develop skeletal reaction models with a fully automated procedure. The 1389-species gasoline surrogate model developed at Lawrence Livermore National Laboratory (LLNL) is selected as the detailed kinetics model. The skeletal reduction procedure is applied to this model in a zero-dimensional constant-pressure reactor over a wide range of initial conditions. The performances of the resulting skeletal models are appraised by comparison against the results via the LLNL detailed model, and also predictions via other skeletal models. Two new skeletal models are developed consisting of 679 and 494 species, respectively. The first is an alternative to an existing model with the same number of species. The predictions with this model reproduces the detailed models vital flame results with less than 1% errors. The errors via the second model are less than 10%.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems</title>
<link>https://arxiv.org/abs/2506.17236</link>
<guid>https://arxiv.org/abs/2506.17236</guid>
<content:encoded><![CDATA[
<div> blockchain, shared resources, fairness, non-commercial networks, Max-min Fair algorithms

Summary:
The dissertation focuses on fair distribution of shared resources in non-commercial blockchain networks. Blockchain networks order and timestamp records in a secure and consensual manner. In non-commercial networks, monetary solutions are not feasible, leading to challenges in fairness. The current faucet mechanism, offering fixed amounts of free tokens, is susceptible to attacks and lacks fairness. The study proposes 6 Max-min Fair algorithms as efficient blockchain faucets, addressing fairness issues. These algorithms are resistant to denial of service attacks, cost-effective in terms of computational economics, and allow for different user weighting policies. By adapting the faucet mechanism for fair distribution, the study aims to improve the efficiency and fairness of resource distribution in non-commercial blockchain networks. 

<br /><br />Summary: <div>
arXiv:2506.17236v1 Announce Type: cross 
Abstract: The present dissertation addresses the problem of fairly distributing shared resources in non-commercial blockchain networks. Blockchains are distributed systems that order and timestamp records of a given network of users, in a public, cryptographically secure, and consensual way. The records, which may in kind be events, transaction orders, sets of rules for structured transactions etc. are placed within well-defined datastructures called blocks, and they are linked to each other by the virtue of cryptographic pointers, in a total ordering which represents their temporal relations of succession. The ability to operate on the blockchain, and/or to contribute a record to the content of a block are shared resources of the blockchain systems. In commercial networks, these resources are exchanged in return for fiat money, and consequently, fairness is not a relevant problem in terms of computer engineering. In non-commercial networks, however, monetary solutions are not available, by definition. The present non-commercial blockchain networks employ trivial distribution mechanisms called faucets, which offer fixed amounts of free tokens (called cryptocurrencies) specific to the given network. This mechanism, although simple and efficient, is prone to denial of service (DoS) attacks and cannot address the fairness problem. In the present dissertation, the faucet mechanism is adapted for fair distribution, in line with Max-min Fairness scheme. In total, we contributed 6 distinct Max-min Fair algorithms as efficient blockchain faucets. The algorithms we contribute are resistant to DoS attacks, low-cost in terms of blockchain computation economics, and they also allow for different user weighting policies.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Beyond Order: A Chaos-Markov-Gaussian Framework for Short-Term Sentiment Forecasting of Any Financial OHLC timeseries Data</title>
<link>https://arxiv.org/abs/2506.17244</link>
<guid>https://arxiv.org/abs/2506.17244</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment forecasting, financial markets, CMG framework, chaos theory, deep learning

Summary:
The paper presents a novel CMG (Chaos-Markov-Gaussian) framework for short-term sentiment forecasting in financial markets. The framework integrates chaos theory, Markov property, and Gaussian processes to enhance prediction accuracy. By incorporating transformer-based deep learning models, temporal patterns can be efficiently captured. The CMG Framework is designed for fast, resource-efficient, and accurate forecasting of any financial instrument's OHLC time series. Unlike traditional models, CMG reduces overhead and generalizes well, making it valuable for analysts and financial institutions. Evaluations on market indices show that CMG consistently outperforms statistical, machine learning, and deep learning baselines in terms of accuracy and efficiency. This framework provides a promising approach for improving sentiment forecasting in financial markets. 

<br /><br />Summary: <div>
arXiv:2506.17244v1 Announce Type: cross 
Abstract: Short-term sentiment forecasting in financial markets (e.g., stocks, indices) is challenging due to volatility, non-linearity, and noise in OHLC (Open, High, Low, Close) data. This paper introduces a novel CMG (Chaos-Markov-Gaussian) framework that integrates chaos theory, Markov property, and Gaussian processes to improve prediction accuracy. Chaos theory captures nonlinear dynamics; the Markov chain models regime shifts; Gaussian processes add probabilistic reasoning. We enhance the framework with transformer-based deep learning models to capture temporal patterns efficiently. The CMG Framework is designed for fast, resource-efficient, and accurate forecasting of any financial instrument's OHLC time series. Unlike traditional models that require heavy infrastructure and instrument-specific tuning, CMG reduces overhead and generalizes well. We evaluate the framework on market indices, forecasting sentiment for the next trading day's first quarter. A comparative study against statistical, ML, and DL baselines trained on the same dataset with no feature engineering shows CMG consistently outperforms in accuracy and efficiency, making it valuable for analysts and financial institutions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation</title>
<link>https://arxiv.org/abs/2506.17747</link>
<guid>https://arxiv.org/abs/2506.17747</guid>
<content:encoded><![CDATA[
<div> Keywords: geological modeling, reservoir characterization, Pix2Geomodel, conditional generative adversarial network, Groningen gas field

Summary:
Accurate geological modeling is essential for understanding reservoir properties, but traditional methods face challenges with complex subsurface heterogeneity and conditioning to observed data. The Pix2Geomodel, a conditional generative adversarial network (cGAN) based on Pix2Pix, was developed to predict reservoir properties in the Groningen gas field. Utilizing a large dataset, the framework achieved high accuracy in predicting facies and water saturation and moderate success with porosity and permeability. The model's performance was validated through evaluation metrics and visualizations, showcasing its ability to capture spatial variability and geological realism. Despite limitations like microstructural variability and 2D constraints, future iterations (Pix2Geomodel v2.0) could potentially integrate multi-modal data for 3D modeling. This study signifies a significant advancement in using generative AI in geoscience, which can enhance reservoir management and support open science initiatives. 

<br /><br />Summary: <div>
arXiv:2506.17747v1 Announce Type: cross 
Abstract: Accurate geological modeling is critical for reservoir characterization, yet traditional methods struggle with complex subsurface heterogeneity, and they have problems with conditioning to observed data. This study introduces Pix2Geomodel, a novel conditional generative adversarial network (cGAN) framework based on Pix2Pix, designed to predict reservoir properties (facies, porosity, permeability, and water saturation) from the Rotliegend reservoir of the Groningen gas field. Utilizing a 7.6 million-cell dataset from the Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology included data preprocessing, augmentation to generate 2,350 images per property, and training with a U-Net generator and PatchGAN discriminator over 19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection over union (mIoU), frequency weighted intersection over union (FWIoU), and visualizations assessed performance in masked property prediction and property-to-property translation tasks. Results demonstrated high accuracy for facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74, FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA 0.98, FWIoU 0.97). The framework captured spatial variability and geological realism, as validated by variogram analysis, and calculated the training loss curves for the generator and discriminator for each property. Compared to traditional methods, Pix2Geomodel offers enhanced fidelity in direct property mapping. Limitations include challenges with microstructural variability and 2D constraints, suggesting future integration of multi-modal data and 3D modeling (Pix2Geomodel v2.0). This study advances the application of generative AI in geoscience, supporting improved reservoir management and open science initiatives.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Six Decades Post-Discovery of Taylor's Power Law: From Ecological and Statistical Universality, Through Prime Number Distributions and Tipping-Point Signals, to Heterogeneity and Stability of Complex Networks</title>
<link>https://arxiv.org/abs/2506.18154</link>
<guid>https://arxiv.org/abs/2506.18154</guid>
<content:encoded><![CDATA[
<div> Taylor's Power Law, insect populations, universality, ecology, statistics<br />
<br />
Summary: Taylor's Power Law (TPL) correlates mean population abundances and variances across insect populations using a power function. TPL has been studied for six decades, with distinct periods and themes identified, including ecological mechanisms, skewed distributions, and mathematical/statistical explanations. Future research directions include fostering interactions between abstract and physical worlds, measuring heterogeneity, and exploring evolutionary contexts. TPL's significance lies in its practical applications in various fields such as agriculture and epidemiology, as well as its theoretical implications related to phase transitions and scale invariance. <div>
arXiv:2506.18154v1 Announce Type: cross 
Abstract: First discovered by L. R. Taylor (1961, Nature), Taylor's Power Law (TPL) correlates the mean (M) population abundances and the corresponding variances (V) across a set of insect populations using a power function (V=aM^b). TPL has demonstrated its 'universality' across numerous fields of sciences, social sciences, and humanities. This universality has inspired two main prongs of exploration: one from mathematicians and statisticians, who might instinctively respond with a convergence theorem similar to the central limit theorem of the Gaussian distribution, and another from biologists, ecologists, physicists, etc., who are more interested in potential underlying ecological or organizational mechanisms. Over the past six decades, TPL studies have produced a punctuated landscape with three relatively distinct periods (1960s-1980s; 1990s-2000s, and 2010s-2020s) across the two prongs of abstract and physical worlds. Eight themes have been identified and reviewed on this landscape, including population spatial aggregation and ecological mechanisms, TPL and skewed statistical distributions, mathematical/statistical mechanisms of TPL, sample vs. population TPL, population stability, synchrony, and early warning signals for tipping points, TPL on complex networks, TPL in macrobiomes, and in microbiomes. Three future research directions including fostering reciprocal interactions between the two prongs, heterogeneity measuring, and exploration in the context of evolution. The significance of TPL research includes practically, population fluctuations captured by TPL are relevant for agriculture, forestry, fishery, wildlife-conservation, epidemiology, tumor heterogeneity, earthquakes, social inequality, stock illiquidity, financial stability, tipping point events, etc.; theoretically, TPL is one form of power laws, which are related to phase transitions, universality, scale-invariance, etc.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airalogy: AI-empowered universal data digitization for research automation</title>
<link>https://arxiv.org/abs/2506.18586</link>
<guid>https://arxiv.org/abs/2506.18586</guid>
<content:encoded><![CDATA[
<div> AI, research data, platform, standardization, multidisciplinary <br />
Summary: <br />
Research data are crucial for AI-driven science, but current applications are limited due to fragmented and inefficient data collection processes. Existing platforms lack the balance between universality and standardization needed to support diverse disciplines. Airalogy aims to bridge this gap by providing a platform that integrates scientific domain knowledge with advanced computing skills. It offers customizable, standardized data records and advanced AI tools for research assistance and automation. Already deployed in laboratories at Westlake University, Airalogy has the potential to accelerate scientific innovation across academia, industry, and the global research community. By addressing the challenges of data standardization and leveraging AI-driven capabilities, Airalogy aims to benefit humanity through enhanced research efficiency and automation. <br /> <div>
arXiv:2506.18586v1 Announce Type: cross 
Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven science, yet current AI applications remain limited to a few fields with readily available, well-structured, digitized datasets. Achieving comprehensive AI empowerment across multiple disciplines is still out of reach. Present-day research data collection is often fragmented, lacking unified standards, inefficiently managed, and difficult to share. Creating a single platform for standardized data digitization needs to overcome the inherent challenge of balancing between universality (supporting the diverse, ever-evolving needs of various disciplines) and standardization (enforcing consistent formats to fully enable AI). No existing platform accommodates both facets. Building a truly multidisciplinary platform requires integrating scientific domain knowledge with sophisticated computing skills. Researchers often lack the computational expertise to design customized and standardized data recording methods, whereas platform developers rarely grasp the intricate needs of multiple scientific domains. These gaps impede research data standardization and hamper AI-driven progress. In this study, we address these challenges by developing Airalogy (https://airalogy.com), the world's first AI- and community-driven platform that balances universality and standardization for digitizing research data across multiple disciplines. Airalogy represents entire research workflows using customizable, standardized data records and offers an advanced AI research copilot for intelligent Q&amp;A, automated data entry, analysis, and research automation. Already deployed in laboratories across all four schools of Westlake University, Airalogy has the potential to accelerate and automate scientific innovation in universities, industry, and the global research community-ultimately benefiting humanity as a whole.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-consistent integration of mechanical systems based on Livens principle</title>
<link>https://arxiv.org/abs/2312.02825</link>
<guid>https://arxiv.org/abs/2312.02825</guid>
<content:encoded><![CDATA[
<div> Hamilton-Pontryagin principle, Livens principle, structure-preserving integrator, mechanical systems, singular mass matrices  
Summary:  
Livens principle, also known as the Hamilton-Pontryagin principle, is utilized to develop a new integrator that preserves the structure of mechanical systems. Unlike traditional Hamiltonian equations, the Euler-Lagrange equations derived from Livens principle avoid the need to invert mass matrices, particularly beneficial for systems with singular mass matrices. This approach unifies Lagrangian and Hamiltonian perspectives, eliminating the requirement to define the system's Hamiltonian. The integrator conserves energy and preserves momentum maps related to system symmetries. An extension is proposed for systems with holonomic constraints, and its performance is evaluated through examples. Overall, the novel integrator based on Livens principle offers a comprehensive and efficient method for simulating mechanical systems while overcoming challenges associated with singular mass matrices. 

<br /><br />Summary: <div>
arXiv:2312.02825v3 Announce Type: replace 
Abstract: In this work we make use of Livens principle (sometimes also referred to as Hamilton-Pontryagin principle) in order to obtain a novel structure-preserving integrator for mechanical systems. In contrast to the canonical Hamiltonian equations of motion, the Euler-Lagrange equations pertaining to Livens principle circumvent the need to invert the mass matrix. This is an essential advantage with respect to singular mass matrices, which can yield severe difficulties for the modelling and simulation of multibody systems. Moreover, Livens principle unifies both Lagrangian and Hamiltonian viewpoints on mechanics. Additionally, the present framework avoids the need to set up the system's Hamiltonian. The novel scheme algorithmically conserves a general energy function and aims at the preservation of momentum maps corresponding to symmetries of the system. We present an extension to mechanical systems subject to holonomic constraints. The performance of the newly devised method is studied in representative examples.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedded Model Form Uncertainty Quantification with Measurement Noise for Bayesian Model Calibration</title>
<link>https://arxiv.org/abs/2410.12037</link>
<guid>https://arxiv.org/abs/2410.12037</guid>
<content:encoded><![CDATA[
<div> parameter calibration, Bayesian methods, model inadequacy, uncertainty quantification, heat flux estimation
Summary: 
This paper addresses the challenge of accurately calibrating simulation parameters in computer models of physical systems. It introduces a novel framework for embedding model inadequacy in Bayesian inference, allowing for more reliable propagation of uncertainties to non-observed quantities of interest (QoIs). By adapting existing likelihood models and proposing new formulations to account for noise and outliers in measurements, the approach improves predictions' representation of observed data points. The study evaluates the method's performance in the presence of discrepancies between measurements and predictions and demonstrates how uncertainty in the model inadequacy term influences QoIs. Through an application to heat flux estimation from transient thermal simulations, the proposed approach enables a more comprehensive statistical analysis of prediction reliability. <div>
arXiv:2410.12037v2 Announce Type: replace 
Abstract: A key factor in ensuring the accuracy of computer simulations that model physical systems is the proper calibration of their parameters based on real-world observations or experimental data. Inevitably, uncertainties arise, and Bayesian methods provide a robust framework for quantifying and propagating these uncertainties to model predictions. Nevertheless, Bayesian methods paired with inexact models usually produce predictions unable to represent the observed datapoints. Additionally, the quantified uncertainties of these overconfident models cannot be propagated to other Quantities of Interest (QoIs) reliably. A promising solution involves embedding a model inadequacy term in the inference parameters, allowing the quantified model form uncertainty to influence non-observed QoIs. This paper introduces a more interpretable framework for embedding the model inadequacy compared to existing methods. To overcome the limitations of current approaches, we adapt the existing likelihood models to properly account for noise in the measurements and propose two new formulations designed to address their shortcomings. Moreover, we evaluate the performance of this inadequacy-embedding approach in the presence of discrepancies between measurements and model predictions, including noise and outliers. Particular attention is given to how the uncertainty associated with the model inadequacy term propagates to the QoIs, enabling a more comprehensive statistical analysis of prediction's reliability. Finally, the proposed approach is applied to estimate the uncertainty in the predicted heat flux from a transient thermal simulation using temperature observations.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilience-based post disaster recovery optimization for infrastructure system via Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.18577</link>
<guid>https://arxiv.org/abs/2410.18577</guid>
<content:encoded><![CDATA[
<div> Keywords: infrastructure systems, post-disaster recovery, deep reinforcement learning, resilience metric, optimization

Summary:
Infrastructure systems face significant challenges during post-disaster recovery, requiring efficient repair-scheduling approaches under resource constraints. Existing methods have limitations in such contexts, prompting the proposal of a novel approach using Deep Reinforcement Learning (DRL) and a specialized resilience metric. The system's recovery process is modeled as a sequential decision-making problem on a graph-based structure, with Deep Q-learning algorithms optimizing recovery strategies. Testing on post-earthquake recovery for an electrical substation system showed Double DQN (DDQN) to be superior. Comparative analysis demonstrated the proposed method's effectiveness in optimizing resilience and rapid recovery while minimizing computational costs. This approach presents an attractive solution for enhancing infrastructure system resilience and response efficiency. 

<br /><br />Summary: <div>
arXiv:2410.18577v2 Announce Type: replace 
Abstract: Infrastructure systems are critical in modern communities but are highly susceptible to various natural and man-made disasters. Efficient post-disaster recovery requires repair-scheduling approaches under the limitation of capped resources that need to be shared across the system. Existing approaches, including component ranking methods, greedy evolutionary algorithms, and data-driven machine learning models, face various limitations when tested within such a context. To tackle these issues, we propose a novel approach to optimize post-disaster recovery of infrastructure systems by leveraging Deep Reinforcement Learning (DRL) methods and incorporating a specialized resilience metric to lead the optimization. The system topology is represented adopting a graph-based structure, where the system's recovery process is formulated as a sequential decision-making problem. Deep Q-learning algorithms are employed to learn optimal recovery strategies by mapping system states to specific actions, as for instance which component ought to be repaired next, with the goal of maximizing long-term recovery from a resilience-oriented perspective. To demonstrate the efficacy of our proposed approach, we implement this scheme on the example of post-earthquake recovery optimization for an electrical substation system. We assess different deep Q-learning algorithms to this end, namely vanilla Deep Q-Networks (DQN), Double DQN(DDQN), Duel DQN, and duel DDQN, demonstrating superiority of the DDQN for the considered problem. A further comparative analysis against baseline methods during testing reveals the superior performance of the proposed method in terms of both optimization effect and computational cost, rendering this an attractive approach in the context of resilience enhancement and rapid response and recovery.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Cancer Gene Identification through Graph Anomaly Analysis</title>
<link>https://arxiv.org/abs/2412.17240</link>
<guid>https://arxiv.org/abs/2412.17240</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph neural networks, protein-protein interaction networks, cancer genes, weight heterogeneity, HIPGNN

Summary: 
This study explores the use of Graph Neural Networks (GNNs) in analyzing protein-protein interaction (PPI) networks to identify cancer genes. It identifies a unique graph anomaly in cancer genes known as weight heterogeneity, characterized by significantly higher variance in edge weights of cancer gene nodes within the graph. The study also highlights how weight heterogeneity can impact the spectral energy distribution, leading to a concentration towards the extremes of the spectrum. To address these insights, the study proposes the HIerarchical-Perspective Graph Neural Network (HIPGNN), which considers variations in spectral energy distribution and protein interaction context. Experimental results on reprocessed datasets demonstrate the superiority of HIPGNN in identifying cancer genes within PPI networks. <div>
arXiv:2412.17240v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) have shown promise in integrating protein-protein interaction (PPI) networks for identifying cancer genes in recent studies. However, due to the insufficient modeling of the biological information in PPI networks, more faithfully depiction of complex protein interaction patterns for cancer genes within the graph structure remains largely unexplored. This study takes a pioneering step toward bridging biological anomalies in protein interactions caused by cancer genes to statistical graph anomaly. We find a unique graph anomaly exhibited by cancer genes, namely weight heterogeneity, which manifests as significantly higher variance in edge weights of cancer gene nodes within the graph. Additionally, from the spectral perspective, we demonstrate that the weight heterogeneity could lead to the "flattening out" of spectral energy, with a concentration towards the extremes of the spectrum. Building on these insights, we propose the HIerarchical-Perspective Graph Neural Network (HIPGNN) that not only determines spectral energy distribution variations on the spectral perspective, but also perceives detailed protein interaction context on the spatial perspective. Extensive experiments are conducted on two reprocessed datasets STRINGdb and CPDB, and the experimental results demonstrate the superiority of HIPGNN.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanics Informatics: A paradigm for efficiently learning constitutive models</title>
<link>https://arxiv.org/abs/2501.08314</link>
<guid>https://arxiv.org/abs/2501.08314</guid>
<content:encoded><![CDATA[
<div> stress state entropy, constitutive model learning, parameter identification, information content, mechanics informatics

Summary:<br />
- Mechanics informatics is introduced as a paradigm for efficient and accurate learning of constitutive laws, focusing on the interplay between experimental data and model parameters.
- The stress state entropy is proposed as a metric for quantifying the information content of experimental data, enabling the analysis of specimen geometries with varying information content for model learning.
- Specimen designs are optimized using stress state entropy in a Bayesian optimization scheme, leading to the creation of cruciform specimens with maximized entropy for accurate parameter identification.
- Tailoring specimen designs for specific experimental goals is demonstrated through minimizing entropy in Peirs shear specimens to achieve a uniform shear stress state.
- The framework addresses experimental uncertainties, explores transfer learning for replacing challenging testing protocols with simpler alternatives, and shows potential for extension to different material laws.

<br /><br />Summary: <div>
arXiv:2501.08314v2 Announce Type: replace 
Abstract: Efficient and accurate learning of constitutive laws is crucial for accurately predicting the mechanical behavior of materials under complex loading conditions. Accurate model calibration hinges on a delicate interplay between the information embedded in experimental data and the parameters that define our constitutive models.The information encoded in the parameters of the constitutive model must be complemented by the information in the data used for calibration. This interplay raises fundamental questions: How can we quantify the information content of test data? How much information does a single test convey? Also, how much information is required to accurately learn a constitutive model? To address these questions, we introduce mechanics informatics, a paradigm for efficient and accurate constitutive model learning. At its core is the stress state entropy, a metric for quantifying the information content of experimental data. Using this framework, we analyzed specimen geometries with varying information content for learning an anisotropic inelastic law. Specimens with limited information enabled accurate identification of a few parameters sensitive to the information in the data. Furthermore, we optimized specimen design by incorporating stress state entropy into a Bayesian optimization scheme. This led to the design of cruciform specimens with maximized entropy for accurate parameter identification. Conversely, minimizing entropy in Peirs shear specimens yielded a uniform shear stress state, showcasing the framework's flexibility in tailoring designs for specific experimental goals. Finally, we addressed experimental uncertainties, demonstrated the potential of transfer learning for replacing challenging testing protocols with simpler alternatives, and extension of the framework to different material laws.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks in Supply Chain Analytics and Optimization: Concepts, Perspectives, Dataset and Benchmarks</title>
<link>https://arxiv.org/abs/2411.08550</link>
<guid>https://arxiv.org/abs/2411.08550</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, supply chain management, benchmark dataset, optimization, real-world applications

Summary: 
Graph Neural Networks (GNNs) have shown promise in various fields but have yet to be extensively studied in supply chain management. This study bridges the gap by providing a conceptual foundation for applying GNNs in supply chain optimization. By connecting supply chains with graph structures, the authors explain formulations, examples, and task guidelines for effective GNN application. They also introduce a benchmark dataset from a leading FMCG company in Bangladesh, focused on supply chain planning. Through various supply chain tasks using GNNs, the study shows that GNN-based models consistently outperform statistical Machine Learning and other Deep Learning models in regression, classification, detection, and anomaly detection tasks. This work lays the groundwork for leveraging GNNs in solving complex supply chain problems with methodological insights and a comprehensive dataset.
<br /><br />Summary: <div>
arXiv:2411.08550v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have recently gained traction in transportation, bioinformatics, language and image processing, but research on their application to supply chain management remains limited. Supply chains are inherently graph-like, making them ideal for GNN methodologies, which can optimize and solve complex problems. The barriers include a lack of proper conceptual foundations, familiarity with graph applications in SCM, and real-world benchmark datasets for GNN-based supply chain research. To address this, we discuss and connect supply chains with graph structures for effective GNN application, providing detailed formulations, examples, mathematical definitions, and task guidelines. Additionally, we present a multi-perspective real-world benchmark dataset from a leading FMCG company in Bangladesh, focusing on supply chain planning. We discuss various supply chain tasks using GNNs and benchmark several state-of-the-art models on homogeneous and heterogeneous graphs across six supply chain analytics tasks. Our analysis shows that GNN-based models consistently outperform statistical Machine Learning and other Deep Learning models by around 10-30% in regression, 10-30% in classification and detection tasks, and 15-40% in anomaly detection tasks on designated metrics. With this work, we lay the groundwork for solving supply chain problems using GNNs, supported by conceptual discussions, methodological insights, and a comprehensive dataset.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Optimization of Physics-Informed Neural Networks: Evo-PINN Frontiers and Opportunities</title>
<link>https://arxiv.org/abs/2501.06572</link>
<guid>https://arxiv.org/abs/2501.06572</guid>
<content:encoded><![CDATA[
<div> Keywords: PINNs, physics-informed neural networks, optimization, generalization, evolutionary algorithms

Summary:
PINNs, or physics-informed neural networks, integrate mathematical laws of nature into their training loss function, providing advantages over data-driven models in limited-data scenarios. However, optimizing and generalizing PINNs present challenges, including training speed, precision, and generalizability. Evolutionary algorithms (EAs) are proposed as a solution to optimize complex loss landscapes in PINNs, potentially improving training efficiency and accuracy. Synergizing gradient descent with EAs can lead to tailored neural architectures and better balancing of physics-informed learning objectives. Additionally, using evolutionary algorithms as meta-learners for generalizable PINN models shows promise. Recent literature demonstrates the early success of these approaches in tackling optimization and generalization issues in PINNs. <div>
arXiv:2501.06572v3 Announce Type: replace-cross 
Abstract: Deep learning models trained on finite data lack a complete understanding of the physical world. On the other hand, physics-informed neural networks (PINNs) are infused with such knowledge through the incorporation of mathematically expressible laws of nature into their training loss function. By complying with physical laws, PINNs provide advantages over purely data-driven models in limited-data regimes and present as a promising route towards Physical AI. This feature has propelled them to the forefront of scientific machine learning, a domain characterized by scarce and costly data. However, the vision of accurate physics-informed learning comes with significant challenges. This work examines PINNs for the first time in terms of model optimization and generalization, shedding light on the need for new algorithmic advances to overcome issues pertaining to the training speed, precision, and generalizability of today's PINN models. Of particular interest are gradient-free evolutionary algorithms (EAs) for optimizing the uniquely complex loss landscapes arising in PINN training. Methods synergizing gradient descent and EAs for discovering bespoke neural architectures and balancing multiple terms in physics-informed learning objectives are positioned as important avenues for future research. Another exciting track is to cast evolutionary as a meta-learner of generalizable PINN models. To substantiate these proposed avenues, we further highlight results from recent literature to showcase the early success of such approaches in addressing the aforementioned challenges in PINN optimization and generalization.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast Iterative Robust Principal Component Analysis Method</title>
<link>https://arxiv.org/abs/2506.16013</link>
<guid>https://arxiv.org/abs/2506.16013</guid>
<content:encoded><![CDATA[
<div> Keywords: Principal Component Analysis, Robust PCA, Outliers, Fast Iterative Robust PCA, Incremental PCA

Summary: 
The article introduces a new method called Fast Iterative Robust (FIR) PCA to improve the robustness of Principal Component Analysis (PCA) in the presence of outliers. Traditional PCA methods are sensitive to outliers, affecting the accuracy of the results. The FIR PCA method efficiently estimates the center location and covariance of inliers, using Incremental PCA (IPCA) to iteratively construct a subset of data points that improve location and covariance estimation while mitigating the impact of outliers. The approach demonstrates competitive accuracy and performance compared to existing robust methods, offering enhanced robustness to outlier contamination. Simulated and real-world datasets are used to evaluate the effectiveness of the method in identifying and preserving underlying data structures in the presence of outliers. <div>
arXiv:2506.16013v1 Announce Type: new 
Abstract: Principal Component Analysis (PCA) is widely used for dimensionality reduction and data analysis. However, PCA results are adversely affected by outliers often observed in real-world data. Existing robust PCA methods are often computationally expensive or exhibit limited robustness. In this work, we introduce a Fast Iterative Robust (FIR) PCA method by efficiently estimating the inliers center location and covariance. Our approach leverages Incremental PCA (IPCA) to iteratively construct a subset of data points that ensures improved location and covariance estimation that effectively mitigates the influence of outliers on PCA projection. We demonstrate that our method achieves competitive accuracy and performance compared to existing robust location and covariance methods while offering improved robustness to outlier contamination. We utilize simulated and real-world datasets to evaluate and demonstrate the efficacy of our approach in identifying and preserving underlying data structures in the presence of contamination.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Converging Single Trace Quasi-local PMCHWT Equation for the Modelling of Composite Systems</title>
<link>https://arxiv.org/abs/2506.16376</link>
<guid>https://arxiv.org/abs/2506.16376</guid>
<content:encoded><![CDATA[
<div> PMCHWT integral equation, scattering, time-harmonic fields, piecewise homogeneous, composite systems<br />
Summary:<br />
The article introduces a single trace quasi-local PMCHWT equation for modeling scattering of time-harmonic fields by composite systems with junctions. Traditional methods for solving such systems rely on Krylov iterative methods, with the number of iterations needed influenced by the system matrix's eigenvalue distribution. While Caldern preconditioning is effective for systems without junction lines, it is insufficient for those with junctions. The new approach, utilizing the global multi-trace method, offers a solution with slower iteration growth as mesh size decreases. The method maintains continuity conditions at domain interfaces and is free from interior resonances. Extensive numerical experiments confirm the method's accuracy, convergence behavior, and efficiency. This advancement represents a significant step forward in the modeling and simulation of complex systems with junctions. <br /> <div>
arXiv:2506.16376v1 Announce Type: new 
Abstract: The PMCHWT integral equation enables the modelling of scattering of time-harmonic fields by penetrable, piecewise homogeneous, systems. They have been generalised to include the modelling of composite systems that may contain junctions, i.e. lines along which three or more materials meet. Linear systems resulting upon discretisation of the PMCHWT are, because of their large dimension, typically solved by Krylov iterative methods. The number of iterations required for this solution critically depends on the eigenvalue distribution of the system matrix. For systems that do not contain junction lines, Calder\'on preconditioning, which was first applied to the electric field integral equation, has been generalised to the PMCHWT equation. When junctions are present, this approach cannot be applied. Alternative approaches, such as the global multi-trace method, conceptually remove the junction lines and as a result are amenable to Calder\'on preconditioning. This approach entails a doubling of the degrees of freedom, and the solution that is produced only approximately fulfils the continuity conditions at interfaces separating domains. In this contribution, a single trace quasi-local PMCHWT equation is introduced that requires a number of iterations for its solution that only slowly increases as the mesh size tends to zero. The method is constructed as a generalisation of the classic PMCHWT, and its discretisation is thoroughly discussed. A comprehensive suite of numerical experiments demonstrates the correctness, convergence behaviour, and efficiency of the method. The integral equation is demonstrated to be free from interior resonances.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aethorix v1.0: AI-Driven Inverse Design of Inorganic Materials for Scalable Industrial Innovation</title>
<link>https://arxiv.org/abs/2506.16609</link>
<guid>https://arxiv.org/abs/2506.16609</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, industrial manufacturing, materials development, Aethorix v1.0, R&amp;D pipelines 

Summary: 
Artificial intelligence for Science (AI4S) is revolutionizing industrial manufacturing by accelerating the discovery and optimization of advanced materials. Aethorix v1.0 is introduced as a platform that combines large language models, generative models for crystal design, and machine-learned potentials for property prediction. This platform aims to streamline the materials development cycle, from design to deployment, while adhering to manufacturing standards. The industrial value of Aethorix v1.0 is validated through a real use case, demonstrating its seamless integration into scalable R&amp;D pipelines. Its capabilities in objective mining, inorganic crystal design, and property prediction enable rapid advancement in materials science, offering a promising solution for the industry's challenges. Aethorix v1.0 showcases the potential of AI in transforming the field of industrial manufacturing and driving innovation in high-performance materials. 

<br /><br />Summary: <div>
arXiv:2506.16609v1 Announce Type: new 
Abstract: Artificial intelligence for Science (AI4S) is poised to transform industrial manufacturing by enabling the accelerated discovery and optimization of advanced (bio)materials, dramatically reducing development cycles, and unlocking novel high-performance solutions. We introduce Aethorix v1.0, a platform that integrates large language models for objective mining, diffusion-based generative models for zero-shot inorganic crystal design, and machine-learned interatomic potentials for rapid property prediction at ab initio accuracy. The platform is developed to enhance the full materials development cycle, ranging from design to deployment in use cases, while incorporating critical operational constraints to meet rigorous manufacturing standards. We validated its industrial value through a real use case, showcasing how the framework can be seamlessly embedded into scalable materials R&amp;D pipelines.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training Time Series Models with Stock Data Customization</title>
<link>https://arxiv.org/abs/2506.16746</link>
<guid>https://arxiv.org/abs/2506.16746</guid>
<content:encoded><![CDATA[
<div> stock selection, pre-training, transformer architecture, financial data, experimental results <br />
Summary:<br />
The paper introduces novel pre-training tasks tailored to stock data characteristics, including stock code classification, stock sector classification, and moving average prediction. The Stock Specialized Pre-trained Transformer (SSPT) is developed based on a two-layer transformer architecture. Experimental results demonstrate the effectiveness of the proposed pre-training methods, outperforming existing methods in terms of cumulative investment return ratio and Sharpe ratio across various stock datasets and time periods. The research also includes evaluations on simulated data to provide insights into understanding price series. The code for the proposed methods is publicly available. <div>
arXiv:2506.16746v1 Announce Type: new 
Abstract: Stock selection, which aims to predict stock prices and identify the most profitable ones, is a crucial task in finance. While existing methods primarily focus on developing model structures and building graphs for improved selection, pre-training strategies remain underexplored in this domain. Current stock series pre-training follows methods from other areas without adapting to the unique characteristics of financial data, particularly overlooking stock-specific contextual information and the non-stationary nature of stock prices. Consequently, the latent statistical features inherent in stock data are underutilized. In this paper, we propose three novel pre-training tasks tailored to stock data characteristics: stock code classification, stock sector classification, and moving average prediction. We develop the Stock Specialized Pre-trained Transformer (SSPT) based on a two-layer transformer architecture. Extensive experimental results validate the effectiveness of our pre-training methods and provide detailed guidance on their application. Evaluations on five stock datasets, including four markets and two time periods, demonstrate that SSPT consistently outperforms the market and existing methods in terms of both cumulative investment return ratio and Sharpe ratio. Additionally, our experiments on simulated data investigate the underlying mechanisms of our methods, providing insights into understanding price series. Our code is publicly available at: https://github.com/astudentuser/Pre-training-Time-Series-Models-with-Stock-Data-Customization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Traditional Technical Analysis with AI: A Multi-Agent LLM-Based Approach to Stock Market Forecasting</title>
<link>https://arxiv.org/abs/2506.16813</link>
<guid>https://arxiv.org/abs/2506.16813</guid>
<content:encoded><![CDATA[
<div> ElliottAgents, multi-agent system, Elliott Wave Principle, AI, stock market forecasting<br />
Summary:<br />
Traditional technical analysis methods struggle to predict trends in complex financial markets. ElliottAgents integrates the Elliott Wave Principle with AI to address these challenges. The system utilizes LLMs to enhance natural language understanding and decision-making in a multi-agent framework. By employing technologies like RAG and DRL, ElliottAgents continuously analyzes market data to identify wave patterns and predict future price movements. Experimental results on historical data from U.S. companies validate its effectiveness in pattern recognition and trend forecasting across different time frames. This research demonstrates the successful combination of traditional technical analysis with modern AI approaches, providing traders with reliable and interpretable market prediction systems. <div>
arXiv:2506.16813v1 Announce Type: new 
Abstract: Traditional technical analysis methods face limitations in accurately predicting trends in today's complex financial markets. This paper introduces ElliottAgents, an multi-agent system that integrates the Elliott Wave Principle with AI for stock market forecasting. The inherent complexity of financial markets, characterized by non-linear dynamics, noise, and susceptibility to unpredictable external factors, poses significant challenges for accurate prediction. To address these challenges, the system employs LLMs to enhance natural language understanding and decision-making capabilities within a multi-agent framework. By leveraging technologies such as Retrieval-Augmented Generation (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs continuous, multi-faceted analysis of market data to identify wave patterns and predict future price movements. The research explores the system's ability to process historical stock data, recognize Elliott wave patterns, and generate actionable insights for traders. Experimental results, conducted on historical data from major U.S. companies, validate the system's effectiveness in pattern recognition and trend forecasting across various time frames. This paper contributes to the field of AI-driven financial analysis by demonstrating how traditional technical analysis methods can be effectively combined with modern AI approaches to create more reliable and interpretable market prediction systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Deprivation Cost Functions for Power Outages During Disasters: A Discrete Choice Modeling Approach</title>
<link>https://arxiv.org/abs/2506.16993</link>
<guid>https://arxiv.org/abs/2506.16993</guid>
<content:encoded><![CDATA[
<div> deprivation costs, electricity outages, stated preference survey, discrete choice model, Harris County<br />
<br />
Summary: 
This study addresses the lack of systematic measurement of deprivation costs related to electricity outages by developing a methodology to estimate deprivation cost functions using stated preference survey data from Harris County, Texas. The analysis compares different model architectures and utility transformations to capture the increasing and convex nature of deprivation cost functions over time. The study also identifies heterogeneity in deprivation valuation, particularly among different income groups. The results show that deprivation cost functions are strictly increasing with time and highlight the importance of flexible modeling approaches to account for systematic and random taste variation. By providing a methodological framework and empirical evidence, this research enables policymakers to better quantify service disruption costs and develop more equitable resilience strategies in infrastructure risk assessments and humanitarian logistics. <br /><br /> <div>
arXiv:2506.16993v1 Announce Type: new 
Abstract: Systems for the generation and distribution of electrical power represents critical infrastructure and, when extreme weather events disrupt such systems, this imposes substantial costs on consumers. These costs can be conceptualized as deprivation costs, an increasing function of time without service, quantifiable through individuals' willingness to pay for power restoration. Despite widespread recognition of outage impacts, a gap in the research literature exists regarding the systematic measurement of deprivation costs. This study addresses this deficiency by developing and implementing a methodology to estimate deprivation cost functions for electricity outages, using stated preference survey data collected from Harris County, Texas. This study compares multiple discrete choice model architectures, including multinomial logit and mixed logit specifications, as well as models incorporating BoxCox and exponential utility transformations for the deprivation time attribute. The analysis examines heterogeneity in deprivation valuation through sociodemographic interactions, particularly across income groups. Results confirm that power outage deprivation cost functions are convex and strictly increasing with time. Additionally, the study reveals both systematic and random taste variation in how individuals value power loss, highlighting the need for flexible modeling approaches. By providing both methodological and empirical foundations for incorporating deprivation costs into infrastructure risk assessments and humanitarian logistics, this research enables policymakers to better quantify service disruption costs and develop more equitable resilience strategies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance Language Model Evaluation (FLaME)</title>
<link>https://arxiv.org/abs/2506.15846</link>
<guid>https://arxiv.org/abs/2506.15846</guid>
<content:encoded><![CDATA[
<div> Language Models, Finance NLP, Evaluation Frameworks, Benchmarking, Reasoning-reinforced LMs <br />
Summary: <br />
This paper introduces a benchmarking suite, FLaME, for evaluating Language Models (LMs) on Finance NLP tasks. It addresses gaps in existing evaluation methodologies and challenges the belief in LMs' lower performance bounds in finance tasks. The study compares 23 LMs across 20 core NLP tasks in finance, including 'reasoning-reinforced' LMs. The research aims to demonstrate the potential of LMs in specialized finance tasks and provides open-source access to the FLaME framework, data, and results. <div>
arXiv:2506.15846v1 Announce Type: cross 
Abstract: Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials</title>
<link>https://arxiv.org/abs/2506.16918</link>
<guid>https://arxiv.org/abs/2506.16918</guid>
<content:encoded><![CDATA[
<div> deep learning, multiscale modeling, FE^2 approach, computational homogenization, viscoelastic material

Summary:
This study focuses on developing a hybrid model that combines data-driven deep learning techniques with physics-based approaches for multiscale simulations in time-dependent solid mechanics problems. By incorporating neural operators to predict microscale physics, the model efficiently predicts homogenized stresses with minimal error (less than 6%) while being approximately 100 times faster computationally. The approach integrates constitutive relations of the microscale into the model architecture and computes internal variables based on established physical principles. This hybrid model allows for physics-guided learning and flexibility for different materials and spatial discretizations, making it a promising method for accurately predicting the global response of materials influenced by microstructure across various time and length scales. <br /><br />Summary: <div>
arXiv:2506.16918v1 Announce Type: cross 
Abstract: The behavior of materials is influenced by a wide range of phenomena occurring across various time and length scales. To better understand the impact of microstructure on macroscopic response, multiscale modeling strategies are essential. Numerical methods, such as the $\text{FE}^2$ approach, account for micro-macro interactions to predict the global response in a concurrent manner. However, these methods are computationally intensive due to the repeated evaluations of the microscale. This challenge has led to the integration of deep learning techniques into computational homogenization frameworks to accelerate multiscale simulations. In this work, we employ neural operators to predict the microscale physics, resulting in a hybrid model that combines data-driven and physics-based approaches. This allows for physics-guided learning and provides flexibility for different materials and spatial discretizations. We apply this method to time-dependent solid mechanics problems involving viscoelastic material behavior, where the state is represented by internal variables only at the microscale. The constitutive relations of the microscale are incorporated into the model architecture and the internal variables are computed based on established physical principles. The results for homogenized stresses ($<6\%$ error) show that the approach is computationally efficient ($\sim 100 \times$ faster).
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete nonlinear elastodynamics in a port-Hamiltonian framework</title>
<link>https://arxiv.org/abs/2306.17740</link>
<guid>https://arxiv.org/abs/2306.17740</guid>
<content:encoded><![CDATA[
<div> Keywords: port-Hamiltonian, discrete elastodynamical systems, nonlinear strains, hyperelastic material behavior, midpoint discrete gradient <br />
<br />
Summary: 
The article presents a fully nonlinear port-Hamiltonian formulation for discrete elastodynamical systems. The governing equations are derived variationaly, resulting in index-1 differential algebraic equations. By performing index reduction, a port-Hamiltonian state space model is obtained, featuring nonlinear strains as independent states. The model captures hyperelastic material behavior through a nonlinear stored energy function and exhibits passivity, losslessness, and symmetry conserving angular momentum. Temporal discretization is achieved using the midpoint discrete gradient, preserving the model's beneficial properties in a discrete sense. Numerical validation in a representative example confirms the effectiveness of the proposed approach. <div>
arXiv:2306.17740v2 Announce Type: replace-cross 
Abstract: We provide a fully nonlinear port-Hamiltonian formulation for discrete elastodynamical systems as well as a structure-preserving time discretization. The governing equations are obtained in a variational manner and represent index-1 differential algebraic equations. Performing an index reduction one obtains the port-Hamiltonian state space model, which features the nonlinear strains as an independent state next to position and velocity. Moreover, hyperelastic material behavior is captured in terms of a nonlinear stored energy function. The model exhibits passivity and losslessness and has an underlying symmetry yielding the conservation of angular momentum. We perform temporal discretization using the midpoint discrete gradient, such that the beneficial properties are inherited by the developed time stepping scheme in a discrete sense. The numerical results obtained in a representative example are demonstrated to validate the findings.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid high-order methods for elasto-acoustic wave propagation in the time domain</title>
<link>https://arxiv.org/abs/2502.10870</link>
<guid>https://arxiv.org/abs/2502.10870</guid>
<content:encoded><![CDATA[
<div> Keywords: Hybrid High-Order method, Acoustic and Elastic wave equations, Time domain, Spectral analysis, Elasto-acoustic wave propagation<br />
<br />
Summary: 
The study introduces a Hybrid High-Order (HHO) method for coupling acoustic and elastic wave equations in the time domain. The method, utilizing first-order time formulation, can employ equal-order and mixed-order settings with polynomial degree k>=0 for face unknowns, along with O(1)- and O(1/h)-stabilizations. An energy-error estimate is established for the time-continuous scenario, and a numerical spectral analysis reveals the necessity of O(1)-stabilization to avoid excessive CFL limitations with explicit time discretizations. Optimal convergence rates of order (k+1) are demonstrated for general mesh analytical solutions in both equal- and mixed-order settings with O(1)-stabilization, while mixed-order setting with O(1/h)-stabilization achieves order (k+2). Test cases using a Ricker wavelet as an initial condition illustrate the method's effectiveness in simulating elasto-acoustic wave propagation in media with differing material properties. <div>
arXiv:2502.10870v2 Announce Type: replace-cross 
Abstract: We devise a Hybrid High-Order (HHO) method for the coupling between the acoustic and elastic wave equations in the time domain. A first-order formulation in time is considered. The HHO method can use equal-order and mixed-order settings with polynomial degree k>=0 for the face unknowns, together with O(1)- and O(1/h)-stabilizations. An energy-error estimate is established in the time-continuous case. A numerical spectral analysis is performed, showing that O(1)-stabilization is required to avoid excessive CFL limitations for explicit time discretizations. Moreover, the spectral radius of the stiffness matrix is fairly independent of the geometry of the mesh cells. For analytical solutions on general meshes, optimal convergence rates of order (k+1) are shown in both equal- and mixed-order settings using O(1)-stabilization, whereas order (k+2) is achieved in the mixed-order setting using O(1/h)-stabilization. Test cases with a Ricker wavelet as an initial condition showcase the relevance of the proposed method for the simulation of elasto-acoustic wave propagation across media with contrasted material properties.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.14933</link>
<guid>https://arxiv.org/abs/2506.14933</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized finance, DeFi, cryptocurrency, financial crime, automated detection tools

Summary:
The article discusses the rapid growth of the decentralized finance (DeFi) community driven by cryptocurrency enthusiasts and the resulting increase in financial crime. The rise of cryptocurrency has created opportunities for criminal activity, and the complex and decentralized nature of the technology makes it challenging to catch and prosecute offenders. To address this issue, the implementation of automated detection tools and policies is essential. These tools can help detect and prevent financial crimes in the cryptocurrency realm, ensuring a safer environment for users and investors. By utilizing technology to monitor and enforce regulations, the DeFi community can mitigate the risks associated with criminal activity and maintain the integrity of the market. <div>
arXiv:2506.14933v1 Announce Type: new 
Abstract: The decentralized finance (DeFi) community has grown rapidly in recent years, pushed forward by cryptocurrency enthusiasts interested in the vast untapped potential of new markets. The surge in popularity of cryptocurrency has ushered in a new era of financial crime. Unfortunately, the novelty of the technology makes the task of catching and prosecuting offenders particularly challenging. Thus, it is necessary to implement automated detection tools related to policies to address the growing criminality in the cryptocurrency realm.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing Structural Vibrations via Guided Flow Matching Design Optimization</title>
<link>https://arxiv.org/abs/2506.15263</link>
<guid>https://arxiv.org/abs/2506.15263</guid>
<content:encoded><![CDATA[
<div> Flow matching, design optimization, structural vibrations, plate-like structures, generative model

Summary: 
This article introduces a novel design optimization approach for reducing structural vibrations in plate-like structures by incorporating guided flow matching. The method combines a generative flow matching model with a surrogate model trained to predict structural vibrations, aiming to create manufacturable designs with low vibrations. By leveraging the flow matching model and its training data, the approach explores a wide range of potential solutions without the need for manually-defined design parameters. Various optimization objectives, including direct optimization of specific eigenfrequencies, are considered to improve passenger comfort in engineering systems like cars, trains, and airplanes. Results show that the method outperforms random search, criterion-based design heuristics, and genetic optimization in generating diverse and effective plate designs with reduced vibrations. The code and data for this approach are publicly available for further research and application. 

<br /><br />Summary: <div>
arXiv:2506.15263v1 Announce Type: new 
Abstract: Structural vibrations are a source of unwanted noise in engineering systems like cars, trains or airplanes. Minimizing these vibrations is crucial for improving passenger comfort. This work presents a novel design optimization approach based on guided flow matching for reducing vibrations by placing beadings (indentations) in plate-like structures. Our method integrates a generative flow matching model and a surrogate model trained to predict structural vibrations. During the generation process, the flow matching model pushes towards manufacturability while the surrogate model pushes to low-vibration solutions. The flow matching model and its training data implicitly define the design space, enabling a broader exploration of potential solutions as no optimization of manually-defined design parameters is required. We apply our method to a range of differentiable optimization objectives, including direct optimization of specific eigenfrequencies through careful construction of the objective function. Results demonstrate that our method generates diverse and manufacturable plate designs with reduced structural vibrations compared to designs from random search, a criterion-based design heuristic and genetic optimization. The code and data are available from https://github.com/ecker-lab/Optimizing_Vibrating_Plates.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation of parametrized cardiac electrophysiology in three dimensions using physics-informed neural networks</title>
<link>https://arxiv.org/abs/2506.15405</link>
<guid>https://arxiv.org/abs/2506.15405</guid>
<content:encoded><![CDATA[
<div> PINNs, cardiac electrophysiology, neural networks, Aliev-Panfilov model, 3D prediction<br />
<br />
Summary:
Physics-informed neural networks (PINNs) are used in cardiac electrophysiology to predict myocardium activity in 3D based on the Aliev-Panfilov model. An FCNN architecture is employed with scaled input normalization and the strong form of partial differential equations. Training data is generated using the finite element method. Variations in spatial dimensions and parameters are studied with comparison to FE simulations. Losses in the network are weighted and controlled to optimize training and prediction accuracy. By investigating optimal hyperparameters, this study aims to improve the accuracy of predicting action potential and recovery variable fields in the myocardium. <div>
arXiv:2506.15405v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) are extensively used to represent various physical systems across multiple scientific domains. The same can be said for cardiac electrophysiology, wherein fully-connected neural networks (FCNNs) have been employed to predict the evolution of an action potential in a 2D space following the two-parameter phenomenological Aliev-Panfilov (AP) model. In this paper, the training behaviour of PINNs is investigated to determine optimal hyperparameters to predict the electrophysiological activity of the myocardium in 3D according to the AP model, with the inclusion of boundary and material parameters. An FCNN architecture is employed with the governing partial differential equations in their strong form, which are scaled consistently with normalization of network inputs. The finite element (FE) method is used to generate training data for the network. Numerical examples with varying spatial dimensions and parameterizations are generated using the trained models. The network predicted fields for both the action potential and the recovery variable are compared with the respective FE simulations. Network losses are weighed with individual scalar values. Their effect on training and prediction is studied to arrive at a method of controlling losses during training.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An improved point-to-surface contact algorithm with penalty method for peridynamics</title>
<link>https://arxiv.org/abs/2408.06556</link>
<guid>https://arxiv.org/abs/2408.06556</guid>
<content:encoded><![CDATA[
<div> contact forces, peridynamics, point-to-surface, accuracy, surface particles identification

Summary:
The article proposes an improved point-to-surface contact model for accurate contact force determination in peridynamics simulations, especially for complex geometries. The outer surface is identified using an eigenvalue method, and a Verlet list efficiently identifies potential contact particle pairs. A point-to-surface contact search algorithm, coupled with a penalty function method, is used to calculate precise contact locations and forces. Validation through various contact examples confirms high accuracy, aligning well with Hertz contact theory solutions. The model automatically recognizes external surface particles and accurately computes contact forces, providing valuable insights for multi-body and complex contact scenarios. <br /><br />Summary: <div>
arXiv:2408.06556v2 Announce Type: replace 
Abstract: It is significantly challenging to obtain accurate contact forces in peridynamics (PD) simulations due to the difficulty of surface particles identification, particularly for complex geometries. Here, an improved point-to-surface contact model is proposed for PD with high accuracy. First, the outer surface is identified using the eigenvalue method and then we construct a Verlet list to identify potential contact particle pairs efficiently. Subsequently, a point-to-surface contact search algorithm is utilized to determine precise contact locations with the penalty function method calculating the contact force. Finally, the accuracy of this point-to-surface contact model is validated through several representative contact examples. The results demonstrate that the point-to-surface contact model model can predict contact forces and deformations with high accuracy, aligning well with the classical Hertz contact theory solutions. This work presents a contact model for PD that automatically recognizes external surface particles and accurately calculates the contact force, which provides guidance for the study of multi-body contact as well as complex contact situations.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing oncology with federated learning: transcending boundaries in breast, lung, and prostate cancer. A systematic review</title>
<link>https://arxiv.org/abs/2408.05249</link>
<guid>https://arxiv.org/abs/2408.05249</guid>
<content:encoded><![CDATA[
<div> FL, oncology, federated learning, cancer, data privacy
<br />
Summary:
This systematic review explores the state-of-the-art of Federated Learning (FL) in oncology, focusing on breast, lung, and prostate cancer. The review highlights FL as a promising solution to overcome privacy concerns and utilize diverse, multi-center data in clinical settings. FL showed superior performance compared to centralised machine learning in 15 out of 25 studies, demonstrating its potential to enhance ML generalisability and data privacy. Despite challenges in reproducibility and methodology standardisation, FL proved effective in integrating multi-modal information for precision medicine. The review calls for future research to address these limitations and explore advanced FL methods to fully leverage data diversity in advancing cancer research. Ultimately, FL presents significant potential for transforming cancer care through its ability to harness real-world data and address clinical needs. 
<br /><br /> <div>
arXiv:2408.05249v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has emerged as a promising solution to address the limitations of centralised machine learning (ML) in oncology, particularly in overcoming privacy concerns and harnessing the power of diverse, multi-center data. This systematic review synthesises current knowledge on the state-of-the-art FL in oncology, focusing on breast, lung, and prostate cancer. Distinct from previous surveys, our comprehensive review critically evaluates the real-world implementation and impact of FL on cancer care, demonstrating its effectiveness in enhancing ML generalisability, performance and data privacy in clinical settings and data. We evaluated state-of-the-art advances in FL, demonstrating its growing adoption amid tightening data privacy regulations. FL outperformed centralised ML in 15 out of the 25 studies reviewed, spanning diverse ML models and clinical applications, and facilitating integration of multi-modal information for precision medicine. Despite the current challenges identified in reproducibility, standardisation and methodology across studies, the demonstrable benefits of FL in harnessing real-world data and addressing clinical needs highlight its significant potential for advancing cancer research. We propose that future research should focus on addressing these limitations and investigating further advanced FL methods, to fully harness data diversity and realise the transformative power of cutting-edge FL in cancer care.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy conservative and entropy stable solid wall boundary conditions for the resistive magnetohydrodynamic equations</title>
<link>https://arxiv.org/abs/2412.11132</link>
<guid>https://arxiv.org/abs/2412.11132</guid>
<content:encoded><![CDATA[
<div> diagonal-norm, summation-by-parts, entropy conservative, resistive magnetohydrodynamic equations, boundary conditions <br />
<br />
Summary: <br /> 
This article presents a novel technique for imposing non-linear entropy conservative and entropy stable wall boundary conditions for the resistive magnetohydrodynamic equations in various scenarios. The method relies on diagonal-norm, summation-by-parts, and simultaneous-approximation-term operators for ensuring entropy stability. By using a penalty flux approach and simultaneous-approximation-term technique, boundary data at the wall are weakly imposed. The proposed technique demonstrates accuracy, robustness, and efficacy in enforcing boundary conditions on high-order unstructured grids. It is shown to be compatible with various spatial operators, including finite element and finite volume schemes. Numerical simulations confirm the stability of the method in three-dimensional flows, making it a valuable tool for accurately modeling complex MHD systems. <div>
arXiv:2412.11132v2 Announce Type: replace-cross 
Abstract: We present a novel technique for imposing non-linear entropy conservative and entropy stable wall boundary conditions for the resistive magnetohydrodynamic equations in the presence of an adiabatic wall or a wall with a prescribed heat entropy flow, addressing three scenarios: electrically insulating walls, thin walls with finite conductivity, and perfectly conducting walls. The procedure relies on the formalism and mimetic properties of diagonal-norm, summation-by-parts, and simultaneous-approximation-term operators. Using the method of lines, a semi-discrete entropy estimate for the entire domain is obtained when the proposed numerical imposition of boundary conditions is coupled with an entropy-conservative or entropy-stable discrete interior operator. The resulting estimate mimics the global entropy estimate obtained at the continuous level. The boundary data at the wall are weakly imposed using a penalty flux approach and a simultaneous-approximation-term technique for both the conservative variables and the gradient of the entropy variables. Discontinuous spectral collocation operators (mass lumped nodal discontinuous Galerkin operators) on high-order unstructured grids are used to demonstrate the new procedure's accuracy, robustness, and efficacy for weakly enforcing boundary conditions. Numerical simulations confirm the non-linear stability of the proposed technique, with applications to three-dimensional flows. The procedure described is compatible with any diagonal-norm summation-by-parts spatial operator, including finite element, finite difference, finite volume, nodal and modal discontinuous Galerkin, and flux reconstruction schemes.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Chain Arbitrage: The Next Frontier of MEV in Decentralized Finance</title>
<link>https://arxiv.org/abs/2501.17335</link>
<guid>https://arxiv.org/abs/2501.17335</guid>
<content:encoded><![CDATA[
<div> arbitrage, DeFi, decentralized exchanges, cross-chain, inventory-based execution 

Summary: 
The study focuses on cross-chain arbitrage in decentralized finance (DeFi) markets, where prices are aligned through arbitrage between Layer-1 (L1) and Layer-2 (L2) blockchains. Centralized exchanges (CEXes) currently play a key role in price discovery, but as trading volume shifts on-chain, cross-chain arbitrage between decentralized exchanges (DEXes) is becoming more significant. The research includes a profit-cost model and a year-long measurement of arbitrage activity across nine blockchains. Findings show a high concentration of market activity on Ethereum-centric pairs, with a surge in volume after a blockchain upgrade. Most trades use pre-positioned inventory and settle quickly, while bridge-based arbitrages face latency issues. The study highlights the centralized nature of cross-chain arbitrage, leading to vertical integration and concentration of economic power, posing risks of censorship and centralization in the DeFi space. Decentralizing block building and reducing entry barriers are suggested as critical measures to address these challenges.<br /><br />Summary: <div>
arXiv:2501.17335v2 Announce Type: replace-cross 
Abstract: Decentralized finance (DeFi) markets spread across Layer-1 (L1) and Layer-2 (L2) blockchains rely on arbitrage to keep prices aligned. Today most price gaps are closed against centralized exchanges (CEXes), whose deep liquidity and fast execution make them the primary venue for price discovery. As trading volume migrates on-chain, cross-chain arbitrage between decentralized exchanges (DEXes) will become the canonical mechanism for price alignment. Yet, despite its importance to DeFi-and the on-chain transparency making real activity tractable in a way CEX-to-DEX arbitrage is not-existing research remains confined to conceptual overviews and hypothetical opportunity analyses.
  We study cross-chain arbitrage with a profit-cost model and a year-long measurement. The model shows that opportunity frequency, bridging time, and token depreciation determine whether inventory- or bridge-based execution is more profitable. Empirically, we analyze one year of transactions (September 2023 - August 2024) across nine blockchains and identify 242,535 executed arbitrages totaling 868.64 million USD volume. Activity clusters on Ethereum-centric L1-L2 pairs, grows 5.5x over the study period, and surges-higher volume, more trades, lower fees-after the Dencun upgrade (March 13, 2024). Most trades use pre-positioned inventory (66.96%) and settle in 9s, whereas bridge-based arbitrages take 242s, underscoring the latency cost of today's bridges. Market concentration is high: the five largest addresses execute more than half of all trades, and one alone captures almost 40% of daily volume post-Dencun. We conclude that cross-chain arbitrage fosters vertical integration, centralizing sequencing infrastructure and economic power and thereby exacerbating censorship, liveness, and finality risks; decentralizing block building and lowering entry barriers are critical to countering these threats.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Oder Splitting Schemes for Fluids with Variable Viscosity</title>
<link>https://arxiv.org/abs/2506.14424</link>
<guid>https://arxiv.org/abs/2506.14424</guid>
<content:encoded><![CDATA[
<div> Keywords: matrix-free, discontinuous Galerkin, Navier-Stokes equations, variable viscosity, nonlinear solver<br />
Summary: <br />
This article explores matrix-free higher-order discontinuous Galerkin (DG) discretizations of the Navier-Stokes equations for incompressible flows with variable viscosity. The study involves comparing linearized variants of saddle point block systems and projection-based splitting time integration schemes in terms of computational performance. The research extends the dual splitting method for non-constant viscosity, presents a higher-order DG method for incompressible flows with variable viscosity, introduces accelerated nonlinear solver variants, and evaluates monolithic and projection-based solvers in terms of their solver performance. The schemes are tested in numerical examples to verify spatial and temporal accuracy, preconditioner performance under increased viscosity contrasts, and efficiency in the backward-facing step benchmark. The study investigates conditions under which fully implicit schemes with improved temporal stability and expensive nonlinear solves outperform stable linearized variants and splitting schemes. <div>
arXiv:2506.14424v1 Announce Type: new 
Abstract: This article investigates matrix-free higher-order discontinuous Galerkin (DG) discretizations of the Navier-Stokes equations for incompressible flows with variable viscosity. The viscosity field may be prescribed analytically or governed by a rheological law, as often found in biomedical or industrial applications. We compare several linearized variants of saddle point block systems and projection-based splitting time integration schemes in terms of their computational performance. Compared to the velocity-pressure block-system for the former, the splitting scheme allows solving a sequence of simple problems such as mass, convection-diffusion and Poisson equations. We investigate under which conditions the improved temporal stability of fully implicit schemes and resulting expensive nonlinear solves outperform the splitting schemes and linearized variants that are stable under hyperbolic time step restrictions.
  The key aspects of this work are i) the extension of the dual splitting method originally proposed by G.E. Karniadakis et al. (J. Comput. Phys. 97, 414-443, 1991) towards non-constant viscosity, ii) a higher-order DG method for incompressible flows with variable viscosity, iii) accelerated nonlinear solver variants and suitable linearizations adopting a matrix-free $hp$-multigrid solver, and iv) a detailed comparison of the monolithic and projection-based solvers in terms of their (non-)linear solver performance.
  The presented schemes are evaluated in a series of numerical examples verifying their spatial and temporal accuracy, and the preconditioner performance under increasing viscosity contrasts, while their efficiency is showcased in the backward-facing step benchmark.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Digital Twins via Active Inference</title>
<link>https://arxiv.org/abs/2506.14453</link>
<guid>https://arxiv.org/abs/2506.14453</guid>
<content:encoded><![CDATA[
<div> Keywords: digital twin, active inference, Bayesian framework, predictive maintenance, railway bridge

Summary:
The paper introduces the concept of active digital twins, which leverage active inference, a neuroscience-inspired Bayesian framework, to enhance adaptability in uncertain and dynamic environments. By formulating the evolution of the active digital twin as a partially observable Markov decision process, the agent continuously refines its generative model through Bayesian updates. Decision-making is based on balancing exploitation and exploration, with actions planned to minimize expected free energy. This approach offers superior exploration capabilities, increasing autonomy and resilience in digital twins. The framework is applied to the health monitoring and predictive maintenance of a railway bridge, demonstrating its effectiveness in real-world applications. <div>
arXiv:2506.14453v1 Announce Type: new 
Abstract: Digital twins are transforming engineering and applied sciences by enabling real-time monitoring, simulation, and predictive analysis of physical systems and processes. However, conventional digital twins rely primarily on passive data assimilation, which limits their adaptability in uncertain and dynamic environments. This paper introduces the active digital twin paradigm, based on active inference. Active inference is a neuroscience-inspired, Bayesian framework for probabilistic reasoning and predictive modeling that unifies inference, decision-making, and learning under a unique, free energy minimization objective. By formulating the evolution of the active digital twin as a partially observable Markov decision process, the active inference agent continuously refines its generative model through Bayesian updates and forecasts future states and observations. Decision-making emerges from an optimization process that balances pragmatic exploitation (maximizing goal-directed utility) and epistemic exploration or information gain (actively resolving uncertainty). Actions are dynamically planned to minimize expected free energy, which quantifies both the divergence between predicted and preferred future observations, and the epistemic value of expected information gain about hidden states. This approach enables a new level of autonomy and resilience in digital twins, offering superior spontaneous exploration capabilities. The proposed framework is assessed on the health monitoring and predictive maintenance of a railway bridge.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Framework for Climate-Resilient Insurance and Real Estate Decisions</title>
<link>https://arxiv.org/abs/2506.14638</link>
<guid>https://arxiv.org/abs/2506.14638</guid>
<content:encoded><![CDATA[
<div> Keywords: Extreme weather events, Insurance viability, Building protection, Climate risks, Sustainability

Summary: 
The SSC-Insurance Model, combining SMOTE, SVM, and C-D-C algorithms, evaluates weather impacts on policies and investments, achieving high accuracy in Zhejiang and Ireland. It determines a critical threshold (43% weather increase) for insurance viability. The TOA-Preservation Model, utilizing TOPSIS-ORM and AHP, prioritizes building protection, with cultural value deemed most significant. Case studies on Nanxun Ancient Town reveal a 65.32% insurability probability and a protection score of 0.512. This research equips insurers, developers, and policymakers with practical tools to manage climate risks sustainably.<br /><br />Summary: <div>
arXiv:2506.14638v1 Announce Type: new 
Abstract: Extreme weather events increasingly threaten the insurance and real estate industries, creating conflicts between profitability and homeowner burdens. To address this, we propose the SSC-Insurance Model, which integrates SMOTE, SVM, and C-D-C algorithms to evaluate weather impacts on policies and investments. Our model achieves 88.3% accuracy in Zhejiang and 79.6% in Ireland, identifying a critical threshold (43% weather increase) for insurance viability. Additionally, we develop the TOA-Preservation Model using TOPSIS-ORM and AHP to prioritize building protection, with cultural value scoring highest (weight: 0.3383). Case studies on Nanxun Ancient Town show a 65.32% insurability probability and a protection score of 0.512. This work provides actionable tools for insurers, developers, and policymakers to manage climate risks sustainably.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimistic MEV in Ethereum Layer 2s: Why Blockspace Is Always in Demand</title>
<link>https://arxiv.org/abs/2506.14768</link>
<guid>https://arxiv.org/abs/2506.14768</guid>
<content:encoded><![CDATA[
<div> optimistic MEV, Layer 2 rollups, DeFi, on-chain arbitrage, Ethereum gas fees
<br />
Summary:
Layer 2 rollups in DeFi have absorbed over $40 billion and nearly half of Ethereum's DEX volume by Q1 2025, but their MEV dynamics are understudied. This study defines and quantifies optimistic MEV, a speculative on-chain arbitrage method prevalent on Arbitrum, Base, and Optimism. In Q1 2025, optimistic MEV accounted for over 50% of on-chain gas on Base and Optimism and 7% on Arbitrum, mainly driven by on-chain computations for arbitrage detection. Despite high gas consumption, optimistic MEV transactions pay only a fraction of total gas fees. Different success rates, code reuse patterns, and sensitivities to sequencing and block production times across networks were observed. OLS regressions linked optimistic MEV trades to ETH volatility, retail trading, and DEX aggregator usage, highlighting how Layer 2 protocols uniquely incentivize speculative MEV. 
<br /> <div>
arXiv:2506.14768v1 Announce Type: new 
Abstract: Layer 2 rollups are rapidly absorbing DeFi activity, securing over $40 billion and accounting for nearly half of Ethereum's DEX volume by Q1 2025, yet their MEV dynamics remain understudied. We address this gap by defining and quantifying optimistic MEV, a form of speculative, on-chain cyclic arbitrage whose detection and execution logic reside largely on-chain in smart contracts. As a result of their speculative nature and lack of off-chain opportunity verification, optimistic MEV transactions frequently fail to execute a profitable arbitrage.
  Applying our multi-stage identification pipeline to Arbitrum, Base, and Optimism, we find that in Q1 2025, optimistic MEV accounts for over 50% of on-chain gas on Base and Optimism and 7% on Arbitrum, driven mainly by "interaction" probes (on-chain computations searching for arbitrage). This speculative probing keeps blocks on Base and Optimism persistently full. Despite consuming over half of on-chain gas, optimistic MEV transactions pay less than one quarter of total gas fees. Cross-network comparison reveals divergent success rates, differing patterns of code reuse, and sensitivity to varying sequencer ordering and block production times. Finally, OLS regressions link optimistic MEV trade count to ETH volatility, retail trading activity, and DEX aggregator usage, showing how Layer 2 protocol parameters uniquely encourage speculative MEV.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Charging Scheduling via Balanced Bounding Box Methods</title>
<link>https://arxiv.org/abs/2506.14461</link>
<guid>https://arxiv.org/abs/2506.14461</guid>
<content:encoded><![CDATA[
<div> Keywords: Electric mobility, charging infrastructure, sustainable urban logistics, collaborative scheduling, bi-objective optimization

Summary:
The paper examines the challenges facing electric mobility and proposes shared charging as a solution. It focuses on sustainable urban logistics by facilitating collaboration between fleet operators. A bi-objective nonlinear integer programming model is formulated to address the scheduling problem for shared charging stations, balancing the cost minimization goals of each company. The Balanced Bounding Box Methods (B3Ms) are introduced to efficiently derive the efficient frontier, reducing computational time while maintaining solution diversity. Cooperative bargaining methods are applied to determine the final solution and promote balanced collaboration. Numerical case studies demonstrate the effectiveness and scalability of the developed methods, showcasing their potential for solving various bi-objective optimization problems beyond the collaborative scheduling issue presented in the study.<br /><br />Summary: <div>
arXiv:2506.14461v1 Announce Type: cross 
Abstract: Electric mobility faces several challenges, most notably the high cost of infrastructure development and the underutilization of charging stations. The concept of shared charging offers a promising solution. The paper explores sustainable urban logistics through horizontal collaboration between two fleet operators and addresses a scheduling problem for the shared use of charging stations. To tackle this, the study formulates a collaborative scheduling problem as a bi-objective nonlinear integer programming model, in which each company aims to minimize its own costs, creating inherent conflicts that require trade-offs. The Balanced Bounding Box Methods (B3Ms) are introduced in order to efficiently derive the efficient frontier, identifying a reduced set of representative solutions. These methods enhance computational efficiency by selectively disregarding closely positioned and competing solutions, preserving the diversity and representativeness of the solutions over the efficient frontier. To determine the final solution and ensure balanced collaboration, cooperative bargaining methods are applied. Numerical case studies demonstrate the viability and scalability of the developed methods, showing that the B3Ms can significantly reduce computational time while maintaining the integrity of the frontier. These methods, along with cooperative bargaining, provide an effective framework for solving various bi-objective optimization problems, extending beyond the collaborative scheduling problem presented here.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and scalable exchange-correlation with deep learning</title>
<link>https://arxiv.org/abs/2506.14665</link>
<guid>https://arxiv.org/abs/2506.14665</guid>
<content:encoded><![CDATA[
<div> machine learning, density functional theory, electronic structure, predictive modeling, atomization energies

Summary:
Skala is a new deep learning-based exchange-correlation (XC) functional designed to improve the accuracy and generality of Density Functional Theory (DFT) for predicting molecular and material properties. Unlike traditional XC functionals, Skala learns representations directly from data without relying on hand-crafted features. By training on a large dataset of high-accuracy reference data, Skala achieves chemical accuracy for atomization energies of small molecules while maintaining computational efficiency. It improves systematically with additional diverse chemistry training data, competing with hybrid functionals in general main group chemistry at the cost of semi-local DFT. Skala's performance is expected to further enhance the predictive power of first-principles simulations as the training dataset expands. 

<br /><br />Summary: <div>
arXiv:2506.14665v1 Announce Type: cross 
Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schr\"odinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative analysis for different finite element types in strain-gradient elasticity simulations performed on Firedrake and FEniCS</title>
<link>https://arxiv.org/abs/2411.12043</link>
<guid>https://arxiv.org/abs/2411.12043</guid>
<content:encoded><![CDATA[
<div> Keywords: additive manufacturing, metamaterials, finite element methods, strain-gradient elasticity, open-source packages<br />
<br />
Summary: 
This study focuses on the use of finite element methods to solve problems in strain-gradient elasticity in architectured materials such as metallic foams. The research compares different finite element formulations, including Lagrange, Argyris, Hermite elements, a mixed formulation, and isogeometric analysis with NURBS. Open-source packages from Firedrake and FEniCS are utilized for the study. The numerical investigation includes one- and two-dimensional problems commonly found in strain-gradient modeling literature. The developed codes are openly accessible to promote research in FEM-based computation of generalized continua. <div>
arXiv:2411.12043v2 Announce Type: replace 
Abstract: The layer-upon-layer approach in additive manufacturing, open or closed cells in polymeric or metallic foams involve an intrinsic microstructure tailored to the underlying applications. Homogenization of such architectured materials creates metamaterials modeled by higher-gradient models, specifically when the microstructure's characteristic length is comparable to the length scale of the structure. In this study, we conduct a comparative analysis of various finite elements methods for solving problems in strain-gradient elasticity. We employ open-source packages from Firedrake and FEniCS. Different finite element formulations are tested: we implement Lagrange, Argyris, Hermite elements, a Hu--Washizu type (mixed) formulation, as well as isogeometric analysis with Non-Uniform Rational B-Splines (NURBS). For the numerical study, we investigate one- and two-dimensional problems discussed in the literature of strain-gradient modeling. All developed codes are open-access to encourage research in Finite Element Method (FEM) based computation of generalized continua.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effect of Selection Format on LLM Performance</title>
<link>https://arxiv.org/abs/2503.06926</link>
<guid>https://arxiv.org/abs/2503.06926</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, classification task, prompts, formatting, performance <br />
Summary: <br />
This paper delves into the impact of different formatting options, specifically bullet points versus plain English, on the performance of large language models (LLMs) in classification tasks. The extensive experimental study conducted by the researchers revealed that presenting classification options via bullet points generally led to better model performance, although there were some exceptions to this trend. The study emphasizes the significance of carefully considering the formatting of options in prompts to enhance the overall performance of LLMs. The research further indicates the need for ongoing exploration and experimentation with various formatting techniques to continue driving improvements in model performance. The findings suggest that the presentation of classification task options can significantly influence the efficiency and effectiveness of LLMs, underscoring the importance of this aspect in the development and optimization of language models. <br /><br />Summary: <div>
arXiv:2503.06926v2 Announce Type: replace-cross 
Abstract: This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks</title>
<link>https://arxiv.org/abs/2503.16974</link>
<guid>https://arxiv.org/abs/2503.16974</guid>
<content:encoded><![CDATA[
<div> consistency, reproducibility, Large Language Model, finance, accounting

Summary:
This study evaluates the consistency and reproducibility of Large Language Models (LLMs) in finance and accounting research. Through extensive experimentation with three OpenAI models and 50 independent runs across five tasks, the study found task-dependent consistency with binary classification and sentiment analysis achieving near-perfect reproducibility. While more advanced models did not consistently demonstrate better consistency, task-specific patterns emerged. LLMs outperformed human annotators in consistency and agreement, even in cases where experts disagreed. Simple aggregation strategies across 3-5 runs improved consistency and accuracy for sentiment analysis. Despite some inconsistency in LLM outputs, downstream statistical inferences remained robust. The study addressed concerns of "G-hacking" by showing that risks of selective reporting from multiple LLM runs are relatively low for finance and accounting tasks. <div>
arXiv:2503.16974v3 Announce Type: replace-cross 
Abstract: This study provides the first comprehensive assessment of consistency and reproducibility in Large Language Model (LLM) outputs in finance and accounting research. We evaluate how consistently LLMs produce outputs given identical inputs through extensive experimentation with 50 independent runs across five common tasks: classification, sentiment analysis, summarization, text generation, and prediction. Using three OpenAI models (GPT-3.5-turbo, GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse financial source texts and data, covering MD&amp;As, FOMC statements, finance news articles, earnings call transcripts, and financial statements. Our findings reveal substantial but task-dependent consistency, with binary classification and sentiment analysis achieving near-perfect reproducibility, while complex tasks show greater variability. More advanced models do not consistently demonstrate better consistency and reproducibility, with task-specific patterns emerging. LLMs significantly outperform expert human annotators in consistency and maintain high agreement even where human experts significantly disagree. We further find that simple aggregation strategies across 3-5 runs dramatically improve consistency. We also find that aggregation may come with an additional benefit of improved accuracy for sentiment analysis when using newer models. Simulation analysis reveals that despite measurable inconsistency in LLM outputs, downstream statistical inferences remain remarkably robust. These findings address concerns about what we term "G-hacking," the selective reporting of favorable outcomes from multiple Generative AI runs, by demonstrating that such risks are relatively low for finance and accounting tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Characterization of Aggregate Flexibility via Generalized Polymatroids</title>
<link>https://arxiv.org/abs/2503.23458</link>
<guid>https://arxiv.org/abs/2503.23458</guid>
<content:encoded><![CDATA[
<div> flexibility, distributed energy resources, aggregation, Minkowski sum, optimization <br />
Summary: 
The article discusses the importance of leveraging the flexibility of distributed energy resources (DERs) to address issues related to renewable generation variability. It highlights the challenge of accurately computing the aggregate flexibility of a population, proposing the use of generalized polymatroids to efficiently represent flexibility sets. The study shows that individual flexibility sets can be categorized under this family, facilitating the exact computation of their Minkowski sum. For homogeneous DER populations, simplified representations of aggregate flexibility are derived. An optimization framework is developed to efficiently allocate aggregate flexibility among individual DERs, with a vertex-based disaggregation method proposed. The approach's optimality and computational efficiency are validated through comparisons with existing methods. <div>
arXiv:2503.23458v2 Announce Type: replace-cross 
Abstract: It is well established that the aggregate flexibility inherent in populations of distributed energy resources (DERs) can be leveraged to mitigate the intermittency and uncertainty associated with renewable generation, while also providing ancillary grid services. To enable this, aggregators must effectively represent the flexibility in the populations they control to the market or system operator. A key challenge is accurately computing the aggregate flexibility of a population, which can be formally expressed as the Minkowski sum of a collection of polytopes, a problem that is generally computationally intractable. However, the flexibility polytopes of many DERs exhibit structural symmetries that can be exploited for computational efficiency. To this end, we introduce generalized polymatroids, a family of polytopes, into the flexibility aggregation literature. We demonstrate that individual flexibility sets belong to this family, enabling efficient computation of their exact Minkowski sum. For homogeneous populations of DERs we further derive simplifications that yield more succinct representations of aggregate flexibility. Additionally, we develop an efficient optimization framework over these sets and propose a vertex-based disaggregation method, to allocate aggregate flexibility among individual DERs. Finally, we validate the optimality and computational efficiency of our approach through comparisons with existing methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Classification of Levantine Ceramic Thin Sections via Neural Networks</title>
<link>https://arxiv.org/abs/2506.12250</link>
<guid>https://arxiv.org/abs/2506.12250</guid>
<content:encoded><![CDATA[
<div> deep learning, Convolutional Neural Networks, Vision Transformers, petrographic analysis, Levantine ceramics 

Summary:
Classification of ceramic thin sections is essential for understanding ancient pottery production techniques and trade networks. This study explores the use of deep learning models like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to classify Levantine ceramics based on their petrographic fabrics. A dataset of 1,424 thin section images from archaeological sites in the Levantine area was used to train these models, with transfer learning significantly improving classification accuracy. The ResNet18 model achieved 92.11% accuracy, while the ViT reached 88.34%. Explainability techniques such as Guided Grad-CAM and attention maps were used to interpret the models' decisions, showing that both CNNs and ViTs focus on key mineralogical features for classification. This study demonstrates the potential of explainable AI in archaeometric studies, providing an efficient and transparent methodology for ceramic analysis. 

<br /><br />Summary: <div>
arXiv:2506.12250v1 Announce Type: new 
Abstract: Classification of ceramic thin sections is fundamental for understanding ancient pottery production techniques, provenance, and trade networks. Although effective, traditional petrographic analysis is time-consuming. This study explores the application of deep learning models, specifically Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), as complementary tools to support the classification of Levantine ceramics based on their petrographic fabrics. A dataset of 1,424 thin section images from 178 ceramic samples belonging to several archaeological sites across the Levantine area, mostly from the Bronze Age, with few samples dating to the Iron Age, was used to train and evaluate these models. The results demonstrate that transfer learning significantly improves classification performance, with a ResNet18 model achieving 92.11% accuracy and a ViT reaching 88.34%. Explainability techniques, including Guided Grad-CAM and attention maps, were applied to interpret and visualize the models' decisions, revealing that both CNNs and ViTs successfully focus on key mineralogical features for the classification of the samples into their respective petrographic fabrics. These findings highlight the potential of explainable AI in archaeometric studies, providing a reproducible and efficient methodology for ceramic analysis while maintaining transparency in model decision-making.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified Newmark/Newton-Raphson method with automatic differentiation for general nonlinear dynamics analysis</title>
<link>https://arxiv.org/abs/2506.13226</link>
<guid>https://arxiv.org/abs/2506.13226</guid>
<content:encoded><![CDATA[
<div> automatic differentiation, Newmark/Newton-Raphson method, nonlinear dynamic systems, Jacobian matrix, complex nonlinear systems 

Summary: 
The study introduces the NNR-AD method, which integrates automatic differentiation into the Newmark/Newton-Raphson method to address limitations in solving complex nonlinear dynamic systems. By utilizing automatic differentiation, the NNR-AD method improves the NNR method's capability to handle complex nonlinear characteristics, simplifies the computation of Jacobian matrices, and enhances modularity for effective solutions. The NNR-AD method has been demonstrated to directly solve dynamic systems with complex nonlinear features, with rigorous validation of its accuracy and generality. This integration of automatic differentiation offers a significant advancement in solving complex nonlinear dynamic systems, providing a more efficient and effective approach compared to traditional methods. <br /><br /> <div>
arXiv:2506.13226v1 Announce Type: new 
Abstract: The Newmark/Newton-Raphson (NNR) method is widely employed for solving nonlinear dynamic systems. However, the current NNR method exhibits limited applicability in complex nonlinear dynamic systems, as the acquisition of the Jacobian matrix required for Newton iterations incurs substantial computational costs and may even prove intractable in certain cases. To address these limitations, we integrate automatic differentiation (AD) into the NNR method, proposing a modified NNR method with AD (NNR-AD) to significantly improve its capability for effectively handling complex nonlinear systems. We have demonstrated that the NNR-AD method can directly solve dynamic systems with complex nonlinear characteristics, and its accuracy and generality have been rigorously validated. Furthermore, automatic differentiation significantly simplifies the computation of Jacobian matrices for such complex nonlinear dynamic systems. This improvement endows the NNR method with enhanced modularity, thereby enabling convenient and effective solutions for complex nonlinear dynamic systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Entropy-Stable/Double-Flux scheme for the multi-component compressible Navier-Stokes equations</title>
<link>https://arxiv.org/abs/2506.13231</link>
<guid>https://arxiv.org/abs/2506.13231</guid>
<content:encoded><![CDATA[
<div> Entropy-Stable Formulation, Double-Flux Scheme, Multi-Component Flows, Specific Heat Ratios, Compressible Flow<br />
Summary:<br />
The article introduces a novel numerical approach for improving multi-component compressible flow simulations. It combines an Entropy-Stable formulation and a Double-Flux scheme designed for multi-component flows. This approach ensures low-dissipation, oscillation-free solutions with enhanced stability compared to traditional methods. A hybrid dissipation strategy further improves robustness by blending the new approach with conventional dissipation mechanisms while maintaining consistency with the second law of thermodynamics. The method also utilizes an explicit Runge-Kutta scheme and adaptive mesh refinement for efficient time integration and capturing local flow features dynamically. Implemented in an existing compressible Navier-Stokes solver based on OpenFOAM, benchmark cases demonstrate the effectiveness of the framework in handling multi-dimensional interface and shock-interface interactions. The results confirm its favorable stability and robustness, making it a promising advancement for high-fidelity simulations of supersonic flows.<br /><br /> <div>
arXiv:2506.13231v1 Announce Type: new 
Abstract: We present a novel combination of numerical techniques to improve the efficiency, accuracy, and robustness of multi-component compressible flow simulations. At the core of our approach is an Entropy-Stable formulation that preserves kinetic energy and integrates a Double-Flux scheme tailored for multi-component flows with variable specific heat ratios. This formulation yields low-dissipation, oscillation-free solutions and enhances stability compared to standard fully conservative methods. To further improve robustness, we introduce a new hybrid dissipation strategy that blends the Entropy-Stable/Double-Flux approach with conventional dissipation mechanisms. We provide a rigorous proof that the resulting numerical flux satisfies a semi-discrete entropy inequality, ensuring consistency with the second law of thermodynamics. For time integration, we employ an explicit Runge-Kutta scheme in combination with adaptive mesh refinement to capture local flow features dynamically. The method is implemented within an existing compressible Navier-Stokes solver based on OpenFOAM. Benchmark cases, including multi-dimensional interface and shock-interface interactions, demonstrate the effectiveness of the proposed framework. The results confirm its favorable stability and robustness, validating the approach as a promising advancement for high-fidelity simulations of supersonic flows.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constitutive Manifold Neural Networks</title>
<link>https://arxiv.org/abs/2506.13648</link>
<guid>https://arxiv.org/abs/2506.13648</guid>
<content:encoded><![CDATA[
<div> neural networks, manifold learning, thermal conductivity, stochastic tensors, engineering applications  
Summary:  
Constitutive Manifold Neural Networks (CMNN) are introduced to address the challenge of accurately modeling stochastic tensor properties, such as thermal conductivity, in engineering applications. These properties, represented as symmetric positive definite tensors on a curved Riemannian manifold, require preservation of their geometric properties during neural network processing. By preprocessing the tensors to a flat vector space, CMNN ensures that the input to the neural network preserves the tensor's symmetry and spatial symmetries. A case study on stochastic anisotropic conductivity in a heat conduction problem demonstrates that CMNN outperforms traditional multi-layer perceptron architectures by preserving the geometric properties of the tensors. This highlights the importance of using manifold-aware techniques in engineering applications involving tensor-valued data. <div>
arXiv:2506.13648v1 Announce Type: new 
Abstract: Important material properties like thermal conductivity are often represented as symmetric positive definite (SPD) tensors, which exhibit variability due to inherent material heterogeneity and manufacturing uncertainties. These tensors reside on a curved Riemannian manifold, and accurately modeling their stochastic nature requires preserving both their symmetric positive definite properties and spatial symmetries. To achieve this, uncertainties are parametrized into scaling (magnitude) and rotation (orientation) components, modeled as independent random variables on a manifold structure derived from the maximum entropy principle. The propagation of such stochastic tensors through physics-based simulations necessitates computationally efficient surrogate models. However, traditional multi-layer perceptron (MLP) architectures are not well-suited for SPD tensors, as directly inputting their components fails to preserve their geometric properties, often leading to suboptimal results. To address this, we introduce Constitutive Manifold Neural Networks (CMNN). This approach introduces a preprocessing layer by mapping the SPD tensor from the curved manifold to the local tangent, a flat vector space, creating an information preserving map for input to the hidden layers of the neural networks. A case study on a steady-state heat conduction problem with stochastic anisotropic conductivity demonstrates that geometry-preserving preprocessing, such as logarithmic maps for scaling data, significantly improves learning performance over conventional MLPs. These findings underscore the importance of manifold-aware techniques when working with tensor-valued data in engineering applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Network for Gene Regulatory Network Inference</title>
<link>https://arxiv.org/abs/2506.13740</link>
<guid>https://arxiv.org/abs/2506.13740</guid>
<content:encoded><![CDATA[
<div> scKAN, gene regulatory networks, single-cell RNA sequencing, explainable AI, cellular dynamics<br />
Summary: <br />
The paper introduces scKAN, a novel model utilizing a Kolmogorov-Arnold network (KAN) with explainable AI to infer gene regulatory networks (GRNs) from single-cell RNA sequencing data. scKAN models gene expression as differentiable functions to accurately detect activation and inhibition regulations through explainable AI and geometric tools. It surpasses existing signed GRN inference models in AUROC and AUPRC metrics on the BEELINE benchmark, showcasing its potential in capturing biological processes in gene regulation without prior knowledge of the graph structure. The model addresses the challenges of high dimensionality and complexity in GRN inference from scRNA-seq data, offering improved scalability, explainability, and precision in detecting regulation types and capturing continuous cellular dynamics. <div>
arXiv:2506.13740v1 Announce Type: new 
Abstract: Gene regulation is central to understanding cellular processes and development, potentially leading to the discovery of new treatments for diseases and personalized medicine. Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing (scRNA-seq) data presents significant challenges due to its high dimensionality and complexity. Existing tree-based models, such as GENIE3 and GRNBOOST2, demonstrated scalability and explainability in GRN inference, but they cannot distinguish regulation types nor effectively capture continuous cellular dynamics. In this paper, we introduce scKAN, a novel model that employs a Kolmogorov-Arnold network (KAN) with explainable AI to infer GRNs from scRNA-seq data. By modeling gene expression as differentiable functions matching the smooth nature of cellular dynamics, scKAN can accurately and precisely detect activation and inhibition regulations through explainable AI and geometric tools. We conducted extensive experiments on the BEELINE benchmark, and scKAN surpasses and improves the leading signed GRN inference models ranging from 5.40\% to 28.37\% in AUROC and from 1.97\% to 40.45\% in AUPRC. These results highlight the potential of scKAN in capturing the underlying biological processes in gene regulation without prior knowledge of the graph structure.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Trust at Scale: Physics-Aware Neural Watermarking for Secure and Verifiable Data Pipelines</title>
<link>https://arxiv.org/abs/2506.12032</link>
<guid>https://arxiv.org/abs/2506.12032</guid>
<content:encoded><![CDATA[
<div> framework, neural watermarking, scientific data integrity, high-dimensional fields, climate modeling

Summary:
The article presents a robust neural watermarking framework designed specifically for scientific data integrity, focusing on high-dimensional fields typically found in climate modeling and fluid simulations. Utilizing a convolutional autoencoder, binary messages can be invisibly embedded into structured data like temperature, vorticity, and geopotential. The method ensures that the watermark persists even under lossy transformations such as noise injection, cropping, and compression while maintaining high fidelity with sub-1% Mean Squared Error (MSE). Compared to traditional singular value decomposition (SVD)-based watermarking, this approach achieves over 98% bit accuracy and provides visually indistinguishable reconstructions for datasets like ERA5 and Navier-Stokes. The system serves as a scalable and model-compatible tool for ensuring data provenance, auditability, and traceability in high-performance scientific workflows. It also contributes to the larger goal of securing AI systems through verifiable watermarking that is informed by physics. The evaluation was conducted on scientifically grounded datasets, demonstrating the frameworks adaptability to other structured domains like satellite imagery and autonomous vehicle perception streams. <br /><br />Summary: <div>
arXiv:2506.12032v1 Announce Type: cross 
Abstract: We present a robust neural watermarking framework for scientific data integrity, targeting high-dimensional fields common in climate modeling and fluid simulations. Using a convolutional autoencoder, binary messages are invisibly embedded into structured data such as temperature, vorticity, and geopotential. Our method ensures watermark persistence under lossy transformations - including noise injection, cropping, and compression - while maintaining near-original fidelity (sub-1\% MSE). Compared to classical singular value decomposition (SVD)-based watermarking, our approach achieves $>$98\% bit accuracy and visually indistinguishable reconstructions across ERA5 and Navier-Stokes datasets. This system offers a scalable, model-compatible tool for data provenance, auditability, and traceability in high-performance scientific workflows, and contributes to the broader goal of securing AI systems through verifiable, physics-aware watermarking. We evaluate on physically grounded scientific datasets as a representative stress-test; the framework extends naturally to other structured domains such as satellite imagery and autonomous-vehicle perception streams.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUST: Quantifying Free-Form Geometric Uncertainty of Metamaterials Using Small Data</title>
<link>https://arxiv.org/abs/2506.12051</link>
<guid>https://arxiv.org/abs/2506.12051</guid>
<content:encoded><![CDATA[
<div> Generative Uncertainty Learning, Self-supervised pretraining, Transfer learning, Geometric Uncertainties, Metamaterials <br />
Summary:
GUST framework uses deep generative models to quantify geometric uncertainties in metamaterial manufacturing. It leverages self-supervised pretraining on synthetic data to capture structure variability and a conditional distribution of fabricated geometries. Transfer learning fine-tunes the model on real-world data to adapt to specific processes. The approach requires only 960 manufactured unit cells for effective uncertainty quantification. Directly training on limited real-world data proves insufficient compared to GUST. This cost-effective method reduces data requirements while accurately modeling complex geometric uncertainties. The scalable approach benefits high-precision industries like aerospace and biomedical engineering by enabling the understanding and mitigation of manufacturing uncertainties. <br /> <div>
arXiv:2506.12051v1 Announce Type: cross 
Abstract: This paper introduces GUST (Generative Uncertainty learning via Self-supervised pretraining and Transfer learning), a framework for quantifying free-form geometric uncertainties inherent in the manufacturing of metamaterials. GUST leverages the representational power of deep generative models to learn a high-dimensional conditional distribution of as-fabricated unit cell geometries given nominal designs, thereby enabling uncertainty quantification. To address the scarcity of real-world manufacturing data, GUST employs a two-stage learning process. First, it leverages self-supervised pretraining on a large-scale synthetic dataset to capture the structure variability inherent in metamaterial geometries and an approximated distribution of as-fabricated geometries given nominal designs. Subsequently, GUST employs transfer learning by fine-tuning the pretrained model on limited real-world manufacturing data, allowing it to adapt to specific manufacturing processes and nominal designs. With only 960 unit cells additively manufactured in only two passes, GUST can capture the variability in geometry and effective material properties. In contrast, directly training a generative model on the same amount of real-world data proves insufficient, as demonstrated through both qualitative and quantitative comparisons. This scalable and cost-effective approach significantly reduces data requirements while maintaining the effectiveness in learning complex, real-world geometric uncertainties, offering an affordable method for free-form geometric uncertainty quantification in the manufacturing of metamaterials. The capabilities of GUST hold significant promise for high-precision industries such as aerospace and biomedical engineering, where understanding and mitigating manufacturing uncertainties are critical.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Green AI Architectures for Circular Economies Through Multi-Layered Sustainable Resource Optimization Framework</title>
<link>https://arxiv.org/abs/2506.12262</link>
<guid>https://arxiv.org/abs/2506.12262</guid>
<content:encoded><![CDATA[
<div> energy-efficient, Green AI, circular economies, sustainable resource consumption, optimization techniques

Summary:<br /><br />In this research paper, a new energy-efficient Green AI architecture is proposed to support circular economies and address sustainable resource consumption. The architecture integrates machine learning algorithms, energy-conscious models, and optimization techniques to facilitate decision-making for resource reuse and waste reduction. Real-world tests on lithium-ion battery recycling and urban waste management show a 25 percent reduction in energy consumption and an 18 percent improvement in resource recovery efficiency. Mathematical models and AI algorithms improve classification accuracy and reduce transportation emissions. Graphical analyses demonstrate the framework's impact on energy efficiency and sustainability, aligning with UN Sustainability Goals. This scalable solution can contribute to sustainable management strategies and technological progress, potentially safeguarding natural capital while advancing AI technologies. <br /> <div>
arXiv:2506.12262v1 Announce Type: cross 
Abstract: In this research paper, we propose a new type of energy-efficient Green AI architecture to support circular economies and address the contemporary challenge of sustainable resource consumption in modern systems. We introduce a multi-layered framework and meta-architecture that integrates state-of-the-art machine learning algorithms, energy-conscious computational models, and optimization techniques to facilitate decision-making for resource reuse, waste reduction, and sustainable production.We tested the framework on real-world datasets from lithium-ion battery recycling and urban waste management systems, demonstrating its practical applicability. Notably, the key findings of this study indicate a 25 percent reduction in energy consumption during workflows compared to traditional methods and an 18 percent improvement in resource recovery efficiency. Quantitative optimization was based on mathematical models such as mixed-integer linear programming and lifecycle assessments. Moreover, AI algorithms improved classification accuracy on urban waste by 20 percent, while optimized logistics reduced transportation emissions by 30 percent. We present graphical analyses and visualizations of the developed framework, illustrating its impact on energy efficiency and sustainability as reflected in the simulation results. This paper combines the principles of Green AI with practical insights into how such architectural models contribute to circular economies, presenting a fully scalable and scientifically rooted solution aligned with applicable UN Sustainability Goals worldwide. These results open avenues for incorporating newly developed AI technologies into sustainable management strategies, potentially safeguarding local natural capital while advancing technological progress.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Software Landscape for the Density Matrix Renormalization Group</title>
<link>https://arxiv.org/abs/2506.12629</link>
<guid>https://arxiv.org/abs/2506.12629</guid>
<content:encoded><![CDATA[
<div> Keywords: density matrix renormalization group, software landscape, modularization, standardization, collaboration

Summary: 
The article discusses the landscape of density matrix renormalization group (DMRG) software, highlighting 35 existing packages and their features. There is significant overlap in features among these packages, particularly in areas such as parallelism strategies and symmetry-adapted formulations. The lack of standard interfaces and modularity suggests a need for more collaboration, standardization, and modularization. The authors advocate for more cohesion and modularity in DMRG software to reduce duplication of efforts and improve interoperability. They believe that the challenges in the software landscape are more social than technical and aim to raise awareness among researchers and developers to encourage collaboration and optimization. Ultimately, greater cohesion and modularity in DMRG software would enhance its capabilities in tackling complex problems. 

<br /><br />Summary: <div>
arXiv:2506.12629v1 Announce Type: cross 
Abstract: The density matrix renormalization group (DMRG) algorithm is a cornerstone computational method for studying quantum many-body systems, renowned for its accuracy and adaptability. Despite DMRG's broad applicability across fields such as materials science, quantum chemistry, and quantum computing, numerous independent implementations have been developed. This survey maps the rapidly expanding DMRG software landscape, providing a comprehensive comparison of features among 35 existing packages. We found significant overlap in features among the packages when comparing key aspects, such as parallelism strategies for high-performance computing and symmetry-adapted formulations that enhance efficiency. This overlap suggests opportunities for modularization of common operations, including tensor operations, symmetry representations, and eigensolvers, as the packages are mostly independent and share few third-party library dependencies where functionality is factored out. More widespread modularization and standardization would result in reduced duplication of efforts and improved interoperability. We believe that the proliferation of packages and the current lack of standard interfaces and modularity are more social than technical. We aim to raise awareness of existing packages, guide researchers in finding a suitable package for their needs, and help developers identify opportunities for collaboration, modularity standardization, and optimization. Ultimately, this work emphasizes the value of greater cohesion and modularity, which would benefit DMRG software, allowing these powerful algorithms to tackle more complex and ambitious problems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimLOB: Learning Representations of Limited Order Book for Financial Market Simulation</title>
<link>https://arxiv.org/abs/2406.19396</link>
<guid>https://arxiv.org/abs/2406.19396</guid>
<content:encoded><![CDATA[
<div> Transformer-based autoencoder, Limit Order Book, financial market simulation, calibration, representation learning <br />
<br />
Summary: <br />
Financial market simulation (FMS) is essential for understanding market anomalies and trading behaviors. Traditional calibration methods focused on mid-price data, resulting in biased models. This study introduces a Transformer-based autoencoder to learn vectorized representations of Limit Order Book (LOB) data, capturing market micro-structure. The learned latent representation preserves temporal auto-correlation and precedence between price levels in LOB. Experiment results demonstrate the effectiveness of the learned representation for downstream calibration tasks. This work advances FMS on LOB data, addressing the challenge of transforming LOB's tabular structure to a vectorized format for calibration purposes. <div>
arXiv:2406.19396v4 Announce Type: replace 
Abstract: Financial market simulation (FMS) serves as a promising tool for understanding market anomalies and the underlying trading behaviors. To ensure high-fidelity simulations, it is crucial to calibrate the FMS model for generating data closely resembling the observed market data. Previous efforts primarily focused on calibrating the mid-price data, leading to essential information loss of the market activities and thus biasing the calibrated model. The Limit Order Book (LOB) data is the fundamental data fully capturing the market micro-structure and is adopted by worldwide exchanges. However, LOB is not applicable to existing calibration objective functions due to its tabular structure not suitable for the vectorized input requirement. This paper proposes to explicitly learn the vectorized representations of LOB with a Transformer-based autoencoder. Then the latent vector, which captures the major information of LOB, can be applied for calibration. Extensive experiments show that the learned latent representation not only preserves the non-linear auto-correlation in the temporal axis, but the precedence between successive price levels of LOB. Besides, it is verified that the performance of the representation learning stage is consistent with the downstream calibration tasks. Thus, this work also progresses the FMS on LOB data, for the first time.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Interpretable Climate Emulators for Economics</title>
<link>https://arxiv.org/abs/2411.10768</link>
<guid>https://arxiv.org/abs/2411.10768</guid>
<content:encoded><![CDATA[
<div> framework, climate emulators, economic models, carbon-cycle, land-use change<br />
<br />
Summary: 
This paper introduces a framework for efficient and interpretable climate emulators for economic models of climate change. It proposes a generalized linear multi-reservoir model for constructing carbon-cycle emulators, customizable for specific applications. The paper evaluates three versions of the model within a representative agent economic model, finding that incorporating land-use change impacts significantly alters atmospheric carbon stocks, temperature trajectories, and optimal mitigation paths. Additionally, the study investigates pattern-scaling techniques to transform global-mean temperature projections into spatially heterogeneous warming fields and examines how regional baseline climates, non-uniform warming, and associated uncertainties influence economic damages. The findings highlight the importance of considering land-use change effects and spatial variations in climate projections for more accurate economic assessments of climate change impacts. <br /><br />Summary: <div>
arXiv:2411.10768v2 Announce Type: replace-cross 
Abstract: We introduce a framework for developing efficient and interpretable climate emulators (CEs) for economic models of climate change. The paper makes two main contributions. First, we propose a general framework for constructing carbon-cycle emulators (CCEs) for macroeconomic models. The framework is implemented as a generalized linear multi-reservoir (box) model that conserves key physical quantities and can be customized for specific applications. We consider three versions of the CCE, which we evaluate within a simple representative agent economic model: (i) a three-box setting comparable to DICE-2016, (ii) a four-box extension, and (iii) a four-box version that explicitly captures land-use change. While the three-box model reproduces benchmark results well and the fourth reservoir adds little, incorporating the impact of land-use change on the carbon storage capacity of the terrestrial biosphere substantially alters atmospheric carbon stocks, temperature trajectories, and the optimal mitigation path. Second, we investigate pattern-scaling techniques that transform global-mean temperature projections from CEs into spatially heterogeneous warming fields. We show how regional baseline climates, non-uniform warming, and the associated uncertainties propagate into economic damages.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOB-Bench: Benchmarking Generative AI for Finance -- an Application to Limit Order Book Data</title>
<link>https://arxiv.org/abs/2502.09172</link>
<guid>https://arxiv.org/abs/2502.09172</guid>
<content:encoded><![CDATA[
<div> benchmark, financial data, generative models, limit order books, evaluation

Summary:
The article introduces LOB-Bench, a benchmarking tool developed in Python, to assess the quality of generative data for limit order books (LOB) in the finance sector. The tool aims to address the challenges posed by noisy and complex financial data by providing a quantitative evaluation framework. LOB-Bench compares generated and real LOB data using various statistical measures, including conditional and unconditional statistics, order book volumes, and order imbalance. The framework also incorporates scores from a discriminator network and market impact metrics, such as cross-correlations and price response functions. The study evaluates different generative models, including generative autoregressive state-space models and (C)GANs, and concludes that the autoregressive GenAI approach outperforms traditional model classes. This research contributes to advancing the field of financial sequence modeling by offering a standardized assessment tool for evaluating generative models in the context of limit order books. 

<br /><br />Summary: <div>
arXiv:2502.09172v2 Announce Type: replace-cross 
Abstract: While financial data presents one of the most challenging and interesting sequence modelling tasks due to high noise, heavy tails, and strategic interactions, progress in this area has been hindered by the lack of consensus on quantitative evaluation paradigms. To address this, we present LOB-Bench, a benchmark, implemented in python, designed to evaluate the quality and realism of generative message-by-order data for limit order books (LOB) in the LOBSTER format. Our framework measures distributional differences in conditional and unconditional statistics between generated and real LOB data, supporting flexible multivariate statistical evaluation. The benchmark also includes features commonly used LOB statistics such as spread, order book volumes, order imbalance, and message inter-arrival times, along with scores from a trained discriminator network. Lastly, LOB-Bench contains "market impact metrics", i.e. the cross-correlations and price response functions for specific events in the data. We benchmark generative autoregressive state-space models, a (C)GAN, as well as a parametric LOB model and find that the autoregressive GenAI approach beats traditional model classes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence of physics-informed neural networks modeling time-harmonic wave fields</title>
<link>https://arxiv.org/abs/2506.11395</link>
<guid>https://arxiv.org/abs/2506.11395</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, Partial Differential Equations, Acoustic Wave Field, Room Acoustics <br />
Summary: <br />
- The study focuses on using physics-informed neural networks (PINNs) to model partial differential equations for solving the acoustic wave field.
- Initial results show promise for simple geometries in two-dimensional domains but face challenges in achieving convergence towards the accurate solution.
- Various factors including 3D dimensionality, realistic source modeling, Neumann boundary conditions, and complex solution quantities play a role in the optimization process.
- The research examines 3D room acoustic scenarios at low frequencies, testing different source definitions, boundary condition sets, and incorporating a complex speed of sound model.
- Convergence studies indicate the need for at least six training points per wavelength to achieve accurate training and predictions with the PINN architecture. This work contributes to the larger goal of modeling low-frequency room acoustics, including absorbers. <br /> <div>
arXiv:2506.11395v1 Announce Type: new 
Abstract: Studying physics-informed neural networks (PINNs) for modeling partial differential equations to solve the acoustic wave field has produced promising results for simple geometries in two-dimensional domains. One option is to compute the time-harmonic wave field using the Helmholtz equation. Compared to existing numerical models, the physics-informed neural networks forward problem has to overcome several topics related to the convergence of the optimization toward the "true" solution. The topics reach from considering the physical dimensionality (from 2D to 3D), the modeling of realistic sources (from a self-similar source to a realistic confined point source), the modeling of sound-hard (Neumann) boundary conditions, and the modeling of the full wave field by considering the complex solution quantities. Within this contribution, we study 3D room acoustic cases at low frequency, varying the source definition and the number of boundary condition sets and using a complex speed of sound model to account for some degree of absorption. We assess the convergence behavior by looking at the loss landscape of the PINN architecture, the $L^2$ error compared to a finite element reference simulation for each network architecture and configuration. The convergence studies showed that at least six training points per wavelength are necessary for accurate training and subsequent predictions of the PINN. The developments are part of an initiative aiming to model the low-frequency behavior of room acoustics, including absorbers.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAN-MI: A Scalable and Efficient Pipeline for Constructing High-Quality Neurodata in Motor Imagery Paradigm</title>
<link>https://arxiv.org/abs/2506.11830</link>
<guid>https://arxiv.org/abs/2506.11830</guid>
<content:encoded><![CDATA[
<div> Keyword: EEG signals, motor imagery, brain-computer interfaces, data construction, neurodata<br />
Summary:<br />
The article introduces CLEAN-MI, a data construction pipeline for motor imagery-based brain-computer interfaces. It addresses challenges such as low signal-to-noise ratio and inter-subject variability in EEG signals. CLEAN-MI combines frequency band filtering, channel template selection, subject screening, and marginal distribution alignment to enhance data quality and standardize multi-source EEG datasets. The pipeline was tested on various public MI datasets, showing consistent improvements in data quality and classification performance. This systematic approach aims to develop large-scale, efficient, and accurate neurodata for building robust and generalizable foundation models in BCI systems. <div>
arXiv:2506.11830v1 Announce Type: new 
Abstract: The construction of large-scale, high-quality datasets is a fundamental prerequisite for developing robust and generalizable foundation models in motor imagery (MI)-based brain-computer interfaces (BCIs). However, EEG signals collected from different subjects and devices are often plagued by low signal-to-noise ratio, heterogeneity in electrode configurations, and substantial inter-subject variability, posing significant challenges for effective model training. In this paper, we propose CLEAN-MI, a scalable and systematic data construction pipeline for constructing large-scale, efficient, and accurate neurodata in the MI paradigm. CLEAN-MI integrates frequency band filtering, channel template selection, subject screening, and marginal distribution alignment to systematically filter out irrelevant or low-quality data and standardize multi-source EEG datasets. We demonstrate the effectiveness of CLEAN-MI on multiple public MI datasets, achieving consistent improvements in data quality and classification performance.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances</title>
<link>https://arxiv.org/abs/2506.10990</link>
<guid>https://arxiv.org/abs/2506.10990</guid>
<content:encoded><![CDATA[
<div> carbon footprint, Follow-the-Sun, Artificial Intelligence, anomaly detection, experiment 

Summary:
- The 'Follow-the-Sun' (FtS) model aims to minimize the carbon footprint of computer workloads by dynamically relocating them to regions with cleaner energy sources.
- A study was conducted to assess the effectiveness of FtS in reducing the carbon emissions of AI training models, comparing it with two other strategies.
- Results from benchmarking four AI algorithms showed that the FtS strategy led to an average reduction of up to 14.6% in carbon emissions, with peaks of 16.3%.
- Additionally, FtS was found to help preserve the training time needed for AI models.
- Utilizing historical carbon intensity data from seven European cities in 2021, the experiment provided scientific evidence supporting the advantages of the FtS strategy in mitigating the carbon footprint of AI workloads.<br /><br />Summary: <div>
arXiv:2506.10990v1 Announce Type: cross 
Abstract: 'Follow-the-Sun' (FtS) is a theoretical computational model aimed at minimizing the carbon footprint of computer workloads. It involves dynamically moving workloads to regions with cleaner energy sources as demand increases and energy production relies more on fossil fuels. With the significant power consumption of Artificial Intelligence (AI) being a subject of extensive debate, FtS is proposed as a strategy to mitigate the carbon footprint of training AI models. However, the literature lacks scientific evidence on the advantages of FtS to mitigate the carbon footprint of AI workloads. In this paper, we present the results of an experiment conducted in a partial synthetic scenario to address this research gap. We benchmarked four AI algorithms in the anomaly detection domain and measured the differences in carbon emissions in four cases: no strategy, FtS, and two strategies previously introduced in the state of the art, namely Flexible Start and Pause and Resume. To conduct our experiment, we utilized historical carbon intensity data from the year 2021 for seven European cities. Our results demonstrate that the FtS strategy not only achieves average reductions of up to 14.6% in carbon emissions (with peaks of 16.3%) but also helps in preserving the time needed for training.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Attention-based Spatio-Temporal Neural Operator for Evolving Physics</title>
<link>https://arxiv.org/abs/2506.11328</link>
<guid>https://arxiv.org/abs/2506.11328</guid>
<content:encoded><![CDATA[
<div> machine learning, SciML, spatio-temporal, interpretability, ASNO <br />
Summary:<br /> 
The article introduces the Attention-based Spatio-Temporal Neural Operator (ASNO) to address challenges in scientific machine learning (SciML). ASNO combines separate attention mechanisms for spatial and temporal interactions to adapt to unknown physical parameters. Inspired by the backward differentiation formula, ASNO learns a transformer for temporal prediction and extrapolation and an attention-based neural operator for handling varying external loads. This enhances interpretability by isolating historical state contributions and external forces, enabling the discovery of underlying physical laws and generalizability to unseen environments. Empirical results on SciML benchmarks show that ASNO outperforms existing models, demonstrating its potential for engineering applications, physics discovery, and interpretable machine learning. <br /> <div>
arXiv:2506.11328v1 Announce Type: cross 
Abstract: In scientific machine learning (SciML), a key challenge is learning unknown, evolving physical processes and making predictions across spatio-temporal scales. For example, in real-world manufacturing problems like additive manufacturing, users adjust known machine settings while unknown environmental parameters simultaneously fluctuate. To make reliable predictions, it is desired for a model to not only capture long-range spatio-temporal interactions from data but also adapt to new and unknown environments; traditional machine learning models excel at the first task but often lack physical interpretability and struggle to generalize under varying environmental conditions. To tackle these challenges, we propose the Attention-based Spatio-Temporal Neural Operator (ASNO), a novel architecture that combines separable attention mechanisms for spatial and temporal interactions and adapts to unseen physical parameters. Inspired by the backward differentiation formula (BDF), ASNO learns a transformer for temporal prediction and extrapolation and an attention-based neural operator for handling varying external loads, enhancing interpretability by isolating historical state contributions and external forces, enabling the discovery of underlying physical laws and generalizability to unseen physical environments. Empirical results on SciML benchmarks demonstrate that ASNO outperforms over existing models, establishing its potential for engineering applications, physics discovery, and interpretable machine learning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design</title>
<link>https://arxiv.org/abs/2506.11420</link>
<guid>https://arxiv.org/abs/2506.11420</guid>
<content:encoded><![CDATA[
<div> protein-binding proteins, affinity, biomedical research, biotechnology, PPDiff<br />
<br />
Summary: 
Designing high-affinity protein-binding proteins is crucial in biomedical research and biotechnology. The PPDiff model, based on SSINC, is introduced to design binders for any protein target in a non-autoregressive manner. PPDiff integrates self-attention layers, graph layers, and causal attention layers to capture global amino acid correlations, local interactions, and simplify interdependencies within the protein sequence. PPDiff is evaluated on the PPBench dataset and achieves success rates of 50.00% for pretraining and 23.16% for target-protein mini-binder complex design. For antigen-antibody complex design, PPDiff achieves a success rate of 16.89%, outperforming baseline methods. This demonstrates the effectiveness of PPDiff in designing high-affinity binders for arbitrary protein targets without extensive wet-lab testing. <div>
arXiv:2506.11420v1 Announce Type: cross 
Abstract: Designing protein-binding proteins with high affinity is critical in biomedical research and biotechnology. Despite recent advancements targeting specific proteins, the ability to create high-affinity binders for arbitrary protein targets on demand, without extensive rounds of wet-lab testing, remains a significant challenge. Here, we introduce PPDiff, a diffusion model to jointly design the sequence and structure of binders for arbitrary protein targets in a non-autoregressive manner. PPDiffbuilds upon our developed Sequence Structure Interleaving Network with Causal attention layers (SSINC), which integrates interleaved self-attention layers to capture global amino acid correlations, k-nearest neighbor (kNN) equivariant graph layers to model local interactions in three-dimensional (3D) space, and causal attention layers to simplify the intricate interdependencies within the protein sequence. To assess PPDiff, we curate PPBench, a general protein-protein complex dataset comprising 706,360 complexes from the Protein Data Bank (PDB). The model is pretrained on PPBenchand finetuned on two real-world applications: target-protein mini-binder complex design and antigen-antibody complex design. PPDiffconsistently surpasses baseline methods, achieving success rates of 50.00%, 23.16%, and 16.89% for the pretraining task and the two downstream applications, respectively.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the performance of multi-fidelity and reduced-dimensional neural emulators for inference of physiologic boundary conditions</title>
<link>https://arxiv.org/abs/2506.11683</link>
<guid>https://arxiv.org/abs/2506.11683</guid>
<content:encoded><![CDATA[
<div> Bayesian parameter estimation, cardiovascular modeling, surrogate modeling, high-fidelity simulations, computational cost <br />
Summary: 
This work focuses on solving inverse problems in cardiovascular modeling through Bayesian parameter estimation. It explores various methods to reduce computational costs by using low-fidelity approximations. Different approaches include constructing surrogates for high-fidelity simulations, building surrogates for the discrepancy between high and low-fidelity models, and treating the discrepancy as random noise to estimate its distribution. Five variations of these methods are validated on analytical test cases, comparing them to high-fidelity posterior distributions in terms of accuracy and computational cost. The approaches are then demonstrated on two cardiovascular examples: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy. The study showcases the effectiveness of leveraging low-fidelity approximations in reducing computational costs while maintaining accuracy in Bayesian parameter estimation for cardiovascular modeling. <br /> <div>
arXiv:2506.11683v1 Announce Type: cross 
Abstract: Solving inverse problems in cardiovascular modeling is particularly challenging due to the high computational cost of running high-fidelity simulations. In this work, we focus on Bayesian parameter estimation and explore different methods to reduce the computational cost of sampling from the posterior distribution by leveraging low-fidelity approximations. A common approach is to construct a surrogate model for the high-fidelity simulation itself. Another is to build a surrogate for the discrepancy between high- and low-fidelity models. This discrepancy, which is often easier to approximate, is modeled with either a fully connected neural network or a nonlinear dimensionality reduction technique that enables surrogate construction in a lower-dimensional space. A third possible approach is to treat the discrepancy between the high-fidelity and surrogate models as random noise and estimate its distribution using normalizing flows. This allows us to incorporate the approximation error into the Bayesian inverse problem by modifying the likelihood function. We validate five different methods which are variations of the above on analytical test cases by comparing them to posterior distributions derived solely from high-fidelity models, assessing both accuracy and computational cost. Finally, we demonstrate our approaches on two cardiovascular examples of increasing complexity: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Level set-based inverse homogenisation of three-dimensional piezoelectric materials</title>
<link>https://arxiv.org/abs/2410.03148</link>
<guid>https://arxiv.org/abs/2410.03148</guid>
<content:encoded><![CDATA[
<div> piezoelectric materials, topology optimisation, iterative solvers, metamaterials, level-set implementation <br />
Summary: 
This paper utilizes memory-distributed level set-based topology optimization to enhance the properties of three-dimensional periodic piezoelectric materials. Various iterative solvers are evaluated for their weak scalability, with the approximate Schur complement preconditioned generalized minimal residual method showing the best performance for solving piezoelectric homogenization equations. Through computational design, high-resolution piezoelectric metamaterials with improved stiffness and piezoelectric properties are created for sensor, hydrophone, and actuator applications. Two robust structures without fine-scale features are proposed, exhibiting significantly enhanced piezoelectric properties compared to the base material. The level-set approach proves effective in piezoelectricity problems by eliminating large regions of intermediate density material. An open-source memory-distributed level-set implementation is provided for the community practitioners to utilize. <br /><br />Summary: <div>
arXiv:2410.03148v3 Announce Type: replace 
Abstract: In this paper we use memory-distributed level set-based topology optimisation to design three-dimensional periodic piezoelectric materials with enhanced properties. We compare and assess several existing iterative solvers with respect to their weak scalability and find that an approximate Schur complement preconditioned generalized minimal residual method method demonstrates the best performance and scalability for solving the piezoelectric homogenisation equations. We use the developed techniques to computationally design high-resolution piezoelectric metamaterials with enhanced stiffness and piezoelectric properties that yield new insights into material design for sensor, hydrophone, and actuator applications. We suggest two robust structures with no fine-scale features that exhibit enhanced piezoelectric properties several times larger than those of the base material. We find that level set-based topology optimisation is well suited to problems involving piezoelectricity and has the advantage of avoiding large regions of intermediate density material. Our memory-distributed level-set implementation is open source and provided for practitioners in the community.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form</title>
<link>https://arxiv.org/abs/2412.15801</link>
<guid>https://arxiv.org/abs/2412.15801</guid>
<content:encoded><![CDATA[
<div> metrics, urban form, performance evaluation, morphology, sustainable urban design
Summary:
- The article discusses the importance of connecting morphology metrics with complex urban forms to enhance performance-driven computational urban design.
- It highlights the need for bi-directional mapping between metrics and urban forms to improve urban performance.
- An approach is presented to formulate metrics that can characterize urban forms and retrieve similar 3D forms.
- The methodology is demonstrated using 3D urban models of New York City, analyzing 14,248 blocks.
- Neural networks and information retrieval techniques are utilized for morphology metric encoding, urban form clustering, and evaluation.
<br /><br />Summary: <div>
arXiv:2412.15801v2 Announce Type: replace 
Abstract: Urban morphology, examining city spatial configurations, links urban design to sustainability. Morphology metrics play a fundamental role in performance-driven computational urban design (CUD) which integrates urban form generation, performance evaluation and optimization. However, a critical gap remains between performance evaluation and complex urban form generation, caused by the disconnection between morphology metrics and urban form, particularly in metric-to-form workflows. It prevents the application of optimized metrics to generate improved urban form with enhanced urban performance. Formulating morphology metrics that not only effectively characterize complex urban forms but also enable the reconstruction of diverse forms is of significant importance. This paper highlights the importance of establishing a bi-directional mapping between morphology metrics and complex urban form to enable the integration of urban form generation with performance evaluation. We present an approach that can 1) formulate morphology metrics to both characterize urban forms and in reverse, retrieve diverse similar 3D urban forms, and 2) evaluate the effectiveness of morphology metrics in representing 3D urban form characteristics of blocks by comparison. We demonstrate the methodology with 3D urban models of New York City, covering 14,248 blocks. We use neural networks and information retrieval for morphology metric encoding, urban form clustering and morphology metric evaluation. We identified an effective set of morphology metrics for characterizing block-scale urban forms through comparison. The proposed methodology tightly couples complex urban forms with morphology metrics, hence it can enable a seamless and bidirectional relationship between urban form generation and optimization in performance-driven urban design towards sustainable urban design and planning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring EEG Responses during Observation of Actions Performed by Human Actor and Humanoid Robot</title>
<link>https://arxiv.org/abs/2506.10170</link>
<guid>https://arxiv.org/abs/2506.10170</guid>
<content:encoded><![CDATA[
<div> action observation therapy humanoid robots EEG sensorimotor brain activity<br />
<br />
Summary: <br />
This pilot study investigated the potential of humanoid robots in supporting action observation therapy for motor and language function in neurological rehabilitation. Three healthy participants were monitored using EEG while observing actions performed by a human actor and a robot. Analysis of their brain activity revealed variability in ERSP patterns, including power suppression in sensorimotor mu and beta rhythms. One participant showed a stronger response to robot conditions compared to human conditions. Positive correlations in ERSP across all conditions implied common cognitive processes or neural networks in the mirror neuron system during action observation. Overall, the results support the feasibility of using EEG to explore differences in neural responses to observing robot- and human-induced actions. <div>
arXiv:2506.10170v1 Announce Type: new 
Abstract: Action observation (AO) therapy is a promising rehabilitative treatment for motor and language function in individuals recovering from neurological conditions, such as stroke. This pilot study aimed to investigate the potential of humanoid robots to support AO therapy in rehabilitation settings. The brain activity of three healthy right-handed participants was monitored with electroencephalography (EEG) while they observed eight different actions performed by two agents, a human actor and a robot, using their left and right arms. Their event-related spectral perturbations (ERSPs, changes in the spectral power of neural oscillations in response to an event or stimulus, compared to baseline) in sensorimotor regions were analyzed. The single-subject analysis showed variability in ERSP patterns among all participants, including power suppression in sensorimotor mu and beta rhythms. One participant showed stronger responses to "robot" AO conditions than to "human" conditions. Strong and positive correlations in ERSP across all conditions were observed for almost all participants and channels, implying common cognitive processes or neural networks at play in the mirror neuron system during AO. The results support the feasibility of using EEG to explore differences in neural responses to observation of robot- and human-induced actions.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable and flexible non-intrusive reduced-order models using reproducing kernel Hilbert spaces</title>
<link>https://arxiv.org/abs/2506.10224</link>
<guid>https://arxiv.org/abs/2506.10224</guid>
<content:encoded><![CDATA[
<div> Regularized kernel interpolation, reduced-order modeling, interpretable models, reproducing kernel Hilbert space, a posteriori error bound <br />
<br />
Summary: <br />
This paper introduces a novel non-intrusive reduced-order modeling technique using regularized kernel interpolation. Unlike traditional methods that rely on least-squares regression, this approach utilizes a reproducing kernel Hilbert space to capture the dynamics of a reduced-order model (ROM) in an interpretable manner. By incorporating carefully selected feature maps into the kernel, the resulting ROMs closely mimic the structure of the full-order model. The flexibility of the approach allows for the inclusion of both structured features and general nonlinear terms in the kernel, providing a comprehensive representation of the system dynamics. Additionally, the authors derive a computable a posteriori error bound that combines error estimates from traditional projection-based ROMs and kernel interpolants. Numerical experiments showcase the effectiveness of the approach compared to operator inference techniques using proper orthogonal decomposition and quadratic manifold dimension reduction. <div>
arXiv:2506.10224v1 Announce Type: new 
Abstract: This paper develops an interpretable, non-intrusive reduced-order modeling technique using regularized kernel interpolation. Existing non-intrusive approaches approximate the dynamics of a reduced-order model (ROM) by solving a data-driven least-squares regression problem for low-dimensional matrix operators. Our approach instead leverages regularized kernel interpolation, which yields an optimal approximation of the ROM dynamics from a user-defined reproducing kernel Hilbert space. We show that our kernel-based approach can produce interpretable ROMs whose structure mirrors full-order model structure by embedding judiciously chosen feature maps into the kernel. The approach is flexible and allows a combination of informed structure through feature maps and closure terms via more general nonlinear terms in the kernel. We also derive a computable a posteriori error bound that combines standard error estimates for intrusive projection-based ROMs and kernel interpolants. The approach is demonstrated in several numerical experiments that include comparisons to operator inference using both proper orthogonal decomposition and quadratic manifold dimension reduction.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment</title>
<link>https://arxiv.org/abs/2506.10711</link>
<guid>https://arxiv.org/abs/2506.10711</guid>
<content:encoded><![CDATA[
<div> refiner models, diffusion models, PDEs, spectral space, Kuramoto-Sivashinsky equation <br />
Summary: 
The article discusses the challenge of generating accurate and stable long rollouts for time-dependent Partial Differential Equations (PDEs). A refiner model called PDERefiner has been developed, utilizing diffusion models to refine outputs for each time step, with a focus on high-frequency accuracy. While this approach works well for some PDEs like the 1-D Kuramoto-Sivashinsky equation by degrading the amplitude of the high-frequency part, it may not be as effective for more complex cases like the Navier-Stokes equation. To address this, adjustments were made in the spectral space, leading to the development of PDE-SpectralRefiner with a v-prediction technique for Blurring diffusion models. This enhancement improved the accuracy of the outputs for both one-step Mean Squared Error (MSE) loss and rollout loss across different model backbones, such as U-Net and neural operators. <div>
arXiv:2506.10711v1 Announce Type: new 
Abstract: Generating accurate and stable long rollouts is a notorious challenge for time-dependent PDEs (Partial Differential Equations). Recently, motivated by the importance of high-frequency accuracy, a refiner model called PDERefiner utilizes diffusion models to refine outputs for every time step, since the denoising process could increase the correctness of modeling high frequency part. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the amplitude of high frequency part better than not doing refinement process. However, for some other cases, the spectrum might be more complicated. For example, for a harder PDE like Navior-Stokes equation, diffusion models could over-degrade the higher frequency part. This motivates us to release the constraint that each frequency weighs the same. We enhance our refiner model with doing adjustments on spectral space, which recovers Blurring diffusion models. We developed a new v-prediction technique for Blurring diffusion models, recovering the MSE training objective on the first refinement step. We show that in this case, for different model backbones, such as U-Net and neural operators, the outputs of PDE-SpectralRefiner are more accurate for both one-step MSE loss and rollout loss.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Analysis of Discretized Boundary Integral Operators in 3D: a High-Frequency Perspective</title>
<link>https://arxiv.org/abs/2506.10880</link>
<guid>https://arxiv.org/abs/2506.10880</guid>
<content:encoded><![CDATA[
<div> wavelength, integral equations, boundary element method, scattering phenomena, discretization<br />
Summary:<br />
The study explores the common practice of using integral equations discretized by the boundary element method to model propagation and scattering phenomena. It focuses on approximating the scatterer's boundary with mesh elements of size around a fraction of the incident wave's wavelength. By analyzing operator matrix spectra, the research reveals a discrepancy compared to continuous operators that increases as the simulation frequency rises. This challenges the conventional assumption that discretizing boundaries at a fraction of the wavelength, like /10, ensures constant solution accuracy at higher frequencies. <div>
arXiv:2506.10880v1 Announce Type: new 
Abstract: When modeling propagation and scattering phenomena using integral equations discretized by the boundary element method, it is common practice to approximate the boundary of the scatterer with a mesh comprising elements of size approximately equal to a fraction of the wavelength $\lambda$ of the incident wave, e.g., $\lambda/10$. In this work, by analyzing the spectra of the operator matrices, we show a discrepancy with respect to the continuous operators which grows with the simulation frequency, challenging the common belief that the aforementioned widely used discretization approach is sufficient to maintain the accuracy of the solution constant when increasing the frequency.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Categorization with Relational Deep Learning in QuickBooks</title>
<link>https://arxiv.org/abs/2506.09234</link>
<guid>https://arxiv.org/abs/2506.09234</guid>
<content:encoded><![CDATA[
<div> Graph-based model, Rel-Cat, transaction categorization, relational database, link prediction <br />
<br />
Summary: 
The paper introduces a novel graph-based model, Rel-Cat, for automatic transaction categorization in QuickBooks. This model addresses challenges such as unique transaction descriptions and a wide variety of categories by formulating categorization as a link prediction task within a graph structure. By integrating natural language processing and graph machine learning techniques, Rel-Cat outperforms the existing QuickBooks model and scales effectively to a growing customer base. The model is designed to handle the cold start problem by adapting to minimal data, providing accurate categorization without compromising on accuracy. This innovative approach simplifies the architecture and enhances the customer experience by providing more accurate accounting and bookkeeping in QuickBooks. <div>
arXiv:2506.09234v1 Announce Type: new 
Abstract: Automatic transaction categorization is crucial for enhancing the customer experience in QuickBooks by providing accurate accounting and bookkeeping. The distinct challenges in this domain stem from the unique formatting of transaction descriptions, the wide variety of transaction categories, and the vast scale of the data involved. Furthermore, organizing transaction data in a relational database creates difficulties in developing a unified model that covers the entire database. In this work, we develop a novel graph-based model, named Rel-Cat, which is built directly over the relational database. We introduce a new formulation of transaction categorization as a link prediction task within this graph structure. By integrating techniques from natural language processing and graph machine learning, our model not only outperforms the existing production model in QuickBooks but also scales effectively to a growing customer base with a simpler, more effective architecture without compromising on accuracy. This design also helps tackle a key challenge of the cold start problem by adapting to minimal data.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Design Structure Matrix Optimization</title>
<link>https://arxiv.org/abs/2506.09749</link>
<guid>https://arxiv.org/abs/2506.09749</guid>
<content:encoded><![CDATA[
<div> machine learning, optimization, engineering design, design structure matrix, Large Language Models 

Summary:
Large Language Models (LLMs) show promise in solving combinatorial optimization (CO) problems in complex engineering systems by leveraging advanced reasoning and contextual understanding. This study proposes an LLM-based framework that combines network topology with domain knowledge to optimize Design Structure Matrix (DSM) element sequencing. Experiments demonstrate that this method achieves faster convergence and better solutions compared to traditional optimization methods. Incorporating contextual domain knowledge enhances optimization performance regardless of the specific LLM backbone used. The findings suggest that LLMs can effectively solve complex CO problems in engineering design by combining semantic and mathematical reasoning, paving the way for a new paradigm in LLM-based engineering design optimization. 

<br /><br />Summary: <div>
arXiv:2506.09749v1 Announce Type: new 
Abstract: In complex engineering systems, the interdependencies among components or development activities are often modeled and analyzed using Design Structure Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and enhance modularity or process efficiency constitutes a challenging combinatorial optimization (CO) problem in engineering design and operations. As problem sizes increase and dependency networks become more intricate, traditional optimization methods that solely use mathematical heuristics often fail to capture the contextual nuances and struggle to deliver effective solutions. In this study, we explore the potential of Large Language Models (LLMs) for helping solve such CO problems by leveraging their capabilities for advanced reasoning and contextual understanding. We propose a novel LLM-based framework that integrates network topology with contextual domain knowledge for iterative optimization of DSM element sequencing - a common CO problem. Experiments on various DSM cases show that our method consistently achieves faster convergence and superior solution quality compared to both stochastic and deterministic baselines. Notably, we find that incorporating contextual domain knowledge significantly enhances optimization performance regardless of the chosen LLM backbone. These findings highlight the potential of LLMs to solve complex engineering CO problems by combining semantic and mathematical reasoning. This approach paves the way towards a new paradigm in LLM-based engineering design optimization.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era</title>
<link>https://arxiv.org/abs/2506.09755</link>
<guid>https://arxiv.org/abs/2506.09755</guid>
<content:encoded><![CDATA[
<div> Keywords: Intelligent Design, Engineering innovation, Foundation Models, Multi-agent collaboration, Autonomous systems

Summary:<br />
The paper introduces Intelligent Design 4.0 (ID 4.0) as a new paradigm in engineering design, utilizing agentic AI systems. It discusses the historical evolution of Intelligent Design through different stages, leading to the emergence of multi-agent collaboration. ID 4.0 aims to automate engineering design processes through coordinated, autonomous multi-agent-based systems. The potential of ID 4.0 lies in supporting end-to-end automation, enhancing adaptivity, autonomy, and effectiveness in tackling complex design challenges. Future perspectives include addressing more complex design scenarios, practical design implementations, novel agent coordination mechanisms, and autonomous design goal-setting with improved human value alignment. The insights presented in the paper lay the groundwork for advancing Intelligent Design towards greater efficiency and innovation in engineering design.<br /><br />Summary: <div>
arXiv:2506.09755v1 Announce Type: new 
Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced engineering innovation, efficiency, quality, and productivity over recent decades, fundamentally reshaping how engineering designers think, behave, and interact with design processes. The recent emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), has demonstrated general knowledge-based reasoning capabilities, and open new paths and avenues for further transformation in engineering design. In this context, this paper introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by agentic AI systems. We review the historical evolution of ID across four distinct stages: rule-based expert systems, task-specific machine learning models, large-scale foundation AI models, and the recent emerging paradigm of multi-agent collaboration. We propose a conceptual framework for ID 4.0 and discuss its potential to support end-to-end automation of engineering design processes through coordinated, autonomous multi-agent-based systems. Furthermore, we discuss future perspectives to enhance and fully realize ID 4.0's potential, including more complex design scenarios, more practical design implementations, novel agent coordination mechanisms, and autonomous design goal-setting with better human value alignment. In sum, these insights lay a foundation for advancing Intelligent Design toward greater adaptivity, autonomy, and effectiveness in addressing increasingly complex design challenges.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superstudent intelligence in thermodynamics</title>
<link>https://arxiv.org/abs/2506.09822</link>
<guid>https://arxiv.org/abs/2506.09822</guid>
<content:encoded><![CDATA[
<div> OpenAI, o3, thermodynamics, exam, artificial intelligence <br />
<br />
Summary: OpenAI's language model o3 has surpassed university students in a thermodynamics exam, showcasing its ability to creatively apply principles. The exam is known for high failure rates and rare A-grades, requiring deep knowledge of thermodynamics. In a zero-shot mode, o3 outperformed students, achieving top scores in comparison to thousands of exams since 1985. This achievement signifies a shift in machine capabilities in complex tasks previously seen as exclusive to human intellect. The implications of this feat raise questions about the role of machines in engineering tasks and the future education of engineers. <div>
arXiv:2506.09822v1 Announce Type: new 
Abstract: In this short note, we report and analyze a striking event: OpenAI's large language model o3 has outwitted all students in a university exam on thermodynamics. The thermodynamics exam is a difficult hurdle for most students, where they must show that they have mastered the fundamentals of this important topic. Consequently, the failure rates are very high, A-grades are rare - and they are considered proof of the students' exceptional intellectual abilities. This is because pattern learning does not help in the exam. The problems can only be solved by knowledgeably and creatively combining principles of thermodynamics. We have given our latest thermodynamics exam not only to the students but also to OpenAI's most powerful reasoning model, o3, and have assessed the answers of o3 exactly the same way as those of the students. In zero-shot mode, the model o3 solved all problems correctly, better than all students who took the exam; its overall score was in the range of the best scores we have seen in more than 10,000 similar exams since 1985. This is a turning point: machines now excel in complex tasks, usually taken as proof of human intellectual capabilities. We discuss the consequences this has for the work of engineers and the education of future engineers.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Guided Ligand-Binding Protein Design</title>
<link>https://arxiv.org/abs/2506.09332</link>
<guid>https://arxiv.org/abs/2506.09332</guid>
<content:encoded><![CDATA[
<div> Keywords: AI protein models, protein design, ligand binding, natural language instructions, machine learning

Summary: In this paper, the authors introduce InstructPro, a family of AI protein generative models trained to design proteins that bind to specific ligands using human language instructions. The models, InstructPro-1B and InstructPro-3B, outperform existing baselines in generating ligand-binding proteins. InstructPro-1B achieves an impressive docking success rate of 81.52% and an average RMSD of 4.026 compared to ground truth structures. InstructPro-3B further improves the average RMSD to 2.527, demonstrating the models' capability to accurately design proteins based on textual descriptions and ligand formulas. The dataset InstructProBench, containing over 9 million triples of function descriptions, ligand formulas, and protein sequences, supports the training and evaluation of the models. These results suggest that AI protein models can effectively follow natural language instructions to design proteins with desired functions, offering a promising approach for protein engineering in various fields. 

<br /><br />Summary: <div>
arXiv:2506.09332v1 Announce Type: cross 
Abstract: Can AI protein models follow human language instructions and design proteins with desired functions (e.g. binding to a ligand)? Designing proteins that bind to a given ligand is crucial in a wide range of applications in biology and chemistry. Most prior AI models are trained on protein-ligand complex data, which is scarce due to the high cost and time requirements of laboratory experiments. In contrast, there is a substantial body of human-curated text descriptions about protein-ligand interactions and ligand formula. In this paper, we propose InstructPro, a family of protein generative models that follow natural language instructions to design ligand-binding proteins. Given a textual description of the desired function and a ligand formula in SMILES, InstructPro generates protein sequences that are functionally consistent with the specified instructions. We develop the model architecture, training strategy, and a large-scale dataset, InstructProBench, to support both training and evaluation. InstructProBench consists of 9,592,829 triples of (function description, ligand formula, protein sequence). We train two model variants: InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion parameters). Both variants consistently outperform strong baselines, including ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking success rate (81.52% at moderate confidence) and the lowest average root mean square deviation (RMSD) compared to ground truth structures (4.026{\AA}). InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating InstructPro's ability to generate ligand-binding proteins that align with the functional specifications.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Climate Emulation with Bayesian Filtering</title>
<link>https://arxiv.org/abs/2506.09891</link>
<guid>https://arxiv.org/abs/2506.09891</guid>
<content:encoded><![CDATA[
<div> climate change, machine learning, emulator, physics-informed, causal relationships

Summary: 
This study introduces a novel approach to climate modeling using machine learning techniques. Traditional climate models are computationally expensive due to their complex systems of equations. The proposed emulator leverages causal representation learning to incorporate physics-informed causal relationships, leading to more accurate climate dynamics predictions. A Bayesian filter ensures stable long-term autoregressive emulation. The emulator is shown to accurately capture climate dynamics on both synthetic and real-world climate model data. This innovative approach opens up new possibilities for quicker and more efficient simulations of climate change dynamics, allowing for improved predictions and analyses of its causes and impacts. <div>
arXiv:2506.09891v1 Announce Type: cross 
Abstract: Traditional models of climate change use complex systems of coupled equations to simulate physical processes across the Earth system. These simulations are highly computationally expensive, limiting our predictions of climate change and analyses of its causes and effects. Machine learning has the potential to quickly emulate data from climate models, but current approaches are not able to incorporate physics-informed causal relationships. Here, we develop an interpretable climate model emulator based on causal representation learning. We derive a physics-informed approach including a Bayesian filter for stable long-term autoregressive emulation. We demonstrate that our emulator learns accurate climate dynamics, and we show the importance of each one of its components on a realistic synthetic dataset and data from two widely deployed climate models.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Note on the Reliability of Goal-Oriented Error Estimates for Galerkin Finite Element Methods with Nonlinear Functionals</title>
<link>https://arxiv.org/abs/2506.09913</link>
<guid>https://arxiv.org/abs/2506.09913</guid>
<content:encoded><![CDATA[
<div> variational problem; Galerkin finite element method; discretization error; nonlinear functionals; error estimates <br />
<br />Summary: 
The study focuses on estimating discretization errors in nonlinear functionals within a variational problem solved using the Galerkin finite element method. It examines error estimates in the form of $J(u) - J(u_h) \approx \eta = L(z) - B(u_h, z)$. The research reveals instances where these error estimates are unreliable, even with an exact adjoint solution. Reliability is defined by the existence of a constant $C$ such that $|J(u) - J(u_h)| \leq C|\eta|$, and multiple examples are provided where this criterion is not met. The analysis highlights the challenges in ensuring the accuracy of error estimates in nonlinear functionals, shedding light on the complexities involved in such estimations. <div>
arXiv:2506.09913v1 Announce Type: cross 
Abstract: We consider estimating the discretization error in a nonlinear functional $J(u)$ in the setting of an abstract variational problem: find $u \in \mathcal{V}$ such that $B(u,\varphi) = L(\varphi) \; \forall \varphi \in \mathcal{V}$, as approximated by a Galerkin finite element method. Here, $\mathcal{V}$ is a Hilbert space, $B(\cdot,\cdot)$ is a bilinear form, and $L(\cdot)$ is a linear functional. We consider well-known error estimates $\eta$ of the form $J(u) - J(u_h) \approx \eta = L(z) - B(u_h, z)$, where $u_h$ denotes a finite element approximation to $u$, and $z$ denotes the solution to an auxiliary adjoint variational problem. We show that there exist nonlinear functionals for which error estimates of this form are not reliable, even in the presence of an exact adjoint solution solution $z$. An estimate $\eta$ is said to be reliable if there exists a constant $C \in \mathbb{R}_{>0}$ independent of $u_h$ such that $|J(u) - J(u_h)| \leq C|\eta|$. We present several example pairs of bilinear forms and nonlinear functionals where reliability of $\eta$ is not achieved.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks</title>
<link>https://arxiv.org/abs/2506.08563</link>
<guid>https://arxiv.org/abs/2506.08563</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Physics Informed Neural Networks, Differential Equations, Kernel Packet, RKHS norm

Summary:<br /> 
1. Differential equations play a crucial role in engineering modeling, and the Physics Informed Neural Networks (PINNs) framework has been utilized to solve complex equations efficiently.
2. The default L2 loss function in PINNs may lead to incorrect and unstable solutions for some complex equations.
3. A new framework called Kernel Packet accelerated PINNs (KP-PINNs) is introduced, which uses the reproducing kernel Hilbert space (RKHS) norm and the Kernel Packet (KP) method to enhance computation speed.
4. Theoretical analysis confirms that KP-PINNs are stable across various differential equations.
5. Numerical experiments demonstrate the effectiveness and efficiency of KP-PINNs in solving differential equations, offering a promising advancement for enhancing the stability and accuracy of PINNs-based solvers in scientific computing.<br /><br /> <div>
arXiv:2506.08563v1 Announce Type: new 
Abstract: Differential equations are involved in modeling many engineering problems. Many efforts have been devoted to solving differential equations. Due to the flexibility of neural networks, Physics Informed Neural Networks (PINNs) have recently been proposed to solve complex differential equations and have demonstrated superior performance in many applications. While the L2 loss function is usually a default choice in PINNs, it has been shown that the corresponding numerical solution is incorrect and unstable for some complex equations. In this work, we propose a new PINNs framework named Kernel Packet accelerated PINNs (KP-PINNs), which gives a new expression of the loss function using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel Packet (KP) method to accelerate the computation. Theoretical results show that KP-PINNs can be stable across various differential equations. Numerical experiments illustrate that KP-PINNs can solve differential equations effectively and efficiently. This framework provides a promising direction for improving the stability and accuracy of PINNs-based solvers in scientific computing.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid cardiac activation prediction for cardiac resynchronization therapy planning using geometric deep learning</title>
<link>https://arxiv.org/abs/2506.08987</link>
<guid>https://arxiv.org/abs/2506.08987</guid>
<content:encoded><![CDATA[
<div> deep learning, cardiac resynchronization therapy, prediction, optimization, in-silico modeling

Summary: 
- The study focuses on predicting cardiac activation time map for cardiac resynchronization therapy (CRT) using geometric deep learning models.
- Two models, GNN and GINO, were developed and trained on a synthetic dataset to predict activation time maps in real-time.
- The GINO model outperformed the GNN model, showing lower prediction errors and better robustness.
- A workflow for optimizing the pacing site in CRT using the GINO model was developed, resulting in a significant reduction in maximum activation time compared to random selection.
- An interactive web-based GUI was also developed to utilize the GINO model as a clinical decision-support tool for personalized CRT optimization. 

<br /><br />Summary: <div>
arXiv:2506.08987v1 Announce Type: new 
Abstract: Cardiac resynchronization therapy (CRT) is a common intervention for patients with dyssynchronous heart failure, yet approximately one-third of recipients fail to respond due to suboptimal lead placement. Identifying optimal pacing sites remains challenging, largely due to patient-specific anatomical variability and the limitations of current individualized planning strategies. In a step towards constructing an in-silico approach to help address this issue, we develop two geometric deep learning (DL) models, based on graph neural network (GNN) and geometry-informed neural operator (GINO), to predict cardiac activation time map in real-time for CRT planning and optimization. Both models are trained on a large synthetic dataset generated from finite-element (FE) simulations over a wide range of left ventricular (LV) geometries, pacing site configurations, and tissue conductivities. The GINO model significantly outperforms the GNN model, with lower prediction errors (1.14% vs 3.14%) and superior robustness to noise and various mesh discretization. Using the GINO model, we also develop a workflow for optimizing the pacing site in CRT from given activation time map and LV geometry. Compared to randomly selecting a pacing site, the CRT optimization workflow produces a larger reduction in maximum activation time (20% vs. 8%). In conjunction with an interactive web-based graphical user interface (GUI) available at https://dcsim.egr.msu.edu/, the GINO model shows promising potential as a clinical decision-support tool for personalized pre-procedural CRT optimization.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Proteins and Language: A Foundation Model for Protein Retrieval</title>
<link>https://arxiv.org/abs/2506.08023</link>
<guid>https://arxiv.org/abs/2506.08023</guid>
<content:encoded><![CDATA[
<div> **Keywords:** protein structures, functional interpretation, cryo-Electron Microscopy, vision-language models, contrastive learning

Summary:
This paper introduces a framework inspired by CLIP-style models to align 3D protein structures with functional annotations using contrastive learning. A dataset of 200,000 protein-caption pairs with detailed functional descriptors is created for model training. The model is evaluated on both in-domain and cross-database retrieval tasks using Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) datasets. Promising zero-shot retrieval performance is demonstrated, showcasing the potential of multimodal foundation models for enhancing our understanding of protein structure-function relationships in biology.<br /><br />Summary: <div>
arXiv:2506.08023v1 Announce Type: cross 
Abstract: This paper aims to retrieve proteins with similar structures and semantics from large-scale protein dataset, facilitating the functional interpretation of protein structures derived by structural determination methods like cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of vision-language models (VLMs), we propose a CLIP-style framework for aligning 3D protein structures with functional annotations using contrastive learning. For model training, we propose a large-scale dataset of approximately 200,000 protein-caption pairs with rich functional descriptors. We evaluate our model in both in-domain and more challenging cross-database retrieval on Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In both cases, our approach demonstrates promising zero-shot retrieval performance, highlighting the potential of multimodal foundation models for structure-function understanding in protein biology.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts</title>
<link>https://arxiv.org/abs/2506.08205</link>
<guid>https://arxiv.org/abs/2506.08205</guid>
<content:encoded><![CDATA[
<div> simulation, residual stress, machine learning, U-Net architecture, experimental data

Summary:
- Residual stresses can affect component performance, and accurately determining their distributions is crucial.
- The proposed Residual Stress Generator (RSG) uses machine learning to infer full-field stresses from limited measurements.
- An extensive dataset was created through process simulations with varied parameters.
- The RSG, based on U-Net architecture, showed excellent predictive accuracy and generalization in generating simulated stresses.
- The RSG successfully learned the latent structure of residual stress distribution and reduced the need for extensive experimental efforts.
- Testing on actual characterization data validated the RSG's effectiveness in predicting experimentally measured residual stresses. 

<br /><br />Summary: <div>
arXiv:2506.08205v1 Announce Type: cross 
Abstract: Residual stresses, which remain within a component after processing, can deteriorate performance. Accurately determining their full-field distributions is essential for optimizing the structural integrity and longevity. However, the experimental effort required for full-field characterization is impractical. Given these challenges, this work proposes a machine learning (ML) based Residual Stress Generator (RSG) to infer full-field stresses from limited measurements. An extensive dataset was initially constructed by performing numerous process simulations with a diverse parameter set. A ML model based on U-Net architecture was then trained to learn the underlying structure through systematic hyperparameter tuning. Then, the model's ability to generate simulated stresses was evaluated, and it was ultimately tested on actual characterization data to validate its effectiveness. The model's prediction of simulated stresses shows that it achieved excellent predictive accuracy and exhibited a significant degree of generalization, indicating that it successfully learnt the latent structure of residual stress distribution. The RSG's performance in predicting experimentally characterized data highlights the feasibility of the proposed approach in providing a comprehensive understanding of residual stress distributions from limited measurements, thereby significantly reducing experimental efforts.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems</title>
<link>https://arxiv.org/abs/2506.08475</link>
<guid>https://arxiv.org/abs/2506.08475</guid>
<content:encoded><![CDATA[
<div> autoencoders, dimensionality reduction, parametric neural networks, reduced-order modeling, thermodynamics

Summary:
The article introduces a novel framework, tLaSDI, for efficient identification of latent space dynamics in parametric nonlinear dynamical systems. By combining autoencoders for dimensionality reduction with parametric neural networks informed by thermodynamic principles, the framework can accurately learn parametric latent dynamics while maintaining important thermodynamic laws. The inclusion of a physics-informed active learning strategy further enhances model performance by adaptively sampling informative training data. Numerical experiments on various equations demonstrate the framework's superior speed and accuracy compared to traditional methods, with significant reductions in training and inference costs. The learned latent space dynamics not only provide accurate representations of physical-space dynamics but also offer valuable insights into the underlying thermodynamic behavior of the system.<br /><br />Summary: <div>
arXiv:2506.08475v1 Announce Type: cross 
Abstract: We propose an efficient thermodynamics-informed latent space dynamics identification (tLaSDI) framework for the reduced-order modeling of parametric nonlinear dynamical systems. This framework integrates autoencoders for dimensionality reduction with newly developed parametric GENERIC formalism-informed neural networks (pGFINNs), which enable efficient learning of parametric latent dynamics while preserving key thermodynamic principles such as free energy conservation and entropy generation across the parameter space. To further enhance model performance, a physics-informed active learning strategy is incorporated, leveraging a greedy, residual-based error indicator to adaptively sample informative training data, outperforming uniform sampling at equivalent computational cost. Numerical experiments on the Burgers' equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed method achieves up to 3,528x speed-up with 1-3% relative errors, and significant reduction in training (50-90%) and inference (57-61%) cost. Moreover, the learned latent space dynamics reveal the underlying thermodynamic behavior of the system, offering valuable insights into the physical-space dynamics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation</title>
<link>https://arxiv.org/abs/2506.08604</link>
<guid>https://arxiv.org/abs/2506.08604</guid>
<content:encoded><![CDATA[
<div> Keywords: generative machine learning, physics-based flow matching, PDE problems, surrogate modeling, uncertainty quantification

Summary: 
Physics-Based Flow Matching (PBFM) is a novel generative framework that incorporates physical constraints, such as PDE residuals and algebraic relations, into the flow matching objective. This method explicitly embeds physics knowledge into the learning process, resulting in improved accuracy in noise-free sample prediction. The inclusion of temporal unrolling during training further enhances prediction accuracy. PBFM minimizes both flow matching loss and physics-based residual loss simultaneously without the need for hyperparameter tuning. By analyzing the role of minimum noise level and implementing a stochastic sampling strategy, physical residuals can be reduced. Benchmarking on three PDE problems demonstrates that PBFM outperforms existing algorithms by up to 8 times in physical residual accuracy and distributional accuracy. This approach offers a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.<br /><br />Summary: <div>
arXiv:2506.08604v1 Announce Type: cross 
Abstract: Generative machine learning methods, such as diffusion models and flow matching, have shown great potential in modeling complex system behaviors and building efficient surrogate models. However, these methods typically learn the underlying physics implicitly from data. We propose Physics-Based Flow Matching (PBFM), a novel generative framework that explicitly embeds physical constraints, both PDE residuals and algebraic relations, into the flow matching objective. We also introduce temporal unrolling at training time that improves the accuracy of the final, noise-free sample prediction. Our method jointly minimizes the flow matching loss and the physics-based residual loss without requiring hyperparameter tuning of their relative weights. Additionally, we analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of physical constraints and evaluate a stochastic sampling strategy that helps to reduce physical residuals. Through extensive benchmarks on three representative PDE problems, we show that our approach yields up to an $8\times$ more accurate physical residuals compared to FM, while clearly outperforming existing algorithms in terms of distributional accuracy. PBFM thus provides a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements</title>
<link>https://arxiv.org/abs/2506.08762</link>
<guid>https://arxiv.org/abs/2506.08762</guid>
<content:encoded><![CDATA[
<div> Benchmark, Japanese financial data, large language model, accounting fraud detection, earnings forecasting

Summary:
The article introduces EDINET-Bench, an open-source Japanese financial benchmark aiming to evaluate the performance of large language models (LLMs) in challenging financial tasks such as accounting fraud detection and earnings forecasting using data from Japan's EDINET. The experiments show that even state-of-the-art LLMs struggle in binary classification for fraud detection and earnings forecasting, only performing slightly better than logistic regression. This indicates significant challenges in applying LLMs to real-world financial tasks and highlights the need for domain-specific adaptation. The dataset, benchmark construction code, and evaluation code are publicly available to support further research in utilizing LLMs for financial analysis.<br /><br />Summary: <div>
arXiv:2506.08762v1 Announce Type: cross 
Abstract: Financial analysis presents complex challenges that could leverage large language model (LLM) capabilities. However, the scarcity of challenging financial datasets, particularly for Japanese financial data, impedes academic innovation in financial analytics. As LLMs advance, this lack of accessible research resources increasingly hinders their development and evaluation in this specialized domain. To address this gap, we introduce EDINET-Bench, an open-source Japanese financial benchmark designed to evaluate the performance of LLMs on challenging financial tasks including accounting fraud detection, earnings forecasting, and industry prediction. EDINET-Bench is constructed by downloading annual reports from the past 10 years from Japan's Electronic Disclosure for Investors' NETwork (EDINET) and automatically assigning labels corresponding to each evaluation task. Our experiments reveal that even state-of-the-art LLMs struggle, performing only slightly better than logistic regression in binary classification for fraud detection and earnings forecasting. These results highlight significant challenges in applying LLMs to real-world financial applications and underscore the need for domain-specific adaptation. Our dataset, benchmark construction code, and evaluation code is publicly available to facilitate future research in finance with LLMs.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)</title>
<link>https://arxiv.org/abs/2506.08844</link>
<guid>https://arxiv.org/abs/2506.08844</guid>
<content:encoded><![CDATA[
<div> dataset, missing data imputation, socioeconomic research, benchmark, IMAGIC-500

Summary:
- The study focuses on missing data imputation in socioeconomic datasets, which are often restricted due to privacy concerns.
- The authors introduce the IMAGIC-500 dataset derived from the World Bank's synthetic dataset, allowing for broad access.
- A comprehensive missing data imputation benchmark is conducted on IMAGIC-500 with various missing mechanisms and ratios.
- Evaluation criteria include imputation accuracy, computational efficiency, and impact on predictive tasks like estimating educational attainment.
- Results assess statistical, traditional machine learning, deep learning, and diffusion-based imputation methods, aiming to improve algorithm development and reproducibility in social science research. 

<br /><br />Summary: <div>
arXiv:2506.08844v1 Announce Type: cross 
Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in data science and machine learning, particularly within socioeconomic research. However, real-world socioeconomic datasets are typically subject to strict data protection protocols, which often prohibit public sharing, even for synthetic derivatives. This severely limits the reproducibility and accessibility of benchmark studies in such settings. Further, there are very few publicly available synthetic datasets. Thus, there is limited availability of benchmarks for systematic evaluation of imputation methods on socioeconomic datasets, whether real or synthetic. In this study, we utilize the World Bank's publicly available synthetic dataset, Synthetic Data for an Imaginary Country, which closely mimics a real World Bank household survey while being fully public, enabling broad access for methodological research. With this as a starting point, we derived the IMAGIC-500 dataset: we select a subset of 500k individuals across approximately 100k households with 19 socioeconomic features, designed to reflect the hierarchical structure of real-world household surveys. This paper introduces a comprehensive missing data imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR, MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation considers the imputation accuracy for continuous and categorical variables, computational efficiency, and impact on downstream predictive tasks, such as estimating educational attainment at the individual level. The results highlight the strengths and weaknesses of statistical, traditional machine learning, and deep learning imputation techniques, including recent diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate the development of robust imputation algorithms and foster reproducible social science research.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market</title>
<link>https://arxiv.org/abs/2506.06356</link>
<guid>https://arxiv.org/abs/2506.06356</guid>
<content:encoded><![CDATA[
<div> deep learning, quantitative trading, Chinese A-share market, backtesting, risk management <br />
Summary:
This paper introduces a multi-day turnover quantitative trading algorithm for the Chinese A-share market that incorporates advanced deep learning techniques for stock prediction. The algorithm consists of five interconnected modules, including initial stock selection, opening signal analysis, position sizing, profit-taking, stop-loss mechanisms, and market timing models. Trained on a decade of A-share data and backtested rigorously, the algorithm achieves impressive annualized returns of 15.2% with controlled maximum drawdown and a high Sharpe ratio. It balances capital efficiency and risk management through adaptive holding periods and optimized entry/exit timing. The strategy maintains a large number of daily positions with a short maximum holding period, utilizing dynamic profit-taking and stop-loss mechanisms to enhance turnover efficiency while preserving risk-adjusted returns. The approach shows robust performance across different market conditions and is suitable for institutional deployment due to its high capital capacity. <br /> <div>
arXiv:2506.06356v1 Announce Type: new 
Abstract: This paper presents a sophisticated multi-day turnover quantitative trading algorithm that integrates advanced deep learning techniques with comprehensive cross-sectional stock prediction for the Chinese A-share market. Our framework combines five interconnected modules: initial stock selection through deep cross-sectional prediction networks, opening signal distribution analysis using mixture models for arbitrage identification, market capitalization and liquidity-based dynamic position sizing, grid-search optimized profit-taking and stop-loss mechanisms, and multi-granularity volatility-based market timing models. The algorithm employs a novel approach to balance capital efficiency with risk management through adaptive holding periods and sophisticated entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and rigorously backtested on 2021-2024 data, our method achieves remarkable performance with 15.2\% annualized returns, maximum drawdown constrained below 5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional scalability by maintaining 50-100 daily positions with a 9-day maximum holding period, incorporating dynamic profit-taking and stop-loss mechanisms that enhance capital turnover efficiency while preserving risk-adjusted returns. Our approach exhibits robust performance across various market regimes while maintaining high capital capacity suitable for institutional deployment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality</title>
<link>https://arxiv.org/abs/2506.06622</link>
<guid>https://arxiv.org/abs/2506.06622</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial analysis, QuantMCP, data APIs, decision-making

Summary:
QuantMCP introduces a framework to ground Large Language Models (LLMs) in financial reality by enabling them to access real-time financial data through standardized and secure tools like the Model Context Protocol (MCP). This allows LLMs to overcome issues such as data hallucination and lack of access to verified information. By leveraging Python-accessible financial data APIs, users can interact with LLMs via natural language to retrieve up-to-date and structured financial data. This enables LLMs to enhance their analytical capabilities, generate insights, and support informed financial decision-making processes. QuantMCP serves as a reliable and secure bridge between conversational AI and complex financial data, aiming to improve the reliability and analytical depth of LLM applications in finance. 

<br /><br />Summary: <div>
arXiv:2506.06622v1 Announce Type: new 
Abstract: Large Language Models (LLMs) hold immense promise for revolutionizing financial analysis and decision-making, yet their direct application is often hampered by issues of data hallucination and lack of access to real-time, verifiable financial information. This paper introduces QuantMCP, a novel framework designed to rigorously ground LLMs in financial reality. By leveraging the Model Context Protocol (MCP) for standardized and secure tool invocation, QuantMCP enables LLMs to accurately interface with a diverse array of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can interact via natural language to precisely retrieve up-to-date financial data, thereby overcoming LLM's inherent limitations in factual data recall. More critically, once furnished with this verified, structured data, the LLM's analytical capabilities are unlocked, empowering it to perform sophisticated data interpretation, generate insights, and ultimately support more informed financial decision-making processes. QuantMCP provides a robust, extensible, and secure bridge between conversational AI and the complex world of financial data, aiming to enhance both the reliability and the analytical depth of LLM applications in finance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hype Index: an NLP-driven Measure of Market News Attention</title>
<link>https://arxiv.org/abs/2506.06329</link>
<guid>https://arxiv.org/abs/2506.06329</guid>
<content:encoded><![CDATA[
<div> Natural Language Processing, Hype Index, Media Attention, Financial News, Stock Volatility

Summary:
The paper introduces the Hype Index as a metric to quantify media attention towards large-cap equities using Natural Language Processing (NLP) techniques. Two versions of the Hype Index are constructed - News Count-Based and Capitalization Adjusted - to measure media exposure relative to stock or sector market capitalization. The Hype Index is evaluated based on its classification into hype groups, associations with returns and volatility, signaling power for market movements, and empirical properties such as correlations and trends. The findings suggest that the Hype Index family is a valuable tool for analyzing stock volatility, market signaling, and extending NLP applications in finance. <div>
arXiv:2506.06329v1 Announce Type: cross 
Abstract: This paper introduces the Hype Index as a novel metric to quantify media attention toward large-cap equities, leveraging advances in Natural Language Processing (NLP) for extracting predictive signals from financial news. Using the S&amp;P 100 as the focus universe, we first construct a News Count-Based Hype Index, which measures relative media exposure by computing the share of news articles referencing each stock or sector. We then extend it to the Capitalization Adjusted Hype Index, adjusts for economic size by taking the ratio of a stock's or sector's media weight to its market capitalization weight within its industry or sector. We compute both versions of the Hype Index at the stock and sector levels, and evaluate them through multiple lenses: (1) their classification into different hype groups, (2) their associations with returns, volatility, and VIX index at various lags, (3) their signaling power for short-term market movements, and (4) their empirical properties including correlations, samplings, and trends. Our findings suggest that the Hype Index family provides a valuable set of tools for stock volatility analysis, market signaling, and NLP extensions in Finance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models</title>
<link>https://arxiv.org/abs/2506.06335</link>
<guid>https://arxiv.org/abs/2506.06335</guid>
<content:encoded><![CDATA[
<div> Keywords: FinBERT2, financial-specific, bidirectional encoder, LLMs, fine-tuned models

Summary:
- FinBERT2 is introduced as a specialized bidirectional encoder pretrained on a financial-specific corpus of 32b tokens, representing the largest Chinese financial pretraining corpus for models of this parameter size.
- FinBERT2 addresses the limitations of LLMs in the financial sector by outperforming other variants on discriminatory tasks by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks.
- FinBERT2 also excels in contrastive fine-tuned models, outperforming open-source and proprietary embedders across five financial retrieval tasks.
- The construction of Fin-TopicModel based on FinBERT2 variants enables superior clustering and topic representation for financial titles. 
<br /><br />Summary: FinBERT2, a specialized bidirectional encoder pretrained on a financial-specific corpus, fills the gap in financial-specific deployment of LLMs by outperforming other variants and LLMs on classification and retrieval tasks. Additionally, the Fin-TopicModel offers enhanced clustering and topic representation for financial titles, showcasing the practical applications and benefits of utilizing FinBERT in the LLMs era. <div>
arXiv:2506.06335v1 Announce Type: cross 
Abstract: In natural language processing (NLP), the focus has shifted from encoder-only tiny language models like BERT to decoder-only large language models(LLMs) such as GPT-3. However, LLMs' practical application in the financial sector has revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT on discriminative tasks despite costing much higher computational resources, such as market sentiment analysis in financial reports; (2) Application on generative tasks heavily relies on retrieval augmented generation (RAG) methods to provide current and specialized information, with general retrievers showing suboptimal performance on domain-specific retrieval tasks; (3) There are additional inadequacies in other feature-based scenarios, such as topic modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. This represents the largest known Chinese financial pretraining corpus for models of this parameter size. As a better backbone, FinBERT2 can bridge the gap in the financial-specific deployment of LLMs through the following achievements: (1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. (2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's text-embedding-3-large) embedders across five financial retrieval tasks; (3) Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables superior clustering and topic representation for financial titles. Our work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment</title>
<link>https://arxiv.org/abs/2506.06355</link>
<guid>https://arxiv.org/abs/2506.06355</guid>
<content:encoded><![CDATA[
<div> Efficient simulation, proactive preparedness, sudden-onset disasters, large language models, world models<br />
<br />
Efficient simulation is crucial for preparing for sudden-onset disasters like earthquakes. This study explores the use of large language models (LLMs) to predict earthquake impacts. By analyzing various datasets, including geospatial and socioeconomic data, the framework generates Modified Mercalli Intensity (MMI) predictions at different scales. Evaluation on past earthquakes shows a high correlation and low error rate compared to real reports. Techniques like RAG and ICL can enhance simulation performance, with visual inputs improving accuracy. These findings highlight the potential of LLMs in simulating disaster impacts, aiding in pre-event planning and response strategies.<br /><br />Summary: <div>
arXiv:2506.06355v1 Announce Type: cross 
Abstract: Efficient simulation is essential for enhancing proactive preparedness for sudden-onset disasters such as earthquakes. Recent advancements in large language models (LLMs) as world models show promise in simulating complex scenarios. This study examines multiple LLMs to proactively estimate perceived earthquake impacts. Leveraging multimodal datasets including geospatial, socioeconomic, building, and street-level imagery data, our framework generates Modified Mercalli Intensity (MMI) predictions at zip code and county scales. Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real reports at the zip code level. Techniques such as RAG and ICL can improve simulation performance, while visual inputs notably enhance accuracy compared to structured numerical data alone. These findings show the promise of LLMs in simulating disaster impacts that can help strengthen pre-event planning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events</title>
<link>https://arxiv.org/abs/2506.06380</link>
<guid>https://arxiv.org/abs/2506.06380</guid>
<content:encoded><![CDATA[
<div> Keywords: extreme events, synthetic data generation, generative modeling techniques, evaluation framework, underexplored areas<br />
Summary:<br />
This article discusses the challenges in predicting extreme events due to the scarcity of data and introduces synthetic data generation as a solution. It reviews generative modeling techniques and large language models tailored for capturing heavy-tailed distributions in extreme event data. The authors propose an evaluation framework encompassing various metrics to assess the performance of models in extreme settings. The article categorizes application domains and highlights underexplored areas like behavioral finance and infectious outbreaks. It also offers insights into statistical theory enhancements and specialized training mechanisms for generating synthetic data. The survey outlines open challenges in synthetic rare-event research, providing a structured foundation for future advancements in this domain. <div>
arXiv:2506.06380v1 Announce Type: cross 
Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events' unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric's applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Probabilistic Framework for Learning with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.07003</link>
<guid>https://arxiv.org/abs/2506.07003</guid>
<content:encoded><![CDATA[
<div> probabilistic forecasting, operational constraints, uncertainty quantification, neural networks, probabilistic projection layer

Summary: 
ProbHardE2E is a probabilistic forecasting framework designed to incorporate operational or physical constraints as hard requirements. It enforces these constraints by utilizing variance information in a unique way, allowing for uncertainty quantification within the model. The framework utilizes a differentiable probabilistic projection layer (DPPL) that can be combined with various neural network architectures to learn the system in an end-to-end manner. ProbHardE2E can optimize a proper scoring rule without assuming a specific target distribution, enabling robust distributional estimates. It can also handle a range of non-linear constraints, increasing modeling power and flexibility. The framework is applied to learning partial differential equations with uncertainty estimates and probabilistic time-series forecasting, demonstrating its broad applicability across diverse domains. <div>
arXiv:2506.07003v1 Announce Type: cross 
Abstract: We present a general purpose probabilistic forecasting framework, ProbHardE2E, to learn systems that can incorporate operational/physical constraints as hard requirements. ProbHardE2E enforces hard constraints by exploiting variance information in a novel way; and thus it is also capable of performing uncertainty quantification (UQ) on the model. Our methodology uses a novel differentiable probabilistic projection layer (DPPL) that can be combined with a wide range of neural network architectures. This DPPL allows the model to learn the system in an end-to-end manner, compared to other approaches where the constraints are satisfied either through a post-processing step or at inference. In addition, ProbHardE2E can optimize a strictly proper scoring rule, without making any distributional assumptions on the target, which enables it to obtain robust distributional estimates (in contrast to existing approaches that generally optimize likelihood-based objectives, which are heavily biased by their distributional assumptions and model choices); and it can incorporate a range of non-linear constraints (increasing the power of modeling and flexibility). We apply ProbHardE2E to problems in learning partial differential equations with uncertainty estimates and to probabilistic time-series forecasting, showcasing it as a broadly applicable general setup that connects these seemingly disparate domains.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning</title>
<link>https://arxiv.org/abs/2506.07551</link>
<guid>https://arxiv.org/abs/2506.07551</guid>
<content:encoded><![CDATA[
<div> Chemistry, Large language models, Chemical tools, Dataset curation, Hierarchical Evolutionary Monte Carlo Tree Search

Summary: 
This article introduces a novel approach to enhance the performance of Large Language Models (LLMs) in chemistry tasks by integrating external chemical tools and dataset curation. The proposed Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework allows for independent optimization of tool planning and execution, leading to improved performance in Chemistry QA and discovery tasks. By generating the ChemToolBench dataset and leveraging self-generated data, the approach supports step-level fine-tuning of the policy model and training task-adaptive PRM and ORM. Experimental evaluations show that this approach surpasses existing models such as GPT-4o, offering a robust solution for integrating specialized tools with LLMs in advanced chemical applications. The datasets and code for this study are available on GitHub at https://github.com/AI4Chem/ChemistryAgent.

<br /><br />Summary: <div>
arXiv:2506.07551v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently demonstrated promising capabilities in chemistry tasks while still facing challenges due to outdated pretraining knowledge and the difficulty of incorporating specialized chemical expertise. To address these issues, we propose an LLM-based agent that synergistically integrates 137 external chemical tools created ranging from basic information retrieval to complex reaction predictions, and a dataset curation pipeline to generate the dataset ChemToolBench that facilitates both effective tool selection and precise parameter filling during fine-tuning and evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework, enabling independent optimization of tool planning and execution. By leveraging self-generated data, our approach supports step-level fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM that surpass GPT-4o. Experimental evaluations demonstrate that our approach significantly improves performance in Chemistry QA and discovery tasks, offering a robust solution to integrate specialized tools with LLMs for advanced chemical applications. All datasets and code are available at https://github.com/AI4Chem/ChemistryAgent .
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</title>
<link>https://arxiv.org/abs/2506.07865</link>
<guid>https://arxiv.org/abs/2506.07865</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scene geometry, appearance, physics, multi-view videos, dynamic scenes

Summary:
FreeGave is a novel approach for modeling complex dynamic 3D scenes solely from multi-view videos, without requiring object priors. The method introduces a physics code and a divergence-free module to estimate per-Gaussian velocity fields, without relying on inefficient PINN losses. Extensive experiments on various datasets demonstrate the superior performance of FreeGave in future frame extrapolation and motion segmentation tasks. The approach is able to learn meaningful 3D physical motion patterns without the need for human labels in training, showcasing its ability to capture complex physical motions at boundaries. By effectively incorporating physics simulation into neural networks, FreeGave shows promising results in learning scene geometry, appearance, and underlying physics, making it a valuable tool for understanding and analyzing dynamic scenes. 

<br /><br />Summary: <div>
arXiv:2506.07865v1 Announce Type: cross 
Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the underlying physics purely from multi-view videos. By applying various governing PDEs as PINN losses or incorporating physics simulation into neural networks, existing works often fail to learn complex physical motions at boundaries or require object priors such as masks or types. In this paper, we propose FreeGave to learn the physics of complex dynamic 3D scenes without needing any object priors. The key to our approach is to introduce a physics code followed by a carefully designed divergence-free module for estimating a per-Gaussian velocity field, without relying on the inefficient PINN losses. Extensive experiments on three public datasets and a newly collected challenging real-world dataset demonstrate the superior performance of our method for future frame extrapolation and motion segmentation. Most notably, our investigation into the learned physics codes reveals that they truly learn meaningful 3D physical motion patterns in the absence of any human labels in training.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>pyCFS-data: Data Processing Framework in Python for openCFS</title>
<link>https://arxiv.org/abs/2405.03437</link>
<guid>https://arxiv.org/abs/2405.03437</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical simulation, multi-field problems, aeroacoustics, open-source framework, data processing

Summary:
openCFS is an open-source framework developed for simulating multi-field problems, focusing on aeroacoustics. It allows for the implementation of partial differential equations using the finite element method. The software, which has been continuously developed since 2000, is now known as openCFS (previously CFS++ Coupled Field Simulations written in C++). In this paper, pyCFS-data, a data processing framework written in Python, is introduced to provide a flexible and user-friendly toolbox for accessing, manipulating, pre- and postprocessing data related to openCFS simulations. This tool aims to make the handling of simulation data more efficient and convenient for users. <div>
arXiv:2405.03437v2 Announce Type: replace 
Abstract: Many numerical simulation tools have been developed and are on the market, but there is still a strong need for appropriate tools capable of simulating multi-field problems, especially in aeroacoustics. Therefore, openCFS provides an open-source framework for implementing partial differential equations using the finite element method. Since 2000, the software has been developed continuously. The result is openCFS (before 2020, known as CFS++ Coupled Field Simulations written in C++). In this paper, we present pyCFS-data, a data processing framework written in Python to provide a flexible and easy-to-use toolbox to access and manipulate, pre- and postprocess data generated by or for usage with openCFS.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting the Canonicalization for Fast and Accurate Crystal Tensor Property Prediction</title>
<link>https://arxiv.org/abs/2410.02372</link>
<guid>https://arxiv.org/abs/2410.02372</guid>
<content:encoded><![CDATA[
<div> canonicalization, crystal tensor property prediction, O(3)-equivariant framework, polar decomposition, computational efficiency

Summary: 
In the field of materials science, predicting the tensor properties of crystalline materials is crucial but computationally expensive due to the need for O(3) group tensor equivariance. The proposed framework, GoeCTP, uses polar decomposition as a form of canonicalization to ensure efficient and accurate crystal tensor property prediction. By incorporating canonicalization, GoeCTP achieves high prediction accuracy and runs significantly faster than existing methods. This novel approach eliminates the need for complex architecture designs to maintain equivariance constraints, making it a more efficient and effective solution for crystal tensor property prediction. <div>
arXiv:2410.02372v3 Announce Type: replace 
Abstract: Predicting the tensor properties of crystalline materials is a fundamental task in materials science. Unlike single-value property prediction, which is inherently invariant, tensor property prediction requires maintaining O(3) group tensor equivariance. Such equivariance constraint often requires specialized architecture designs to achieve effective predictions, inevitably introducing tremendous computational costs. Canonicalization, a classical technique for geometry, has recently been explored for efficient learning with symmetry. In this work, we revisit the problem of crystal tensor property prediction through the lens of canonicalization. Specifically, we demonstrate how polar decomposition, a simple yet efficient algebraic method, can serve as a form of canonicalization and be leveraged to ensure equivariant tensor property prediction. Building upon this insight, we propose a general O(3)-equivariant framework for fast and accurate crystal tensor property prediction, referred to as GoeCTP. By utilizing canonicalization, GoeCTP achieves high efficiency without requiring the explicit incorporation of equivariance constraints into the network architecture. Experimental results indicate that GoeCTP achieves the best prediction accuracy and runs up to 13 times faster compared to existing state-of-the-art methods in benchmarking datasets, underscoring its effectiveness and efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-PINN: A Physics-Informed Neural Network with Finite Geometric Encoding for Solid Mechanics</title>
<link>https://arxiv.org/abs/2412.09453</link>
<guid>https://arxiv.org/abs/2412.09453</guid>
<content:encoded><![CDATA[
<div> finite element method, solid mechanics, PINN, neural network, hybrid space <br />
Summary: <br />
This study addresses challenges in using Physics-Informed Neural Networks (PINN) for solid mechanics problems compared to traditional methods like the finite element method (FEM). The main challenges are the infinite domain generated by PINN conflicting with finite boundaries in solid structures and the Euclidean solution space being insufficient for complex solid geometries. The Finite-PINN model proposed in this work overcomes these challenges by incorporating finite geometric encoding into neural network inputs, creating a hybrid Euclidean-topological solution space. This model is trained using both strong-form and weak-form loss formulations, allowing it to efficiently solve forward problems with preprocessed structural geometric information and reconstruct full-field solutions from sparse observations in inverse problems by embedding physical laws and geometric information. The Finite-PINN model shows promise for addressing a wide range of solid mechanics problems effectively. <div>
arXiv:2412.09453v2 Announce Type: replace 
Abstract: PINN models have demonstrated capabilities in addressing fluid PDE problems, and their potential in solid mechanics is beginning to emerge. This study identifies two key challenges when using PINN to solve general solid mechanics problems. These challenges become evident when comparing the limitations of PINN with the well-established numerical methods commonly used in solid mechanics, such as the finite element method (FEM). Specifically: a) PINN models generate solutions over an infinite domain, which conflicts with the finite boundaries typical of most solid structures; and b) the solution space utilised by PINN is Euclidean, which is inadequate for addressing the complex geometries often present in solid structures.
  This work presents a PINN architecture for general solid mechanics problems, referred to as the Finite-PINN model. The model is designed to effectively tackle two key challenges, while retaining as much of the original PINN framework as possible. To this end, the Finite-PINN incorporates finite geometric encoding into the neural network inputs, thereby transforming the solution space from a conventional Euclidean space into a hybrid Euclidean-topological space. The model is comprehensively trained using both strong-form and weak-form loss formulations, enabling its application to a wide range of forward and inverse problems in solid mechanics. For forward problems, the Finite-PINN model efficiently approximates solutions to solid mechanics problems when the geometric information of a given structure has been preprocessed. For inverse problems, it effectively reconstructs full-field solutions from very sparse observations by embedding both physical laws and geometric information within its architecture.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Value of Battery Energy Storage in the Continuous Intraday Market: Forecast vs. Perfect Foresight Strategies</title>
<link>https://arxiv.org/abs/2501.07121</link>
<guid>https://arxiv.org/abs/2501.07121</guid>
<content:encoded><![CDATA[
<div> modeling, grid-scale battery, energy storage system, intraday market, price forecast

Summary:
Grid-scale battery energy storage systems can optimize trading in the European continuous intraday market by using a forecast-driven model. This model utilizes price forecasts to optimize trading schedules, resulting in significant earning potential. The approach outperforms key market indices and demonstrates the profitability of participating in the CID market despite its complexity. By comparing profits across various spot markets, the forecast-driven model proves to be effective in capturing market dynamics and maximizing revenue. Using real data, a 1 MW/1 MWh system earns EUR 146,237, showcasing the model's success in estimating earnings potential and optimizing trading strategies. The method surpasses key market indices by a significant margin, confirming its reliability and effectiveness in trading energy in the CID market. <div>
arXiv:2501.07121v2 Announce Type: replace 
Abstract: Grid-scale battery energy storage systems (BESSs) can provide flexibility to the power system and capture shortterm price volatility by shifting energy in time through controlled charging and discharging. The highly volatile European continuous intraday (CID) market allows trading until just a few minutes before physical delivery, offering significant earning potential. However, its high trading frequency poses substantial modeling challenges. Accurate modeling of BESSs trading in the CID market is essential to estimate revenue potential and optimize trading strategies. Additionally, comparing CID profits with other spot markets helps determine whether participating in the CID is worthwhile despite its complexity. We propose a forecast-driven model to optimize BESS trading in the CID market. Our strategy employs a rolling window modeling framework to capture market dynamics. Price forecasts for impending CID products are generated at the beginning of each window and used to optimize trading schedules for subsequent execution. We also benchmark our approach across various spot markets, offering a broad cross-market profit comparison. We evaluate our forecast-driven model across different BESS power-to-capacity ratios, comparing it to a perfect-foresight scenario and key CID market indices, such as ID1 and ID3. Using real 2023 German CID data, a 1 MW/1 MWh system adopting our method earns EUR 146 237, only 11% below perfect foresight, surpassing all other markets and indices. Our approach surpasses ID1 and ID3 by over 4% and 32%, respectively, confirming ID1 as a reliable lower-bound estimate for earnings potential in the CID market.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Deep Learning Surrogate Models for Uncertainty Propagation in Microstructure-Properties of Ceramic Aerogels</title>
<link>https://arxiv.org/abs/2501.13255</link>
<guid>https://arxiv.org/abs/2501.13255</guid>
<content:encoded><![CDATA[
<div> surrogate models, deep learning, ceramic aerogel, microstructure, mechanical response
Summary:
This study introduces a computational framework that combines physics-based simulations with deep learning surrogate models to predict the microstructural morphology and mechanical behavior of ceramic aerogel porous materials. Lattice Boltzmann simulations model microstructure formation during material synthesis, while a finite element model calculates mechanical properties. To address the computational demands of analyzing microstructural randomness, Convolutional Neural Networks (CNNs) are used to develop surrogate models for microstructure generation and mapping. CNN training is treated as a Bayesian inference problem to enable uncertainty quantification in predictions. The surrogate models produce microstructural images consistent with training data and accurately predict strain energy for in-distribution microstructures. The study investigates the generalization capability of the surrogate models and utilizes them for efficient uncertainty propagation to quantify the influence of microstructural variability on macroscopic mechanical properties.<br /><br />Summary: <div>
arXiv:2501.13255v3 Announce Type: replace 
Abstract: This study presents an integrated computational framework that, given synthesis parameters, predicts the resulting microstructural morphology and mechanical response of ceramic aerogel porous materials by combining physics-based simulations with deep learning surrogate models. Lattice Boltzmann simulations are employed to model microstructure formation during material synthesis process, while a finite element model is used to compute the corresponding mechanical properties. To overcome the prohibitive computational demands of repeated physics-based simulations required for characterizing the impact of microstructure randomness on mechanical properties, surrogate models are developed using Convolutional Neural Networks (CNNs) for both microstructure generation and microstructure-property mapping. CNN training is formulated as a Bayesian inference problem to enable uncertainty quantification and provide confidence estimates in surrogate model predictions, under limited training data furnished by physics-based simulations. Numerical results demonstrate that the microstructure surrogate model effectively generates microstructural images consistent with the morphology of training data across larger domains. The Bayesian CNN surrogate accurately predicts strain energy for in-distribution microstructures and its generalization capability to interpolated morphologies are further investigated. Finally, the surrogate models are employed for efficient uncertainty propagation, quantifying the influence of microstructural variability on macroscopic mechanical property.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay</title>
<link>https://arxiv.org/abs/2502.16789</link>
<guid>https://arxiv.org/abs/2502.16789</guid>
<content:encoded><![CDATA[
<div> alpha decay, alpha mining, Large Language Models, genetic programming, factor overfitting
<br />
Summary: 
The article introduces AlphaAgent, a framework designed to address the challenge of alpha decay in quantitative investment. By integrating Large Language Models with ad hoc regularizations, AlphaAgent aims to generate decay-resistant alpha factors. It enforces originality by comparing generated factors with existing ones, aligns hypotheses with factors for market consistency, and controls complexity to prevent overfitting. Through extensive evaluations in Chinese CSI 500 and US S&amp;P 500 markets, AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay. It demonstrates significant resistance to alpha decay and consistently delivers substantial alpha over the past four years, offering promising potential for powerful factors. <div>
arXiv:2502.16789v2 Announce Type: replace 
Abstract: Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay, where factors lose their predictive power over time, poses a significant challenge for alpha mining. Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and US S&amp;P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications</title>
<link>https://arxiv.org/abs/2408.11878</link>
<guid>https://arxiv.org/abs/2408.11878</guid>
<content:encoded><![CDATA[
<div> Financial LLMs, Open-FinLLMs, multimodal capabilities, zero-shot, few-shot, fine-tuning.

Summary:
Open-FinLLMs are introduced as the first open-source multimodal financial Language Models (LLMs) to handle various financial tasks using text, tabular, time-series, and chart data. The suite includes FinLLaMA pre-trained on a large corpus, FinLLaMA-Instruct fine-tuned with financial instructions, and FinLLaVA enhanced with multimodal tuning pairs for cross-modal reasoning. Evaluation across 14 financial tasks and 4 multimodal tasks shows that Open-FinLLMs outperform other financial and general LLMs such as GPT-4 in tasks like financial NLP and decision-making, indicating their potential to address real-world challenges. Codes and models are released under OSI-approved licenses to encourage collaboration between academia and industry. <br /><br />Summary: Financial LLMs Open-FinLLMs introduced as open-source multimodal models, excel in diverse tasks, outperforming GPT-4, released with codes and models for collaboration. <div>
arXiv:2408.11878v3 Announce Type: replace-cross 
Abstract: Financial LLMs hold promise for advancing financial tasks and domain-specific applications. However, they are limited by scarce corpora, weak multimodal capabilities, and narrow evaluations, making them less suited for real-world application. To address this, we introduce \textit{Open-FinLLMs}, the first open-source multimodal financial LLMs designed to handle diverse tasks across text, tabular, time-series, and chart data, excelling in zero-shot, few-shot, and fine-tuning settings. The suite includes FinLLaMA, pre-trained on a comprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K financial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning pairs for strong cross-modal reasoning. We comprehensively evaluate Open-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in zero-shot, few-shot, and supervised fine-tuning settings, introducing two new multimodal evaluation datasets. Our results show that Open-FinLLMs outperforms afvanced financial and general LLMs such as GPT-4, across financial NLP, decision-making, and multi-modal tasks, highlighting their potential to tackle real-world challenges. To foster innovation and collaboration across academia and industry, we release all codes (https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE) and models under OSI-approved licenses.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Deep Learning Model for Line-integral Diagnostics Across Fusion Devices</title>
<link>https://arxiv.org/abs/2412.00087</link>
<guid>https://arxiv.org/abs/2412.00087</guid>
<content:encoded><![CDATA[
<div> physics-informed model, deep learning, plasma profiles, line-integral measurements, nuclear fusion <br />
Summary: <br />
The paper introduces a physics-informed model architecture called Onion for rapid 2D plasma profile reconstruction from line-integral measurements in nuclear fusion research. By incorporating physical information through a multiplication process and applying a physics-informed loss function based on the principle of line integration, the model shows improvements in performance. Results indicate a reduction in average relative error in reconstruction profiles compared to target profiles on both synthetic and experimental datasets. The use of Softplus activation function in the final two fully connected layers further enhances model performance. The physics-informed loss function corrects prediction errors, bringing back-projections closer to actual inputs and reducing inversion algorithm errors. Synthetic data models for generating customized diagnostic datasets and collection of soft x-ray diagnostic datasets from EAST and HL-2A contribute to the study's success in reducing reconstruction errors and accelerating the development of surrogate models in fusion research. <br /> <div>
arXiv:2412.00087v3 Announce Type: replace-cross 
Abstract: Rapid reconstruction of 2D plasma profiles from line-integral measurements is important in nuclear fusion. This paper introduces a physics-informed model architecture called Onion, that can enhance the performance of models and be adapted to various backbone networks. The model under Onion incorporates physical information by a multiplication process and applies the physics-informed loss function according to the principle of line integration. Prediction results demonstrate that the additional input of physical information improves the deep learning model's ability, leading to a reduction in the average relative error E_1 between the reconstruction profiles and the target profiles by approximately 0.84x10^(-2) on synthetic datasets and about 0.06x10^(-2) on experimental datasets. Furthermore, the implementation of the Softplus activation function in the final two fully connected layers improves model performance. This enhancement results in a reduction in the E_1 by approximately 1.06x10^(-2) on synthetic datasets and about 0.11x10^(-2) on experimental datasets. The incorporation of the physics-informed loss function has been shown to correct the model's predictions, bringing the back-projections closer to the actual inputs and reducing the errors associated with inversion algorithms. Besides, we have developed a synthetic data model to generate customized line-integral diagnostic datasets and have also collected soft x-ray diagnostic datasets from EAST and HL-2A. This study achieves reductions in reconstruction errors, and accelerates the development of surrogate models in fusion research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven inventory management for new products: An adjusted Dyna-$Q$ approach with transfer learning</title>
<link>https://arxiv.org/abs/2501.08109</link>
<guid>https://arxiv.org/abs/2501.08109</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, inventory management, transfer learning, model-based approach, Dyna-Q

Summary:
The paper presents a novel reinforcement learning algorithm for inventory management of new products without historical demand data. It combines model-free and model-based approaches in a Dyna-Q structure, enhancing training efficiency and reducing model discrepancy. By incorporating transfer learning from similar existing products' demand data, the algorithm stabilizes early-stage training and improves policy estimation accuracy. Validation through a bakery inventory case study demonstrates up to a 23.7% cost reduction compared to Q-learning and up to a 77.5% faster training time than traditional Dyna-Q. Utilizing transfer learning, the adjusted Dyna-Q outperforms benchmark algorithms in terms of total cost, cost variance, and shortage percentages during a 30-day testing period. <div>
arXiv:2501.08109v4 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel reinforcement learning algorithm for inventory management of newly launched products with no historical demand information. The algorithm follows the classic Dyna-$Q$ structure, balancing the model-free and model-based approaches, while accelerating the training process of Dyna-$Q$ and mitigating the model discrepancy generated by the model-based feedback. Based on the idea of transfer learning, warm-start information from the demand data of existing similar products can be incorporated into the algorithm to further stabilize the early-stage training and reduce the variance of the estimated optimal policy. Our approach is validated through a case study of bakery inventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\% reduction in average daily cost compared with $Q$-learning, and up to a 77.5\% reduction in training time within the same horizon compared with classic Dyna-$Q$. By using transfer learning, it can be found that the adjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and relatively low shortage percentages among all the benchmarking algorithms under a 30-day testing.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Aspects of Strategic Trading</title>
<link>https://arxiv.org/abs/2502.07606</link>
<guid>https://arxiv.org/abs/2502.07606</guid>
<content:encoded><![CDATA[
<div> Algorithmic trading, position building, equilibrium strategies, temporary market impact, permanent market impact <br />
<br />Summary: 
This study focuses on algorithmic trading in financial markets, particularly in the context of position building with temporary and permanent market impact. The research presents an efficient algorithm for computing best responses in trading strategies, highlighting the challenges of convergence in the general setting. While the temporary impact-only scenario forms a potential game, convergence is not guaranteed in the broader context. The concept of Coarse Correlated Equilibria (CCE) is introduced as an alternative solution, computable via Follow the Perturbed Leader (FTPL) implementation. An experimental investigation demonstrates the behavior of FTPL in varying conditions of temporary and permanent market impact weighting, shedding light on strategic behaviors in complex trading environments. <div>
arXiv:2502.07606v2 Announce Type: replace-cross 
Abstract: Algorithmic trading in modern financial markets is widely acknowledged to exhibit strategic, game-theoretic behaviors whose complexity can be difficult to model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress in the setting of trading for position building. Here parties wish to buy or sell a fixed number of shares in a fixed time period in the presence of both temporary and permanent market impact, resulting in exponentially large strategy spaces. While these papers primarily consider the existence and structural properties of equilibrium strategies, in this work we focus on the algorithmic aspects of the proposed model. We give an efficient algorithm for computing best responses, and show that while the temporary impact only setting yields a potential game, best response dynamics do not generally converge for the general setting, for which no fast algorithm for (Nash) equilibrium computation is known. This leads us to consider the broader notion of Coarse Correlated Equilibria (CCE), which we show can be computed efficiently via an implementation of Follow the Perturbed Leader (FTPL). We illustrate the model and our results with an experimental investigation, where FTPL exhibits interesting behavior in different regimes of the relative weighting between temporary and permanent market impact.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation</title>
<link>https://arxiv.org/abs/2503.09409</link>
<guid>https://arxiv.org/abs/2503.09409</guid>
<content:encoded><![CDATA[
<div> AI-based framework, automation, cable connector mating, force control, deep visuotactile learning<br />
<br />
Summary: 
An AI-based framework has been developed to automate the manual process of cable connector mating in automotive assembly. The system integrates force control with deep visuotactile learning to optimize search-and-insertion strategies using multimodal transformer architecture. A novel automated data collection and optimization pipeline reduces the need for machine learning expertise. The framework optimizes robot programs that can run on standard industrial controllers, allowing for human auditing and certification. Experimental validations on a center console assembly task show improved cycle times and robustness compared to traditional robot programming methods. Videos demonstrating the system's capabilities are available for further reference. <div>
arXiv:2503.09409v2 Announce Type: replace-cross 
Abstract: Despite the widespread adoption of industrial robots in automotive assembly, wire harness installation remains a largely manual process, as it requires precise and flexible manipulation. To address this challenge, we design a novel AI-based framework that automates cable connector mating by integrating force control with deep visuotactile learning. Our system optimizes search-and-insertion strategies using first-order optimization over a multimodal transformer architecture trained on visual, tactile, and proprioceptive data. Additionally, we design a novel automated data collection and optimization pipeline that minimizes the need for machine learning expertise. The framework optimizes robot programs that run natively on standard industrial controllers, permitting human experts to audit and certify them. Experimental validations on a center console assembly task demonstrate significant improvements in cycle times and robustness compared to conventional robot programming approaches. Videos are available under https://claudius-kienle.github.io/AppMuTT.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Informer for Option Pricing: A Transformer-Based Approach</title>
<link>https://arxiv.org/abs/2506.05565</link>
<guid>https://arxiv.org/abs/2506.05565</guid>
<content:encoded><![CDATA[
<div> neural network, option pricing, financial forecasting, long-term dependencies, market fluctuations

Summary:
This paper explores the use of the Informer neural network for accurate option pricing in financial markets. Traditional models like Black-Scholes are limited in capturing market volatility, but Informer's efficient architecture allows for better prediction accuracy by incorporating long-term dependencies and dynamically adapting to market fluctuations. The study demonstrates that Informer surpasses traditional approaches in option pricing, highlighting its potential to enhance data-driven financial forecasting in this field. The research contributes to advancing the capabilities of option pricing by providing a more adaptable and resilient framework for effective trading and risk management in financial markets.<br /><br />Summary: <div>
arXiv:2506.05565v1 Announce Type: new 
Abstract: Accurate option pricing is essential for effective trading and risk management in financial markets, yet it remains challenging due to market volatility and the limitations of traditional models like Black-Scholes. In this paper, we investigate the application of the Informer neural network for option pricing, leveraging its ability to capture long-term dependencies and dynamically adjust to market fluctuations. This research contributes to the field of financial forecasting by introducing Informer's efficient architecture to enhance prediction accuracy and provide a more adaptable and resilient framework compared to existing methods. Our results demonstrate that Informer outperforms traditional approaches in option pricing, advancing the capabilities of data-driven financial forecasting in this domain.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction</title>
<link>https://arxiv.org/abs/2506.05427</link>
<guid>https://arxiv.org/abs/2506.05427</guid>
<content:encoded><![CDATA[
<div> prediction, drug discovery, material design, computational methods, interaction details 

Summary:
The article presents the Multi-Grained Target Perception network (MTPNet) for activity cliff prediction in drug discovery and material design. MTPNet integrates Macro-level Target Semantic (MTS) guidance and Micro-level Pocket Semantic (MPS) guidance to optimize molecular representations based on protein interactions. This approach, utilizing receptor proteins as guiding information, outperforms previous methods by achieving an average RMSE improvement of 18.95% on various datasets. By incorporating interaction patterns through conditional deep learning, MTPNet provides accurate predictions of activity cliffs, aiding in compound optimization and design. The availability of codes for MTPNet on GitHub enhances its accessibility and applicability in research and industry. <div>
arXiv:2506.05427v1 Announce Type: cross 
Abstract: Activity cliff prediction is a critical task in drug discovery and material design. Existing computational methods are limited to handling single binding targets, which restricts the applicability of these prediction models. In this paper, we present the Multi-Grained Target Perception network (MTPNet) to incorporate the prior knowledge of interactions between the molecules and their target proteins. Specifically, MTPNet is a unified framework for activity cliff prediction, which consists of two components: Macro-level Target Semantic (MTS) guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet dynamically optimizes molecular representations through multi-grained protein semantic conditions. To our knowledge, it is the first time to employ the receptor proteins as guiding information to effectively capture critical interaction details. Extensive experiments on 30 representative activity cliff datasets demonstrate that MTPNet significantly outperforms previous approaches, achieving an average RMSE improvement of 18.95% on top of several mainstream GNN architectures. Overall, MTPNet internalizes interaction patterns through conditional deep learning to achieve unified predictions of activity cliffs, helping to accelerate compound optimization and design. Codes are available at: https://github.com/ZishanShu/MTPNet.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Network Model of Spatial and Feature-Based Attention</title>
<link>https://arxiv.org/abs/2506.05487</link>
<guid>https://arxiv.org/abs/2506.05487</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual attention, neural network model, human cognition, spatial attention, feature-based attention

Summary: 
This article introduces a neural network model inspired by human visual attention mechanisms. The model consists of two networks, one performing a basic task and the other guiding attention based on contextual information for more complex tasks. The trained model's attention patterns closely resemble spatial and feature-based attention observed in human vision. This similarity suggests that neural network models can effectively mimic human cognition, offering valuable insights into the workings of visual attention. The study highlights the potential of using neural network models to explore and understand human cognitive processes, particularly in the realm of visual attention. <div>
arXiv:2506.05487v1 Announce Type: cross 
Abstract: Visual attention is a mechanism closely intertwined with vision and memory. Top-down information influences visual processing through attention. We designed a neural network model inspired by aspects of human visual attention. This model consists of two networks: one serves as a basic processor performing a simple task, while the other processes contextual information and guides the first network through attention to adapt to more complex tasks. After training the model and visualizing the learned attention response, we discovered that the model's emergent attention patterns corresponded to spatial and feature-based attention. This similarity between human visual attention and attention in computer vision suggests a promising direction for studying human cognition using neural network models.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Stabilization Protocol for Cross-Chain Digital Assets Using Adaptor Signatures and AI-Driven Arbitrage</title>
<link>https://arxiv.org/abs/2506.05708</link>
<guid>https://arxiv.org/abs/2506.05708</guid>
<content:encoded><![CDATA[
<div> decentralization, stability, regulatory compliance, stablecoins, hybrid stabilization protocol

Summary:
The article discusses the challenges faced by stablecoins in balancing decentralization, stability, and regulatory compliance. A hybrid stabilization protocol is proposed, combining crypto-collateralized reserves, algorithmic futures contracts, and cross-chain liquidity pools. Stabilization futures contracts (SFCs) are introduced as non-collateralized derivatives that incentivize third-party arbitrageurs to maintain price adherence. Autonomous AI agents optimize delta hedging on decentralized exchanges, while zkSNARKs ensure compliance with anti-money laundering regulations without revealing user identities. The cryptographic design reduces cross-chain liquidity concentration and ensures atomicity in transactions. The protocol's layered architecture, including SFCs, AI market making, and zero-knowledge regulatory proofs, serves as a blueprint for advanced decentralized financial infrastructure. <div>
arXiv:2506.05708v1 Announce Type: cross 
Abstract: Stablecoins face an unresolved trilemma of balancing decentralization, stability, and regulatory compliance. We present a hybrid stabilization protocol that combines crypto-collateralized reserves, algorithmic futures contracts, and cross-chain liquidity pools to achieve robust price adherence while preserving user privacy. At its core, the protocol introduces stabilization futures contracts (SFCs), non-collateralized derivatives that programmatically incentivize third-party arbitrageurs to counteract price deviations via adaptor signature atomic swaps. Autonomous AI agents optimize delta hedging across decentralized exchanges (DEXs), while zkSNARKs prove compliance with anti-money laundering (AML) regulations without exposing identities or transaction details. Our cryptographic design reduces cross-chain liquidity concentration (Herfindahl-Hirschman Index: 2,400 vs. 4,900 in single-chain systems) and ensures atomicity under standard cryptographic assumptions. The protocol's layered architecture encompassing incentive-compatible SFCs, AI-driven market making, and zero-knowledge regulatory proofs. It provides a blueprint for next-generation decentralized financial infrastructure.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts</title>
<link>https://arxiv.org/abs/2506.05755</link>
<guid>https://arxiv.org/abs/2506.05755</guid>
<content:encoded><![CDATA[
<div> flow matching models, optimal execution, financial markets, imitation learning framework, market impact costs

Summary:
flowOE is introduced as a novel imitation learning framework for optimal execution in financial markets. It learns from expert traditional strategies and selects the most suitable behavior based on market conditions, incorporating a refining loss function to improve upon learned actions. This approach outperforms both expert models and traditional benchmarks in empirical evaluations, achieving higher profits with reduced risk. FlowOE demonstrates practical applicability and potential to enhance adaptive optimal execution in dynamic financial markets. <br /><br />Summary: <div>
arXiv:2506.05755v1 Announce Type: cross 
Abstract: Optimal execution in financial markets refers to the process of strategically transacting a large volume of assets over a period to achieve the best possible outcome by balancing the trade-off between market impact costs and timing or volatility risks. Traditional optimal execution strategies, such as static Almgren-Chriss models, often prove suboptimal in dynamic financial markets. This paper propose flowOE, a novel imitation learning framework based on flow matching models, to address these limitations. FlowOE learns from a diverse set of expert traditional strategies and adaptively selects the most suitable expert behavior for prevailing market conditions. A key innovation is the incorporation of a refining loss function during the imitation process, enabling flowOE not only to mimic but also to improve upon the learned expert actions. To the best of our knowledge, this work is the first to apply flow matching models in a stochastic optimal execution problem. Empirical evaluations across various market conditions demonstrate that flowOE significantly outperforms both the specifically calibrated expert models and other traditional benchmarks, achieving higher profits with reduced risk. These results underscore the practical applicability and potential of flowOE to enhance adaptive optimal execution.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator</title>
<link>https://arxiv.org/abs/2506.05797</link>
<guid>https://arxiv.org/abs/2506.05797</guid>
<content:encoded><![CDATA[
<div> equivariant neural fields simulator, deformable objects, collisions, Graph Neural Network, scalability
Summary:
EqCollide is a novel end-to-end neural fields simulator designed for simulating collisions of deformable objects. The model incorporates an equivariant encoder to map object geometry and velocity into latent control points, followed by a Graph Neural Network-based Neural Ordinary Differential Equation to model interactions among control points via collision-aware message passing. The approach allows for accurate and stable simulations across diverse object configurations, with a significant reduction in MSE compared to baseline models. Furthermore, EqCollide demonstrates scalability, generalization to more colliding objects and extended temporal horizons, and robustness to input transformations with group action. The model also enables continuous and resolution-independent motion predictions, showcasing its potential for a wide range of applications in simulating complex interactions among deformable objects. 
<br /><br />Summary: <div>
arXiv:2506.05797v1 Announce Type: cross 
Abstract: Simulating collisions of deformable objects is a fundamental yet challenging task due to the complexity of modeling solid mechanics and multi-body interactions. Existing data-driven methods often suffer from lack of equivariance to physical symmetries, inadequate handling of collisions, and limited scalability. Here we introduce EqCollide, the first end-to-end equivariant neural fields simulator for deformable objects and their collisions. We propose an equivariant encoder to map object geometry and velocity into latent control points. A subsequent equivariant Graph Neural Network-based Neural Ordinary Differential Equation models the interactions among control points via collision-aware message passing. To reconstruct velocity fields, we query a neural field conditioned on control point features, enabling continuous and resolution-independent motion predictions. Experimental results show that EqCollide achieves accurate, stable, and scalable simulations across diverse object configurations, and our model achieves 24.34% to 35.82% lower rollout MSE even compared with the best-performing baseline model. Furthermore, our model could generalize to more colliding objects and extended temporal horizons, and stay robust to input transformed with group action.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging</title>
<link>https://arxiv.org/abs/2506.05828</link>
<guid>https://arxiv.org/abs/2506.05828</guid>
<content:encoded><![CDATA[
<div> Keywords: FinanceReasoning, benchmark, large reasoning models, financial concepts, numerical precision

Summary: 
FinanceReasoning introduces a new benchmark to evaluate the reasoning abilities of large reasoning models (LRMs) in financial numerical tasks. The benchmark includes updated questions with detailed Python solutions, covering 67.8% of financial concepts and formulas. LRMs benefit from 3,133 Python-formatted functions, enhancing their financial reasoning capabilities. The benchmark also presents 238 challenging problems requiring precise numerical reasoning. The best-performing model achieves 89.1% accuracy, highlighting the ongoing challenges faced by LRMs in numerical precision. The study demonstrates that combining Reasoner and Programmer models can improve LRMs' performance. This advancement in evaluating LRMs in specific reasoning tasks opens avenues for future research in complex domain-specific reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2506.05828v1 Announce Type: cross 
Abstract: We introduce FinanceReasoning, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) Credibility: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) Comprehensiveness: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMs' financial reasoning capabilities through refined knowledge (e.g., 83.2% $\rightarrow$ 91.6% for GPT-4o). (3) Challenge: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMs' performance (e.g., 83.2% $\rightarrow$ 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design Tasks and Their Complexity for the European Train Control System with Hybrid Train Detection</title>
<link>https://arxiv.org/abs/2308.02572</link>
<guid>https://arxiv.org/abs/2308.02572</guid>
<content:encoded><![CDATA[
<div> Keywords: railway networks, train control system, ETCS L2 HTD, design tasks, computational complexity

Summary:
Railway networks play a crucial role in transportation, especially for freight and public transit. Maximizing rail capacity is essential, but existing constraints must be considered. The European Train Control System (ETCS L2 HTD) introduces virtual subsections to improve train following times and increase track capacity. Design tasks for ETCS L2 HTD present new challenges that can benefit from automated methods. This paper provides a formal description of these design tasks and proves their computational complexity to be NP-complete or NP-hard. This research forms the foundation for developing methods to address these tasks and will be integrated into the Munich Train Control Toolkit for the railway industry. <div>
arXiv:2308.02572v4 Announce Type: replace-cross 
Abstract: Railway networks have become increasingly important in recent times, especially in moving freight and public transportation from road traffic and planes to more environmentally friendly trains. Since expanding the global railway network is time- and resource-consuming, maximizing the rail capacity of the existing infrastructure is desirable. However, simply running more trains is infeasible as certain constraints enforced by the train control system must be satisfied. The capacity of a network depends (amongst others) on the distance between trains allowed by this safety system. While most signaling systems rely on fixed blocks defined by costly hardware, new specifications provided by Level 2 with Hybrid Train Detection of the European Train Control System (ETCS L2 HTD), formerly known as ETCS Hybrid Level 3, allow the usage of virtual subsections. This additional degree of freedom allows for shorter train following times and, thus, more trains on existing railway tracks. On the other hand, new design tasks arise on which automated methods might be helpful for designers of modern railway networks. However, although first approaches exist that solve design problems arising within ETCS L2 HTD, neither formal descriptions nor results on the computational complexity of the corresponding design tasks exist. In this paper, we fill this gap by providing a formal description of design tasks for ETCS L2 HTD and proof that these tasks are NP-complete or NP-hard, respectively. By that, we are providing a solid basis for the future development of methods to solve those tasks, which will be integrated into the Munich Train Control Toolkit available open-source on GitHub at https://github.com/cda-tum/mtct.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and efficient predictions of keyhole dynamics in laser materials processing using machine learning-aided simulations</title>
<link>https://arxiv.org/abs/2402.16190</link>
<guid>https://arxiv.org/abs/2402.16190</guid>
<content:encoded><![CDATA[
<div> machine learning, simulation, laser materials processing, keyhole phenomenon, defects

Summary:
- The study focuses on the keyhole phenomenon in laser materials processing, which leads to the formation of pores and affects product performance.
- Pores are associated with the dynamic behavior of the keyhole, making accurate characterization and prediction challenging.
- In situ characterization using synchrotron X-ray technique is informative but complex and costly.
- The developed machine learning-aided simulation method accurately predicts keyhole dynamics, particularly keyhole depth fluctuations, for a wide range of processing parameters.
- The method achieved a mean absolute percentage error of 10%, surpassing ray-tracing simulations with a 30% error margin and reducing computational time.
- This cost-effective and efficient model can serve as an alternative to synchrotron experiments, offering potential for defect elimination or reduction in various laser materials processing techniques. 

<br /><br />Summary: <div>
arXiv:2402.16190v2 Announce Type: replace-cross 
Abstract: The keyhole phenomenon has been widely observed in laser materials processing, including laser welding, remelting, cladding, drilling, and additive manufacturing. Keyhole-induced defects, primarily pores, dramatically affect the performance of final products, impeding the broad use of these laser-based technologies. The formation of these pores is typically associated with the dynamic behavior of the keyhole. So far, the accurate characterization and prediction of keyhole features, particularly keyhole depth, as a function of time, has been a challenging task. In situ characterization of keyhole dynamic behavior using the synchrotron X-ray technique is informative but complicated and expensive. Current simulations are generally hindered by their poor accuracy and generalization abilities in predicting keyhole depths due to the lack of accurate laser absorptance data. In this study, we develop a machine learning-aided simulation method that accurately predicts keyhole dynamics, especially in keyhole depth fluctuations, over a wide range of processing parameters. In two case studies involving titanium and aluminum alloys, we achieve keyhole depth prediction with a mean absolute percentage error of 10%, surpassing those simulated using the ray-tracing method with an error margin of 30%, while also reducing computational time. This exceptional fidelity and efficiency empower our model to serve as a cost-effective alternative to synchrotron experiments. Our machine learning-aided simulation method is affordable and readily deployable for a large variety of materials, opening new doors to eliminate or reduce defects for a wide range of laser materials processing techniques.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Structure of Financial Equity Research Reports -- Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4</title>
<link>https://arxiv.org/abs/2407.18327</link>
<guid>https://arxiv.org/abs/2407.18327</guid>
<content:encoded><![CDATA[
<div> Keywords: financial equity research reports, automation, language models, information extraction, question archetypes

Summary:
- The research aims to categorize the content of financial equity research reports (ERRs) by analyzing 72 reports sentence-by-sentence.
- 4940 sentences from ERRs were classified into 169 unique question archetypes without predefined questions, providing an unbiased view of their content.
- 78.7% of the questions in ERRs were found to be automatable, with 48.2% as text-extractable and 30.5% as database-extractable.
- Only 21.3% of questions required human judgment, suggesting a potential for automation in the ERR writing process.
- Empirical validation using advanced language models like Llama-3-70B and GPT-4-turbo-2024-04-09 confirmed the feasibility of automating approximately 80% of ERR content.
- The study highlights the potential benefits of introducing large language models in the ERR writing process to enhance quality and efficiency. 

<br /><br />Summary: This research analyzes financial equity research reports to identify question archetypes and determine the potential for automation in answering these questions. The study reveals that a significant percentage of questions in ERRs can be automated, with text-extractable and database-extractable questions being prominent. By leveraging advanced language models, such as Llama-3-70B and GPT-4-turbo-2024-04-09, the automation of ERR writing process is feasible, providing opportunities to improve quality and efficiency. <div>
arXiv:2407.18327v2 Announce Type: replace-cross 
Abstract: This research dissects financial equity research reports (ERRs) by mapping their content into categories. There is insufficient empirical analysis of the questions answered in ERRs. In particular, it is not understood how frequently certain information appears, what information is considered essential, and what information requires human judgment to distill into an ERR. The study analyzes 72 ERRs sentence-by-sentence, classifying their 4940 sentences into 169 unique question archetypes. We did not predefine the questions but derived them solely from the statements in the ERRs. This approach provides an unbiased view of the content of the observed ERRs. Subsequently, we used public corporate reports to classify the questions' potential for automation. Answers were labeled "text-extractable" if the answers to the question were accessible in corporate reports. 78.7% of the questions in ERRs can be automated. Those automatable question consist of 48.2% text-extractable (suited to processing by large language models, LLMs) and 30.5% database-extractable questions. Only 21.3% of questions require human judgment to answer. We empirically validate using Llama-3-70B and GPT-4-turbo-2024-04-09 that recent advances in language generation and information extraction enable the automation of approximately 80% of the statements in ERRs. Surprisingly, the models complement each other's strengths and weaknesses well. The research confirms that the current writing process of ERRs can likely benefit from additional automation, improving quality and efficiency. The research thus allows us to quantify the potential impacts of introducing large language models in the ERR writing process. The full question list, including the archetypes and their frequency, will be made available online after peer review.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemReservoir -- An Open-Source Framework for Chemically-Inspired Reservoir Computing</title>
<link>https://arxiv.org/abs/2506.04249</link>
<guid>https://arxiv.org/abs/2506.04249</guid>
<content:encoded><![CDATA[
<div> Keywords: reservoir computing, cheminformatics, open-source framework, chemically-inspired reservoirs, memory capacity tasks

Summary:
ChemReservoir is introduced as an open-source framework for chemically-inspired reservoir computing, addressing the limitations of previous studies focused on DNA chemistry. Unlike previous tools, ChemReservoir is a general framework for constructing and analyzing chemically-inspired reservoirs, ensuring enhanced testing, evaluation, and reproducibility. The tool was evaluated using various cycle-based reservoir topologies and showed stable performance across different configurations in memory capacity tasks. This framework allows for the development of reservoir models not limited to DNA chemistry, making it a versatile tool for cheminformatics research. By providing a user-friendly and accessible platform, ChemReservoir contributes to the advancement of reservoir computing in the field of cheminformatics. <br /><br />Summary: <div>
arXiv:2506.04249v1 Announce Type: new 
Abstract: Reservoir computing is a type of a recurrent neural network, mapping the inputs into higher dimensional space using fixed and nonlinear dynamical systems, called reservoirs. In the literature, there are various types of reservoirs ranging from in-silico to in-vitro. In cheminformatics, previous studies contributed to the field by developing simulation-based chemically inspired in-silico reservoir models. Yahiro used a DNA-based chemical reaction network as its reservoir and Nguyen developed a DNA chemistry-inspired tool based on Gillespie algorithm. However, these software tools were designed mainly with the focus on DNA chemistry and their maintenance status has limited their current usability. Due to these limitations, there was a need for a proper open-source tool. This study introduces ChemReservoir, an open-source framework for chemically-inspired reservoir computing. In contrast to the former studies focused on DNA-chemistry, ChemReservoir is a general framework for the construction and analysis of chemically-inspired reservoirs, which also addresses the limitations in these previous studies by ensuring enhanced testing, evaluation, and reproducibility. The tool was evaluated using various cycle-based reservoir topologies and demonstrated stable performance across a range of configurations in memory capacity tasks.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive recycled plastic architecture: Vacuum-Sealed Chainmail Structures Through Computational Design</title>
<link>https://arxiv.org/abs/2506.04660</link>
<guid>https://arxiv.org/abs/2506.04660</guid>
<content:encoded><![CDATA[
<div> Keywords: recycled plastics, construction industry, modular chainmail systems, sustainability, innovative applications<br />
<br />
Summary: This paper explores the use of recycled plastics as a primary construction material in modular chainmail systems. The research demonstrates the optimization of design, testing, and fabrication of vacuum-sealed chainmail structures made of recycled plastic filaments. The study identifies the rectangular chainmail configuration as the most efficient for architectural use, with superior deformation capacity, material efficiency, and load-bearing performance. Optimization strategies for temporary structures are also proposed to balance material savings, usable area, and water drainage efficiency. The findings suggest innovative applications for extreme conditions such as disaster-prone areas, high-altitude environments, underwater platforms, and extraterrestrial habitats, leveraging the properties of recycled plastics and modular chainmail systems. This research bridging waste management and high-performance design offers solutions for challenges in harsh and resource-constrained environments. <br /><br /> <div>
arXiv:2506.04660v1 Announce Type: new 
Abstract: The construction industry is a major consumer of raw materials, accounting for nearly half of global material usage annually, while generating significant waste that poses sustainability challenges. This paper explores the untapped potential of recycled plastics as a primary construction material, leveraging their lightweight, flexible, and customizable properties for advanced applications in modular chainmail systems. Through a computational workflow, the study optimizes the design, testing, and fabrication of vacuum-sealed chainmail structures composed of recycled plastic filaments, demonstrating their adaptability and structural performance for architectural use.
  Key contributions include a novel methodology for integrating recycled plastic filaments into chainmail geometries, validated through 2D sectional testing, 3D shell structure generation, and physical modeling under vacuum constraints. The research identifies the rectangular chainmail configuration as the most efficient and adaptable, achieving superior deformation capacity, material efficiency, and load-bearing performance. Optimization strategies for temporary structures highlight practical deployment potential, balancing material savings, usable area, and water drainage efficiency.
  The findings offer a foundation for innovative applications in extreme conditions, including disaster-prone areas, high-altitude environments, underwater platforms, and extraterrestrial habitats. These applications leverage the lightweight, adaptable, and durable properties of recycled plastics and modular chainmail systems, bridging the gap between waste management and high-performance design while addressing unique challenges in harsh and resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear elastodynamic material identification of heterogeneous isogeometric Bernoulli-Euler beams</title>
<link>https://arxiv.org/abs/2506.04960</link>
<guid>https://arxiv.org/abs/2506.04960</guid>
<content:encoded><![CDATA[
<div> Finite Element Model Updating, Material Identification, Isogeometric Formulation, Bernoulli-Euler Beams, Elastic Properties<br />
<br />
Summary: This paper presents a Finite Element Model Updating framework for identifying heterogeneous material distributions in planar Bernoulli-Euler beams. The process involves identifying elastic properties from quasi-static displacements and determining density from modal data. Three independent discretizations are used, including isogeometric finite element mesh, high-resolution experimental measurement grid, and material mesh with low-order Lagrange elements. The method minimizes errors between experiments and numerical model using local optimization with trust-region method. Results from numerical examples show effectiveness in handling large displacements and noise in experimental data. B2M1 discretization is used to alleviate membrane locking. Regularization ensures stable solutions for dense material meshes. The proposed framework can be extended to shells and 3D continua. <div>
arXiv:2506.04960v1 Announce Type: new 
Abstract: This paper presents a Finite Element Model Updating framework for identifying heterogeneous material distributions in planar Bernoulli-Euler beams based on a rotation-free isogeometric formulation. The procedure follows two steps: First, the elastic properties are identified from quasi-static displacements; then, the density is determined from modal data (low frequencies and mode shapes), given the previously obtained elastic properties. The identification relies on three independent discretizations: the isogeometric finite element mesh, a high-resolution grid of experimental measurements, and a material mesh composed of low-order Lagrange elements. The material mesh approximates the unknown material distributions, with its nodal values serving as design variables. The error between experiments and numerical model is expressed in a least squares manner. The objective is minimized using local optimization with the trust-region method, providing analytical derivatives to accelerate computations. Several numerical examples exhibiting large displacements are provided to test the proposed approach. To alleviate membrane locking, the B2M1 discretization is employed when necessary. Quasi-experimental data is generated using refined finite element models with random noise applied up to 4%. The method yields satisfactory results as long as a sufficient amount of experimental data is available, even for high measurement noise. Regularization is used to ensure a stable solution for dense material meshes. The density can be accurately reconstructed based on the previously identified elastic properties. The proposed framework can be straightforwardly extended to shells and 3D continua.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series Analysis</title>
<link>https://arxiv.org/abs/2506.05019</link>
<guid>https://arxiv.org/abs/2506.05019</guid>
<content:encoded><![CDATA[
<div> Dataset, financial news, structured tables, K-line charts, stock prices  
Summary:  
- The paper introduces FinMultiTime, a large-scale multimodal financial time series dataset that includes financial news, structured financial tables, K-line technical charts, and stock price time series.
- The dataset encompasses 5,105 stocks from the S&amp;P 500 and HS 300 universes, covering the period from 2009 to 2025 in the U.S. and China.
- It provides data at minute-level, daily, and quarterly resolutions, enhancing the capturing of short, medium, and long-term market signals with high fidelity.
- Experiments show that the dataset's scale and data quality significantly improve prediction accuracy.
- Multimodal fusion in Transformer models results in moderate performance gains.
<br /><br />Summary: <div>
arXiv:2506.05019v1 Announce Type: new 
Abstract: Pure time series forecasting tasks typically focus exclusively on numerical features; however, real-world financial decision-making demands the comparison and analysis of heterogeneous sources of information. Recent advances in deep learning and large scale language models (LLMs) have made significant strides in capturing sentiment and other qualitative signals, thereby enhancing the accuracy of financial time series predictions. Despite these advances, most existing datasets consist solely of price series and news text, are confined to a single market, and remain limited in scale. In this paper, we introduce FinMultiTime, the first large scale, multimodal financial time series dataset. FinMultiTime temporally aligns four distinct modalities financial news, structured financial tables, K-line technical charts, and stock price time series across both the S&amp;P 500 and HS 300 universes. Covering 5,105 stocks from 2009 to 2025 in the United States and China, the dataset totals 112.6 GB and provides minute-level, daily, and quarterly resolutions, thus capturing short, medium, and long term market signals with high fidelity. Our experiments demonstrate that (1) scale and data quality markedly boost prediction accuracy; (2) multimodal fusion yields moderate gains in Transformer models; and (3) a fully reproducible pipeline enables seamless dataset updates.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmark for Antibody Binding Affinity Maturation and Design</title>
<link>https://arxiv.org/abs/2506.04235</link>
<guid>https://arxiv.org/abs/2506.04235</guid>
<content:encoded><![CDATA[
<div> Benchmarking, Antibody binding affinity maturation, Design, AbBiBench, Protein models <br />
Summary: <br />
The article introduces AbBiBench, a benchmarking framework for evaluating antibody binding affinity maturation and design. Unlike current methods that focus on the antibody alone, AbBiBench considers the antibody-antigen complex as a functional unit. The framework includes 9 datasets with 9 antigens and 155,853 mutated antibodies, allowing for systematic comparison of 14 protein models. The correlation between model likelihood and experimental affinity values is used to assess model performance. In a case study involving increasing binding affinity of antibody F045-092 to influenza H1N1, structure-conditioned inverse folding models prove to be the most effective. AbBiBench offers a biologically grounded evaluation framework to enhance the development of antibody design models that are more effective and function-aware. <br /> <div>
arXiv:2506.04235v1 Announce Type: cross 
Abstract: We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking framework for antibody binding affinity maturation and design. Unlike existing antibody evaluation strategies that rely on antibody alone and its similarity to natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench considers an antibody-antigen (Ab-Ag) complex as a functional unit and evaluates the potential of an antibody design binding to given antigen by measuring protein model's likelihood on the Ab-Ag complex. We first curate, standardize, and share 9 datasets containing 9 antigens (involving influenza, anti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain mutated antibodies. Using these datasets, we systematically compare 14 protein models including masked language models, autoregressive language models, inverse folding models, diffusion-based generative models, and geometric graph models. The correlation between model likelihood and experimental affinity values is used to evaluate model performance. Additionally, in a case study to increase binding affinity of antibody F045-092 to antigen influenza H1N1, we evaluate the generative power of the top-performing models by sampling a set of new antibodies binding to the antigen and ranking them based on structural integrity and biophysical properties of the Ab-Ag complex. As a result, structure-conditioned inverse folding models outperform others in both affinity correlation and generation tasks. Overall, AbBiBench provides a unified, biologically grounded evaluation framework to facilitate the development of more effective, function-aware antibody design models.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding</title>
<link>https://arxiv.org/abs/2506.04353</link>
<guid>https://arxiv.org/abs/2506.04353</guid>
<content:encoded><![CDATA[
<div> Keywords: ReXVQA, visual question answering, chest radiology, multimodal large language models, AI performance

Summary: 
ReXVQA is a new benchmark for visual question answering in chest radiology, encompassing a vast dataset of questions paired with X-ray studies. It introduces diverse and authentic tasks reflecting various radiological reasoning skills. State-of-the-art multimodal large language models were evaluated, with MedGemma achieving the highest accuracy. A human reader study revealed that MedGemma outperformed radiology residents in chest X-ray interpretation, indicating a milestone where AI surpassed expert human evaluation. The study also highlighted distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists. ReXVQA establishes a standard for evaluating radiological AI systems, offering public leaderboards and detailed evaluation metrics. The dataset will be open-sourced, laying the groundwork for AI systems capable of expert-level clinical reasoning. 

<br /><br />Summary: <div>
arXiv:2506.04353v1 Announce Type: cross 
Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual question answering (VQA) in chest radiology, comprising approximately 696,000 questions paired with 160,000 chest X-rays studies across training, validation, and test sets. Unlike prior efforts that rely heavily on template based queries, ReXVQA introduces a diverse and clinically authentic task suite reflecting five core radiological reasoning skills: presence assessment, location analysis, negation detection, differential diagnosis, and geometric reasoning. We evaluate eight state-of-the-art multimodal large language models, including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge the gap between AI performance and clinical expertise, we conducted a comprehensive human reader study involving 3 radiology residents on 200 randomly sampled cases. Our evaluation demonstrates that MedGemma achieved superior performance (83.84% accuracy) compared to human readers (best radiology resident: 77.27%), representing a significant milestone where AI performance exceeds expert human evaluation on chest X-ray interpretation. The reader study reveals distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists while showing more variable agreement patterns between human readers and AI models. ReXVQA establishes a new standard for evaluating generalist radiological AI systems, offering public leaderboards, fine-grained evaluation splits, structured explanations, and category-level breakdowns. This benchmark lays the foundation for next-generation AI systems capable of mimicking expert-level clinical reasoning beyond narrow pathology classification. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor-based multivariate function approximation: methods benchmarking and comparison</title>
<link>https://arxiv.org/abs/2506.04791</link>
<guid>https://arxiv.org/abs/2506.04791</guid>
<content:encoded><![CDATA[
<div> evaluation, tensor-based multivariate function construction, approximation, machine learning, benchmark 

Summary: 
This note evaluates methods for tensor-based multivariate function construction and approximation, using a collection of functions with varying complexity. The performance, features, and user experience of each method are assessed based on accuracy, computational time, and parameter tuning impact. The goal is to provide a fair comparison of available strategies to guide users in understanding the process, advantages, and limitations of each tool. The note introduces a benchmark collection of tools for tensor approximation by surrogate models and gives explicit attention to the multivariate Loewner Framework approach. The detailed comparison allows readers to grasp the capabilities and limitations of each method, with examples provided for clarity. <div>
arXiv:2506.04791v1 Announce Type: cross 
Abstract: In this note, we evaluate the performances, the features and the user-experience of some methods (and their implementations) designed for tensor- (or data-) based multivariate function construction and approximation. To this aim, a collection of multivariate functions extracted from contributive works coming from different communities, is suggested. First, these functions with varying complexity (e.g. number and degree of the variables) and nature (e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to construct tensors, each of different dimension and size on the disk. Second, grounded on this tensor, we inspect performances of each considered method (e.g. the accuracy, the computational time, the parameters tuning impact, etc.). Finally, considering the "best" parameter tuning set, we compare each method using multiple evaluation criteria. The purpose of this note is not to rank the methods but rather to evaluate as fairly as possible the different available strategies, with the idea in mind to guide users to understand the process, the possibilities, the advantages and the limits brought by each tools. The contribution claimed is to suggest a complete benchmark collection of some available tools for tensor approximation by surrogate models (e.g. rational functions, networks, etc.). In addition, as contributors of the multivariate Loewner Framework (mLF) approach (and its side implementation in MDSPACK), attention and details of the latter are more explicitly given, in order to provide readers a digest of this contributive work and some details with simple examples.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Private Smart Wallet with Probabilistic Compliance</title>
<link>https://arxiv.org/abs/2506.04853</link>
<guid>https://arxiv.org/abs/2506.04853</guid>
<content:encoded><![CDATA[
<div> smart wallet, privacy-preserving, private onboarding mechanism, compliance checks, digital payments

Summary: 
The article presents a privacy-preserving smart wallet that incorporates a novel invitation-based private onboarding mechanism. It combines proof of innocence and ancestral commitment tracking systems for compliance with an authority party. Performance analysis reveals efficient private transfers with compliance checks completing in seconds on a standard laptop, and low proof generation. On-chain costs remain minimal for affordability on a Base layer 2 network. The smart wallet enables encrypted contact list management and transaction unlinkability for enhanced privacy. The evaluation confirms the effectiveness of the approach for compliance-aware digital payments, with reduced computational and financial burdens. <div>
arXiv:2506.04853v1 Announce Type: cross 
Abstract: We propose a privacy-preserving smart wallet with a novel invitation-based private onboarding mechanism. The solution integrates two levels of compliance in concert with an authority party: a proof of innocence mechanism and an ancestral commitment tracking system using bloom filters for probabilistic UTXO chain states. Performance analysis demonstrates practical efficiency: private transfers with compliance checks complete within seconds on a consumer-grade laptop, and overall with proof generation remaining low. On-chain costs stay minimal, ensuring affordability for all operations on Base layer 2 network. The wallet facilitates private contact list management through encrypted data blobs while maintaining transaction unlinkability. Our evaluation validates the approach's viability for privacy-preserving, compliance-aware digital payments with minimized computational and financial overhead.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRC20 Pinning Attack</title>
<link>https://arxiv.org/abs/2410.11295</link>
<guid>https://arxiv.org/abs/2410.11295</guid>
<content:encoded><![CDATA[
<div> tokens, Bitcoin network, security, attack vector, Binance

Summary:
BRC20 tokens are unique assets on the Bitcoin network that allow users to embed custom content within satoshis. Despite their growing market size, the transfer mechanism of BRC20 tokens has not been thoroughly examined for security vulnerabilities. A new attack vector, known as the BRC20 pinning attack, exploits the fee levels of bundled transactions to disrupt liquidity and withdrawal requests. This attack was successfully validated in collaboration with Binance researchers, resulting in a temporary suspension of withdrawals. The impact of the attack extends to a significant portion of inscription-based tokens in the Bitcoin ecosystem, highlighting the potential risks associated with BRC20 tokens and similar assets. Security measures and further analysis are necessary to mitigate the threat posed by such attacks. 

<br /><br />Summary: <div>
arXiv:2410.11295v3 Announce Type: replace-cross 
Abstract: BRC20 tokens are a type of non-fungible asset on the Bitcoin network. They allow users to embed customised content within Bitcoin's satoshis. The token frenzy reached a market size of US\$2.811\,b (2023Q3--2025Q1). However, this intuitive design has not undergone serious security scrutiny.
  We present the first analysis of BRC20's \emph{transfer} mechanism and identify a new attack vector. A typical BRC20 transfer involves two "bundled" on-chain transactions with different fee levels: the first (i.e., \textbf{Tx1}) with a lower fee inscribes the \textsf{transfer} request, while the second (i.e., \textbf{Tx2}) with a higher fee finalizes the actual transfer. An adversary can send a manipulated fee transaction (falling between the two fee levels), which causes \textbf{Tx1} to be processed while \textbf{Tx2} is pinned in the mempool. This locks BRC20 liquidity and disrupts normal withdrawal requests from users. We term this the \emph{BRC20 pinning attack}.
  We validated the attack in real-world settings in collaboration with Binance researchers. With their knowledge and permission, we conducted a controlled test against Binance's ORDI hot wallet, resulting in a temporary suspension of ORDI withdrawals for 3.5 hours. Recovery was performed shortly after. Further analysis confirms that the attack can be applied to over \textbf{90\%} of inscription-based tokens within the Bitcoin ecosystem.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.08435</link>
<guid>https://arxiv.org/abs/2412.08435</guid>
<content:encoded><![CDATA[
<div> Proactive Adaptation, Concept Drift Estimation, Forecast Model, Online Learning, Time Series<br />
<br />
Summary: Proceed is a proactive model adaptation framework designed for online time series forecasting. It addresses the challenge of concept drift by estimating and adapting to changes between training and test samples. By utilizing an adaptation generator, Proceed translates estimated drift into parameter adjustments, enhancing model performance. Trained on diverse synthetic concept drifts, Proceed demonstrates improved resilience against concept drift compared to existing online learning methods. Extensive experiments on real-world datasets confirm the effectiveness of Proceed in enhancing forecast model generalization capability and performance. The code for Proceed is available on GitHub for further exploration and application in time series forecasting tasks. <br /><br /> <div>
arXiv:2412.08435v4 Announce Type: replace-cross 
Abstract: Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Functional Materials Design with Optimal Initial Data in Surrogate-Based Active Learning</title>
<link>https://arxiv.org/abs/2506.03329</link>
<guid>https://arxiv.org/abs/2506.03329</guid>
<content:encoded><![CDATA[
<div> Keywords: functional materials, optimization, data-driven algorithms, surrogate-based active learning, quantum computing

Summary: 
This study focuses on optimizing functional materials through data-driven algorithms, which efficiently explore complex design spaces by learning relationships between material structures and performance metrics. Surrogate-based active learning, coupled with quantum computing, is highlighted as a cost-effective approach for material optimization. The use of a special surrogate model called quadratic unconstrained binary optimization is emphasized. The research investigates the impact of initial data sizes on optimization efficiency, showing that adequate initial data is crucial for achieving fast convergence and reducing computational costs. Averaged piecewise linear regression is used to identify the optimal initiation points for convergence, emphasizing the importance of proper initial data in efficient optimization of functional materials. This work contributes to improving optimization processes for functional materials by ensuring faster convergence and reduced computational expenses in surrogate-based active learning. 

<br /><br />Summary: <div>
arXiv:2506.03329v1 Announce Type: new 
Abstract: The optimization of functional materials is important to enhance their properties, but their complex geometries pose great challenges to optimization. Data-driven algorithms efficiently navigate such complex design spaces by learning relationships between material structures and performance metrics to discover high-performance functional materials. Surrogate-based active learning, continually improving its surrogate model by iteratively including high-quality data points, has emerged as a cost-effective data-driven approach. Furthermore, it can be coupled with quantum computing to enhance optimization processes, especially when paired with a special form of surrogate model ($i.e.$, quadratic unconstrained binary optimization), formulated by factorization machine. However, current practices often overlook the variability in design space sizes when determining the initial data size for optimization. In this work, we investigate the optimal initial data sizes required for efficient convergence across various design space sizes. By employing averaged piecewise linear regression, we identify initiation points where convergence begins, highlighting the crucial role of employing adequate initial data in achieving efficient optimization. These results contribute to the efficient optimization of functional materials by ensuring faster convergence and reducing computational costs in surrogate-based active learning.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the robustness of Dirichlet-Neumann coupling schemes for fluid-structure-interaction problems with nearly-closed fluid domains</title>
<link>https://arxiv.org/abs/2506.04027</link>
<guid>https://arxiv.org/abs/2506.04027</guid>
<content:encoded><![CDATA[
<div> Dirichlet-Neumann split, incompressible fluid, added-mass effect, flow resistance, nearly-closed fluid-domain<br />
<br />
Summary:<br />
Partitioned methods for fluid-structure interaction (FSI) typically use a Dirichlet-Neumann (DN) split for interface conditions. However, for nearly-closed fluid domains with incompressible fluids and Robin conditions, the DN scheme can become unstable due to increasing flow resistance. Convergence deteriorates as resistance increases, leading to instability at high resistances. This instability is linked to an added-damping effect, affecting the convergence rate of the partitioned method. Understanding this effect can improve the robustness and efficiency of FSI simulations, especially for applications like valves. The analysis also sheds light on the incompressibility dilemma for FSI problems with nearly closed fluid domains. Numerical experiments confirm these findings in more complex scenarios, highlighting the challenges and potential solutions for such FSI problems. <div>
arXiv:2506.04027v1 Announce Type: new 
Abstract: Partitioned methods for fluid-structure interaction (FSI) involve solving the structural and flow problems sequentially. These methods allow for separate settings for the fluid and solid subsystems and thus modularity, enabling reuse of advanced commercial and open-source software. Most partitioned FSI schemes apply a Dirichlet-Neumann (DN) split of the interface conditions. The DN scheme is adequate in a wide range of applications, but it is sensitive to the added-mass effect, and it is susceptible to the incompressibility dilemma, i.e. it completely fails for FSI problems with an incompressible fluid furnished with Dirichlet boundary conditions on the part of its boundary complementary to the interface. In this paper, we show that if the fluid is incompressible and the fluid domain is nearly-closed, i.e. it carries Dirichlet conditions except for a permeable part of the boundary carrying a Robin condition, then the DN partitioned approach is sensitive to the flow resistance at the permeable part, and convergence of the partitioned approach deteriorates as the flow resistance increases. The DN scheme then becomes unstable in the limit as the flow resistance passes to infinity. Based on a simple model problem, we show that in the nearly-closed case, the convergence rate of the DN partitioned method depends on a so-called added-damping effect. The analysis gives insights that can aid to improve robustness and efficiency of partitioned method for FSI problems with contact, e.g. valve applications. In addition, the results elucidate the incompressibility dilemma as a limit of the added-damping effect passing to infinity, and the corresponding challenges related to FSI problems with nearly closed fluid-domain configurations. Via numerical experiments, we consider the generalization of the results of the simple model problem to more complex nearly-closed FSI problems.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk and Reward of Transitioning from a National to a Zonal Electricity Market in Great Britain</title>
<link>https://arxiv.org/abs/2506.04107</link>
<guid>https://arxiv.org/abs/2506.04107</guid>
<content:encoded><![CDATA[
<div> consumer savings, producer surplus impacts, socioeconomic benefits, zonal market, electricity market  

Summary:  
The study evaluates the potential benefits of transitioning from a single-price national wholesale market to a zonal market design in Great Britain. Using an open-source electricity market model, the analysis shows that a six-zone market could result in significant consumer savings of around 9.4/MWh annually, totaling over 2.3 billion per year. However, generators in northern regions may experience revenue reductions of 30-40%. Policy interventions could mitigate these negative impacts, allowing for up to 97% restoration of national market revenues for affected units while still preserving around 3.1/MWh in consumer savings. The current system could achieve an annual welfare gain of 380-770 million through operational efficiency improvements alone during 2022-2024, with potential annual benefits exceeding 1-2 billion beyond 2029. These benefits outweigh potential downsides associated with increased capital costs. <div>
arXiv:2506.04107v1 Announce Type: cross 
Abstract: More spatially granular electricity wholesale markets promise more efficient operation and better asset siting in highly renewable power systems. Great Britain is considering moving from its current single-price national wholesale market to a zonal design. Existing studies reach varying and difficult-to-reconcile conclusions about the desirability of a zonal market in GB, partly because they rely on models that vary in their transparency and assumptions about future power systems. Using a novel open-source electricity market model, calibrated to match observed network behaviour, this article quantifies consumer savings, unit-level producer surplus impacts, and broader socioeconomic benefits that would have arisen had a six-zone market operated in Great Britain during 2022-2024. In the absence of mitigating policies, it is estimated that during those three years GB consumers would save approximately {\pounds}9.4/MWh (equalling an average of more than {\pounds}2.3B per year), but generators in northern regions would experience revenue reductions of 30-40\%. Policy interventions can restore these units' national market revenues to up to 97\% while still preserving around {\pounds}3.1/MWh in consumer savings (about {\pounds}750M per year). It is further estimated that the current system could achieve approximately {\pounds}380-{\pounds}770 million in annual welfare gain during 2022-2024 through improved operational efficiency alone. The drivers behind these benefits, notably wind curtailment volumes, are expected to become more pronounced towards 2030, suggesting that purely operationally achieved annual benefits of around {\pounds}1-2 billion beyond 2029 are likely. It is found that the scale of these benefits would outweigh the potential downsides related to increases in the cost of capital that have been estimated elsewhere.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.04171</link>
<guid>https://arxiv.org/abs/2506.04171</guid>
<content:encoded><![CDATA[
<div> Flow-Based Generative Models, Physics-Constrained Inference, Partial Differential Equations, Constraint Satisfaction, Zero-Shot Inference<br />
Summary:<br />
The article introduces Physics-Constrained Flow Matching (PCFM), a method for enforcing nonlinear constraints in pretrained flow-based generative models. Existing methods struggle to enforce physical constraints effectively, but PCFM addresses this by guiding the sampling process with physics-based corrections while maintaining alignment with learned flow. The framework outperforms both unconstrained and constrained baselines on various PDEs, including those with shocks and sharp features. PCFM ensures exact satisfaction of constraints at the final solution. This approach presents a general framework for enforcing hard constraints in scientific and general-purpose generative models, particularly valuable in applications where constraint satisfaction is critical. <br /><br />Summary: <div>
arXiv:2506.04171v1 Announce Type: cross 
Abstract: Deep generative models have recently been applied to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty-aware inference. However, enforcing physical constraints, such as conservation laws (linear and nonlinear) and physical consistencies, remains challenging. Existing methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints. In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints. Empirically, PCFM outperforms both unconstrained and constrained baselines on a range of PDEs, including those with shocks, discontinuities, and sharp features, while ensuring exact constraint satisfaction at the final solution. Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models, especially in applications where constraint satisfaction is essential.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Financial Foundation Models (MFFMs): Progress, Prospects, and Challenges</title>
<link>https://arxiv.org/abs/2506.01973</link>
<guid>https://arxiv.org/abs/2506.01973</guid>
<content:encoded><![CDATA[
<div> Keywords: FinLLMs, MFFMs, financial data, multimodal, research

Summary:
Financial Large Language Models (FinLLMs) and Multimodal Financial Foundation Models (MFFMs) are revolutionizing the analysis of financial data. While FinLLMs focus on language-centric approaches, MFFMs can process a wide range of multimodal financial data, offering a more comprehensive understanding of complex financial tasks. The progress and potential of MFFMs were discussed in a position paper presented at the MFFM Workshop at the ACM International Conference on AI in Finance 2024. Ongoing research on FinAgents at the SecureFinAI Lab at Columbia University aims to further explore the capabilities of MFFMs. By leveraging diverse data sources such as fundamental data, market data, and alternative data, MFFMs have the potential to streamline financial operations and investment processes. The Github repository for MFFMs provides a platform for collaboration and development in this emerging field. <div>
arXiv:2506.01973v1 Announce Type: new 
Abstract: Financial Large Language Models (FinLLMs), such as open FinGPT and proprietary BloombergGPT, have demonstrated great potential in select areas of financial services. Beyond this earlier language-centric approach, Multimodal Financial Foundation Models (MFFMs) can digest interleaved multimodal financial data, including fundamental data, market data, data analytics, macroeconomic, and alternative data (e.g., natural language, audio, images, and video). In this position paper, presented at the MFFM Workshop joined with ACM International Conference on AI in Finance (ICAIF) 2024, we describe the progress, prospects, and challenges of MFFMs. This paper also highlights ongoing research on FinAgents in the \textbf{SecureFinAI Lab}\footnote{\https://openfin.engineering.columbia.edu/} at Columbia University. We believe that MFFMs will enable a deeper understanding of the underlying complexity associated with numerous financial tasks and data, streamlining the operation of financial services and investment processes. Github Repo https://github.com/Open-Finance-Lab/Awesome-MFFMs/.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Polymer Property Predictions</title>
<link>https://arxiv.org/abs/2506.02129</link>
<guid>https://arxiv.org/abs/2506.02129</guid>
<content:encoded><![CDATA[
<div> Machine learning, polymer informatics, large language models, thermal properties, molecular embeddings
Summary: 
- The study explores the use of large language models (LLMs) in predicting key thermal properties in polymer science, comparing them to traditional fingerprinting-based methods.
- LLaMA-3 outperforms GPT-3.5 in predictive accuracy, likely due to its open-source architecture.
- Single-task learning proves more effective than multi-task learning, as LLMs struggle with capturing cross-property correlations.
- Analysis of molecular embeddings shows limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features.
- The findings provide guidance on selecting LLMs for polymer informatics, highlighting the interplay between molecular embeddings and natural language processing. 

<br /><br />Summary: <div>
arXiv:2506.02129v1 Announce Type: new 
Abstract: Machine learning has revolutionized polymer science by enabling rapid property prediction and generative design. Large language models (LLMs) offer further opportunities in polymer informatics by simplifying workflows that traditionally rely on large labeled datasets, handcrafted representations, and complex feature engineering. LLMs leverage natural language inputs through transfer learning, eliminating the need for explicit fingerprinting and streamlining training. In this study, we finetune general purpose LLMs -- open-source LLaMA-3-8B and commercial GPT-3.5 -- on a curated dataset of 11,740 entries to predict key thermal properties: glass transition, melting, and decomposition temperatures. Using parameter-efficient fine-tuning and hyperparameter optimization, we benchmark these models against traditional fingerprinting-based approaches -- Polymer Genome, polyGNN, and polyBERT -- under single-task (ST) and multi-task (MT) learning. We find that while LLM-based methods approach traditional models in performance, they generally underperform in predictive accuracy and efficiency. LLaMA-3 consistently outperforms GPT-3.5, likely due to its tunable open-source architecture. Additionally, ST learning proves more effective than MT, as LLMs struggle to capture cross-property correlations, a key strength of traditional methods. Analysis of molecular embeddings reveals limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features and domain-specific embeddings. These findings provide insight into the interplay between molecular embeddings and natural language processing, guiding LLM selection for polymer informatics.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singularity Blockchain Key Management via non-custodial key management</title>
<link>https://arxiv.org/abs/2506.02282</link>
<guid>https://arxiv.org/abs/2506.02282</guid>
<content:encoded><![CDATA[
<div> web3 wallets, user identity, blockchain, key management, non-custodial<br />
<br />
Summary: <br />
Web3 wallets play a crucial role in managing user identity on the blockchain by storing and providing access to private keys. Key management schemes can be either custodial or non-custodial, with the latter placing the burden of key storage and recovery on the user. Existing non-custodial schemes often require users to remember seed phrases, leading to onboarding challenges and the risk of asset loss if the key is forgotten. This paper introduces a novel non-custodial key management approach that allows users to back up and recover their private key using third-party sign-in methods such as google-oAuth. By enabling users to securely backup their keys through independent authentication methods, this technique aims to enhance user experience and reduce the likelihood of key loss. <div>
arXiv:2506.02282v1 Announce Type: new 
Abstract: web3 wallets are key to managing user identity on blockchain. The main purpose of a web3 wallet application is to manage the private key for the user and provide an interface to interact with the blockchain. The key management scheme ( KMS ) used by the wallet to store and recover the private key can be either custodial, where the keys are permissioned and in custody of the wallet provider or noncustodial where the keys are in custody of the user. The existing non-custodial key management schemes tend to offset the burden of storing and recovering the key entirely on the user by asking them to remember seed-phrases. This creates onboarding hassles for the user and introduces the risk that the user may lose their assets if they forget or lose their seedphrase/private key. In this paper, we propose a novel method of backing up user keys using a non-custodial key management technique that allows users to save and recover a backup of their private key using any independent sign-in method such as google-oAuth or other 3P oAuth.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the fracture mechanics validity of small scale tests</title>
<link>https://arxiv.org/abs/2506.02538</link>
<guid>https://arxiv.org/abs/2506.02538</guid>
<content:encoded><![CDATA[
<div> Keywords: fracture behaviour, micro-scale mechanical tests, crack growth, material properties, hydrogen embrittlement

Summary: 
This study focuses on conducting small-scale tests to understand the fracture behavior of materials. Using numerical and semi-analytical approaches, the researchers determine the conditions necessary for a valid and quantitative fracture experiment, considering factors such as sample geometry, material properties, and crack lengths. They establish the maximum value of the J-integral, known as Jmax, where fracture must occur for accurate results. Maps are generated to show the maximum valid J value as a function of yield strength, strain hardening, and sample size, providing guidance for conducting experiments. The analysis is extended to metals embrittled by hydrogen exposure, overlaying the response of these materials on the established maps to determine the conditions required for obtaining quantitative insight into such materials. <div>
arXiv:2506.02538v1 Announce Type: new 
Abstract: There is growing interest in conducting small-scale tests to gain additional insight into the fracture behaviour of components across a wide range of materials. For example, micro-scale mechanical tests inside of a microscope (\emph{in situ}) enable direct, high-resolution observation of the interplay between crack growth and microstructural phenomena (e.g., dislocation behaviour or the fracture resistance of a particular interface), and sub-size samples are increasingly used when only a limited amount of material is available. However, to obtain quantitative insight and extract relevant fracture parameters, the sample must be sufficiently large for a $J$- (HRR) or a $K$-field to exist. We conduct numerical and semi-analytical studies to map the conditions (sample geometry, material) that result in a valid, quantitative fracture experiment. Specifically, for a wide range of material properties, crack lengths and sample dimensions, we establish the maximum value of the $J$-integral where an HRR field ceases to exist (i.e., the maximum $J$ value at which fracture must occur for the test to be valid, $J_\mathrm{max}$). Maps are generated to establish the maximum valid $J$ value ($J_\mathrm{max}$) as a function of yield strength, strain hardening and minimum sample size. These maps are then used to discuss the existing experimental literature and provide guidance on how to conduct quantitative experiments. Finally, our study is particularised to the analysis of metals that have been embrittled due to hydrogen exposure. The response of relevant materials under hydrogen-containing environments are superimposed on the aforementioned maps, determining the conditions that will enable quantitative insight.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriching Location Representation with Detailed Semantic Information</title>
<link>https://arxiv.org/abs/2506.02744</link>
<guid>https://arxiv.org/abs/2506.02744</guid>
<content:encoded><![CDATA[
<div> Embeddings, Urban Modeling, Contrastive Learning, Point-of-Interest, Multimodal<br />
<br />
Summary: 
The study introduces CaLLiPer+, an urban modeling approach that integrates Point-of-Interest names with categorical labels in a contrastive learning framework. The model shows improved performance in land use classification and socioeconomic status mapping compared to baseline methods, with gains of 4% to 11%. By incorporating POI names, the model enhances location retrieval and captures complex urban concepts accurately. Ablation studies demonstrate the complementary role of POI names and the benefits of using pretrained text encoders for spatial representations. The research underscores the significance of integrating fine-grained semantic attributes and multimodal learning techniques for advancing urban foundation models. <div>
arXiv:2506.02744v1 Announce Type: new 
Abstract: Spatial representations that capture both structural and semantic characteristics of urban environments are essential for urban modeling. Traditional spatial embeddings often prioritize spatial proximity while underutilizing fine-grained contextual information from places. To address this limitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that systematically integrates Point-of-Interest (POI) names alongside categorical labels within a multimodal contrastive learning framework. We evaluate its effectiveness on two downstream tasks, land use classification and socioeconomic status distribution mapping, demonstrating consistent performance gains of 4% to 11% over baseline methods. Additionally, we show that incorporating POI names enhances location retrieval, enabling models to capture complex urban concepts with greater precision. Ablation studies further reveal the complementary role of POI names and the advantages of leveraging pretrained text encoders for spatial representations. Overall, our findings highlight the potential of integrating fine-grained semantic attributes and multimodal learning techniques to advance the development of urban foundation models.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Aware Density Estimation in Multiple Dimensions</title>
<link>https://arxiv.org/abs/2506.02323</link>
<guid>https://arxiv.org/abs/2506.02323</guid>
<content:encoded><![CDATA[
<div> Keywords: optimization, probability densities, multidimensional problems, splines, PET rebinning  
Summary:  
- The article presents an optimization problem to estimate probability densities in multidimensional problems with uneven sampling probability.  
- Detector sensitivity is considered as an heterogeneous density, utilizing splines on a grid for computational speed and flexible boundary conditions.  
- The method uses nuclear norm regularization on the spline's Hessian to promote sparsity, making it spatially adaptive and stable against the choice of regularization parameter.  
- The computational pipeline is tested on standard densities, with provided software for implementation.  
- A new approach to PET rebinning is showcased as an application of the framework.<br /><br />Summary: <div>
arXiv:2506.02323v1 Announce Type: cross 
Abstract: We formulate an optimization problem to estimate probability densities in the context of multidimensional problems that are sampled with uneven probability. It considers detector sensitivity as an heterogeneous density and takes advantage of the computational speed and flexible boundary conditions offered by splines on a grid. We choose to regularize the Hessian of the spline via the nuclear norm to promote sparsity. As a result, the method is spatially adaptive and stable against the choice of the regularization parameter, which plays the role of the bandwidth. We test our computational pipeline on standard densities and provide software. We also present a new approach to PET rebinning as an application of our framework.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning</title>
<link>https://arxiv.org/abs/2506.02485</link>
<guid>https://arxiv.org/abs/2506.02485</guid>
<content:encoded><![CDATA[
<div> Generative AI, Wildfire prediction, Multimodal approaches, 2D fire spread forecasting, 3D simulations 

Summary:
Generative AI models like GANs and VAEs show promise in improving wildfire prediction by integrating multimodal data and generating diverse scenarios. These models can enhance 2D fire spread forecasting and enable more realistic 3D simulations. A human-AI collaboration framework using large language models aids in automated knowledge extraction and literature synthesis. Five key visions for integrating generative AI into wildfire management include multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. The challenges of implementing these visions are also addressed, with proposed solutions to overcome them.<br /><br />Summary: <div>
arXiv:2506.02485v1 Announce Type: cross 
Abstract: Wildfires continue to inflict devastating human, environmental, and economic losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and the urgent demand for more effective response strategies. While physics-based and deep learning models have advanced wildfire simulation, they face critical limitations in predicting and visualizing multimodal fire spread in real time, particularly in both 2D and 3D spatial domains using dynamically updated GIS data. These limitations hinder timely emergency response, infrastructure protection, and community safety. Generative AI has recently emerged as a transformative approach across research and industry. Models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and diffusion-based architectures offer distinct advantages over traditional methods, including the integration of multimodal data, generation of diverse scenarios under uncertainty, and improved modeling of wildfire dynamics across spatial and temporal scales. This position paper advocates for the adoption of generative AI as a foundational framework for wildfire prediction. We explore how such models can enhance 2D fire spread forecasting and enable more realistic, scalable 3D simulations. Additionally, we employ a novel human-AI collaboration framework using large language models (LLMs) for automated knowledge extraction, literature synthesis, and bibliometric mapping. Looking ahead, we identify five key visions for integrating generative AI into wildfire management: multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. We also address three major challenges accompanying these opportunities and propose potential solutions to support their implementation.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression</title>
<link>https://arxiv.org/abs/2506.02678</link>
<guid>https://arxiv.org/abs/2506.02678</guid>
<content:encoded><![CDATA[
<div> Dynamic ratio-based training, Large Language Models, efficient language reasoning, inference, System-1, System-2 <br />
<br />
Dynamic ratio-based training is proposed in this work to improve the efficiency of language reasoning in large language models. The method continuously balances the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. The approach is validated on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B models across a variety of benchmarks with varying difficulty levels. Results show a reduction of nearly 40% in the number of output tokens while maintaining reasoning accuracy. The research presents an innovative solution to the challenge of performing efficient language reasoning, particularly during inference with long outputs, without the need for sophisticated data annotations or interpolation between models. Code and data for the research will be made available soon. <br /><br />Summary: <div>
arXiv:2506.02678v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A mesoscale phase-field model of intergranular liquid lithium corrosion of ferritic/martensitic steels</title>
<link>https://arxiv.org/abs/2506.02776</link>
<guid>https://arxiv.org/abs/2506.02776</guid>
<content:encoded><![CDATA[
<div> phase-field model, intergranular corrosion, ferritic/martensitic steels, liquid lithium, chromium concentration

Summary:
The article presents a phase-field model for simulating intergranular corrosion in ferritic/martensitic steels exposed to liquid lithium. By tracking the chromium concentration in the material, mass transport within the metal and liquid phases is analyzed. The model effectively captures intergranular corrosion by enhancing chromium diffusion along grain boundaries without the need for specific treatment. Results from simulations align closely with experimental measurements of weight loss and corrosion depth in a 9 wt% Cr steel at 600C. A sensitivity analysis reveals the influence of microstructural factors such as near-surface grain density and grain size on the corrosion process. The study also evaluates the impact of saturation on corrosion behavior. Overall, near-surface grain density is identified as a critical factor, while grain size is found to influence susceptibility to intergranular corrosion. <div>
arXiv:2506.02776v1 Announce Type: cross 
Abstract: A phase-field model is developed to simulate intergranular corrosion of ferritic/martensitic steels exposed to liquid lithium. The chromium concentration of the material is used to track the mass transport within the metal and liquid (corrosive) phase. The framework naturally captures intergranular corrosion by enhancing the diffusion of chromium along grain boundaries relative to the grain bulk with no special treatment for the corrosion front evolution. The formulation applies to arbitrary 2D and 3D polycrystalline geometries. The framework reproduces experimental measurements of weight loss and corrosion depth for a 9 wt\% Cr ferritic/martensitic steel exposed to static lithium at 600 $^\circ$C. A sensitivity analysis, varying near-surface grain density, grain size, and chromium depletion thickness, highlights the microstructural influence in the corrosion process. Moreover, the significance of saturation is considered and evaluated. Simulation results show that near-surface grain density is a deciding factor, whereas grain size dictates the susceptibility to intergranular corrosion.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.02911</link>
<guid>https://arxiv.org/abs/2506.02911</guid>
<content:encoded><![CDATA[
<div> cell type annotation, single-cell RNA sequencing data, CellPuzzles, large language models, batch-level accuracy

Summary:
Cell type annotation plays a critical role in analyzing single-cell RNA sequencing data heterogeneity. Current foundation models lack the ability to consider batch-level cellular context and provide explanatory reasoning in cell type annotation tasks. To address this, the CellPuzzles task was introduced to mimic expert annotation workflows, requiring unique cell type assignment across batches of cells. Existing large language models struggle with this task, with limited accuracy. In response, Cell-o1, a 7B large language model, was developed through supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 surpasses previous models, demonstrating superior performance and generalization across different contexts. The training dynamics and reasoning behaviors of Cell-o1 provide insights into improved batch-level annotation performance and expert-like reasoning strategies. <div>
arXiv:2506.02911v1 Announce Type: cross 
Abstract: Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to optimize convex risk measures: The cases of utility-based shortfall risk and optimized certainty equivalent risk</title>
<link>https://arxiv.org/abs/2506.01101</link>
<guid>https://arxiv.org/abs/2506.01101</guid>
<content:encoded><![CDATA[
<div> risk measures, estimation, optimization, gradient estimators, stochastic gradient algorithm

Summary: 
The article introduces the estimation and optimization of convex risk measures, specifically utility-based shortfall risk (UBSR) and Optimized Certainty Equivalent (OCE) risk, covering unbounded random variables. It extends various risk measures like entropic risk and Value-at-Risk. Non-asymptotic bounds are derived for mean absolute error and mean-squared error in estimation using sample average approximation (SAA) estimators. Expressions for UBSR and OCE gradients under smooth parameterization are provided, with gradient estimators proposed using the SAA estimator of UBSR and non-asymptotic bounds on error. A stochastic gradient algorithm is developed for optimization using these gradient estimators. Non-asymptotic convergence rate bounds are derived for the optimization of UBSR and OCE risk measures. <div>
arXiv:2506.01101v1 Announce Type: new 
Abstract: We consider the problems of estimation and optimization of two popular convex risk mea- sures: utility-based shortfall risk (UBSR) and Optimized Certainty Equivalent (OCE) risk. We extend these risk measures to cover possibly unbounded random variables. We cover prominent risk measures like the entropic risk, expectile risk, monotone mean-variance risk, Value-at-Risk, and Conditional Value-at-Risk as few special cases of either the UBSR or the OCE risk. In the context of estimation, we derive non-asymptotic bounds on the mean absolute error (MAE) and mean-squared error (MSE) of the classical sample average approximation (SAA) estimators of both, the UBSR and the OCE. Next, in the context of optimization, we derive expressions for the UBSR gradient and the OCE gradient under a smooth parameterization. Utilizing these expres- sions, we propose gradient estimators for both, the UBSR and the OCE. We use the SAA estimator of UBSR in both these gradient estimators, and derive non-asymptotic bounds on MAE and MSE for the proposed gradient estimation schemes. We incorporate the aforementioned gradient estima- tors into a stochastic gradient (SG) algorithm for optimization. Finally, we derive non-asymptotic bounds that quantify the rate of convergence of our SG algorithm for the optimization of the UBSR and the OCE risk measure
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast numerical generation of Lie closure</title>
<link>https://arxiv.org/abs/2506.01120</link>
<guid>https://arxiv.org/abs/2506.01120</guid>
<content:encoded><![CDATA[
<div> Keywords: Lie algebra, matrix, numerical construction, quantum computing, linear independence<br />
Summary:<br />
The article discusses the importance of finding the Lie-algebraic closure of matrices in quantum computing and quantum control. Analytically determining the closure is challenging for most cases, leading to a need for numerical construction. The standard algorithm for this construction relies on a subroutine to check linear independence, which can be computationally intensive. The authors present efficient methods for linear independence checks that reduce computational complexity and memory usage. One of these methods is implemented and validated against known results. These new algorithms allow for the exploration of Lie closure in larger system sizes that were previously unattainable, opening up possibilities for numerical studies in quantum computing and control applications. <div>
arXiv:2506.01120v1 Announce Type: new 
Abstract: Finding the Lie-algebraic closure of a handful of matrices has important applications in quantum computing and quantum control. For most realistic cases, the closure cannot be determined analytically, necessitating an explicit numerical construction. The standard construction algorithm makes repeated calls to a subroutine that determines whether a matrix is linearly independent from a potentially large set of matrices. Because the common implementation of this subroutine has a high complexity, the construction of Lie closure is practically limited to trivially small matrix sizes. We present efficient alternative methods of linear independence check that simultaneously reduce the computational complexity and memory footprint. An implementation of one of the methods is validated against known results. Our new algorithms enable numerical studies of Lie closure in larger system sizes than was previously possible.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging ML-AI Techniques for Analog and RF EDA</title>
<link>https://arxiv.org/abs/2506.00007</link>
<guid>https://arxiv.org/abs/2506.00007</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, EDA workflows, analog design, RF circuits, optimization techniques

Summary: 
This survey delves into the integration of machine learning (ML) into electronic design automation (EDA) workflows specifically tailored for analog and RF circuits. The challenges unique to analog design, such as complex constraints, nonlinear design spaces, and high computational costs, are addressed. The review encompasses state-of-the-art ML and optimization techniques for various circuit tasks, including constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey emphasizes how ML can enhance automation, elevate design quality, and reduce time-to-market while meeting desired circuit specifications. In addition, emerging trends and cross-cutting challenges, like robustness to variations and considerations of interconnect parasitics, are explored. Overall, the survey underscores the potential of ML to revolutionize analog and RF circuit design by optimizing workflow efficiency and achieving superior design outcomes. 

<br /><br />Summary: <div>
arXiv:2506.00007v1 Announce Type: cross 
Abstract: This survey explores the integration of machine learning (ML) into EDA workflows for analog and RF circuits, addressing challenges unique to analog design, which include complex constraints, nonlinear design spaces, and high computational costs. State-of-the-art learning and optimization techniques are reviewed for circuit tasks such as constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey highlights the capability of ML to enhance automation, improve design quality, and reduce time-to-market while meeting the target specifications of an analog or RF circuit. Emerging trends and cross-cutting challenges, including robustness to variations and considerations of interconnect parasitics, are also discussed.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Spatio-Temporal Vessel Behavior using AIS Trajectory Data and Markovian Models in the Gulf of St. Lawrence</title>
<link>https://arxiv.org/abs/2506.00025</link>
<guid>https://arxiv.org/abs/2506.00025</guid>
<content:encoded><![CDATA[
<div> Keywords: maritime mobility, spatio-temporal analysis, vessel movement patterns, discrete-time Markov chains, COVID-19 pandemic <br />
Summary: <br />
This article presents a spatio-temporal analytical framework using discrete-time Markov chains to analyze vessel movement patterns in the Gulf of St. Lawrence, focusing on changes during the COVID-19 pandemic. The ocean space is divided into hexagonal cells, and mobility signatures for different vessel types are constructed based on cell transitions and dwell time. Origin-destination matrices and spatial transition probability models are developed to understand vessel dynamics at different time scales. The study reveals consistent mobility signatures for specific vessel types across different regions, suggesting underlying behavioral patterns. During the pandemic, passenger and fishing vessels show significant temporal deviations, reflecting the impact of social isolation measures and operational restrictions on non-essential maritime activities in the region. These findings contribute to a better understanding of maritime mobility patterns and highlight the influence of external factors on vessel movements. <br /> <div>
arXiv:2506.00025v1 Announce Type: cross 
Abstract: Maritime Mobility is at the center of the global economy, and analyzing and understanding such data at scale is critical for ocean conservation and governance. Accordingly, this work introduces a spatio-temporal analytical framework based on discrete-time Markov chains to analyze vessel movement patterns in the Gulf of St. Lawrence, emphasizing changes induced during the COVID-19 pandemic. We discretize the ocean space into hexagonal cells and construct mobility signatures for individual vessel types using the frequency of cell transitions and the dwell time within each cell. These features are used to build origin-destination matrices and spatial transition probability models that characterize vessel dynamics at different temporal resolutions. Under multiple vessel types, we contribute with a temporal evolution analysis of mobility patterns during pandemic times, highlighting significant but transient changes to recurring transportation behaviors. Our findings indicate vessel-specific mobility signatures consistent across spatially disjoint regions, suggesting that those are latent behavioral invariants. Besides, we observe significant temporal deviations among passenger and fishing vessels during the pandemic, indicating a strong influence of social isolation policies and operational limitations imposed on non-essential maritime activity in this region.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risks of AI-driven product development and strategies for their mitigation</title>
<link>https://arxiv.org/abs/2506.00047</link>
<guid>https://arxiv.org/abs/2506.00047</guid>
<content:encoded><![CDATA[
<div> progressing, automated product development, risks, mitigation strategies, AI-driven product development
<br />
Summary:
Humanity is moving towards automated product development to accelerate technological progress, but this trend poses risks that must be addressed. To mitigate these risks, principles for safer AI-driven product development are outlined, emphasizing human oversight, accountability, and explainable design. The risk assessment includes technical risks affecting product quality and safety, as well as sociotechnical risks impacting society. While AI-driven product development is still evolving, this discussion aims to balance opportunities and risks without hindering progress in understanding, norm-setting, and regulation. <div>
arXiv:2506.00047v1 Announce Type: cross 
Abstract: Humanity is progressing towards automated product development, a trend that promises faster creation of better products and thus the acceleration of technological progress. However, increasing reliance on non-human agents for this process introduces many risks. This perspective aims to initiate a discussion on these risks and appropriate mitigation strategies. To this end, we outline a set of principles for safer AI-driven product development which emphasize human oversight, accountability, and explainable design, among others. The risk assessment covers both technical risks which affect product quality and safety, and sociotechnical risks which affect society. While AI-driven product development is still in its early stages, this discussion will help balance its opportunities and risks without delaying essential progress in understanding, norm-setting, and regulation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance</title>
<link>https://arxiv.org/abs/2506.00494</link>
<guid>https://arxiv.org/abs/2506.00494</guid>
<content:encoded><![CDATA[
<div> Keywords: Soft Fin-Ray grippers, multi-objective optimization, finite element method, multilayer perception, non-dominated sorting genetic algorithm

Summary:<br />
Soft Fin-Ray grippers are effective for delicate manipulation but face challenges in modeling grasp force and deformation for design purposes. The study uses finite element method (FEM) to estimate deflections and contact forces of the gripper when grasping cylindrical objects, creating a dataset for predicting contact force and tip displacement using a multilayer perception (MLP). The dataset includes design variables related to beam thickness and spacing, with target features of maximum contact forces and tip displacements. A multi-objective optimization problem is addressed, balancing the trade-off between force and delicate manipulation. The non-dominated sorting genetic algorithm (NSGA-II) is used to find optimized design solutions. The methodologies presented in the study can enhance the design and gripping performance of soft robotic grippers, aiding in choosing designs suitable for both delicate grasping and high-force applications.<br />Summary: <div>
arXiv:2506.00494v1 Announce Type: cross 
Abstract: Soft Fin-Ray grippers can perform delicate and careful manipulation, which has caused notable attention in different fields. These grippers can handle objects of various forms and sizes safely. The internal structure of the Fin-Ray finger plays a significant role in its adaptability and grasping performance. However, modeling the non-linear grasp force and deformation behaviors for design purposes is challenging. Moreover, when the Fin-Ray finger becomes more rigid and capable of exerting higher forces, it becomes less delicate in handling objects. The contrast between these two objectives gives rise to a multi-objective optimization problem. In this study, we employ finite element method (FEM) to estimate the deflections and contact forces of the Fin-Ray, grasping cylindrical objects. This dataset is then used to construct a multilayer perception (MLP) for prediction of the contact force and the tip displacement. The FEM dataset consists of three input and four target features. The three input features of the MLP and optimization design variables are the thickness of the front and supporting beams, the thickness of the cross beams, and the equal spacing between the cross beams. In addition, the target features are the maximum contact forces and maximum tip displacements in x- and y-directions. The magnitude of maximum contact force and magnitude of maximum tip displacement are the two objectives, showing the trade-off between force and delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized set of solutions are found using multi-objective optimal techniques. We use non-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our findings demonstrate that our methodologies can be used to improve the design and gripping performance of soft robotic grippers, helping us to choose a design not only for delicate grasping but also for high-force applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling the Spread of Epidemics on Networks with Differential Privacy</title>
<link>https://arxiv.org/abs/2506.00745</link>
<guid>https://arxiv.org/abs/2506.00745</guid>
<content:encoded><![CDATA[
<div> strategies, epidemic spread, vaccination, contact network, differential privacy<br />
<br />
Summary: Designing effective vaccination strategies for controlling epidemic spread on heterogeneous contact networks is crucial, especially when sensitive information is involved and privacy guarantees are needed. This study introduces $(\varepsilon,\delta)$-differentially private algorithms for reducing the maximum degree and spectral radius to design optimal vaccination strategies. A private algorithm for the multi-set multi-cover problem is developed to control network properties while preserving privacy. The tradeoff between privacy and utility of these algorithms is evaluated on various synthetic and real-world networks, demonstrating their effectiveness in controlling epidemic spread while maintaining privacy. <div>
arXiv:2506.00745v1 Announce Type: cross 
Abstract: Designing effective strategies for controlling epidemic spread by vaccination is an important question in epidemiology, especially in the early stages when vaccines are limited. This is a challenging question when the contact network is very heterogeneous, and strategies based on controlling network properties, such as the degree and spectral radius, have been shown to be effective. Implementation of such strategies requires detailed information on the contact structure, which might be sensitive in many applications. Our focus here is on choosing effective vaccination strategies when the edges are sensitive and differential privacy guarantees are needed. Our main contributions are $(\varepsilon,\delta)$-differentially private algorithms for designing vaccination strategies by reducing the maximum degree and spectral radius. Our key technique is a private algorithm for the multi-set multi-cover problem, which we use for controlling network properties. We evaluate privacy-utility tradeoffs of our algorithms on multiple synthetic and real-world networks, and show their effectiveness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking</title>
<link>https://arxiv.org/abs/2506.01093</link>
<guid>https://arxiv.org/abs/2506.01093</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time transaction monitoring, graph-based modeling, narrative field embedding, generative explanation, financial compliance <br />
Summary: 
The paper introduces a real-time transaction monitoring framework that combines graph-based modeling, narrative field embedding, and generative explanation to automate financial compliance. The system creates dynamic transaction graphs, extracts features, and detects suspicious behavior using a graph neural network. It also generates natural language explanations aligned with regulatory clauses for flagged transactions. Experimental results on simulated financial data demonstrate high performance metrics with a 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation confirms the quality and interpretability of generated justifications. The study showcases the potential of integrating graph intelligence and generative models for explainable compliance in high-risk financial settings. <br /><br />Summary: <div>
arXiv:2506.01093v1 Announce Type: cross 
Abstract: This paper presents a real-time transaction monitoring framework that integrates graph-based modeling, narrative field embedding, and generative explanation to support automated financial compliance. The system constructs dynamic transaction graphs, extracts structural and contextual features, and classifies suspicious behavior using a graph neural network. A retrieval-augmented generation module generates natural language explanations aligned with regulatory clauses for each flagged transaction. Experiments conducted on a simulated stream of financial data show that the proposed method achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation further confirms the quality and interpretability of generated justifications. The findings demonstrate the potential of combining graph intelligence and generative models to support explainable, audit-ready compliance in high-risk financial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents</title>
<link>https://arxiv.org/abs/2506.01900</link>
<guid>https://arxiv.org/abs/2506.01900</guid>
<content:encoded><![CDATA[
<div> framework, autonomous LLM agents, cost optimization, task outsourcing, agent economies

Summary:
COALESCE is a framework designed to optimize resource utilization in autonomous Large Language Model (LLM) agents by enabling them to outsource specific subtasks to cost-effective third-party agents. The framework incorporates hybrid skill representation, dynamic skill discovery, task decomposition, cost comparison models, decision-making algorithms, and a communication protocol. The theoretical simulations show a 41.8% cost reduction potential, while empirical validation confirms a 20.3% cost reduction with epsilon-greedy exploration. The framework aims to leverage open standards like Google's Agent2Agent protocol to foster efficient agent interactions, reduce operational costs, enhance scalability, and create specialized agent economies. By facilitating a dynamic market for agent capabilities, COALESCE enables complex LLM agent functionalities to become more accessible and economically viable. 

<br /><br />Summary: <div>
arXiv:2506.01900v1 Announce Type: cross 
Abstract: The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Guided Diffusion Model for Accelerating Computational Fluid Dynamics</title>
<link>https://arxiv.org/abs/2504.04375</link>
<guid>https://arxiv.org/abs/2504.04375</guid>
<content:encoded><![CDATA[
<div> diffusion models, fluid dynamics computation, machine learning, numerical solvers, turbulent flow<br />
Summary:<br />
Machine learning methods, such as diffusion models, aim to accelerate high-fidelity fluid dynamics computation by utilizing low-fidelity data produced by numerical solvers. However, existing approaches struggle to reconstruct fine-scale details when using solver-generated low-fidelity inputs. To address this issue, SG-Diff, a novel diffusion model, is proposed. It incorporates an Importance Weight strategy during training to focus on intricate fluid details and a Predictor-Corrector-Advancer SDE solver to embed physical guidance into the diffusion sampling process. Experimental results on turbulent flow datasets demonstrate SG-Diff's effectiveness in achieving more accurate reconstructions compared to state-of-the-art baselines. <br />Summary: <div>
arXiv:2504.04375v2 Announce Type: replace 
Abstract: Machine learning methods, such as diffusion models, are widely explored as a promising way to accelerate high-fidelity fluid dynamics computation via a super-resolution process from faster-to-compute low-fidelity input. However, existing approaches usually make impractical assumptions that the low-fidelity data is down-sampled from high-fidelity data. In reality, low-fidelity data is produced by numerical solvers that use a coarser resolution. Solver-generated low-fidelity data usually sacrifices fine-grained details, such as small-scale vortices compared to high-fidelity ones. Our findings show that SOTA diffusion models struggle to reconstruct fine-scale details when faced with solver-generated low-fidelity inputs. To bridge this gap, we propose SG-Diff, a novel diffusion model for reconstruction, where both low-fidelity inputs and high-fidelity targets are generated from numerical solvers. We propose an \textit{Importance Weight} strategy during training that serves as a form of self-guidance, focusing on intricate fluid details, and a \textit{Predictor-Corrector-Advancer} SDE solver that embeds physical guidance into the diffusion sampling process. Together, these techniques steer the diffusion model toward more accurate reconstructions. Experimental results on four 2D turbulent flow datasets demonstrate the efficacy of \model~against state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning in Business Analytics: A Clash of Expectations and Reality</title>
<link>https://arxiv.org/abs/2205.09337</link>
<guid>https://arxiv.org/abs/2205.09337</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, machine learning, structured datasets, gradient boosting, business analytics

Summary:
Deep learning, a popular tool in the field of artificial intelligence and machine learning, faces challenges in widespread adoption within business analytics. These challenges include computational complexity, lack of big data architecture, black-box transparency, skill shortages, and leadership commitment. Despite its benefits, deep learning may not outperform traditional machine learning models for structured datasets with fixed-length feature vectors. The study suggests that gradient boosting models are more suitable for making predictions on structured datasets in business analytics. This finding highlights the importance of viewing deep learning as a complement to existing machine learning models rather than a universal solution. The paper provides insights from empirical studies in three industry use cases, discussing practical implications and outlining future research directions. 

<br /><br />Summary: <div>
arXiv:2205.09337v2 Announce Type: replace-cross 
Abstract: Our fast-paced digital economy shaped by global competition requires increased data-driven decision-making based on artificial intelligence (AI) and machine learning (ML). The benefits of deep learning (DL) are manifold, but it comes with limitations that have, so far, interfered with widespread industry adoption. This paper explains why DL, despite its popularity, has difficulties speeding up its adoption within business analytics. It is shown that the adoption of deep learning is not only affected by computational complexity, lacking big data architecture, lack of transparency (black-box), skill shortage, and leadership commitment, but also by the fact that DL does not outperform traditional ML models in the case of structured datasets with fixed-length feature vectors. Deep learning should be regarded as a powerful addition to the existing body of ML models instead of a one size fits all solution. The results strongly suggest that gradient boosting can be seen as the go-to model for predictions on structured datasets within business analytics. In addition to the empirical study based on three industry use cases, the paper offers a comprehensive discussion of those results, practical implications, and a roadmap for future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated machine learning: AI-driven decision making in business analytics</title>
<link>https://arxiv.org/abs/2205.10538</link>
<guid>https://arxiv.org/abs/2205.10538</guid>
<content:encoded><![CDATA[
<div> AutoML, industrial machine learning, H2O AutoML framework, business analytics, automated decision-making <br />
Summary: <br />
The rise of AI-driven decision-making in the business world has led to a surge in interest in industrial machine learning applications. The shortage of analytics experts can be addressed by enhancing the user-friendliness of ML frameworks. Automated machine learning (AutoML) offers a solution by providing automated off-the-shelf solutions for model selection and hyperparameter tuning. In a study comparing the H2O AutoML framework with a manually tuned stacked ML model, the manual model outperformed in all three case studies, but the H2O AutoML package showed promising results. It is fast, easy to use, and delivers reliable results close to a professionally tuned model. This tool can aid in fast prototyping, shorten development cycles, and bridge the gap between demand and supply for ML experts. AutoML has the potential to empower individuals in an increasingly automated and digital world. <br /> <div>
arXiv:2205.10538v2 Announce Type: replace-cross 
Abstract: The realization that AI-driven decision-making is indispensable in today's fast-paced and ultra-competitive marketplace has raised interest in industrial machine learning (ML) applications significantly. The current demand for analytics experts vastly exceeds the supply. One solution to this problem is to increase the user-friendliness of ML frameworks to make them more accessible for the non-expert. Automated machine learning (AutoML) is an attempt to solve the problem of expertise by providing fully automated off-the-shelf solutions for model choice and hyperparameter tuning. This paper analyzed the potential of AutoML for applications within business analytics, which could help to increase the adoption rate of ML across all industries. The H2O AutoML framework was benchmarked against a manually tuned stacked ML model on three real-world datasets. The manually tuned ML model could reach a performance advantage in all three case studies used in the experiment. Nevertheless, the H2O AutoML package proved to be quite potent. It is fast, easy to use, and delivers reliable results, which come close to a professionally tuned ML model. The H2O AutoML framework in its current capacity is a valuable tool to support fast prototyping with the potential to shorten development and deployment cycles. It can also bridge the existing gap between supply and demand for ML experts and is a big step towards automated decisions in business analytics. Finally, AutoML has the potential to foster human empowerment in a world that is rapidly becoming more automated and digital.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETS-C: Leveraging Text Embedding for Time Series Classification</title>
<link>https://arxiv.org/abs/2407.06533</link>
<guid>https://arxiv.org/abs/2407.06533</guid>
<content:encoded><![CDATA[
<div> Keywords: language modeling, time series data, text embedding model, convolutional neural networks, lightweight model architecture <br />
Summary: 
This study introduces a novel approach for time series classification using a text embedding model in conjunction with a lightweight classification head. While previous methods focused on fine-tuning large language models for time series data, this new approach, called LETS-C, outperforms the state-of-the-art models in classification accuracy. The LETS-C model combines text embeddings with a simple classification head composed of convolutional neural networks and multilayer perceptron. By utilizing text embeddings to encode time series data, the LETS-C model achieves high performance while keeping a lightweight architecture. Through extensive experiments on a well-established time series classification benchmark, it was found that LETS-C requires only 14.5% of the trainable parameters compared to the current SOTA model. This suggests that leveraging text embedding models for time series classification tasks presents a promising direction for achieving high performance with a simpler model architecture. <br /><br />Summary: <div>
arXiv:2407.06533v2 Announce Type: replace-cross 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much is Enough? The Diminishing Returns of Tokenization Training Data</title>
<link>https://arxiv.org/abs/2502.20273</link>
<guid>https://arxiv.org/abs/2502.20273</guid>
<content:encoded><![CDATA[
<div> tokenization, hyperparameter, training data size, BPE, UnigramLM, WordPiece
<br />
Summary: 
This study examines the impact of tokenizer training data size on tokenization quality in natural language processing. By training BPE, UnigramLM, and WordPiece tokenizers on varying English training data sizes from 1GB to 900GB, it is found that improvements in tokenization quality diminish beyond 150GB due to constraints introduced by the pre-tokenization stage. The saturation effect is observed in both English and Russian data, indicating a practical limit to tokenization quality improvements achievable through increasing data size. This insight can guide the optimization of tokenization processes, reducing the computational resources needed for training on large corpora. Further research directions in tokenization algorithms are suggested based on these findings. 
<br /> <div>
arXiv:2502.20273v2 Announce Type: replace-cross 
Abstract: Tokenization, a crucial initial step in natural language processing, is governed by several key parameters, such as the tokenization algorithm, vocabulary size, pre-tokenization strategy, inference strategy, and training data corpus. This paper investigates the impact of an often-overlooked hyperparameter, tokenizer training data size. We train BPE, UnigramLM, and WordPiece tokenizers across various vocabulary sizes using English training data ranging from 1GB to 900GB. Our findings reveal diminishing returns as training data size increases beyond roughly 150GB, suggesting a practical limit to the improvements in tokenization quality achievable through additional data. We analyze this phenomenon and attribute the saturation effect to constraints introduced by the pre-tokenization stage. We then demonstrate the extent to which these findings can generalize by experimenting on data in Russian, a language typologically distant from English. While the limit appears to materialize at a later phase of pre-training, around 200GB, it is in fact observed. These results provide valuable insights for optimizing the tokenization process by reducing the compute required for training on large corpora and suggest promising directions for future research in tokenization algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singularity Protocol for Cross Chain AMM without Intermediate Tokens or Bridges</title>
<link>https://arxiv.org/abs/2505.24337</link>
<guid>https://arxiv.org/abs/2505.24337</guid>
<content:encoded><![CDATA[
<div> AMMs, decentralized exchange, cross-chain swaps, liquidity, blockchain<br />
Summary:<br />
Automated Market Makers (AMMs) have revolutionized decentralized exchanges but face scalability challenges with cross-chain swaps. The current double-sided AMMs are inefficient and introduce risks like volatility and blockchain issues. This paper proposes a new class of AMMs that eliminate the need for intermediate tokens or bridging, enabling efficient cross-chain swaps with lower gas requirements. The new technology is based on an invariant that does not rely on bi-state dependency between assets being swapped. This innovation supports cross-chain swaps across various blockchain layers and offers a more streamlined and secure method for value transfer swaps. The proposed solution addresses the limitations of existing AMMs and provides a promising approach for improving liquidity and efficiency in cross-chain transactions. <br /><br /> <div>
arXiv:2505.24337v1 Announce Type: new 
Abstract: Automated Market Makers (AMMs) are decentralized exchange protocols that provide continuous access to token liquidity without the need for order books or traditional market makers. However, this innovation has failed to scale when it comes to cross-chain swaps. Modern cross-chain swaps employ double-sided AMMs, which are not only inefficient due to liquidity fragmentation but also require an intermediate token. This introduces inherent volatility risk as well as blockchain and bridging risk, especially in the case of wrapped tokens. This paper describes the inefficiencies of existing AMM invariants, particularly their mixed polynomial nature, and derives a new class of AMMs that do not have bi-state dependency between the assets being swapped. We propose a novel method of value transfer swaps using the described invariant that mitigates the need for bi-state dependency and eliminates the need for intermediate tokens or bridging. Furthermore, we show how this mechanism enables efficient cross-chain swaps with lower gas requirements and no bridging risks. The proposed technology is designed to support cross-chain swaps across any permutation of L1, L2, and L3 blockchains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Black Box: Interpretability of LLMs in Finance</title>
<link>https://arxiv.org/abs/2505.24650</link>
<guid>https://arxiv.org/abs/2505.24650</guid>
<content:encoded><![CDATA[
<div> interpretability, financial services, large language models, transparency, regulatory compliance

Summary: 
This paper introduces the concept of mechanistic interpretability in the context of Large Language Models (LLMs) in the financial services sector. LLMs have shown great potential in various financial tasks but their complexity and lack of transparency raise concerns in the regulated financial industry. Mechanistic interpretability offers a transparent way to understand LLM behavior by reverse-engineering their internal workings, providing insights into how specific features influence predictions and allowing for the modification of model behavior. The paper explores the theoretical aspects of mechanistic interpretability and demonstrates its practical relevance through financial use cases such as trading strategies, sentiment analysis, bias detection, and hallucination detection. The adoption of advanced interpretability tools is expected to be crucial as LLM usage increases, ensuring that AI systems in finance are ethical, transparent, and compliant with evolving regulations. The paper emphasizes how these techniques can address interpretability requirements for regulatory and compliance purposes in the financial sector. 

<br /><br />Summary: <div>
arXiv:2505.24650v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across a spectrum of tasks in financial services, including report generation, chatbots, sentiment analysis, regulatory compliance, investment advisory, financial knowledge retrieval, and summarization. However, their intrinsic complexity and lack of transparency pose significant challenges, especially in the highly regulated financial sector, where interpretability, fairness, and accountability are critical. As far as we are aware, this paper presents the first application in the finance domain of understanding and utilizing the inner workings of LLMs through mechanistic interpretability, addressing the pressing need for transparency and control in AI systems. Mechanistic interpretability is the most intuitive and transparent way to understand LLM behavior by reverse-engineering their internal workings. By dissecting the activations and circuits within these models, it provides insights into how specific features or components influence predictions - making it possible not only to observe but also to modify model behavior. In this paper, we explore the theoretical aspects of mechanistic interpretability and demonstrate its practical relevance through a range of financial use cases and experiments, including applications in trading strategies, sentiment analysis, bias, and hallucination detection. While not yet widely adopted, mechanistic interpretability is expected to become increasingly vital as adoption of LLMs increases. Advanced interpretability tools can ensure AI systems remain ethical, transparent, and aligned with evolving financial regulations. In this paper, we have put special emphasis on how these techniques can help unlock interpretability requirements for regulatory and compliance purposes - addressing both current needs and anticipating future expectations from financial regulators globally.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Bayesian multi-fidelity inverse analysis for expensive and non-differentiable physics-based simulations in high stochastic dimensions</title>
<link>https://arxiv.org/abs/2505.24708</link>
<guid>https://arxiv.org/abs/2505.24708</guid>
<content:encoded><![CDATA[
<div> Bayesian, multi-fidelity, inverse analysis, high-dimensional, computational<br />
Summary:<br />
This article introduces a novel approach called Bayesian multi-fidelity inverse analysis (BMFIA) to address the challenges of high-dimensional Bayesian inverse analysis for computationally demanding, nonlinear physics-based high-fidelity models. The method leverages simpler lower-fidelity models designed to provide model derivatives, learning a probabilistic dependence between the lower and higher-fidelity models. This allows for statistically correcting the inaccurate lower-fidelity responses in an altered likelihood formulation. BMFIA is fully differentiable and can be applied to a wide range of scenarios, including finely-resolved spatial reconstruction problems for nonlinear and transient coupled poro-elastic media physics. The approach is demonstrated to solve Bayesian inverse problems efficiently, even with a small amount of data, making it a valuable tool for addressing complex multi-physics problems. <div>
arXiv:2505.24708v1 Announce Type: new 
Abstract: High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible for computationally demanding, nonlinear physics-based high-fidelity (HF) models. Usually, the use of more efficient gradient-based inference schemes is impeded if the multi-physics models are provided by complex legacy codes. Adjoint-based derivatives are either exceedingly cumbersome to derive or non-existent for practically relevant large-scale nonlinear and coupled multi-physics problems. Similarly, holistic automated differentiation w.r.t. primary variables of multi-physics codes is usually not yet an option and requires extensive code restructuring if not considered from the outset in the software design. This absence of differentiability further exacerbates the already present computational challenges. To overcome the existing limitations, we propose a novel inference approach called Bayesian multi-fidelity inverse analysis (BMFIA), which leverages simpler and computationally cheaper lower-fidelity (LF) models that are designed to provide model derivatives. BMFIA learns a simple, probabilistic dependence of the LF and HF models, which is then employed in an altered likelihood formulation to statistically correct the inaccurate LF response. From a Bayesian viewpoint, this dependence represents a multi-fidelity conditional density (discriminative model). We demonstrate how this multi-fidelity conditional density can be learned robustly in the small data regime from only a few HF and LF simulations (50 to 300), which would not be sufficient for naive surrogate approaches. The formulation is fully differentiable and allows the flexible design of a wide range of LF models. We demonstrate that BMFIA solves Bayesian inverse problems for scenarios that used to be prohibitive, such as finely-resolved spatial reconstruction problems for nonlinear and transient coupled poro-elastic media physics.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Surprising Soupability of Documents in State Space Models</title>
<link>https://arxiv.org/abs/2505.24033</link>
<guid>https://arxiv.org/abs/2505.24033</guid>
<content:encoded><![CDATA[
<div> Keywords: Structured State Space Models, document souping, Mamba2 models, multi-hop QA, long-document reasoning

Summary:
Structured State Space Models (SSMs) are investigated to determine if their hidden states can be merged post-hoc to support downstream reasoning. A strategy called document souping is proposed, where documents are encoded independently and their representations are pooled into a single context state. This approach allows for modular encoding and reuse without the need to reprocess the full input for each query. Mamba2 models are modified to produce soupable representations, enabling support for multi-hop QA, sparse retrieval, and long-document reasoning with high accuracy. In experiments on HotpotQA, souping ten independently encoded documents achieves performance near that of a cross-encoder trained on the same inputs. Overall, this method demonstrates the effectiveness of combining independently encoded document representations for improved downstream reasoning tasks. 

Summary: <div>
arXiv:2505.24033v1 Announce Type: cross 
Abstract: We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post-hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled -- via simple operations like averaging -- into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We finetune Mamba2 models to produce soupable representations and find that they support multi-hop QA, sparse retrieval, and long-document reasoning with strong accuracy. On HotpotQA, souping ten independently encoded documents nearly matches the performance of a cross-encoder trained on the same inputs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Proximity: A Graph-Based Approach to Blockchain Fraud Prevention</title>
<link>https://arxiv.org/abs/2505.24284</link>
<guid>https://arxiv.org/abs/2505.24284</guid>
<content:encoded><![CDATA[
<div> Keywords: fraud-deterrent, public blockchains, transaction proximity, Easily Attainable Identities (EAIs), directed graph analysis

Summary: 
This paper presents a fraud-deterrent access validation system for public blockchains using Transaction Proximity and Easily Attainable Identities (EAIs) concepts. The system analyzes transaction patterns to identify wallets closely connected to centralized exchanges, aiming to prevent fraudulent activities. The analysis of the Ethereum blockchain reveals a high percentage of large USDC wallets being EAI or within one transaction hop of an EAI. Moreover, a significant number of past exploits were found to not involve EAIs, highlighting the need for such a validation system. Three implementation approaches are proposed, balancing gas cost and privacy considerations. This approach allows for programmatic compliance without restricting access or sharing personal information, maintaining blockchain openness and enabling protocols to implement customized validation systems.<br /><br />Summary: <div>
arXiv:2505.24284v1 Announce Type: cross 
Abstract: This paper introduces a fraud-deterrent access validation system for public blockchains, leveraging two complementary concepts: "Transaction Proximity", which measures the distance between wallets in the transaction graph, and "Easily Attainable Identities (EAIs)", wallets with direct transaction connections to centralized exchanges. Recognizing the limitations of traditional approaches like blocklisting (reactive, slow) and strict allow listing (privacy-invasive, adoption barriers), we propose a system that analyzes transaction patterns to identify wallets with close connections to centralized exchanges.
  Our directed graph analysis of the Ethereum blockchain reveals that 56% of large USDC wallets (with a lifetime maximum balance greater than \$10,000) are EAI and 88% are within one transaction hop of an EAI. For transactions exceeding \$2,000, 91% involve at least one EAI. Crucially, an analysis of past exploits shows that 83% of the known exploiter addresses are not EAIs, with 21% being more than five hops away from any regulated exchange. We present three implementation approaches with varying gas cost and privacy tradeoffs, demonstrating that EAI-based access control can potentially prevent most of these incidents while preserving blockchain openness. Importantly, our approach does not restrict access or share personally identifiable information, but it provides information for protocols to implement their own validation or risk scoring systems based on specific needs. This middle-ground solution enables programmatic compliance while maintaining the core values of open blockchain.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking for Attention: Randomized Attention Test Design for Validator Monitoring in Optimistic Rollups</title>
<link>https://arxiv.org/abs/2505.24393</link>
<guid>https://arxiv.org/abs/2505.24393</guid>
<content:encoded><![CDATA[
<div> scalability, blockchain, Optimistic Rollups, Randomized Attention Test, game-theoretic analysis

Summary:
Optimistic Rollups (ORUs) enhance blockchain scalability but face the verifier's dilemma due to a lack of mechanisms ensuring validator attentiveness. The Randomized Attention Test (RAT) protocol is introduced to challenge validators in ORUs, verifying their liveness and readiness. Game-theoretic analysis shows that an Ideal Security Equilibrium can be achieved with RAT, where validators are attentive and proposers honest. This equilibrium is attainable with low penalties for non-responsive validators and a low attention test frequency. RAT serves as a practical mechanism to enforce validator diligence, increasing the security and integrity of ORU systems without significant additional costs. <div>
arXiv:2505.24393v1 Announce Type: cross 
Abstract: Optimistic Rollups (ORUs) significantly enhance blockchain scalability but inherently suffer from the verifier's dilemma, particularly concerning validator attentiveness. Current systems lack mechanisms to proactively ensure validators are diligently monitoring L2 state transitions, creating a vulnerability where fraudulent states could be finalized. This paper introduces the Randomized Attention Test (RAT), a novel L1-based protocol designed to probabilistically challenge validators in ORUs, thereby verifying their liveness and computational readiness. Our game-theoretic analysis demonstrates that an Ideal Security Equilibrium, where all validators are attentive and proposers are honest, can be achieved with RAT. Notably, this equilibrium is attainable and stable with relatively low economic penalties (e.g., under $1000) for non-responsive validators and a low attention test frequency (e.g., under 1% per epoch). RAT thus provides a crucial, practical mechanism to enforce validator diligence, fortifying the overall security and integrity of ORU systems with minimizing additional costs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV</title>
<link>https://arxiv.org/abs/2505.24781</link>
<guid>https://arxiv.org/abs/2505.24781</guid>
<content:encoded><![CDATA[
<div> Regularized Tyler's M-estimator, shrinkage coefficient, leave-one-out cross-validation, computational efficiency, high-dimensional data<br />
<br />
Summary: <br />
The study focuses on estimating a shrinkage coefficient for Regularized Tyler's M-estimator using leave-one-out cross-validation (LOOCV) log-likelihood loss. The proposed approach aims to find an optimal shrinkage coefficient by solving a selected objective function, enhancing computational efficiency by approximating the LOOCV log-likelihood loss. This approximation significantly reduces the running time complexity for the LOOCV procedure by O(n), offering a faster computation of the LOOCV estimate. The efficiency and accuracy of the method were demonstrated through synthetic high-dimensional data and real datasets for object recognition, face recognition, and handwritten digit recognition. Results indicate the proposed approach is both efficient and more precise compared to existing methods for shrinkage coefficient estimation. <div>
arXiv:2505.24781v1 Announce Type: cross 
Abstract: We consider the problem of estimating a regularization parameter, or a shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator (RTME). In particular, we propose to estimate an optimal shrinkage coefficient by setting $\alpha$ as the solution to a suitably chosen objective function; namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since LOOCV is computationally prohibitive even for moderate sample size $n$, we propose a computationally efficient approximation for the LOOCV log-likelihood loss that eliminates the need for invoking the RTME procedure $n$ times for each sample left out during the LOOCV procedure. This approximation yields an $O(n)$ reduction in the running time complexity for the LOOCV procedure, which results in a significant speedup for computing the LOOCV estimate. We demonstrate the efficiency and accuracy of the proposed approach on synthetic high-dimensional data sampled from heavy-tailed elliptical distributions, as well as on real high-dimensional datasets for object recognition, face recognition, and handwritten digit's recognition. Our experiments show that the proposed approach is efficient and consistently more accurate than other methods in the literature for shrinkage coefficient estimation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpolating Neural Network-Tensor Decomposition (INN-TD): a scalable and interpretable approach for large-scale physics-based problems</title>
<link>https://arxiv.org/abs/2503.02041</link>
<guid>https://arxiv.org/abs/2503.02041</guid>
<content:encoded><![CDATA[
<div> Interpolating Neural Network-Tensor Decomposition, scalable, interpretable, machine learning, finite element methods, large-scale physical systems
Summary:
Interpolating Neural Network-Tensor Decomposition (INN-TD) is introduced as a framework that combines machine learning and finite element methods to model large-scale physical systems accurately and efficiently. By incorporating locally supported interpolation functions from finite element methods into the network architecture, INN-TD achieves a sparse learning structure, leading to enhanced accuracy, faster training/solving speed, and reduced memory usage. This framework is well-suited for addressing large-scale high-dimensional parametric partial differential equations in physical problems that require high precision in tasks such as training, solving, and inverse optimization. Its effectiveness lies in its ability to provide interpretable solutions for industrial problems while maintaining computational efficiency and accuracy in modeling complex physics-based systems. <br /><br />Summary: <div>
arXiv:2503.02041v3 Announce Type: replace 
Abstract: Deep learning has been extensively employed as a powerful function approximator for modeling physics-based problems described by partial differential equations (PDEs). Despite their popularity, standard deep learning models often demand prohibitively large computational resources and yield limited accuracy when scaling to large-scale, high-dimensional physical problems. Their black-box nature further hinders the application in industrial problems where interpretability and high precision are critical. To overcome these challenges, this paper introduces Interpolating Neural Network-Tensor Decomposition (INN-TD), a scalable and interpretable framework that has the merits of both machine learning and finite element methods for modeling large-scale physical systems. By integrating locally supported interpolation functions from finite element into the network architecture, INN-TD achieves a sparse learning structure with enhanced accuracy, faster training/solving speed, and reduced memory footprint. This makes it particularly effective for tackling large-scale high-dimensional parametric PDEs in training, solving, and inverse optimization tasks in physical problems where high precision is required.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>